import feedparser
import datetime
import os

def arxiv_search(query, max_results=5):
    base_url = 'http://export.arxiv.org/api/query?'
    search_url = f'search_query={query}&start=0&max_results={max_results}'
    response = feedparser.parse(base_url + search_url)
    return response.entries

def save_results_as_html(entries):
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    os.makedirs("docs", exist_ok=True)  # ç¡®ä¿ docs ç›®å½•å­˜åœ¨
    filename = f"docs/arxiv_{today}.html"  # æ¯æ—¥çˆ¬å–ç»“æœä»¥æ—¥æœŸå‘½å

    # ç”Ÿæˆæ¯æ—¥çš„ç»“æœé¡µé¢
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"<!DOCTYPE html>\n<html>\n<head>\n<title>arXiv Papers - {today}</title>\n</head>\n<body>\n")
        f.write(f"<h1>ğŸ“š arXiv Papers on {today}</h1>\n")

        for entry in entries:
            f.write(f"<h2><a href='{entry.link}' target='_blank'>{entry.title}</a></h2>\n")
            authors = ', '.join(author.name for author in entry.authors)
            f.write(f"<p><strong>Authors:</strong> {authors}</p>\n")
            f.write(f"<p><strong>Summary:</strong> {entry.summary[:500]}...</p>\n")
            f.write("<hr>\n")

        f.write("</body>\n</html>")

    # æ›´æ–° index.html é¡µé¢ï¼Œæ·»åŠ å†å²é“¾æ¥
    update_index(today)

    print(f"âœ… Results saved to {filename}")

def update_index(latest_date):
    index_file = "docs/index.html"
    if not os.path.exists(index_file):
        with open(index_file, "w", encoding="utf-8") as f:
            f.write("<!DOCTYPE html>\n<html>\n<head>\n<title>arXiv Archive</title>\n</head>\n<body>\n")
            f.write("<h1>ğŸ“š arXiv Paper Archive</h1>\n")
            f.write("<ul>\n")
            f.write("</ul>\n")
            f.write("</body>\n</html>")

    # è¯»å–ç°æœ‰ index.html
    with open(index_file, "r", encoding="utf-8") as f:
        content = f.read()

    # é˜²æ­¢é‡å¤æ·»åŠ åŒä¸€å¤©çš„é“¾æ¥
    new_entry = f'<li><a href="arxiv_{latest_date}.html">{latest_date} Papers</a></li>\n'
    if new_entry not in content:
        content = content.replace("<ul>\n", f"<ul>\n{new_entry}")

    # ä¿å­˜æ›´æ–°åçš„ index.html
    with open(index_file, "w", encoding="utf-8") as f:
        f.write(content)

if __name__ == "__main__":
    query = 'all:planning+AND+all:"reinforcement learning"'
    results = arxiv_search(query, max_results=100)
    save_results_as_html(results)
