<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-04-08</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-04-08</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04691v1' target='_blank'>Large-Scale Mixed-Traffic and Intersection Control using Multi-agent
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Songyang Liu, Muyang Fan, Weizi Li, Jing Du, Shuai Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-07 02:52:39</h6>
<p class='card-text'>Traffic congestion remains a significant challenge in modern urban networks.
Autonomous driving technologies have emerged as a potential solution. Among
traffic control methods, reinforcement learning has shown superior performance
over traffic signals in various scenarios. However, prior research has largely
focused on small-scale networks or isolated intersections, leaving large-scale
mixed traffic control largely unexplored. This study presents the first attempt
to use decentralized multi-agent reinforcement learning for large-scale mixed
traffic control in which some intersections are managed by traffic signals and
others by robot vehicles. Evaluating a real-world network in Colorado Springs,
CO, USA with 14 intersections, we measure traffic efficiency via average
waiting time of vehicles at intersections and the number of vehicles reaching
their destinations within a time window (i.e., throughput). At 80% RV
penetration rate, our method reduces waiting time from 6.17 s to 5.09 s and
increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500
seconds, outperforming the baseline of fully signalized intersections. These
findings suggest that integrating reinforcement learning-based control
large-scale traffic can improve overall efficiency and may inform future urban
planning strategies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04675v1' target='_blank'>HypRL: Reinforcement Learning of Control Policies for Hyperproperties</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tzu-Han Hsu, Arshia Rafieioskouei, Borzoo Bonakdarpour</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-07 01:58:36</h6>
<p class='card-text'>We study the problem of learning control policies for complex tasks whose
requirements are given by a hyperproperty. The use of hyperproperties is
motivated by their significant power to formally specify requirements of
multi-agent systems as well as those that need expressiveness in terms of
multiple execution traces (e.g., privacy and fairness). Given a Markov decision
process M with unknown transitions (representing the environment) and a
HyperLTL formula $\varphi$, our approach first employs Skolemization to handle
quantifier alternations in $\varphi$. We introduce quantitative robustness
functions for HyperLTL to define rewards of finite traces of M with respect to
$\varphi$. Finally, we utilize a suitable reinforcement learning algorithm to
learn (1) a policy per trace quantifier in $\varphi$, and (2) the probability
distribution of transitions of M that together maximize the expected reward
and, hence, probability of satisfaction of $\varphi$ in M. We present a set of
case studies on (1) safety-preserving multi-agent path planning, (2) fairness
in resource allocation, and (3) the post-correspondence problem (PCP).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04469v1' target='_blank'>AI2STOW: End-to-End Deep Reinforcement Learning to Construct Master
  Stowage Plans under Demand Uncertainty</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jaike Van Twiller, Djordje Grbic, Rune Møller Jensen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-06 12:45:25</h6>
<p class='card-text'>The worldwide economy and environmental sustainability depend on eff icient
and reliable supply chains, in which container shipping plays a crucial role as
an environmentally friendly mode of transport. Liner shipping companies seek to
improve operational efficiency by solving the stowage planning problem. Due to
many complex combinatorial aspects, stowage planning is challenging and often
decomposed into two NP-hard subproblems: master and slot planning. This article
proposes AI2STOW, an end-to-end deep reinforcement learning model with
feasibility projection and an action mask to create master plans under demand
uncertainty with global objectives and constraints, including paired block
stowage patterms. Our experimental results demonstrate that AI2STOW outperforms
baseline methods from reinforcement learning and stochastic programming in
objective performance and computational efficiency, based on simulated
instances reflecting the scale of realistic vessels and operational planning
horizons.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04421v1' target='_blank'>Deliberate Planning of 3D Bin Packing on Packing Configuration Trees</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hang Zhao, Juzhan Xu, Kexiong Yu, Ruizhen Hu, Chenyang Zhu, Kai Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-06 09:07:10</h6>
<p class='card-text'>Online 3D Bin Packing Problem (3D-BPP) has widespread applications in
industrial automation. Existing methods usually solve the problem with limited
resolution of spatial discretization, and/or cannot deal with complex practical
constraints well. We propose to enhance the practical applicability of online
3D-BPP via learning on a novel hierarchical representation, packing
configuration tree (PCT). PCT is a full-fledged description of the state and
action space of bin packing which can support packing policy learning based on
deep reinforcement learning (DRL). The size of the packing action space is
proportional to the number of leaf nodes, making the DRL model easy to train
and well-performing even with continuous solution space. We further discover
the potential of PCT as tree-based planners in deliberately solving packing
problems of industrial significance, including large-scale packing and
different variations of BPP setting. A recursive packing method is proposed to
decompose large-scale packing into smaller sub-trees while a spatial ensemble
mechanism integrates local solutions into global. For different BPP variations
with additional decision variables, such as lookahead, buffering, and offline
packing, we propose a unified planning framework enabling out-of-the-box
problem solving. Extensive evaluations demonstrate that our method outperforms
existing online BPP baselines and is versatile in incorporating various
practical constraints. The planning process excels across large-scale problems
and diverse problem variations. We develop a real-world packing robot for
industrial warehousing, with careful designs accounting for constrained
placement and transportation stability. Our packing robot operates reliably and
efficiently on unprotected pallets at 10 seconds per box. It achieves averagely
19 boxes per pallet with 57.4% space utilization for relatively large-size
boxes.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.04366v1' target='_blank'>Solving Sokoban using Hierarchical Reinforcement Learning with Landmarks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sergey Pastukhov</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-06 05:30:21</h6>
<p class='card-text'>We introduce a novel hierarchical reinforcement learning (HRL) framework that
performs top-down recursive planning via learned subgoals, successfully applied
to the complex combinatorial puzzle game Sokoban. Our approach constructs a
six-level policy hierarchy, where each higher-level policy generates subgoals
for the level below. All subgoals and policies are learned end-to-end from
scratch, without any domain knowledge. Our results show that the agent can
generate long action sequences from a single high-level call. While prior work
has explored 2-3 level hierarchies and subgoal-based planning heuristics, we
demonstrate that deep recursive goal decomposition can emerge purely from
learning, and that such hierarchies can scale effectively to hard puzzle
domains.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.03622v1' target='_blank'>Align to Structure: Aligning Large Language Models with Structural
  Information</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-04 17:40:04</h6>
<p class='card-text'>Generating long, coherent text remains a challenge for large language models
(LLMs), as they lack hierarchical planning and structured organization in
discourse generation. We introduce Structural Alignment, a novel method that
aligns LLMs with human-like discourse structures to enhance long-form text
generation. By integrating linguistically grounded discourse frameworks into
reinforcement learning, our approach guides models to produce coherent and
well-organized outputs. We employ a dense reward scheme within a Proximal
Policy Optimization framework, assigning fine-grained, token-level rewards
based on the discourse distinctiveness relative to human writing. Two
complementary reward models are evaluated: the first improves readability by
scoring surface-level textual features to provide explicit structuring, while
the second reinforces deeper coherence and rhetorical sophistication by
analyzing global discourse patterns through hierarchical discourse motifs,
outperforming both standard and RLHF-enhanced models in tasks such as essay
generation and long-document summarization. All training data and code will be
publicly shared at https://github.com/minnesotanlp/struct_align.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.03160v2' target='_blank'>DeepResearcher: Scaling Deep Research via Reinforcement Learning in
  Real-world Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-04 04:41:28</h6>
<p class='card-text'>Large Language Models (LLMs) equipped with web search capabilities have
demonstrated impressive potential for deep research tasks. However, current
approaches predominantly rely on either manually engineered prompts (prompt
engineering-based) with brittle performance or reinforcement learning within
controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that
fail to capture the complexities of real-world interaction. In this paper, we
introduce DeepResearcher, the first comprehensive framework for end-to-end
training of LLM-based deep research agents through scaling reinforcement
learning (RL) in real-world environments with authentic web search
interactions. Unlike RAG-based approaches that assume all necessary information
exists within a fixed corpus, our method trains agents to navigate the noisy,
unstructured, and dynamic nature of the open web. We implement a specialized
multi-agent architecture where browsing agents extract relevant information
from various webpage structures and overcoming significant technical
challenges. Extensive experiments on open-domain research tasks demonstrate
that DeepResearcher achieves substantial improvements of up to 28.9 points over
prompt engineering-based baselines and up to 7.2 points over RAG-based RL
agents. Our qualitative analysis reveals emergent cognitive behaviors from
end-to-end RL training, including the ability to formulate plans,
cross-validate information from multiple sources, engage in self-reflection to
redirect research, and maintain honesty when unable to find definitive answers.
Our results highlight that end-to-end training in real-world web environments
is not merely an implementation detail but a fundamental requirement for
developing robust research capabilities aligned with real-world applications.
We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.02688v1' target='_blank'>Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication
  using DRL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Achilles Kiwanuka Machumilane, Alberto Gotta, Pietro Cassarà</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-03 15:28:04</h6>
<p class='card-text'>Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted
next-generation wireless networks is critical for mobility management and
ensuring UAV safety and ubiquitous connectivity, especially in dense urban
environments with street canyons and tall buildings. Traditional statistical
and model-based techniques have been successfully used for path optimization in
communication networks. However, when dynamic channel propagation
characteristics such as line-of-sight (LOS), interference, handover, and
signal-to-interference and noise ratio (SINR) are included in path
optimization, statistical and model-based path planning solutions become
obsolete since they cannot adapt to the dynamic and time-varying wireless
channels, especially in the mmWave bands. In this paper, we propose a novel
model-free actor-critic deep reinforcement learning (AC-DRL) framework for path
optimization in UAV-assisted 5G mmWave wireless networks, which combines four
important aspects of UAV communication: \textit{flight time, handover,
connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to
a gNB to determine the optimal path to a desired destination in the shortest
possible time with minimal gNB handover, while maintaining connectivity and the
highest possible SINR. We train our model with data from a powerful ray tracing
tool called Wireless InSite, which uses 3D images of the propagation
environment and provides data that closely resembles the real propagation
environment. The simulation results show that our system has superior
performance in tracking high SINR compared to other selected RL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.02161v1' target='_blank'>Preference-Driven Active 3D Scene Representation for Robotic Inspection
  in Nuclear Decommissioning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Meng, Kan Chen, Xiangmin Xu, Erwin Jose Lopez Pulgarin, Emma Li, Philip G. Zhao, David Flynn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-02 22:20:48</h6>
<p class='card-text'>Active 3D scene representation is pivotal in modern robotics applications,
including remote inspection, manipulation, and telepresence. Traditional
methods primarily optimize geometric fidelity or rendering accuracy, but often
overlook operator-specific objectives, such as safety-critical coverage or
task-driven viewpoints. This limitation leads to suboptimal viewpoint
selection, particularly in constrained environments such as nuclear
decommissioning. To bridge this gap, we introduce a novel framework that
integrates expert operator preferences into the active 3D scene representation
pipeline. Specifically, we employ Reinforcement Learning from Human Feedback
(RLHF) to guide robotic path planning, reshaping the reward function based on
expert input. To capture operator-specific priorities, we conduct interactive
choice experiments that evaluate user preferences in 3D scene representation.
We validate our framework using a UR3e robotic arm for reactor tile inspection
in a nuclear decommissioning scenario. Compared to baseline methods, our
approach enhances scene representation while optimizing trajectory efficiency.
The RLHF-based policy consistently outperforms random selection, prioritizing
task-critical details. By unifying explicit 3D geometric modeling with implicit
human-in-the-loop optimization, this work establishes a foundation for
adaptive, safety-critical robotic perception systems, paving the way for
enhanced automation in nuclear decommissioning, remote maintenance, and other
high-risk environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.01871v1' target='_blank'>Interpreting Emergent Planning in Model-Free Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-02 16:24:23</h6>
<p class='card-text'>We present the first mechanistic evidence that model-free reinforcement
learning agents can learn to plan. This is achieved by applying a methodology
based on concept-based interpretability to a model-free agent in Sokoban -- a
commonly used benchmark for studying planning. Specifically, we demonstrate
that DRC, a generic model-free agent introduced by Guez et al. (2019), uses
learned concept representations to internally formulate plans that both predict
the long-term effects of actions on the environment and influence action
selection. Our methodology involves: (1) probing for planning-relevant
concepts, (2) investigating plan formation within the agent's representations,
and (3) verifying that discovered plans (in the agent's representations) have a
causal effect on the agent's behavior through interventions. We also show that
the emergence of these plans coincides with the emergence of a planning-like
property: the ability to benefit from additional test-time compute. Finally, we
perform a qualitative analysis of the planning algorithm learned by the agent
and discover a strong resemblance to parallelized bidirectional search. Our
findings advance understanding of the internal mechanisms underlying planning
behavior in agents, which is important given the recent trend of emergent
planning and reasoning capabilities in LLMs through RL</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.00277v1' target='_blank'>Rack Position Optimization in Large-Scale Heterogeneous Data Centers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chang-Lin Chen, Jiayu Chen, Tian Lan, Zhaoxia Zhao, Hongbo Dong, Vaneet Aggarwal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 22:55:37</h6>
<p class='card-text'>As rapidly growing AI computational demands accelerate the need for new
hardware installation and maintenance, this work explores optimal data center
resource management by balancing operational efficiency with fault tolerance
through strategic rack positioning considering diverse resources and locations.
Traditional mixed-integer programming (MIP) approaches often struggle with
scalability, while heuristic methods may result in significant sub-optimality.
To address these issues, this paper presents a novel two-tier optimization
framework using a high-level deep reinforcement learning (DRL) model to guide a
low-level gradient-based heuristic for local search. The high-level DRL agent
employs Leader Reward for optimal rack type ordering, and the low-level
heuristic efficiently maps racks to positions, minimizing movement counts and
ensuring fault-tolerant resource distribution. This approach allows scalability
to over 100,000 positions and 100 rack types. Our method outperformed the
gradient-based heuristic by 7\% on average and the MIP solver by over 30\% in
objective value. It achieved a 100\% success rate versus MIP's 97.5\% (within a
20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes
(i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which
showed performance variability under time constraints and high penalties, our
algorithm consistently delivered stable, efficient results - an essential
feature for large-scale data center management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.24376v1' target='_blank'>Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 17:55:23</h6>
<p class='card-text'>Recent advancements in Chain of Thought (COT) generation have significantly
improved the reasoning capabilities of Large Language Models (LLMs), with
reinforcement learning (RL) emerging as an effective post-training approach.
Multimodal Large Language Models (MLLMs) inherit this reasoning potential but
remain underexplored in tasks requiring both perception and logical reasoning.
To address this, we introduce SEED-Bench-R1, a benchmark designed to
systematically evaluate post-training methods for MLLMs in video understanding.
It includes intricate real-world videos and complex everyday planning tasks in
the format of multiple-choice questions, requiring sophisticated perception and
reasoning. SEED-Bench-R1 assesses generalization through a three-level
hierarchy: in-distribution, cross-environment, and cross-environment-task
scenarios, equipped with a large-scale training dataset with easily verifiable
ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL
with supervised fine-tuning (SFT), demonstrating RL's data efficiency and
superior performance on both in-distribution and out-of-distribution tasks,
even outperforming SFT on general video understanding benchmarks like
LongVideoBench. Our detailed analysis reveals that RL enhances visual
perception but often produces less logically coherent reasoning chains. We
identify key limitations such as inconsistent reasoning and overlooked visual
cues, and suggest future improvements in base model reasoning, reward modeling,
and RL robustness against noisy signals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.24214v1' target='_blank'>Moving Edge for On-Demand Edge Computing: An Uncertainty-aware Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fangtong Zhou, Ruozhou Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 15:32:05</h6>
<p class='card-text'>We study an edge demand response problem where, based on historical edge
workload demands, an edge provider needs to dispatch moving computing units,
e.g. truck-carried modular data centers, in response to emerging hotspots
within service area. The goal of edge provider is to maximize the expected
revenue brought by serving congested users with satisfactory performance, while
minimizing the costs of moving units and the potential service-level agreement
violation penalty for interrupted services. The challenge is to make robust
predictions for future demands, as well as optimized moving unit dispatching
decisions. We propose a learning-based, uncertain-aware moving unit scheduling
framework, URANUS, to address this problem. Our framework novelly combines
Bayesian deep learning and distributionally robust approximation to make
predictions that are robust to data, model and distributional uncertainties in
deep learning-based prediction models. Based on the robust prediction outputs,
we further propose an efficient planning algorithm to optimize moving unit
scheduling in an online manner. Simulation experiments show that URANUS can
significantly improve robustness in decision making, and achieve superior
performance compared to state-of-the-art reinforcement learning,
uncertainty-agnostic learning-based methods, and other baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23975v1' target='_blank'>A Reactive Framework for Whole-Body Motion Planning of Mobile
  Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic
  Programmi</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenyu Zhang, Shiying Sun, Kuan Liu, Chuanbao Zhou, Xiaoguang Zhao, Min Tan, Yanlong Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 11:37:02</h6>
<p class='card-text'>As an important branch of embodied artificial intelligence, mobile
manipulators are increasingly applied in intelligent services, but their
redundant degrees of freedom also limit efficient motion planning in cluttered
environments. To address this issue, this paper proposes a hybrid learning and
optimization framework for reactive whole-body motion planning of mobile
manipulators. We develop the Bayesian distributional soft actor-critic
(Bayes-DSAC) algorithm to improve the quality of value estimation and the
convergence performance of the learning. Additionally, we introduce a quadratic
programming method constrained by the signed distance field to enhance the
safety of the obstacle avoidance motion. We conduct experiments and make
comparison with standard benchmark. The experimental results verify that our
proposed framework significantly improves the efficiency of reactive whole-body
motion planning, reduces the planning time, and improves the success rate of
motion planning. Additionally, the proposed reinforcement learning method
ensures a rapid learning process in the whole-body planning task. The novel
framework allows mobile manipulators to adapt to complex environments more
safely and efficiently.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23908v1' target='_blank'>MAER-Nav: Bidirectional Motion Learning Through Mirror-Augmented
  Experience Replay for Robot Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shanze Wang, Mingao Tan, Zhibo Yang, Biao Huang, Xiaoyu Shen, Hailong Huang, Wei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 09:58:28</h6>
<p class='card-text'>Deep Reinforcement Learning (DRL) based navigation methods have demonstrated
promising results for mobile robots, but suffer from limited action flexibility
in confined spaces. Conventional DRL approaches predominantly learn
forward-motion policies, causing robots to become trapped in complex
environments where backward maneuvers are necessary for recovery. This paper
presents MAER-Nav (Mirror-Augmented Experience Replay for Robot Navigation), a
novel framework that enables bidirectional motion learning without requiring
explicit failure-driven hindsight experience replay or reward function
modifications. Our approach integrates a mirror-augmented experience replay
mechanism with curriculum learning to generate synthetic backward navigation
experiences from successful trajectories. Experimental results in both
simulation and real-world environments demonstrate that MAER-Nav significantly
outperforms state-of-the-art methods while maintaining strong forward
navigation capabilities. The framework effectively bridges the gap between the
comprehensive action space utilization of traditional planning methods and the
environmental adaptability of learning-based approaches, enabling robust
navigation in scenarios where conventional DRL methods consistently fail.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23766v1' target='_blank'>Accelerating High-Efficiency Organic Photovoltaic Discovery via
  Pretrained Graph Neural Networks and Generative Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiangjie Qiu, Hou Hei Lam, Xiuyuan Hu, Wentao Li, Siwei Fu, Fankun Zeng, Hao Zhang, Xiaonan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 06:31:15</h6>
<p class='card-text'>Organic photovoltaic (OPV) materials offer a promising avenue toward
cost-effective solar energy utilization. However, optimizing donor-acceptor
(D-A) combinations to achieve high power conversion efficiency (PCE) remains a
significant challenge. In this work, we propose a framework that integrates
large-scale pretraining of graph neural networks (GNNs) with a GPT-2
(Generative Pretrained Transformer 2)-based reinforcement learning (RL)
strategy to design OPV molecules with potentially high PCE. This approach
produces candidate molecules with predicted efficiencies approaching 21\%,
although further experimental validation is required. Moreover, we conducted a
preliminary fragment-level analysis to identify structural motifs recognized by
the RL model that may contribute to enhanced PCE, thus providing design
guidelines for the broader research community. To facilitate continued
discovery, we are building the largest open-source OPV dataset to date,
expected to include nearly 3,000 donor-acceptor pairs. Finally, we discuss
plans to collaborate with experimental teams on synthesizing and characterizing
AI-designed molecules, which will provide new data to refine and improve our
predictive and generative models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23650v1' target='_blank'>A Survey of Reinforcement Learning-Based Motion Planning for Autonomous
  Driving: Lessons Learned from a Driving Task Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoren Li, Guizhe Jin, Ran Yu, Zhiwen Chen, Nan Li, Wei Han, Lu Xiong, Bo Leng, Jia Hu, Ilya Kolmanovsky, Dimitar Filev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 01:31:14</h6>
<p class='card-text'>Reinforcement learning (RL), with its ability to explore and optimize
policies in complex, dynamic decision-making tasks, has emerged as a promising
approach to addressing motion planning (MoP) challenges in autonomous driving
(AD). Despite rapid advancements in RL and AD, a systematic description and
interpretation of the RL design process tailored to diverse driving tasks
remains underdeveloped. This survey provides a comprehensive review of RL-based
MoP for AD, focusing on lessons from task-specific perspectives. We first
outline the fundamentals of RL methodologies, and then survey their
applications in MoP, analyzing scenario-specific features and task requirements
to shed light on their influence on RL design choices. Building on this
analysis, we summarize key design experiences, extract insights from various
driving task applications, and provide guidance for future implementations.
Additionally, we examine the frontier challenges in RL-based MoP, review recent
efforts to addresse these challenges, and propose strategies for overcoming
unresolved issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23486v1' target='_blank'>A Systematic Decade Review of Trip Route Planning with Travel Time
  Estimation based on User Preferences and Behavior</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nikil Jayasuriya, Deshan Sumanathilaka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-30 15:41:44</h6>
<p class='card-text'>This paper systematically explores the advancements in adaptive trip route
planning and travel time estimation (TTE) through Artificial Intelligence (AI).
With the increasing complexity of urban transportation systems, traditional
navigation methods often struggle to accommodate dynamic user preferences,
real-time traffic conditions, and scalability requirements. This study explores
the contributions of established AI techniques, including Machine Learning
(ML), Reinforcement Learning (RL), and Graph Neural Networks (GNNs), alongside
emerging methodologies like Meta-Learning, Explainable AI (XAI), Generative AI,
and Federated Learning. In addition to highlighting these innovations, the
paper identifies critical challenges such as ethical concerns, computational
scalability, and effective data integration, which must be addressed to advance
the field. The paper concludes with recommendations for leveraging AI to build
efficient, transparent, and sustainable navigation systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.22942v1' target='_blank'>Adaptive Interactive Navigation of Quadruped Robots using Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kangjie Zhou, Yao Mu, Haoyang Song, Yi Zeng, Pengying Wu, Han Gao, Chang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-29 02:17:52</h6>
<p class='card-text'>Robotic navigation in complex environments remains a critical research
challenge. Traditional navigation methods focus on optimal trajectory
generation within free space, struggling in environments lacking viable paths
to the goal, such as disaster zones or cluttered warehouses. To address this
gap, we propose an adaptive interactive navigation approach that proactively
interacts with environments to create feasible paths to reach originally
unavailable goals. Specifically, we present a primitive tree for task planning
with large language models (LLMs), facilitating effective reasoning to
determine interaction objects and sequences. To ensure robust subtask
execution, we adopt reinforcement learning to pre-train a comprehensive skill
library containing versatile locomotion and interaction behaviors for motion
planning. Furthermore, we introduce an adaptive replanning method featuring two
LLM-based modules: an advisor serving as a flexible replanning trigger and an
arborist for autonomous plan adjustment. Integrated with the tree structure,
the replanning mechanism allows for convenient node addition and pruning,
enabling rapid plan modification in unknown environments. Comprehensive
simulations and experiments have demonstrated our method's effectiveness and
adaptivity in diverse scenarios. The supplementary video is available at page:
https://youtu.be/W5ttPnSap2g.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.22925v2' target='_blank'>Predictive Traffic Rule Compliance using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanliang Huang, Sebastian Mair, Zhuoqi Zeng, Matthias Althoff</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-29 01:04:08</h6>
<p class='card-text'>Autonomous vehicle path planning has reached a stage where safety and
regulatory compliance are crucial. This paper presents an approach that
integrates a motion planner with a deep reinforcement learning model to predict
potential traffic rule violations. Our main innovation is replacing the
standard actor network in an actor-critic method with a motion planning module,
which ensures both stable and interpretable trajectory generation. In this
setup, we use traffic rule robustness as the reward to train a reinforcement
learning agent's critic, and the output of the critic is directly used as the
cost function of the motion planner, which guides the choices of the
trajectory. We incorporate some key interstate rules from the German Road
Traffic Regulation into a rule book and use a graph-based state representation
to handle complex traffic information. Experiments on an open German highway
dataset show that the model can predict and prevent traffic rule violations
beyond the planning horizon, increasing safety and rule compliance in
challenging traffic scenarios.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>