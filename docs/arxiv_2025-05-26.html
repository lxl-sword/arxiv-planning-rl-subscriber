<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-05-26</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-05-26</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18098v1' target='_blank'>Planning without Search: Refining Frontier LLMs with Offline
  Goal-Conditioned RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joey Hong, Anca Dragan, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 16:51:54</h6>
<p class='card-text'>Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18083v1' target='_blank'>What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Quentin Clark, Florian Shkurti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 16:41:08</h6>
<p class='card-text'>In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17866v1' target='_blank'>DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 13:16:01</h6>
<p class='card-text'>Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17795v1' target='_blank'>DialogXpert: Driving Intelligent and Emotion-Aware Conversations through
  Online Value-Based Reinforcement Learning with LLM Priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 12:12:40</h6>
<p class='card-text'>Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17659v1' target='_blank'>Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 09:22:19</h6>
<p class='card-text'>Safe and feasible trajectory planning is essential for real-world autonomous
driving systems. However, existing learning-based planning methods often rely
on expert demonstrations, which not only lack explicit safety awareness but
also risk inheriting unsafe behaviors such as speeding from suboptimal human
driving data. Inspired by the success of large language models, we propose
Plan-R1, a novel two-stage trajectory planning framework that formulates
trajectory planning as a sequential prediction task, guided by explicit
planning principles such as safety, comfort, and traffic rule compliance. In
the first stage, we train an autoregressive trajectory predictor via next
motion token prediction on expert data. In the second stage, we design
rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the
model using Group Relative Policy Optimization (GRPO), a reinforcement learning
strategy, to align its predictions with these planning principles. Experiments
on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves
planning safety and feasibility, achieving state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17352v1' target='_blank'>Alignment and Safety of Diffusion Models via Reinforcement Learning and
  Reward Modeling: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 00:08:49</h6>
<p class='card-text'>Diffusion models have emerged as leading generative models for images and
other modalities, but aligning their outputs with human preferences and safety
constraints remains a critical challenge. This thesis proposal investigates
methods to align diffusion models using reinforcement learning (RL) and reward
modeling. We survey recent advances in fine-tuning text-to-image diffusion
models with human feedback, including reinforcement learning from human and AI
feedback, direct preference optimization, and differentiable reward approaches.
We classify these methods based on the type of feedback (human, automated,
binary or ranked preferences), the fine-tuning technique (policy gradient,
reward-weighted likelihood, direct backpropagation, etc.), and their efficiency
and safety outcomes. We compare key algorithms and frameworks, highlighting how
they improve alignment with user intent or safety standards, and discuss
inter-relationships such as how newer methods build on or diverge from earlier
ones. Based on the survey, we identify five promising research directions for
the next two years: (1) multi-objective alignment with combined rewards, (2)
efficient human feedback usage and active learning, (3) robust safety alignment
against adversarial inputs, (4) continual and online alignment of diffusion
models, and (5) interpretable and trustworthy reward modeling for generative
images. Each direction is elaborated with its problem statement, challenges,
related work, and a proposed research plan. The proposal is organized as a
comprehensive document with literature review, comparative tables of methods,
and detailed research plans, aiming to contribute new insights and techniques
for safer and value-aligned diffusion-based generative AI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17249v1' target='_blank'>Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 19:56:03</h6>
<p class='card-text'>Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16833v1' target='_blank'>Strategically Linked Decisions in Long-Term Planning and Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alihan Hüyük, Finale Doshi-Velez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 16:04:17</h6>
<p class='card-text'>Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16787v1' target='_blank'>Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashish Sundar, Chunbo Luo, Xiaoyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:28:50</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16547v1' target='_blank'>Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for
  Occlusion Aware Plant Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 11:37:39</h6>
<p class='card-text'>This paper presents an end-to-end deep reinforcement learning (RL) framework
for occlusion-aware robotic manipulation in cluttered plant environments. Our
approach enables a robot to interact with a deformable plant to reveal hidden
objects of interest, such as fruits, using multimodal observations. We decouple
the kinematic planning problem from robot control to simplify zero-shot
sim2real transfer for the trained policy. Our results demonstrate that the
trained policy, deployed using our framework, achieves up to 86.7% success in
real-world trials across diverse initial conditions. Our findings pave the way
toward autonomous, perception-driven agricultural robots that intelligently
interact with complex foliage plants to "find the fruit" in challenging
occluded scenarios, without the need for explicitly designed geometric and
dynamic models of every plant scenario.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16394v1' target='_blank'>Raw2Drive: Reinforcement Learning with Aligned World Models for
  End-to-End Autonomous Driving (in CARLA v2)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:46:53</h6>
<p class='card-text'>Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16377v1' target='_blank'>VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with
  World Models for Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:29:59</h6>
<p class='card-text'>Reinforcement learning (RL)-based autonomous driving policy learning faces
critical limitations such as low sample efficiency and poor generalization; its
reliance on online interactions and trial-and-error learning is especially
unacceptable in safety-critical scenarios. Existing methods including safe RL
often fail to capture the true semantic meaning of "safety" in complex driving
contexts, leading to either overly conservative driving behavior or constraint
violations. To address these challenges, we propose VL-SAFE, a world
model-based safe RL framework with Vision-Language model
(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.
Specifically, we construct offline datasets containing data collected by expert
agents and labeled with safety scores derived from VLMs. A world model is
trained to generate imagined rollouts together with safety estimations,
allowing the agent to perform safe planning without interacting with the real
environment. Based on these imagined trajectories and safety evaluations,
actor-critic learning is conducted under VLM-based safety guidance to optimize
the driving policy more safely and efficiently. Extensive evaluations
demonstrate that VL-SAFE achieves superior sample efficiency, generalization,
safety, and overall performance compared to existing baselines. To the best of
our knowledge, this is the first work that introduces a VLM-guided world
model-based approach for safe autonomous driving. The demo video and code can
be accessed at: https://ys-qu.github.io/vlsafe-website/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15793v2' target='_blank'>HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for
  Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiwen Chen, Bo Leng, Zhuoren Li, Hanming Deng, Guizhe Jin, Ran Yu, Huanxi Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 17:47:24</h6>
<p class='card-text'>Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15754v1' target='_blank'>Improving planning and MBRL with temporally-extended actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Palash Chatterjee, Roni Khardon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:59:32</h6>
<p class='card-text'>Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15607v1' target='_blank'>From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with
  Pedagogy using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 15:00:07</h6>
<p class='card-text'>Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15146v1' target='_blank'>lmgame-Bench: How Good are LLMs at Playing Games?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 06:02:55</h6>
<p class='card-text'>Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.14970v1' target='_blank'>Self-Evolving Curriculum for LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 23:17:15</h6>
<p class='card-text'>Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.14443v1' target='_blank'>Semantically-driven Deep Reinforcement Learning for Inspection Path
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Grzegorz Malczyk, Mihir Kulkarni, Kostas Alexis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 14:45:16</h6>
<p class='card-text'>This paper introduces a novel semantics-aware inspection planning policy
derived through deep reinforcement learning. Reflecting the fact that within
autonomous informative path planning missions in unknown environments, it is
often only a sparse set of objects of interest that need to be inspected, the
method contributes an end-to-end policy that simultaneously performs semantic
object visual inspection combined with collision-free navigation. Assuming
access only to the instantaneous depth map, the associated segmentation image,
the ego-centric local occupancy, and the history of past positions in the
robot's neighborhood, the method demonstrates robust generalizability and
successful crossing of the sim2real gap. Beyond simulations and extensive
comparison studies, the approach is verified in experimental evaluations
onboard a flying robot deployed in novel environments with previously unseen
semantics and overall geometric configurations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13921v1' target='_blank'>APEX: Empowering LLMs with Physics-Based Task Planning for Real-time
  Insight</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 04:34:58</h6>
<p class='card-text'>Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13834v1' target='_blank'>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal
  Robot Teams</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 02:20:54</h6>
<p class='card-text'>Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>