<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-05-28</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-05-28</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.21457v1' target='_blank'>Active-O3: Empowering Multimodal Large Language Models with Active
  Perception via GRPO</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 17:29:31</h6>
<p class='card-text'>Active vision, also known as active perception, refers to the process of
actively selecting where and how to look in order to gather task-relevant
information. It is a critical component of efficient perception and
decision-making in humans and advanced embodied agents. Recently, the use of
Multimodal Large Language Models (MLLMs) as central planning and
decision-making modules in robotic systems has gained extensive attention.
However, despite the importance of active perception in embodied intelligence,
there is little to no exploration of how MLLMs can be equipped with or learn
active perception capabilities. In this paper, we first provide a systematic
definition of MLLM-based active perception tasks. We point out that the
recently proposed GPT-o3 model's zoom-in search strategy can be regarded as a
special case of active perception; however, it still suffers from low search
efficiency and inaccurate region selection. To address these issues, we propose
ACTIVE-O3, a purely reinforcement learning based training framework built on
top of GRPO, designed to equip MLLMs with active perception capabilities. We
further establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across
both general open-world tasks, such as small-object and dense object grounding,
and domain-specific scenarios, including small object detection in remote
sensing and autonomous driving, as well as fine-grained interactive
segmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot
reasoning abilities on the V* Benchmark, without relying on any explicit
reasoning data. We hope that our work can provide a simple codebase and
evaluation protocol to facilitate future research on active perception in
MLLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.20737v1' target='_blank'>RRO: LLM Agent Optimization Through Rising Reward Trajectories</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zilong Wang, Jingfeng Yang, Sreyashi Nag, Samarth Varshney, Xianfeng Tang, Haoming Jiang, Jingbo Shang, Sheikh Muhammad Sarwar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-27 05:27:54</h6>
<p class='card-text'>Large language models (LLMs) have exhibited extraordinary performance in a
variety of tasks while it remains challenging for them to solve complex
multi-step tasks as agents. In practice, agents sensitive to the outcome of
certain key steps which makes them likely to fail the task because of a subtle
mistake in the planning trajectory. Recent approaches resort to calibrating the
reasoning process through reinforcement learning. They reward or penalize every
reasoning step with process supervision, as known as Process Reward Models
(PRMs). However, PRMs are difficult and costly to scale up with a large number
of next action candidates since they require extensive computations to acquire
the training data through the per-step trajectory exploration. To mitigate this
issue, we focus on the relative reward trend across successive reasoning steps
and propose maintaining an increasing reward in the collected trajectories for
process supervision, which we term Reward Rising Optimization (RRO).
Specifically, we incrementally augment the process supervision until
identifying a step exhibiting positive reward differentials, i.e. rising
rewards, relative to its preceding iteration. This method dynamically expands
the search space for the next action candidates, efficiently capturing
high-quality data. We provide mathematical groundings and empirical results on
the WebShop and InterCode-SQL benchmarks, showing that our proposed RRO
achieves superior performance while requiring much less exploration cost.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.20573v1' target='_blank'>Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM
  Planners</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiabao Ji, Yongchao Chen, Yang Zhang, Ramana Rao Kompella, Chuchu Fan, Gaowen Liu, Shiyu Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 23:14:16</h6>
<p class='card-text'>Large language models (LLMs) have demonstrated strong performance in various
robot control tasks. However, their deployment in real-world applications
remains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently
produce invalid action plans that violate physical constraints, such as
directing a robot to an unreachable location or causing collisions between
robots. This issue primarily arises from a lack of awareness of these physical
constraints during the reasoning process. To address this issue, we propose a
novel framework that integrates reinforcement learning with verifiable rewards
(RLVR) to incentivize knowledge of physical constraints into LLMs to induce
constraints-aware reasoning during plan generation. In this approach, only
valid action plans that successfully complete a control task receive positive
rewards. We applied our method to two small-scale LLMs: a non-reasoning
Qwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results
demonstrate that constraint-aware small LLMs largely outperform large-scale
models without constraints, grounded on both the BoxNet task and a newly
developed BoxNet3D environment built using MuJoCo. This work highlights the
effectiveness of grounding even small LLMs with physical constraints to enable
scalable and efficient multi-robot control in complex, physically constrained
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.20285v2' target='_blank'>MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search
  Capability</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weiqi Wu, Xin Guan, Shen Huang, Yong Jiang, Pengjun Xie, Fei Huang, Jiuxin Cao, Hai Zhao, Jingren Zhou</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 17:58:50</h6>
<p class='card-text'>Retrieval-Augmented Language Models (RALMs) represent a classic paradigm
where models enhance generative capabilities using external knowledge retrieved
via a specialized module. Recent advancements in Agent techniques enable Large
Language Models (LLMs) to autonomously utilize tools for retrieval, planning,
and reasoning. While existing training-based methods show promise, their
agentic abilities are limited by inherent characteristics of the task-specific
data used during training. To further enhance the universal search capability
of agents, we propose a novel pre-training framework, MaskSearch. In the
pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)
task, where the model learns to leverage search tools to fill masked spans on a
large number of pre-training data, thus acquiring universal retrieval and
reasoning capabilities for LLMs. After that, the model is trained on downstream
tasks to achieve further improvement. We apply both Supervised Fine-tuning
(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine
agent-based and distillation-based methods to generate training data, starting
with a multi-agent system consisting of a planner, rewriter, observer, and
followed by a self-evolving teacher model. While for RL, we employ DAPO as the
training framework and adopt a hybrid reward system consisting of answer
rewards and format rewards. Additionally, we introduce a curriculum learning
approach that allows the model to learn progressively from easier to more
challenging instances based on the number of masked spans. We evaluate the
effectiveness of our framework in the scenario of open-domain multi-hop
question answering. Through extensive experiments, we demonstrate that
MaskSearch significantly enhances the performance of LLM-based search agents on
both in-domain and out-of-domain downstream tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.20175v1' target='_blank'>URPlanner: A Universal Paradigm For Collision-Free Robotic Motion
  Planning Based on Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fengkang Ying, Hanwen Zhang, Haozhe Wang, Huishi Huang, Marcelo H. Ang Jr</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 16:15:42</h6>
<p class='card-text'>Collision-free motion planning for redundant robot manipulators in complex
environments is yet to be explored. Although recent advancements at the
intersection of deep reinforcement learning (DRL) and robotics have highlighted
its potential to handle versatile robotic tasks, current DRL-based
collision-free motion planners for manipulators are highly costly, hindering
their deployment and application. This is due to an overreliance on the minimum
distance between the manipulator and obstacles, inadequate exploration and
decision-making by DRL, and inefficient data acquisition and utilization. In
this article, we propose URPlanner, a universal paradigm for collision-free
robotic motion planning based on DRL. URPlanner offers several advantages over
existing approaches: it is platform-agnostic, cost-effective in both training
and deployment, and applicable to arbitrary manipulators without solving
inverse kinematics. To achieve this, we first develop a parameterized task
space and a universal obstacle avoidance reward that is independent of minimum
distance. Second, we introduce an augmented policy exploration and evaluation
algorithm that can be applied to various DRL algorithms to enhance their
performance. Third, we propose an expert data diffusion strategy for efficient
policy learning, which can produce a large-scale trajectory dataset from only a
few expert demonstrations. Finally, the superiority of the proposed methods is
comprehensively verified through experiments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19964v1' target='_blank'>The Limits of Preference Data for Post-Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eric Zhao, Jessica Dai, Pranjal Awasthi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 13:26:15</h6>
<p class='card-text'>Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19867v1' target='_blank'>Deep Active Inference Agents for Delayed and Long-Horizon Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 11:50:22</h6>
<p class='card-text'>With the recent success of world-model agents, which extend the core idea of
model-based reinforcement learning by learning a differentiable model for
sample-efficient control across diverse tasks, active inference (AIF) offers a
complementary, neuroscience-grounded paradigm that unifies perception,
learning, and action within a single probabilistic framework powered by a
generative model. Despite this promise, practical AIF agents still rely on
accurate immediate predictions and exhaustive planning, a limitation that is
exacerbated in delayed environments requiring plans over long horizons, tens to
hundreds of steps. Moreover, most existing agents are evaluated on robotic or
vision benchmarks which, while natural for biological agents, fall short of
real-world industrial complexity. We address these limitations with a
generative-policy architecture featuring (i) a multi-step latent transition
that lets the generative model predict an entire horizon in a single
look-ahead, (ii) an integrated policy network that enables the transition and
receives gradients of the expected free energy, (iii) an alternating
optimization scheme that updates model and policy from a replay buffer, and
(iv) a single gradient step that plans over long horizons, eliminating
exhaustive planning from the control loop. We evaluate our agent in an
environment that mimics a realistic industrial scenario with delayed and
long-horizon settings. The empirical results confirm the effectiveness of the
proposed approach, demonstrating the coupled world-model with the AIF formalism
yields an end-to-end probabilistic controller capable of effective decision
making in delayed, long-horizon settings without handcrafted rewards or
expensive planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19761v1' target='_blank'>Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents
  via Offline Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 09:43:40</h6>
<p class='card-text'>While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19713v1' target='_blank'>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric
  Reward</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 09:01:56</h6>
<p class='card-text'>In this work, we introduce CAD-Coder, a novel framework that reformulates
text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric
CAD language. This representation enables direct geometric validation, a richer
modeling vocabulary, and seamless integration with existing LLMs. To further
enhance code validity and geometric fidelity, we propose a two-stage learning
pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2)
reinforcement learning with Group Reward Policy Optimization (GRPO), guided by
a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and
a format reward. We also introduce a chain-of-thought (CoT) planning process to
improve model reasoning, and construct a large-scale, high-quality dataset of
110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated
pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to
generate diverse, valid, and complex CAD models directly from natural language,
advancing the state of the art of text-to-CAD generation and geometric
reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19219v1' target='_blank'>Where Paths Collide: A Comprehensive Survey of Classic and
  Learning-Based Multi-Agent Pathfinding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiyue Wang, Haozheng Xu, Yuhan Zhang, Jingran Lin, Changhong Lu, Xiangfeng Wang, Wenhao Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 16:28:06</h6>
<p class='card-text'>Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial
intelligence and robotics, requiring the computation of collision-free paths
for multiple agents navigating from their start locations to designated goals.
As autonomous systems become increasingly prevalent in warehouses, urban
transportation, and other complex environments, MAPF has evolved from a
theoretical challenge to a critical enabler of real-world multi-robot
coordination. This comprehensive survey bridges the long-standing divide
between classical algorithmic approaches and emerging learning-based methods in
MAPF research. We present a unified framework that encompasses search-based
methods (including Conflict-Based Search, Priority-Based Search, and Large
Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP
formulations), and data-driven techniques (reinforcement learning, supervised
learning, and hybrid strategies). Through systematic analysis of experimental
practices across 200+ papers, we uncover significant disparities in evaluation
methodologies, with classical methods typically tested on larger-scale
instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based
approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy
of evaluation metrics, environment types, and baseline selections, highlighting
the need for standardized benchmarking protocols. Finally, we outline promising
future directions including mixed-motive MAPF with game-theoretic
considerations, language-grounded planning with large language models, and
neural solver architectures that combine the rigor of classical methods with
the flexibility of deep learning. This survey serves as both a comprehensive
reference for researchers and a practical guide for deploying MAPF solutions in
increasingly complex real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19053v1' target='_blank'>Structured Reinforcement Learning for Combinatorial Decision-Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heiko Hoppe, Léo Baty, Louis Bouvier, Axel Parmentier, Maximilian Schiffer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 09:17:10</h6>
<p class='card-text'>Reinforcement learning (RL) is increasingly applied to real-world problems
involving complex and structured decisions, such as routing, scheduling, and
assortment planning. These settings challenge standard RL algorithms, which
struggle to scale, generalize, and exploit structure in the presence of
combinatorial action spaces. We propose Structured Reinforcement Learning
(SRL), a novel actor-critic framework that embeds combinatorial optimization
layers into the actor neural network. We enable end-to-end learning of the
actor via Fenchel-Young losses and provide a geometric interpretation of SRL as
a primal-dual algorithm in the dual of the moment polytope. Across six
environments with exogenous and endogenous uncertainty, SRL matches or
surpasses the performance of unstructured RL and imitation learning on static
tasks and improves over these baselines by up to 92% on dynamic problems, with
improved stability and convergence speed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18989v1' target='_blank'>SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of
  Liver Tumours</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Catalina Tan, Yipeng Hu, Shaheer U. Saeed</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 06:14:41</h6>
<p class='card-text'>Accurate tumour segmentation is vital for various targeted diagnostic and
therapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.
Manual delineation is extremely labour-intensive, requiring substantial expert
time. Fully-supervised machine learning models aim to automate such
localisation tasks, but require a large number of costly and often subjective
3D voxel-level labels for training. The high-variance and subjectivity in such
labels impacts model generalisability, even when large datasets are available.
Histopathology labels may offer more objective labels but the infeasibility of
acquiring pixel-level annotations to develop tumour localisation methods based
on histology remains challenging in-vivo. In this work, we propose a novel
weakly-supervised semantic segmentation framework called SPARS (Self-Play
Adversarial Reinforcement Learning for Segmentation), which utilises an object
presence classifier, trained on a small number of image-level binary cancer
presence labels, to localise cancerous regions on CT scans. Such binary labels
of patient-level cancer presence can be sourced more feasibly from biopsies and
histopathology reports, enabling a more objective cancer localisation on
medical images. Evaluating with real patient data, we observed that SPARS
yielded a mean dice score of $77.3 \pm 9.4$, which outperformed other
weakly-supervised methods by large margins. This performance was comparable
with recent fully-supervised methods that require voxel-level annotations. Our
results demonstrate the potential of using SPARS to reduce the need for
extensive human-annotated labels to detect cancer in real-world healthcare
settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18831v1' target='_blank'>Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities
  through Fine-Tuning and Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinzheng Li, Sibo Ju, Yanzhou Su, Hongguang Li, Yiqing Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-24 19:00:36</h6>
<p class='card-text'>Existing large language models (LLMs) driven search agents typically rely on
prompt engineering to decouple the user queries into search plans, limiting
their effectiveness in complex scenarios requiring reasoning. Furthermore, they
suffer from excessive token consumption due to Python-based search plan
representations and inadequate integration of multimedia elements for both
input processing and response generation. To address these challenges, we
introduce SearchExpert, a training method for LLMs to improve their multimedia
search capabilities in response to complex search queries. Firstly, we
reformulate the search plan in an efficient natural language representation to
reduce token consumption. Then, we propose the supervised fine-tuning for
searching (SFTS) to fine-tune LLM to adapt to these representations, together
with an automated dataset construction pipeline. Secondly, to improve
reasoning-intensive search capabilities, we propose the reinforcement learning
from search feedback (RLSF) that takes the search results planned by LLM as the
reward signals. Thirdly, we propose a multimedia understanding and generation
agent that enables the fine-tuned LLM to process visual input and produce
visual output during inference. Finally, we establish an automated benchmark
construction pipeline and a human evaluation framework. Our resulting
benchmark, SearchExpertBench-25, comprises 200 multiple-choice questions
spanning financial and international news scenarios that require reasoning in
searching. Experiments demonstrate that SearchExpert outperforms the commercial
LLM search method (Perplexity Pro) by 36.60% on the existing FinSearchBench-24
benchmark and 54.54% on our proposed SearchExpertBench-25. Human evaluations
further confirm the superior readability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18098v1' target='_blank'>Planning without Search: Refining Frontier LLMs with Offline
  Goal-Conditioned RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joey Hong, Anca Dragan, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 16:51:54</h6>
<p class='card-text'>Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18083v1' target='_blank'>What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Quentin Clark, Florian Shkurti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 16:41:08</h6>
<p class='card-text'>In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17866v1' target='_blank'>DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 13:16:01</h6>
<p class='card-text'>Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17795v1' target='_blank'>DialogXpert: Driving Intelligent and Emotion-Aware Conversations through
  Online Value-Based Reinforcement Learning with LLM Priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 12:12:40</h6>
<p class='card-text'>Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17659v2' target='_blank'>Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 09:22:19</h6>
<p class='card-text'>Safe and feasible trajectory planning is essential for real-world autonomous
driving systems. However, existing learning-based planning methods often rely
on expert demonstrations, which not only lack explicit safety awareness but
also risk inheriting unsafe behaviors such as speeding from suboptimal human
driving data. Inspired by the success of large language models, we propose
Plan-R1, a novel two-stage trajectory planning framework that formulates
trajectory planning as a sequential prediction task, guided by explicit
planning principles such as safety, comfort, and traffic rule compliance. In
the first stage, we train an autoregressive trajectory predictor via next
motion token prediction on expert data. In the second stage, we design
rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the
model using Group Relative Policy Optimization (GRPO), a reinforcement learning
strategy, to align its predictions with these planning principles. Experiments
on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves
planning safety and feasibility, achieving state-of-the-art performance. Our
code will be made public soon.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17352v1' target='_blank'>Alignment and Safety of Diffusion Models via Reinforcement Learning and
  Reward Modeling: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 00:08:49</h6>
<p class='card-text'>Diffusion models have emerged as leading generative models for images and
other modalities, but aligning their outputs with human preferences and safety
constraints remains a critical challenge. This thesis proposal investigates
methods to align diffusion models using reinforcement learning (RL) and reward
modeling. We survey recent advances in fine-tuning text-to-image diffusion
models with human feedback, including reinforcement learning from human and AI
feedback, direct preference optimization, and differentiable reward approaches.
We classify these methods based on the type of feedback (human, automated,
binary or ranked preferences), the fine-tuning technique (policy gradient,
reward-weighted likelihood, direct backpropagation, etc.), and their efficiency
and safety outcomes. We compare key algorithms and frameworks, highlighting how
they improve alignment with user intent or safety standards, and discuss
inter-relationships such as how newer methods build on or diverge from earlier
ones. Based on the survey, we identify five promising research directions for
the next two years: (1) multi-objective alignment with combined rewards, (2)
efficient human feedback usage and active learning, (3) robust safety alignment
against adversarial inputs, (4) continual and online alignment of diffusion
models, and (5) interpretable and trustworthy reward modeling for generative
images. Each direction is elaborated with its problem statement, challenges,
related work, and a proposed research plan. The proposal is organized as a
comprehensive document with literature review, comparative tables of methods,
and detailed research plans, aiming to contribute new insights and techniques
for safer and value-aligned diffusion-based generative AI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17249v1' target='_blank'>Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 19:56:03</h6>
<p class='card-text'>Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>