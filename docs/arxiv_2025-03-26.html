<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-03-26</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-03-26</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.19699v1' target='_blank'>Optimal Path Planning and Cost Minimization for a Drone Delivery System
  Via Model Predictive Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Muhammad Al-Zafar Khan, Jamal Al-Karaki</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-25 14:27:29</h6>
<p class='card-text'>In this study, we formulate the drone delivery problem as a control problem
and solve it using Model Predictive Control. Two experiments are performed: The
first is on a less challenging grid world environment with lower
dimensionality, and the second is with a higher dimensionality and added
complexity. The MPC method was benchmarked against three popular Multi-Agent
Reinforcement Learning (MARL): Independent $Q$-Learning (IQL), Joint Action
Learners (JAL), and Value-Decomposition Networks (VDN). It was shown that the
MPC method solved the problem quicker and required fewer optimal numbers of
drones to achieve a minimized cost and navigate the optimal path.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18366v1' target='_blank'>Reinforcement Learning for Adaptive Planner Parameter Tuning: A
  Perspective on Hierarchical Architecture</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lu Wangtao, Wei Yufei, Xu Jiadong, Jia Wenhao, Li Liang, Xiong Rong, Wang Yue</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 06:02:41</h6>
<p class='card-text'>Automatic parameter tuning methods for planning algorithms, which integrate
pipeline approaches with learning-based techniques, are regarded as promising
due to their stability and capability to handle highly constrained
environments. While existing parameter tuning methods have demonstrated
considerable success, further performance improvements require a more
structured approach. In this paper, we propose a hierarchical architecture for
reinforcement learning-based parameter tuning. The architecture introduces a
hierarchical structure with low-frequency parameter tuning, mid-frequency
planning, and high-frequency control, enabling concurrent enhancement of both
upper-layer parameter tuning and lower-layer control through iterative
training. Experimental evaluations in both simulated and real-world
environments show that our method surpasses existing parameter tuning
approaches. Furthermore, our approach achieves first place in the Benchmark for
Autonomous Robot Navigation (BARN) Challenge.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18349v1' target='_blank'>Human-Object Interaction with Vision-Language Model Guided Relative
  Movement Dynamics</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zekai Deng, Ye Shi, Kaiyang Ji, Lan Xu, Shaoli Huang, Jingya Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-24 05:18:04</h6>
<p class='card-text'>Human-Object Interaction (HOI) is vital for advancing simulation, animation,
and robotics, enabling the generation of long-term, physically plausible
motions in 3D environments. However, existing methods often fall short of
achieving physics realism and supporting diverse types of interactions. To
address these challenges, this paper introduces a unified Human-Object
Interaction framework that provides unified control over interactions with
static scenes and dynamic objects using language commands. The interactions
between human and object parts can always be described as the continuous stable
Relative Movement Dynamics (RMD) between human and object parts. By leveraging
the world knowledge and scene perception capabilities of Vision-Language Models
(VLMs), we translate language commands into RMD diagrams, which are used to
guide goal-conditioned reinforcement learning for sequential interaction with
objects. Our framework supports long-horizon interactions among dynamic,
articulated, and static objects. To support the training and evaluation of our
framework, we present a new dataset named Interplay, which includes multi-round
task plans generated by VLMs, covering both static and dynamic HOI tasks.
Extensive experiments demonstrate that our proposed framework can effectively
handle a wide range of HOI tasks, showcasing its ability to maintain long-term,
multi-round transitions. For more details, please refer to our project webpage:
https://rmd-hoi.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.18161v1' target='_blank'>Active Inference for Energy Control and Planning in Smart Buildings and
  Communities</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Seyyed Danial Nazemi, Mohsen A. Jafari, Andrea Matta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-23 18:03:01</h6>
<p class='card-text'>Active Inference (AIF) is emerging as a powerful framework for
decision-making under uncertainty, yet its potential in engineering
applications remains largely unexplored. In this work, we propose a novel
dual-layer AIF architecture that addresses both building-level and
community-level energy management. By leveraging the free energy principle,
each layer adapts to evolving conditions and handles partial observability
without extensive sensor information and respecting data privacy. We validate
the continuous AIF model against both a perfect optimization baseline and a
reinforcement learning-based approach. We also test the community AIF framework
under extreme pricing scenarios. The results highlight the model's robustness
in handling abrupt changes. This study is the first to show how a distributed
AIF works in engineering. It also highlights new opportunities for
privacy-preserving and uncertainty-aware control strategies in engineering
applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17985v1' target='_blank'>Optimizing Navigation And Chemical Application in Precision Agriculture
  With Deep Reinforcement Learning And Conditional Action Tree</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mahsa Khosravi, Zhanhong Jiang, Joshua R Waite, Sarah Jonesc, Hernan Torres, Arti Singh, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-23 08:38:13</h6>
<p class='card-text'>This paper presents a novel reinforcement learning (RL)-based planning scheme
for optimized robotic management of biotic stresses in precision agriculture.
The framework employs a hierarchical decision-making structure with conditional
action masking, where high-level actions direct the robot's exploration, while
low-level actions optimize its navigation and efficient chemical spraying in
affected areas. The key objectives of optimization include improving the
coverage of infected areas with limited battery power and reducing chemical
usage, thus preventing unnecessary spraying of healthy areas of the field. Our
numerical experimental results demonstrate that the proposed method,
Hierarchical Action Masking Proximal Policy Optimization (HAM-PPO),
significantly outperforms baseline practices, such as LawnMower navigation +
indiscriminate spraying (Carpet Spray), in terms of yield recovery and resource
efficiency. HAM-PPO consistently achieves higher yield recovery percentages and
lower chemical costs across a range of infection scenarios. The framework also
exhibits robustness to observation noise and generalizability under diverse
environmental conditions, adapting to varying infection ranges and spatial
distribution patterns.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17693v1' target='_blank'>Conditional Diffusion Model with OOD Mitigation as High-Dimensional
  Offline Resource Allocation Planner in Clustered Ad Hoc Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kechen Meng, Sinuo Zhang, Rongpeng Li, Chan Wang, Ming Lei, Zhifeng Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-22 08:27:09</h6>
<p class='card-text'>Due to network delays and scalability limitations, clustered ad hoc networks
widely adopt Reinforcement Learning (RL) for on-demand resource allocation.
Albeit its demonstrated agility, traditional Model-Free RL (MFRL) solutions
struggle to tackle the huge action space, which generally explodes
exponentially along with the number of resource allocation units, enduring low
sampling efficiency and high interaction cost. In contrast to MFRL, Model-Based
RL (MBRL) offers an alternative solution to boost sample efficiency and
stabilize the training by explicitly leveraging a learned environment model.
However, establishing an accurate dynamic model for complex and noisy
environments necessitates a careful balance between model accuracy and
computational complexity $\&$ stability. To address these issues, we propose a
Conditional Diffusion Model Planner (CDMP) for high-dimensional offline
resource allocation in clustered ad hoc networks. By leveraging the astonishing
generative capability of Diffusion Models (DMs), our approach enables the
accurate modeling of high-quality environmental dynamics while leveraging an
inverse dynamics model to plan a superior policy. Beyond simply adopting DMs in
offline RL, we further incorporate the CDMP algorithm with a theoretically
guaranteed, uncertainty-aware penalty metric, which theoretically and
empirically manifests itself in mitigating the Out-of-Distribution
(OOD)-induced distribution shift issue underlying scarce training data.
Extensive experiments also show that our model outperforms MFRL in average
reward and Quality of Service (QoS) while demonstrating comparable performance
to other MBRL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17553v1' target='_blank'>Autonomous Radiotherapy Treatment Planning Using DOLA: A
  Privacy-Preserving, LLM-Based Optimization Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Humza Nusrat, Bing Luo, Ryan Hall, Joshua Kim, Hassan Bagher-Ebadian, Anthony Doemer, Benjamin Movsas, Kundan Thind</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-21 22:01:19</h6>
<p class='card-text'>Radiotherapy treatment planning is a complex and time-intensive process,
often impacted by inter-planner variability and subjective decision-making. To
address these challenges, we introduce Dose Optimization Language Agent (DOLA),
an autonomous large language model (LLM)-based agent designed for optimizing
radiotherapy treatment plans while rigorously protecting patient privacy. DOLA
integrates the LLaMa3.1 LLM directly with a commercial treatment planning
system, utilizing chain-of-thought prompting, retrieval-augmented generation
(RAG), and reinforcement learning (RL). Operating entirely within secure local
infrastructure, this agent eliminates external data sharing. We evaluated DOLA
using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in
20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and
optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations.
The 70B model demonstrated significantly improved performance, achieving
approximately 16.4% higher final scores than the 8B model. The RAG approach
outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated
convergence, highlighting the synergy of retrieval-based memory and
reinforcement learning. Optimal temperature hyperparameter analysis identified
0.4 as providing the best balance between exploration and exploitation. This
proof of concept study represents the first successful deployment of locally
hosted LLM agents for autonomous optimization of treatment plans within a
commercial radiotherapy planning system. By extending human-machine interaction
through interpretable natural language reasoning, DOLA offers a scalable and
privacy-conscious framework, with significant potential for clinical
implementation and workflow improvement.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17194v1' target='_blank'>Curriculum RL meets Monte Carlo Planning: Optimization of a Real World
  Container Management Problem</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Abhijeet Pendyala, Tobias Glasmachers</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-21 14:43:11</h6>
<p class='card-text'>In this work, we augment reinforcement learning with an inference-time
collision model to ensure safe and efficient container management in a
waste-sorting facility with limited processing capacity. Each container has two
optimal emptying volumes that trade off higher throughput against overflow
risk. Conventional reinforcement learning (RL) approaches struggle under
delayed rewards, sparse critical events, and high-dimensional uncertainty --
failing to consistently balance higher-volume empties with the risk of
safety-limit violations. To address these challenges, we propose a hybrid
method comprising: (1) a curriculum-learning pipeline that incrementally trains
a PPO agent to handle delayed rewards and class imbalance, and (2) an offline
pairwise collision model used at inference time to proactively avert collisions
with minimal online cost. Experimental results show that our targeted
inference-time collision checks significantly improve collision avoidance,
reduce safety-limit violations, maintain high throughput, and scale effectively
across varying container-to-PU ratios. These findings offer actionable
guidelines for designing safe and efficient container-management systems in
real-world facilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15865v2' target='_blank'>Active management of battery degradation in wireless sensor network
  using deep reinforcement learning for group battery replacement</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jong-Hyun Jeong, Hongki Jo, Qiang Zhou, Tahsin Afroz Hoque Nishat, Lang Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-20 05:36:33</h6>
<p class='card-text'>Wireless sensor networks (WSNs) have become a promising solution for
structural health monitoring (SHM), especially in hard-to-reach or remote
locations. Battery-powered WSNs offer various advantages over wired systems,
however limited battery life has always been one of the biggest obstacles in
practical use of the WSNs, regardless of energy harvesting methods. While
various methods have been studied for battery health management, existing
methods exclusively aim to extend lifetime of individual batteries, lacking a
system level view. A consequence of applying such methods is that batteries in
a WSN tend to fail at different times, posing significant difficulty on
planning and scheduling of battery replacement trip. This study investigate a
deep reinforcement learning (DRL) method for active battery degradation
management by optimizing duty cycle of WSNs at the system level. This active
management strategy effectively reduces earlier failure of battery individuals
which enable group replacement without sacrificing WSN performances. A
simulated environment based on a real-world WSN setup was developed to train a
DRL agent and learn optimal duty cycle strategies. The performance of the
strategy was validated in a long-term setup with various network sizes,
demonstrating its efficiency and scalability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15290v1' target='_blank'>Reinforcement Learning for Robust Athletic Intelligence: Lessons from
  the 2nd 'AI Olympics with RealAIGym' Competition</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Felix Wiebe, Niccol√≤ Turcato, Alberto Dalla Libera, Jean Seong Bjorn Choe, Bumkyu Choi, Tim Lukas Faust, Habib Maraqten, Erfan Aghadavoodi, Marco Cali, Alberto Sinigaglia, Giulio Giacomuzzo, Diego Romeres, Jong-kook Kim, Gian Antonio Susto, Shubham Vyas, Dennis Mronga, Boris Belousov, Jan Peters, Frank Kirchner, Shivesh Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 15:10:02</h6>
<p class='card-text'>In the field of robotics many different approaches ranging from classical
planning over optimal control to reinforcement learning (RL) are developed and
borrowed from other fields to achieve reliable control in diverse tasks. In
order to get a clear understanding of their individual strengths and weaknesses
and their applicability in real world robotic scenarios is it important to
benchmark and compare their performances not only in a simulation but also on
real hardware. The '2nd AI Olympics with RealAIGym' competition was held at the
IROS 2024 conference to contribute to this cause and evaluate different
controllers according to their ability to solve a dynamic control problem on an
underactuated double pendulum system with chaotic dynamics. This paper
describes the four different RL methods submitted by the participating teams,
presents their performance in the swing-up task on a real double pendulum,
measured against various criteria, and discusses their transferability from
simulation to real hardware and their robustness to external disturbances.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.17398v1' target='_blank'>Reachable Sets-based Trajectory Planning Combining Reinforcement
  Learning and iLQR</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wenjie Huang, Yang Li, Shijie Yuan, Jingjia Teng, Hongmao Qin, Yougang Bian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 13:07:10</h6>
<p class='card-text'>The driving risk field is applicable to more complex driving scenarios,
providing new approaches for safety decision-making and active vehicle control
in intricate environments. However, existing research often overlooks the
driving risk field and fails to consider the impact of risk distribution within
drivable areas on trajectory planning, which poses challenges for enhancing
safety. This paper proposes a trajectory planning method for intelligent
vehicles based on the risk reachable set to further improve the safety of
trajectory planning. First, we construct the reachable set incorporating the
driving risk field to more accurately assess and avoid potential risks in
drivable areas. Then, the initial trajectory is generated based on safe
reinforcement learning and projected onto the reachable set. Finally, we
introduce a trajectory planning method based on a constrained iterative
quadratic regulator to optimize the initial solution, ensuring that the planned
trajectory achieves optimal comfort, safety, and efficiency. We conduct
simulation tests of trajectory planning in high-speed lane-changing scenarios.
The results indicate that the proposed method can guarantee trajectory comfort
and driving efficiency, with the generated trajectory situated outside
high-risk boundaries, thereby ensuring vehicle safety during operation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.15108v1' target='_blank'>VIPER: Visual Perception and Explainable Reasoning for Sequential
  Decision-Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohamed Salim Aissi, Clemence Grislain, Mohamed Chetouani, Olivier Sigaud, Laure Soulier, Nicolas Thome</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 11:05:42</h6>
<p class='card-text'>While Large Language Models (LLMs) excel at reasoning on text and
Vision-Language Models (VLMs) are highly effective for visual perception,
applying those models for visual instruction-based planning remains a widely
open problem. In this paper, we introduce VIPER, a novel framework for
multimodal instruction-based planning that integrates VLM-based perception with
LLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM
generates textual descriptions of image observations, which are then processed
by an LLM policy to predict actions based on the task goal. We fine-tune the
reasoning module using behavioral cloning and reinforcement learning, improving
our agent's decision-making capabilities. Experiments on the ALFWorld benchmark
show that VIPER significantly outperforms state-of-the-art visual
instruction-based planners while narrowing the gap with purely text-based
oracles. By leveraging text as an intermediate representation, VIPER also
enhances explainability, paving the way for a fine-grained analysis of
perception and reasoning components.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.14809v1' target='_blank'>Learning with Expert Abstractions for Efficient Multi-Task Continuous
  Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jeff Jewett, Sandhya Saisubramanian</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-19 00:44:23</h6>
<p class='card-text'>Decision-making in complex, continuous multi-task environments is often
hindered by the difficulty of obtaining accurate models for planning and the
inefficiency of learning purely from trial and error. While precise environment
dynamics may be hard to specify, human experts can often provide high-fidelity
abstractions that capture the essential high-level structure of a task and user
preferences in the target environment. Existing hierarchical approaches often
target discrete settings and do not generalize across tasks. We propose a
hierarchical reinforcement learning approach that addresses these limitations
by dynamically planning over the expert-specified abstraction to generate
subgoals to learn a goal-conditioned policy. To overcome the challenges of
learning under sparse rewards, we shape the reward based on the optimal state
value in the abstract model. This structured decision-making process enhances
sample efficiency and facilitates zero-shot generalization. Our empirical
evaluation on a suite of procedurally generated continuous control environments
demonstrates that our approach outperforms existing hierarchical reinforcement
learning methods in terms of sample efficiency, task completion rate,
scalability to complex tasks, and generalization to novel scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13842v1' target='_blank'>Counterfactual experience augmented off-policy reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sunbowen Lee, Yicheng Gong, Chao Deng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-18 02:32:50</h6>
<p class='card-text'>Reinforcement learning control algorithms face significant challenges due to
out-of-distribution and inefficient exploration problems. While model-based
reinforcement learning enhances the agent's reasoning and planning capabilities
by constructing virtual environments, training such virtual environments can be
very complex. In order to build an efficient inference model and enhance the
representativeness of learning data, we propose the Counterfactual Experience
Augmentation (CEA) algorithm. CEA leverages variational autoencoders to model
the dynamic patterns of state transitions and introduces randomness to model
non-stationarity. This approach focuses on expanding the learning data in the
experience pool through counterfactual inference and performs exceptionally
well in environments that follow the bisimulation assumption. Environments with
bisimulation properties are usually represented by discrete observation and
action spaces, we propose a sampling method based on maximum kernel density
estimation entropy to extend CEA to various environments. By providing reward
signals for counterfactual state transitions based on real information, CEA
constructs a complete counterfactual experience to alleviate the
out-of-distribution problem of the learning data, and outperforms general SOTA
algorithms in environments with difference properties. Finally, we discuss the
similarities, differences and properties of generated counterfactual
experiences and real experiences. The code is available at
https://github.com/Aegis1863/CEA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12538v1' target='_blank'>EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with
  Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Zhu, Abirath Raju, Abdulaziz Shamsah, Anqi Wu, Seth Hutchinson, Ye Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 15:11:57</h6>
<p class='card-text'>This study presents an emotion-aware navigation framework -- EmoBipedNav --
using deep reinforcement learning (DRL) for bipedal robots walking in socially
interactive environments. The inherent locomotion constraints of bipedal robots
challenge their safe maneuvering capabilities in dynamic environments. When
combined with the intricacies of social environments, including pedestrian
interactions and social cues, such as emotions, these challenges become even
more pronounced. To address these coupled problems, we propose a two-stage
pipeline that considers both bipedal locomotion constraints and complex social
environments. Specifically, social navigation scenarios are represented using
sequential LiDAR grid maps (LGMs), from which we extract latent features,
including collision regions, emotion-related discomfort zones, social
interactions, and the spatio-temporal dynamics of evolving environments. The
extracted features are directly mapped to the actions of reduced-order models
(ROMs) through a DRL architecture. Furthermore, the proposed framework
incorporates full-order dynamics and locomotion constraints during training,
effectively accounting for tracking errors and restrictions of the locomotion
controller while planning the trajectory with ROMs. Comprehensive experiments
demonstrate that our approach exceeds both model-based planners and DRL-based
baselines. The hardware videos and open-source code are available at
https://gatech-lidar.github.io/emobipednav.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12434v1' target='_blank'>A Survey on the Optimization of Large Language Model-based Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 10:09:10</h6>
<p class='card-text'>With the rapid development of Large Language Models (LLMs), LLM-based agents
have been widely adopted in various fields, becoming essential for autonomous
decision-making and interactive tasks. However, current work typically relies
on prompt design or fine-tuning strategies applied to vanilla LLMs, which often
leads to limited effectiveness or suboptimal performance in complex
agent-related environments. Although LLM optimization techniques can improve
model performance across many general tasks, they lack specialized optimization
towards critical agent functionalities such as long-term planning, dynamic
environmental interaction, and complex decision-making. Although numerous
recent studies have explored various strategies to optimize LLM-based agents
for complex agent tasks, a systematic review summarizing and comparing these
methods from a holistic perspective is still lacking. In this survey, we
provide a comprehensive review of LLM-based agent optimization approaches,
categorizing them into parameter-driven and parameter-free methods. We first
focus on parameter-driven optimization, covering fine-tuning-based
optimization, reinforcement learning-based optimization, and hybrid strategies,
analyzing key aspects such as trajectory data construction, fine-tuning
techniques, reward function design, and optimization algorithms. Additionally,
we briefly discuss parameter-free strategies that optimize agent behavior
through prompt engineering and external knowledge retrieval. Finally, we
summarize the datasets and benchmarks used for evaluation and tuning, review
key applications of LLM-based agents, and discuss major challenges and
promising future directions. Our repository for related references is available
at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12036v1' target='_blank'>Hierarchical Reinforcement Learning for Safe Mapless Navigation with
  Congestion Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianqi Gao, Xizheng Pang, Qi Liu, Yanjie Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 08:03:50</h6>
<p class='card-text'>Reinforcement learning-based mapless navigation holds significant potential.
However, it faces challenges in indoor environments with local minima area.
This paper introduces a safe mapless navigation framework utilizing
hierarchical reinforcement learning (HRL) to enhance navigation through such
areas. The high-level policy creates a sub-goal to direct the navigation
process. Notably, we have developed a sub-goal update mechanism that considers
environment congestion, efficiently avoiding the entrapment of the robot in
local minimum areas. The low-level motion planning policy, trained through safe
reinforcement learning, outputs real-time control instructions based on
acquired sub-goal. Specifically, to enhance the robot's environmental
perception, we introduce a new obstacle encoding method that evaluates the
impact of obstacles on the robot's motion planning. To validate the performance
of our HRL-based navigation framework, we conduct simulations in office, home,
and restaurant environments. The findings demonstrate that our HRL-based
navigation framework excels in both static and dynamic scenarios. Finally, we
implement the HRL-based navigation framework on a TurtleBot3 robot for physical
validation experiments, which exhibits its strong generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.11449v1' target='_blank'>Optimizing 6G Dense Network Deployment for the Metaverse Using Deep
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Zhang, Swarna Chetty, Qiao Wang, Chenrui Sun, Paul Daniel Mitchell, David Grace, Hamed Ahmadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-14 14:34:36</h6>
<p class='card-text'>As the Metaverse envisions deeply immersive and pervasive connectivity in 6G
networks, Integrated Access and Backhaul (IAB) emerges as a critical enabler to
meet the demanding requirements of massive and immersive communications. IAB
networks offer a scalable solution for expanding broadband coverage in urban
environments. However, optimizing IAB node deployment to ensure reliable
coverage while minimizing costs remains challenging due to location constraints
and the dynamic nature of cities. Existing heuristic methods, such as Greedy
Algorithms, have been employed to address these optimization problems. This
work presents a novel Deep Reinforcement Learning ( DRL) approach for IAB
network planning, tailored to future 6G scenarios that seek to support
ultra-high data rates and dense device connectivity required by immersive
Metaverse applications. We utilize Deep Q-Network (DQN) with action elimination
and integrate DQN, Double Deep Q-Network ( DDQN), and Dueling DQN architectures
to effectively manage large state and action spaces. Simulations with various
initial donor configurations demonstrate the effectiveness of our DRL approach,
with Dueling DQN reducing node count by an average of 12.3% compared to
traditional heuristics. The study underscores how advanced DRL techniques can
address complex network planning challenges in 6G-enabled Metaverse contexts,
providing an efficient and adaptive solution for IAB deployment in diverse
urban environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10822v1' target='_blank'>Rotated Bitboards in FUSc# and Reinforcement Learning in Computer Chess
  and Beyond</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johannes Buchner</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 19:13:51</h6>
<p class='card-text'>There exist several techniques for representing the chess board inside the
computer. In the first part of this paper, the concepts of the
bitboard-representation and the advantages of (rotated) bitboards in move
generation are explained. In order to illustrate those ideas practice, the
concrete implementation of the move-generator in FUSc# is discussed and we
explain a technique how to verify the move-generator with the "perft"-command.
We show that the move-generator of FUSc# works 100% correct.
  The second part of this paper deals with reinforcement learning in computer
chess (and beyond). We exemplify the progress that has been made in this field
in the last 15-20 years by comparing the "state of the art" from 2002-2008,
when FUSc# was developed, with recent innovations connected to "AlphaZero". We
discuss how a "FUSc#-Zero" could be implemented and what would be necessary to
reduce the number of training games necessary to achieve a good performance.
This can be seen as a test case to the general prblem of improving "sample
effciency" in reinforcement learning.
  In the final part, we move beyond computer chess, as the importance of sample
effciency extends far beyond board games into a wide range of applications
where data is costly, diffcult to obtain, or time consuming to generate. We
review some application of the ideas developed in AlphaZero in other domains,
i.e. the "other Alphas" like AlphaFold, AlphaTensor, AlphaGeometry and
AlphaProof. We also discuss future research and the potential for such methods
for ecological economic planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10434v1' target='_blank'>Finetuning Generative Trajectory Model with Reinforcement Learning from
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 14:56:17</h6>
<p class='card-text'>Generating human-like and adaptive trajectories is essential for autonomous
driving in dynamic environments. While generative models have shown promise in
synthesizing feasible trajectories, they often fail to capture the nuanced
variability of human driving styles due to dataset biases and distributional
shifts. To address this, we introduce TrajHF, a human feedback-driven
finetuning framework for generative trajectory models, designed to align motion
planning with diverse driving preferences. TrajHF incorporates
multi-conditional denoiser and reinforcement learning with human feedback to
refine multi-modal trajectory generation beyond conventional imitation
learning. This enables better alignment with human driving preferences while
maintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95
on NavSim benchmark, significantly exceeding other methods. TrajHF sets a new
paradigm for personalized and adaptable trajectory generation in autonomous
driving.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>