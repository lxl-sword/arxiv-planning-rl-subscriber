<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-05-27</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-05-27</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19964v1' target='_blank'>The Limits of Preference Data for Post-Training</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Eric Zhao, Jessica Dai, Pranjal Awasthi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 13:26:15</h6>
<p class='card-text'>Recent progress in strengthening the capabilities of large language models
has stemmed from applying reinforcement learning to domains with automatically
verifiable outcomes. A key question is whether we can similarly use RL to
optimize for outcomes in domains where evaluating outcomes inherently requires
human feedback; for example, in tasks like deep research and trip planning,
outcome evaluation is qualitative and there are many possible degrees of
success. One attractive and scalable modality for collecting human feedback is
preference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$
given outcomes, which one is preferred. In this work, we study a critical
roadblock: preference data fundamentally and significantly limits outcome-based
optimization. Even with idealized preference data (infinite, noiseless, and
online), the use of ordinal feedback can prevent obtaining even approximately
optimal solutions. We formalize this impossibility using voting theory, drawing
an analogy between how a model chooses to answer a query with how voters choose
a candidate to elect. This indicates that grounded human scoring and
algorithmic innovations are necessary for extending the success of RL
post-training to domains demanding human feedback. We also explore why these
limitations have disproportionately impacted RLHF when it comes to eliciting
reasoning behaviors (e.g., backtracking) versus situations where RLHF has been
historically successful (e.g., instruction-tuning and safety training), finding
that the limitations of preference data primarily suppress RLHF's ability to
elicit robust strategies -- a class that encompasses most reasoning behaviors.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19867v1' target='_blank'>Deep Active Inference Agents for Delayed and Long-Horizon Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yavar Taheri Yeganeh, Mohsen Jafari, Andrea Matta</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 11:50:22</h6>
<p class='card-text'>With the recent success of world-model agents, which extend the core idea of
model-based reinforcement learning by learning a differentiable model for
sample-efficient control across diverse tasks, active inference (AIF) offers a
complementary, neuroscience-grounded paradigm that unifies perception,
learning, and action within a single probabilistic framework powered by a
generative model. Despite this promise, practical AIF agents still rely on
accurate immediate predictions and exhaustive planning, a limitation that is
exacerbated in delayed environments requiring plans over long horizons, tens to
hundreds of steps. Moreover, most existing agents are evaluated on robotic or
vision benchmarks which, while natural for biological agents, fall short of
real-world industrial complexity. We address these limitations with a
generative-policy architecture featuring (i) a multi-step latent transition
that lets the generative model predict an entire horizon in a single
look-ahead, (ii) an integrated policy network that enables the transition and
receives gradients of the expected free energy, (iii) an alternating
optimization scheme that updates model and policy from a replay buffer, and
(iv) a single gradient step that plans over long horizons, eliminating
exhaustive planning from the control loop. We evaluate our agent in an
environment that mimics a realistic industrial scenario with delayed and
long-horizon settings. The empirical results confirm the effectiveness of the
proposed approach, demonstrating the coupled world-model with the AIF formalism
yields an end-to-end probabilistic controller capable of effective decision
making in delayed, long-horizon settings without handcrafted rewards or
expensive planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19761v1' target='_blank'>Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents
  via Offline Hierarchical Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zican Hu, Wei Liu, Xiaoye Qu, Xiangyu Yue, Chunlin Chen, Zhi Wang, Yu Cheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 09:43:40</h6>
<p class='card-text'>While showing sophisticated reasoning abilities, large language models (LLMs)
still struggle with long-horizon decision-making tasks due to deficient
exploration and long-term credit assignment, especially in sparse-reward
scenarios. Inspired by the divide-and-conquer principle, we propose an
innovative framework **GLIDER** (**G**rounding **L**anguage Models as
Eff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical
**R**einforcement Learning) that introduces a parameter-efficient and generally
applicable hierarchy to LLM policies. We develop a scheme where the low-level
controller is supervised with abstract, step-by-step plans that are learned and
instructed by the high-level policy. This design decomposes complicated
problems into a series of coherent chain-of-thought reasoning sub-tasks,
providing flexible temporal abstraction to significantly enhance exploration
and learning for long-horizon tasks. Furthermore, GLIDER facilitates fast
online adaptation to non-stationary environments owing to the strong
transferability of its task-agnostic low-level skills. Experiments on
ScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent
performance gains, along with enhanced generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19713v1' target='_blank'>CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric
  Reward</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yandong Guan, Xilin Wang, Xingxi Ming, Jing Zhang, Dong Xu, Qian Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-26 09:01:56</h6>
<p class='card-text'>In this work, we introduce CAD-Coder, a novel framework that reformulates
text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric
CAD language. This representation enables direct geometric validation, a richer
modeling vocabulary, and seamless integration with existing LLMs. To further
enhance code validity and geometric fidelity, we propose a two-stage learning
pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2)
reinforcement learning with Group Reward Policy Optimization (GRPO), guided by
a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and
a format reward. We also introduce a chain-of-thought (CoT) planning process to
improve model reasoning, and construct a large-scale, high-quality dataset of
110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated
pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to
generate diverse, valid, and complex CAD models directly from natural language,
advancing the state of the art of text-to-CAD generation and geometric
reasoning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19219v1' target='_blank'>Where Paths Collide: A Comprehensive Survey of Classic and
  Learning-Based Multi-Agent Pathfinding</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shiyue Wang, Haozheng Xu, Yuhan Zhang, Jingran Lin, Changhong Lu, Xiangfeng Wang, Wenhao Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 16:28:06</h6>
<p class='card-text'>Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial
intelligence and robotics, requiring the computation of collision-free paths
for multiple agents navigating from their start locations to designated goals.
As autonomous systems become increasingly prevalent in warehouses, urban
transportation, and other complex environments, MAPF has evolved from a
theoretical challenge to a critical enabler of real-world multi-robot
coordination. This comprehensive survey bridges the long-standing divide
between classical algorithmic approaches and emerging learning-based methods in
MAPF research. We present a unified framework that encompasses search-based
methods (including Conflict-Based Search, Priority-Based Search, and Large
Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP
formulations), and data-driven techniques (reinforcement learning, supervised
learning, and hybrid strategies). Through systematic analysis of experimental
practices across 200+ papers, we uncover significant disparities in evaluation
methodologies, with classical methods typically tested on larger-scale
instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based
approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy
of evaluation metrics, environment types, and baseline selections, highlighting
the need for standardized benchmarking protocols. Finally, we outline promising
future directions including mixed-motive MAPF with game-theoretic
considerations, language-grounded planning with large language models, and
neural solver architectures that combine the rigor of classical methods with
the flexibility of deep learning. This survey serves as both a comprehensive
reference for researchers and a practical guide for deploying MAPF solutions in
increasingly complex real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.19053v1' target='_blank'>Structured Reinforcement Learning for Combinatorial Decision-Making</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heiko Hoppe, Léo Baty, Louis Bouvier, Axel Parmentier, Maximilian Schiffer</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 09:17:10</h6>
<p class='card-text'>Reinforcement learning (RL) is increasingly applied to real-world problems
involving complex and structured decisions, such as routing, scheduling, and
assortment planning. These settings challenge standard RL algorithms, which
struggle to scale, generalize, and exploit structure in the presence of
combinatorial action spaces. We propose Structured Reinforcement Learning
(SRL), a novel actor-critic framework that embeds combinatorial optimization
layers into the actor neural network. We enable end-to-end learning of the
actor via Fenchel-Young losses and provide a geometric interpretation of SRL as
a primal-dual algorithm in the dual of the moment polytope. Across six
environments with exogenous and endogenous uncertainty, SRL matches or
surpasses the performance of unstructured RL and imitation learning on static
tasks and improves over these baselines by up to 92% on dynamic problems, with
improved stability and convergence speed.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18989v1' target='_blank'>SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of
  Liver Tumours</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Catalina Tan, Yipeng Hu, Shaheer U. Saeed</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-25 06:14:41</h6>
<p class='card-text'>Accurate tumour segmentation is vital for various targeted diagnostic and
therapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.
Manual delineation is extremely labour-intensive, requiring substantial expert
time. Fully-supervised machine learning models aim to automate such
localisation tasks, but require a large number of costly and often subjective
3D voxel-level labels for training. The high-variance and subjectivity in such
labels impacts model generalisability, even when large datasets are available.
Histopathology labels may offer more objective labels but the infeasibility of
acquiring pixel-level annotations to develop tumour localisation methods based
on histology remains challenging in-vivo. In this work, we propose a novel
weakly-supervised semantic segmentation framework called SPARS (Self-Play
Adversarial Reinforcement Learning for Segmentation), which utilises an object
presence classifier, trained on a small number of image-level binary cancer
presence labels, to localise cancerous regions on CT scans. Such binary labels
of patient-level cancer presence can be sourced more feasibly from biopsies and
histopathology reports, enabling a more objective cancer localisation on
medical images. Evaluating with real patient data, we observed that SPARS
yielded a mean dice score of $77.3 \pm 9.4$, which outperformed other
weakly-supervised methods by large margins. This performance was comparable
with recent fully-supervised methods that require voxel-level annotations. Our
results demonstrate the potential of using SPARS to reduce the need for
extensive human-annotated labels to detect cancer in real-world healthcare
settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18831v1' target='_blank'>Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities
  through Fine-Tuning and Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jinzheng Li, Sibo Ju, Yanzhou Su, Hongguang Li, Yiqing Shen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-24 19:00:36</h6>
<p class='card-text'>Existing large language models (LLMs) driven search agents typically rely on
prompt engineering to decouple the user queries into search plans, limiting
their effectiveness in complex scenarios requiring reasoning. Furthermore, they
suffer from excessive token consumption due to Python-based search plan
representations and inadequate integration of multimedia elements for both
input processing and response generation. To address these challenges, we
introduce SearchExpert, a training method for LLMs to improve their multimedia
search capabilities in response to complex search queries. Firstly, we
reformulate the search plan in an efficient natural language representation to
reduce token consumption. Then, we propose the supervised fine-tuning for
searching (SFTS) to fine-tune LLM to adapt to these representations, together
with an automated dataset construction pipeline. Secondly, to improve
reasoning-intensive search capabilities, we propose the reinforcement learning
from search feedback (RLSF) that takes the search results planned by LLM as the
reward signals. Thirdly, we propose a multimedia understanding and generation
agent that enables the fine-tuned LLM to process visual input and produce
visual output during inference. Finally, we establish an automated benchmark
construction pipeline and a human evaluation framework. Our resulting
benchmark, SearchExpertBench-25, comprises 200 multiple-choice questions
spanning financial and international news scenarios that require reasoning in
searching. Experiments demonstrate that SearchExpert outperforms the commercial
LLM search method (Perplexity Pro) by 36.60% on the existing FinSearchBench-24
benchmark and 54.54% on our proposed SearchExpertBench-25. Human evaluations
further confirm the superior readability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18098v1' target='_blank'>Planning without Search: Refining Frontier LLMs with Offline
  Goal-Conditioned RL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Joey Hong, Anca Dragan, Sergey Levine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 16:51:54</h6>
<p class='card-text'>Large language models (LLMs) excel in tasks like question answering and
dialogue, but complex tasks requiring interaction, such as negotiation and
persuasion, require additional long-horizon reasoning and planning.
Reinforcement learning (RL) fine-tuning can enable such planning in principle,
but suffers from drawbacks that hinder scalability. In particular, multi-turn
RL training incurs high memory and computational costs, which are exacerbated
when training LLMs as policies. Furthermore, the largest LLMs do not expose the
APIs necessary to be trained in such manner. As a result, modern methods to
improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather
than RL fine-tuning. To remedy this, we propose a novel approach that uses
goal-conditioned value functions to guide the reasoning of LLM agents, that
scales even to large API-based models. These value functions predict how a task
will unfold given an action, allowing the LLM agent to evaluate multiple
possible outcomes, both positive and negative, to plan effectively. In
addition, these value functions are trained over reasoning steps rather than
full actions, to be a concise and light-weight module that facilitates
decision-making in multi-turn interactions. We validate our method on tasks
requiring interaction, including tool use, social deduction, and dialogue,
demonstrating superior performance over both RL fine-tuning and prompting
methods while maintaining efficiency and scalability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.18083v1' target='_blank'>What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Quentin Clark, Florian Shkurti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 16:41:08</h6>
<p class='card-text'>In planning, stitching is an ability of algorithms to piece together
sub-trajectories of data they are trained on to generate new and diverse
behaviours. While stitching is historically a strength of offline reinforcement
learning, recent generative behavioural cloning (BC) methods have also shown
proficiency at stitching. However, the main factors behind this are poorly
understood, hindering the development of new algorithms that can reliably
stitch. Focusing on diffusion planners trained via BC, we find two properties
are needed to compose: \emph{positional equivariance} and \emph{local
receptiveness}. We use these two properties to explain architecture, data, and
inference choices in existing generative BC methods based on diffusion
planning, including replanning frequency, data augmentation, and data scaling.
Experimental comparisions show that (1) while locality is more important than
positional equivariance in creating a diffusion planner capable of composition,
both are crucial (2) enabling these properties through relatively simple
architecture choices can be competitive with more computationally expensive
methods such as replanning or scaling data, and (3) simple inpainting-based
guidance can guide architecturally compositional models to enable
generalization in goal-conditioned settings.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17866v1' target='_blank'>DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hongshu Guo, Zeyuan Ma, Yining Ma, Xinglin Zhang, Wei-Neng Chen, Yue-Jiao Gong</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 13:16:01</h6>
<p class='card-text'>Designing effective black-box optimizers is hampered by limited
problem-specific knowledge and manual control that spans months for almost
every detail. In this paper, we present DesignX, the first automated algorithm
design framework that generates an effective optimizer specific to a given
black-box optimization problem within seconds. Rooted in the first principles,
we identify two key sub-tasks: 1) algorithm structure generation and 2)
hyperparameter control. To enable systematic construction, a comprehensive
modular algorithmic space is first built, embracing hundreds of algorithm
components collected from decades of research. We then introduce a dual-agent
reinforcement learning system that collaborates on structural and parametric
design through a novel cooperative training objective, enabling large-scale
meta-training across 10k diverse instances. Remarkably, through days of
autonomous learning, the DesignX-generated optimizers continuously surpass
human-crafted optimizers by orders of magnitude, either on synthetic testbed or
on realistic optimization scenarios such as Protein-docking, AutoML and UAV
path planning. Further in-depth analysis reveals DesignX's capability to
discover non-trivial algorithm patterns beyond expert intuition, which,
conversely, provides valuable design insights for the optimization community.
We provide DesignX's inference code at https://github.com/MetaEvo/DesignX.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17795v1' target='_blank'>DialogXpert: Driving Intelligent and Emotion-Aware Conversations through
  Online Value-Based Reinforcement Learning with LLM Priors</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 12:12:40</h6>
<p class='card-text'>Large-language-model (LLM) agents excel at reactive dialogue but struggle
with proactive, goal-driven interactions due to myopic decoding and costly
planning. We introduce DialogXpert, which leverages a frozen LLM to propose a
small, high-quality set of candidate actions per turn and employs a compact
Q-network over fixed BERT embeddings trained via temporal-difference learning
to select optimal moves within this reduced space. By tracking the user's
emotions, DialogXpert tailors each decision to advance the task while nurturing
a genuine, empathetic connection. Across negotiation, emotional support, and
tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with
success rates exceeding 94\% and, with a larger LLM prior, pushes success above
97\% while markedly improving negotiation outcomes. This framework delivers
real-time, strategic, and emotionally intelligent dialogue planning at scale.
Code available at https://github.com/declare-lab/dialogxpert/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17659v1' target='_blank'>Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaolong Tang, Meina Kan, Shiguang Shan, Xilin Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 09:22:19</h6>
<p class='card-text'>Safe and feasible trajectory planning is essential for real-world autonomous
driving systems. However, existing learning-based planning methods often rely
on expert demonstrations, which not only lack explicit safety awareness but
also risk inheriting unsafe behaviors such as speeding from suboptimal human
driving data. Inspired by the success of large language models, we propose
Plan-R1, a novel two-stage trajectory planning framework that formulates
trajectory planning as a sequential prediction task, guided by explicit
planning principles such as safety, comfort, and traffic rule compliance. In
the first stage, we train an autoregressive trajectory predictor via next
motion token prediction on expert data. In the second stage, we design
rule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the
model using Group Relative Policy Optimization (GRPO), a reinforcement learning
strategy, to align its predictions with these planning principles. Experiments
on the nuPlan benchmark demonstrate that our Plan-R1 significantly improves
planning safety and feasibility, achieving state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17352v1' target='_blank'>Alignment and Safety of Diffusion Models via Reinforcement Learning and
  Reward Modeling: A Survey</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Preeti Lamba, Kiran Ravish, Ankita Kushwaha, Pawan Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-23 00:08:49</h6>
<p class='card-text'>Diffusion models have emerged as leading generative models for images and
other modalities, but aligning their outputs with human preferences and safety
constraints remains a critical challenge. This thesis proposal investigates
methods to align diffusion models using reinforcement learning (RL) and reward
modeling. We survey recent advances in fine-tuning text-to-image diffusion
models with human feedback, including reinforcement learning from human and AI
feedback, direct preference optimization, and differentiable reward approaches.
We classify these methods based on the type of feedback (human, automated,
binary or ranked preferences), the fine-tuning technique (policy gradient,
reward-weighted likelihood, direct backpropagation, etc.), and their efficiency
and safety outcomes. We compare key algorithms and frameworks, highlighting how
they improve alignment with user intent or safety standards, and discuss
inter-relationships such as how newer methods build on or diverge from earlier
ones. Based on the survey, we identify five promising research directions for
the next two years: (1) multi-objective alignment with combined rewards, (2)
efficient human feedback usage and active learning, (3) robust safety alignment
against adversarial inputs, (4) continual and online alignment of diffusion
models, and (5) interpretable and trustworthy reward modeling for generative
images. Each direction is elaborated with its problem statement, challenges,
related work, and a proposed research plan. The proposal is organized as a
comprehensive document with literature review, comparative tables of methods,
and detailed research plans, aiming to contribute new insights and techniques
for safer and value-aligned diffusion-based generative AI.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.17249v1' target='_blank'>Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuran Sun, Susu Xu, Chenguang Wang, Xilei Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 19:56:03</h6>
<p class='card-text'>Big trajectory data hold great promise for human mobility analysis, but their
utility is often constrained by the absence of critical traveler attributes,
particularly sociodemographic information. While prior studies have explored
predicting such attributes from mobility patterns, they often overlooked
underlying cognitive mechanisms and exhibited low predictive accuracy. This
study introduces SILIC, short for Sociodemographic Inference with LLM-guided
Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a
theoretically grounded framework that leverages LLMs to infer sociodemographic
attributes from observed mobility patterns by capturing latent behavioral
intentions and reasoning through psychological constructs. Particularly, our
approach explicitly follows the Theory of Planned Behavior (TPB), a
foundational behavioral framework in transportation research, to model
individuals' latent cognitive processes underlying travel decision-making. The
LLMs further provide heuristic guidance to improve IRL reward function
initialization and update by addressing its ill-posedness and optimization
challenges arising from the vast and unstructured reward space. Evaluated in
the 2017 Puget Sound Regional Council Household Travel Survey, our method
substantially outperforms state-of-the-art baselines and shows great promise
for enriching big trajectory data to support more behaviorally grounded
applications in transportation planning and beyond.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16833v1' target='_blank'>Strategically Linked Decisions in Long-Term Planning and Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alihan Hüyük, Finale Doshi-Velez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 16:04:17</h6>
<p class='card-text'>Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16787v1' target='_blank'>Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashish Sundar, Chunbo Luo, Xiaoyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:28:50</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16547v1' target='_blank'>Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for
  Occlusion Aware Plant Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 11:37:39</h6>
<p class='card-text'>This paper presents an end-to-end deep reinforcement learning (RL) framework
for occlusion-aware robotic manipulation in cluttered plant environments. Our
approach enables a robot to interact with a deformable plant to reveal hidden
objects of interest, such as fruits, using multimodal observations. We decouple
the kinematic planning problem from robot control to simplify zero-shot
sim2real transfer for the trained policy. Our results demonstrate that the
trained policy, deployed using our framework, achieves up to 86.7% success in
real-world trials across diverse initial conditions. Our findings pave the way
toward autonomous, perception-driven agricultural robots that intelligently
interact with complex foliage plants to "find the fruit" in challenging
occluded scenarios, without the need for explicitly designed geometric and
dynamic models of every plant scenario.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16394v1' target='_blank'>Raw2Drive: Reinforcement Learning with Aligned World Models for
  End-to-End Autonomous Driving (in CARLA v2)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:46:53</h6>
<p class='card-text'>Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16377v1' target='_blank'>VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with
  World Models for Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:29:59</h6>
<p class='card-text'>Reinforcement learning (RL)-based autonomous driving policy learning faces
critical limitations such as low sample efficiency and poor generalization; its
reliance on online interactions and trial-and-error learning is especially
unacceptable in safety-critical scenarios. Existing methods including safe RL
often fail to capture the true semantic meaning of "safety" in complex driving
contexts, leading to either overly conservative driving behavior or constraint
violations. To address these challenges, we propose VL-SAFE, a world
model-based safe RL framework with Vision-Language model
(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.
Specifically, we construct offline datasets containing data collected by expert
agents and labeled with safety scores derived from VLMs. A world model is
trained to generate imagined rollouts together with safety estimations,
allowing the agent to perform safe planning without interacting with the real
environment. Based on these imagined trajectories and safety evaluations,
actor-critic learning is conducted under VLM-based safety guidance to optimize
the driving policy more safely and efficiently. Extensive evaluations
demonstrate that VL-SAFE achieves superior sample efficiency, generalization,
safety, and overall performance compared to existing baselines. To the best of
our knowledge, this is the first work that introduces a VLM-guided world
model-based approach for safe autonomous driving. The demo video and code can
be accessed at: https://ys-qu.github.io/vlsafe-website/</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>