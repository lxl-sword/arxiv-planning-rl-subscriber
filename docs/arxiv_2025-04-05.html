<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-04-05</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-04-05</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.02688v1' target='_blank'>Handover and SINR-Aware Path Optimization in 5G-UAV mmWave Communication
  using DRL</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Achilles Kiwanuka Machumilane, Alberto Gotta, Pietro Cassarà</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-03 15:28:04</h6>
<p class='card-text'>Path planning and optimization for unmanned aerial vehicles (UAVs)-assisted
next-generation wireless networks is critical for mobility management and
ensuring UAV safety and ubiquitous connectivity, especially in dense urban
environments with street canyons and tall buildings. Traditional statistical
and model-based techniques have been successfully used for path optimization in
communication networks. However, when dynamic channel propagation
characteristics such as line-of-sight (LOS), interference, handover, and
signal-to-interference and noise ratio (SINR) are included in path
optimization, statistical and model-based path planning solutions become
obsolete since they cannot adapt to the dynamic and time-varying wireless
channels, especially in the mmWave bands. In this paper, we propose a novel
model-free actor-critic deep reinforcement learning (AC-DRL) framework for path
optimization in UAV-assisted 5G mmWave wireless networks, which combines four
important aspects of UAV communication: \textit{flight time, handover,
connectivity and SINR}. We train an AC-RL agent that enables a UAV connected to
a gNB to determine the optimal path to a desired destination in the shortest
possible time with minimal gNB handover, while maintaining connectivity and the
highest possible SINR. We train our model with data from a powerful ray tracing
tool called Wireless InSite, which uses 3D images of the propagation
environment and provides data that closely resembles the real propagation
environment. The simulation results show that our system has superior
performance in tracking high SINR compared to other selected RL algorithms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.02161v1' target='_blank'>Preference-Driven Active 3D Scene Representation for Robotic Inspection
  in Nuclear Decommissioning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhen Meng, Kan Chen, Xiangmin Xu, Erwin Jose Lopez Pulgarin, Emma Li, Philip G. Zhao, David Flynn</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-02 22:20:48</h6>
<p class='card-text'>Active 3D scene representation is pivotal in modern robotics applications,
including remote inspection, manipulation, and telepresence. Traditional
methods primarily optimize geometric fidelity or rendering accuracy, but often
overlook operator-specific objectives, such as safety-critical coverage or
task-driven viewpoints. This limitation leads to suboptimal viewpoint
selection, particularly in constrained environments such as nuclear
decommissioning. To bridge this gap, we introduce a novel framework that
integrates expert operator preferences into the active 3D scene representation
pipeline. Specifically, we employ Reinforcement Learning from Human Feedback
(RLHF) to guide robotic path planning, reshaping the reward function based on
expert input. To capture operator-specific priorities, we conduct interactive
choice experiments that evaluate user preferences in 3D scene representation.
We validate our framework using a UR3e robotic arm for reactor tile inspection
in a nuclear decommissioning scenario. Compared to baseline methods, our
approach enhances scene representation while optimizing trajectory efficiency.
The RLHF-based policy consistently outperforms random selection, prioritizing
task-critical details. By unifying explicit 3D geometric modeling with implicit
human-in-the-loop optimization, this work establishes a foundation for
adaptive, safety-critical robotic perception systems, paving the way for
enhanced automation in nuclear decommissioning, remote maintenance, and other
high-risk environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.01871v1' target='_blank'>Interpreting Emergent Planning in Model-Free Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-02 16:24:23</h6>
<p class='card-text'>We present the first mechanistic evidence that model-free reinforcement
learning agents can learn to plan. This is achieved by applying a methodology
based on concept-based interpretability to a model-free agent in Sokoban -- a
commonly used benchmark for studying planning. Specifically, we demonstrate
that DRC, a generic model-free agent introduced by Guez et al. (2019), uses
learned concept representations to internally formulate plans that both predict
the long-term effects of actions on the environment and influence action
selection. Our methodology involves: (1) probing for planning-relevant
concepts, (2) investigating plan formation within the agent's representations,
and (3) verifying that discovered plans (in the agent's representations) have a
causal effect on the agent's behavior through interventions. We also show that
the emergence of these plans coincides with the emergence of a planning-like
property: the ability to benefit from additional test-time compute. Finally, we
perform a qualitative analysis of the planning algorithm learned by the agent
and discover a strong resemblance to parallelized bidirectional search. Our
findings advance understanding of the internal mechanisms underlying planning
behavior in agents, which is important given the recent trend of emergent
planning and reasoning capabilities in LLMs through RL</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.00277v1' target='_blank'>Rack Position Optimization in Large-Scale Heterogeneous Data Centers</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chang-Lin Chen, Jiayu Chen, Tian Lan, Zhaoxia Zhao, Hongbo Dong, Vaneet Aggarwal</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 22:55:37</h6>
<p class='card-text'>As rapidly growing AI computational demands accelerate the need for new
hardware installation and maintenance, this work explores optimal data center
resource management by balancing operational efficiency with fault tolerance
through strategic rack positioning considering diverse resources and locations.
Traditional mixed-integer programming (MIP) approaches often struggle with
scalability, while heuristic methods may result in significant sub-optimality.
To address these issues, this paper presents a novel two-tier optimization
framework using a high-level deep reinforcement learning (DRL) model to guide a
low-level gradient-based heuristic for local search. The high-level DRL agent
employs Leader Reward for optimal rack type ordering, and the low-level
heuristic efficiently maps racks to positions, minimizing movement counts and
ensuring fault-tolerant resource distribution. This approach allows scalability
to over 100,000 positions and 100 rack types. Our method outperformed the
gradient-based heuristic by 7\% on average and the MIP solver by over 30\% in
objective value. It achieved a 100\% success rate versus MIP's 97.5\% (within a
20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes
(i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which
showed performance variability under time constraints and high penalties, our
algorithm consistently delivered stable, efficient results - an essential
feature for large-scale data center management.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.24376v1' target='_blank'>Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 17:55:23</h6>
<p class='card-text'>Recent advancements in Chain of Thought (COT) generation have significantly
improved the reasoning capabilities of Large Language Models (LLMs), with
reinforcement learning (RL) emerging as an effective post-training approach.
Multimodal Large Language Models (MLLMs) inherit this reasoning potential but
remain underexplored in tasks requiring both perception and logical reasoning.
To address this, we introduce SEED-Bench-R1, a benchmark designed to
systematically evaluate post-training methods for MLLMs in video understanding.
It includes intricate real-world videos and complex everyday planning tasks in
the format of multiple-choice questions, requiring sophisticated perception and
reasoning. SEED-Bench-R1 assesses generalization through a three-level
hierarchy: in-distribution, cross-environment, and cross-environment-task
scenarios, equipped with a large-scale training dataset with easily verifiable
ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL
with supervised fine-tuning (SFT), demonstrating RL's data efficiency and
superior performance on both in-distribution and out-of-distribution tasks,
even outperforming SFT on general video understanding benchmarks like
LongVideoBench. Our detailed analysis reveals that RL enhances visual
perception but often produces less logically coherent reasoning chains. We
identify key limitations such as inconsistent reasoning and overlooked visual
cues, and suggest future improvements in base model reasoning, reward modeling,
and RL robustness against noisy signals.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.24214v1' target='_blank'>Moving Edge for On-Demand Edge Computing: An Uncertainty-aware Approach</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fangtong Zhou, Ruozhou Yu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 15:32:05</h6>
<p class='card-text'>We study an edge demand response problem where, based on historical edge
workload demands, an edge provider needs to dispatch moving computing units,
e.g. truck-carried modular data centers, in response to emerging hotspots
within service area. The goal of edge provider is to maximize the expected
revenue brought by serving congested users with satisfactory performance, while
minimizing the costs of moving units and the potential service-level agreement
violation penalty for interrupted services. The challenge is to make robust
predictions for future demands, as well as optimized moving unit dispatching
decisions. We propose a learning-based, uncertain-aware moving unit scheduling
framework, URANUS, to address this problem. Our framework novelly combines
Bayesian deep learning and distributionally robust approximation to make
predictions that are robust to data, model and distributional uncertainties in
deep learning-based prediction models. Based on the robust prediction outputs,
we further propose an efficient planning algorithm to optimize moving unit
scheduling in an online manner. Simulation experiments show that URANUS can
significantly improve robustness in decision making, and achieve superior
performance compared to state-of-the-art reinforcement learning,
uncertainty-agnostic learning-based methods, and other baselines.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23975v1' target='_blank'>A Reactive Framework for Whole-Body Motion Planning of Mobile
  Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic
  Programmi</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Chenyu Zhang, Shiying Sun, Kuan Liu, Chuanbao Zhou, Xiaoguang Zhao, Min Tan, Yanlong Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 11:37:02</h6>
<p class='card-text'>As an important branch of embodied artificial intelligence, mobile
manipulators are increasingly applied in intelligent services, but their
redundant degrees of freedom also limit efficient motion planning in cluttered
environments. To address this issue, this paper proposes a hybrid learning and
optimization framework for reactive whole-body motion planning of mobile
manipulators. We develop the Bayesian distributional soft actor-critic
(Bayes-DSAC) algorithm to improve the quality of value estimation and the
convergence performance of the learning. Additionally, we introduce a quadratic
programming method constrained by the signed distance field to enhance the
safety of the obstacle avoidance motion. We conduct experiments and make
comparison with standard benchmark. The experimental results verify that our
proposed framework significantly improves the efficiency of reactive whole-body
motion planning, reduces the planning time, and improves the success rate of
motion planning. Additionally, the proposed reinforcement learning method
ensures a rapid learning process in the whole-body planning task. The novel
framework allows mobile manipulators to adapt to complex environments more
safely and efficiently.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23908v1' target='_blank'>MAER-Nav: Bidirectional Motion Learning Through Mirror-Augmented
  Experience Replay for Robot Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shanze Wang, Mingao Tan, Zhibo Yang, Biao Huang, Xiaoyu Shen, Hailong Huang, Wei Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 09:58:28</h6>
<p class='card-text'>Deep Reinforcement Learning (DRL) based navigation methods have demonstrated
promising results for mobile robots, but suffer from limited action flexibility
in confined spaces. Conventional DRL approaches predominantly learn
forward-motion policies, causing robots to become trapped in complex
environments where backward maneuvers are necessary for recovery. This paper
presents MAER-Nav (Mirror-Augmented Experience Replay for Robot Navigation), a
novel framework that enables bidirectional motion learning without requiring
explicit failure-driven hindsight experience replay or reward function
modifications. Our approach integrates a mirror-augmented experience replay
mechanism with curriculum learning to generate synthetic backward navigation
experiences from successful trajectories. Experimental results in both
simulation and real-world environments demonstrate that MAER-Nav significantly
outperforms state-of-the-art methods while maintaining strong forward
navigation capabilities. The framework effectively bridges the gap between the
comprehensive action space utilization of traditional planning methods and the
environmental adaptability of learning-based approaches, enabling robust
navigation in scenarios where conventional DRL methods consistently fail.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23766v1' target='_blank'>Accelerating High-Efficiency Organic Photovoltaic Discovery via
  Pretrained Graph Neural Networks and Generative Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiangjie Qiu, Hou Hei Lam, Xiuyuan Hu, Wentao Li, Siwei Fu, Fankun Zeng, Hao Zhang, Xiaonan Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 06:31:15</h6>
<p class='card-text'>Organic photovoltaic (OPV) materials offer a promising avenue toward
cost-effective solar energy utilization. However, optimizing donor-acceptor
(D-A) combinations to achieve high power conversion efficiency (PCE) remains a
significant challenge. In this work, we propose a framework that integrates
large-scale pretraining of graph neural networks (GNNs) with a GPT-2
(Generative Pretrained Transformer 2)-based reinforcement learning (RL)
strategy to design OPV molecules with potentially high PCE. This approach
produces candidate molecules with predicted efficiencies approaching 21\%,
although further experimental validation is required. Moreover, we conducted a
preliminary fragment-level analysis to identify structural motifs recognized by
the RL model that may contribute to enhanced PCE, thus providing design
guidelines for the broader research community. To facilitate continued
discovery, we are building the largest open-source OPV dataset to date,
expected to include nearly 3,000 donor-acceptor pairs. Finally, we discuss
plans to collaborate with experimental teams on synthesizing and characterizing
AI-designed molecules, which will provide new data to refine and improve our
predictive and generative models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23650v1' target='_blank'>A Survey of Reinforcement Learning-Based Motion Planning for Autonomous
  Driving: Lessons Learned from a Driving Task Perspective</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhuoren Li, Guizhe Jin, Ran Yu, Zhiwen Chen, Nan Li, Wei Han, Lu Xiong, Bo Leng, Jia Hu, Ilya Kolmanovsky, Dimitar Filev</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-31 01:31:14</h6>
<p class='card-text'>Reinforcement learning (RL), with its ability to explore and optimize
policies in complex, dynamic decision-making tasks, has emerged as a promising
approach to addressing motion planning (MoP) challenges in autonomous driving
(AD). Despite rapid advancements in RL and AD, a systematic description and
interpretation of the RL design process tailored to diverse driving tasks
remains underdeveloped. This survey provides a comprehensive review of RL-based
MoP for AD, focusing on lessons from task-specific perspectives. We first
outline the fundamentals of RL methodologies, and then survey their
applications in MoP, analyzing scenario-specific features and task requirements
to shed light on their influence on RL design choices. Building on this
analysis, we summarize key design experiences, extract insights from various
driving task applications, and provide guidance for future implementations.
Additionally, we examine the frontier challenges in RL-based MoP, review recent
efforts to addresse these challenges, and propose strategies for overcoming
unresolved issues.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.23486v1' target='_blank'>A Systematic Decade Review of Trip Route Planning with Travel Time
  Estimation based on User Preferences and Behavior</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nikil Jayasuriya, Deshan Sumanathilaka</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-30 15:41:44</h6>
<p class='card-text'>This paper systematically explores the advancements in adaptive trip route
planning and travel time estimation (TTE) through Artificial Intelligence (AI).
With the increasing complexity of urban transportation systems, traditional
navigation methods often struggle to accommodate dynamic user preferences,
real-time traffic conditions, and scalability requirements. This study explores
the contributions of established AI techniques, including Machine Learning
(ML), Reinforcement Learning (RL), and Graph Neural Networks (GNNs), alongside
emerging methodologies like Meta-Learning, Explainable AI (XAI), Generative AI,
and Federated Learning. In addition to highlighting these innovations, the
paper identifies critical challenges such as ethical concerns, computational
scalability, and effective data integration, which must be addressed to advance
the field. The paper concludes with recommendations for leveraging AI to build
efficient, transparent, and sustainable navigation systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.22942v1' target='_blank'>Adaptive Interactive Navigation of Quadruped Robots using Large Language
  Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kangjie Zhou, Yao Mu, Haoyang Song, Yi Zeng, Pengying Wu, Han Gao, Chang Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-29 02:17:52</h6>
<p class='card-text'>Robotic navigation in complex environments remains a critical research
challenge. Traditional navigation methods focus on optimal trajectory
generation within free space, struggling in environments lacking viable paths
to the goal, such as disaster zones or cluttered warehouses. To address this
gap, we propose an adaptive interactive navigation approach that proactively
interacts with environments to create feasible paths to reach originally
unavailable goals. Specifically, we present a primitive tree for task planning
with large language models (LLMs), facilitating effective reasoning to
determine interaction objects and sequences. To ensure robust subtask
execution, we adopt reinforcement learning to pre-train a comprehensive skill
library containing versatile locomotion and interaction behaviors for motion
planning. Furthermore, we introduce an adaptive replanning method featuring two
LLM-based modules: an advisor serving as a flexible replanning trigger and an
arborist for autonomous plan adjustment. Integrated with the tree structure,
the replanning mechanism allows for convenient node addition and pruning,
enabling rapid plan modification in unknown environments. Comprehensive
simulations and experiments have demonstrated our method's effectiveness and
adaptivity in diverse scenarios. The supplementary video is available at page:
https://youtu.be/W5ttPnSap2g.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.22925v1' target='_blank'>Predictive Traffic Rule Compliance using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanliang Huang, Sebastian Mair, Zhuoqi Zeng, Amr Alanwar, Matthias Althoff</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-29 01:04:08</h6>
<p class='card-text'>Autonomous vehicle path planning has reached a stage where safety and
regulatory compliance are crucial. This paper presents a new approach that
integrates a motion planner with a deep reinforcement learning model to predict
potential traffic rule violations. In this setup, the predictions of the critic
directly affect the cost function of the motion planner, guiding the choices of
the trajectory. We incorporate key interstate rules from the German Road
Traffic Regulation into a rule book and use a graph-based state representation
to handle complex traffic information. Our main innovation is replacing the
standard actor network in an actor-critic setup with a motion planning module,
which ensures both predictable trajectory generation and prevention of
long-term rule violations. Experiments on an open German highway dataset show
that the model can predict and prevent traffic rule violations beyond the
planning horizon, significantly increasing safety in challenging traffic
conditions.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.22496v1' target='_blank'>Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving
  Simulation Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Luke Rowe, Roger Girgis, Anthony Gosselin, Liam Paull, Christopher Pal, Felix Heide</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-28 15:03:41</h6>
<p class='card-text'>We introduce Scenario Dreamer, a fully data-driven generative simulator for
autonomous vehicle planning that generates both the initial traffic scene -
comprising a lane graph and agent bounding boxes - and closed-loop agent
behaviours. Existing methods for generating driving simulation environments
encode the initial traffic scene as a rasterized image and, as such, require
parameter-heavy networks that perform unnecessary computation due to many empty
pixels in the rasterized scene. Moreover, we find that existing methods that
employ rule-based agent behaviours lack diversity and realism. Scenario Dreamer
instead employs a novel vectorized latent diffusion model for initial scene
generation that directly operates on the vectorized scene elements and an
autoregressive Transformer for data-driven agent behaviour simulation. Scenario
Dreamer additionally supports scene extrapolation via diffusion inpainting,
enabling the generation of unbounded simulation environments. Extensive
experiments show that Scenario Dreamer outperforms existing generative
simulators in realism and efficiency: the vectorized scene-generation base
model achieves superior generation quality with around 2x fewer parameters, 6x
lower generation latency, and 10x fewer GPU training hours compared to the
strongest baseline. We confirm its practical utility by showing that
reinforcement learning planning agents are more challenged in Scenario Dreamer
environments than traditional non-generative simulation environments,
especially on long and adversarial driving environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.22162v1' target='_blank'>Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration
  Maps</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ning Liu, Sen Shen, Xiangrui Kong, Hongtao Zhang, Thomas Bräunl</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-28 05:57:23</h6>
<p class='card-text'>Multi-Agent Pathfinding is used in areas including multi-robot formations,
warehouse logistics, and intelligent vehicles. However, many environments are
incomplete or frequently change, making it difficult for standard centralized
planning or pure reinforcement learning to maintain both global solution
quality and local flexibility. This paper introduces a hybrid framework that
integrates D* Lite global search with multi-agent reinforcement learning, using
a switching mechanism and a freeze-prevention strategy to handle dynamic
conditions and crowded settings. We evaluate the framework in the discrete
POGEMA environment and compare it with baseline methods. Experimental outcomes
indicate that the proposed framework substantially improves success rate,
collision rate, and path efficiency. The model is further tested on the EyeSim
platform, where it maintains feasible Pathfinding under frequent changes and
large-scale robot deployments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21989v1' target='_blank'>Bresa: Bio-inspired Reflexive Safe Reinforcement Learning for
  Contact-Rich Robotic Tasks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Heng Zhang, Gokhan Solak, Arash Ajoudani</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 21:11:32</h6>
<p class='card-text'>Ensuring safety in reinforcement learning (RL)-based robotic systems is a
critical challenge, especially in contact-rich tasks within unstructured
environments. While the state-of-the-art safe RL approaches mitigate risks
through safe exploration or high-level recovery mechanisms, they often overlook
low-level execution safety, where reflexive responses to potential hazards are
crucial. Similarly, variable impedance control (VIC) enhances safety by
adjusting the robot's mechanical response, yet lacks a systematic way to adapt
parameters, such as stiffness and damping throughout the task. In this paper,
we propose Bresa, a Bio-inspired Reflexive Hierarchical Safe RL method inspired
by biological reflexes. Our method decouples task learning from safety
learning, incorporating a safety critic network that evaluates action risks and
operates at a higher frequency than the task solver. Unlike existing
recovery-based methods, our safety critic functions at a low-level control
layer, allowing real-time intervention when unsafe conditions arise. The
task-solving RL policy, running at a lower frequency, focuses on high-level
planning (decision-making), while the safety critic ensures instantaneous
safety corrections. We validate Bresa on multiple tasks including a
contact-rich robotic task, demonstrating its reflexive ability to enhance
safety, and adaptability in unforeseen dynamic environments. Our results show
that Bresa outperforms the baseline, providing a robust and reflexive safety
mechanism that bridges the gap between high-level planning and low-level
execution. Real-world experiments and supplementary material are available at
project website https://jack-sherman01.github.io/Bresa.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21969v1' target='_blank'>Data-Agnostic Robotic Long-Horizon Manipulation with
  Vision-Language-Guided Closed-Loop Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuan Meng, Xiangtong Yao, Haihui Ye, Yirui Zhou, Shengqiang Zhang, Zhenshan Bing, Alois Knoll</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 20:32:58</h6>
<p class='card-text'>Recent advances in language-conditioned robotic manipulation have leveraged
imitation and reinforcement learning to enable robots to execute tasks from
human commands. However, these methods often suffer from limited
generalization, adaptability, and the lack of large-scale specialized datasets,
unlike data-rich domains such as computer vision, making long-horizon task
execution challenging. To address these gaps, we introduce DAHLIA, a
data-agnostic framework for language-conditioned long-horizon robotic
manipulation, leveraging large language models (LLMs) for real-time task
planning and execution. DAHLIA employs a dual-tunnel architecture, where an
LLM-powered planner collaborates with co-planners to decompose tasks and
generate executable plans, while a reporter LLM provides closed-loop feedback,
enabling adaptive re-planning and ensuring task recovery from potential
failures. Moreover, DAHLIA integrates chain-of-thought (CoT) in task reasoning
and temporal abstraction for efficient action execution, enhancing traceability
and robustness. Our framework demonstrates state-of-the-art performance across
diverse long-horizon tasks, achieving strong generalization in both simulated
and real-world scenarios. Videos and code are available at
https://ghiara.github.io/DAHLIA/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21683v1' target='_blank'>LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku
  with Self-Play and Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hui Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 16:52:25</h6>
<p class='card-text'>In recent years, large language models (LLMs) have shown significant
advancements in natural language processing (NLP), with strong capa-bilities in
generation, comprehension, and rea-soning. These models have found applications
in education, intelligent decision-making, and gaming. However, effectively
utilizing LLMs for strategic planning and decision-making in the game of Gomoku
remains a challenge. This study aims to develop a Gomoku AI system based on
LLMs, simulating the human learning process of playing chess. The system is
de-signed to understand and apply Gomoku strat-egies and logic to make rational
decisions. The research methods include enabling the model to "read the board,"
"understand the rules," "select strategies," and "evaluate positions," while
en-hancing its abilities through self-play and rein-forcement learning. The
results demonstrate that this approach significantly improves the se-lection of
move positions, resolves the issue of generating illegal positions, and reduces
pro-cess time through parallel position evaluation. After extensive self-play
training, the model's Gomoku-playing capabilities have been notably enhanced.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.21677v1' target='_blank'>A tale of two goals: leveraging sequentiality in multi-goal scenarios</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Olivier Serris, Stéphane Doncieux, Olivier Sigaud</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-27 16:47:46</h6>
<p class='card-text'>Several hierarchical reinforcement learning methods leverage planning to
create a graph or sequences of intermediate goals, guiding a lower-level
goal-conditioned (GC) policy to reach some final goals. The low-level policy is
typically conditioned on the current goal, with the aim of reaching it as
quickly as possible. However, this approach can fail when an intermediate goal
can be reached in multiple ways, some of which may make it impossible to
continue toward subsequent goals. To address this issue, we introduce two
instances of Markov Decision Process (MDP) where the optimization objective
favors policies that not only reach the current goal but also subsequent ones.
In the first, the agent is conditioned on both the current and final goals,
while in the second, it is conditioned on the next two goals in the sequence.
We conduct a series of experiments on navigation and pole-balancing tasks in
which sequences of intermediate goals are given. By evaluating policies trained
with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,
in most cases, conditioning on the next two goals improves stability and sample
efficiency over other approaches.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.20685v2' target='_blank'>Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast
  Ultrasound</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, Ruobing Huang, Alejandro F Frangi, Lingyun Bao, Xin Yang, Dong Ni</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-26 16:20:02</h6>
<p class='card-text'>Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D
automated breast ultrasound (ABUS) is crucial for clinical diagnosis and
treatment planning. Therefore, developing an automated system for nodule
segmentation can enhance user independence and expedite clinical analysis.
Unlike fully-supervised learning, weakly-supervised segmentation (WSS) can
streamline the laborious and intricate annotation process. However, current WSS
methods face challenges in achieving precise nodule segmentation, as many of
them depend on inaccurate activation maps or inefficient pseudo-mask generation
algorithms. In this study, we introduce a novel multi-agent reinforcement
learning-based WSS framework called Flip Learning, which relies solely on 2D/3D
boxes for accurate segmentation. Specifically, multiple agents are employed to
erase the target from the box to facilitate classification tag flipping, with
the erased region serving as the predicted segmentation mask. The key
contributions of this research are as follows: (1) Adoption of a
superpixel/supervoxel-based approach to encode the standardized environment,
capturing boundary priors and expediting the learning process. (2) Introduction
of three meticulously designed rewards, comprising a classification score
reward and two intensity distribution rewards, to steer the agents' erasing
process precisely, thereby avoiding both under- and over-segmentation. (3)
Implementation of a progressive curriculum learning strategy to enable agents
to interact with the environment in a progressively challenging manner, thereby
enhancing learning efficiency. Extensively validated on the large in-house BUS
and ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS
methods and foundation models, and achieves comparable performance as
fully-supervised learning algorithms.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>