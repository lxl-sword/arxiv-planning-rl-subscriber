<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-03-19</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-03-19</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.13842v1' target='_blank'>Counterfactual experience augmented off-policy reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sunbowen Lee, Yicheng Gong, Chao Deng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-18 02:32:50</h6>
<p class='card-text'>Reinforcement learning control algorithms face significant challenges due to
out-of-distribution and inefficient exploration problems. While model-based
reinforcement learning enhances the agent's reasoning and planning capabilities
by constructing virtual environments, training such virtual environments can be
very complex. In order to build an efficient inference model and enhance the
representativeness of learning data, we propose the Counterfactual Experience
Augmentation (CEA) algorithm. CEA leverages variational autoencoders to model
the dynamic patterns of state transitions and introduces randomness to model
non-stationarity. This approach focuses on expanding the learning data in the
experience pool through counterfactual inference and performs exceptionally
well in environments that follow the bisimulation assumption. Environments with
bisimulation properties are usually represented by discrete observation and
action spaces, we propose a sampling method based on maximum kernel density
estimation entropy to extend CEA to various environments. By providing reward
signals for counterfactual state transitions based on real information, CEA
constructs a complete counterfactual experience to alleviate the
out-of-distribution problem of the learning data, and outperforms general SOTA
algorithms in environments with difference properties. Finally, we discuss the
similarities, differences and properties of generated counterfactual
experiences and real experiences. The code is available at
https://github.com/Aegis1863/CEA.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12538v1' target='_blank'>EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with
  Deep Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wei Zhu, Abirath Raju, Abdulaziz Shamsah, Anqi Wu, Seth Hutchinson, Ye Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 15:11:57</h6>
<p class='card-text'>This study presents an emotion-aware navigation framework -- EmoBipedNav --
using deep reinforcement learning (DRL) for bipedal robots walking in socially
interactive environments. The inherent locomotion constraints of bipedal robots
challenge their safe maneuvering capabilities in dynamic environments. When
combined with the intricacies of social environments, including pedestrian
interactions and social cues, such as emotions, these challenges become even
more pronounced. To address these coupled problems, we propose a two-stage
pipeline that considers both bipedal locomotion constraints and complex social
environments. Specifically, social navigation scenarios are represented using
sequential LiDAR grid maps (LGMs), from which we extract latent features,
including collision regions, emotion-related discomfort zones, social
interactions, and the spatio-temporal dynamics of evolving environments. The
extracted features are directly mapped to the actions of reduced-order models
(ROMs) through a DRL architecture. Furthermore, the proposed framework
incorporates full-order dynamics and locomotion constraints during training,
effectively accounting for tracking errors and restrictions of the locomotion
controller while planning the trajectory with ROMs. Comprehensive experiments
demonstrate that our approach exceeds both model-based planners and DRL-based
baselines. The hardware videos and open-source code are available at
https://gatech-lidar.github.io/emobipednav.github.io/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12434v1' target='_blank'>A Survey on the Optimization of Large Language Model-based Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, Liang He</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-16 10:09:10</h6>
<p class='card-text'>With the rapid development of Large Language Models (LLMs), LLM-based agents
have been widely adopted in various fields, becoming essential for autonomous
decision-making and interactive tasks. However, current work typically relies
on prompt design or fine-tuning strategies applied to vanilla LLMs, which often
leads to limited effectiveness or suboptimal performance in complex
agent-related environments. Although LLM optimization techniques can improve
model performance across many general tasks, they lack specialized optimization
towards critical agent functionalities such as long-term planning, dynamic
environmental interaction, and complex decision-making. Although numerous
recent studies have explored various strategies to optimize LLM-based agents
for complex agent tasks, a systematic review summarizing and comparing these
methods from a holistic perspective is still lacking. In this survey, we
provide a comprehensive review of LLM-based agent optimization approaches,
categorizing them into parameter-driven and parameter-free methods. We first
focus on parameter-driven optimization, covering fine-tuning-based
optimization, reinforcement learning-based optimization, and hybrid strategies,
analyzing key aspects such as trajectory data construction, fine-tuning
techniques, reward function design, and optimization algorithms. Additionally,
we briefly discuss parameter-free strategies that optimize agent behavior
through prompt engineering and external knowledge retrieval. Finally, we
summarize the datasets and benchmarks used for evaluation and tuning, review
key applications of LLM-based agents, and discuss major challenges and
promising future directions. Our repository for related references is available
at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.12036v1' target='_blank'>Hierarchical Reinforcement Learning for Safe Mapless Navigation with
  Congestion Estimation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jianqi Gao, Xizheng Pang, Qi Liu, Yanjie Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-15 08:03:50</h6>
<p class='card-text'>Reinforcement learning-based mapless navigation holds significant potential.
However, it faces challenges in indoor environments with local minima area.
This paper introduces a safe mapless navigation framework utilizing
hierarchical reinforcement learning (HRL) to enhance navigation through such
areas. The high-level policy creates a sub-goal to direct the navigation
process. Notably, we have developed a sub-goal update mechanism that considers
environment congestion, efficiently avoiding the entrapment of the robot in
local minimum areas. The low-level motion planning policy, trained through safe
reinforcement learning, outputs real-time control instructions based on
acquired sub-goal. Specifically, to enhance the robot's environmental
perception, we introduce a new obstacle encoding method that evaluates the
impact of obstacles on the robot's motion planning. To validate the performance
of our HRL-based navigation framework, we conduct simulations in office, home,
and restaurant environments. The findings demonstrate that our HRL-based
navigation framework excels in both static and dynamic scenarios. Finally, we
implement the HRL-based navigation framework on a TurtleBot3 robot for physical
validation experiments, which exhibits its strong generalization capabilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.11449v1' target='_blank'>Optimizing 6G Dense Network Deployment for the Metaverse Using Deep
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jie Zhang, Swarna Chetty, Qiao Wang, Chenrui Sun, Paul Daniel Mitchell, David Grace, Hamed Ahmadi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-14 14:34:36</h6>
<p class='card-text'>As the Metaverse envisions deeply immersive and pervasive connectivity in 6G
networks, Integrated Access and Backhaul (IAB) emerges as a critical enabler to
meet the demanding requirements of massive and immersive communications. IAB
networks offer a scalable solution for expanding broadband coverage in urban
environments. However, optimizing IAB node deployment to ensure reliable
coverage while minimizing costs remains challenging due to location constraints
and the dynamic nature of cities. Existing heuristic methods, such as Greedy
Algorithms, have been employed to address these optimization problems. This
work presents a novel Deep Reinforcement Learning ( DRL) approach for IAB
network planning, tailored to future 6G scenarios that seek to support
ultra-high data rates and dense device connectivity required by immersive
Metaverse applications. We utilize Deep Q-Network (DQN) with action elimination
and integrate DQN, Double Deep Q-Network ( DDQN), and Dueling DQN architectures
to effectively manage large state and action spaces. Simulations with various
initial donor configurations demonstrate the effectiveness of our DRL approach,
with Dueling DQN reducing node count by an average of 12.3% compared to
traditional heuristics. The study underscores how advanced DRL techniques can
address complex network planning challenges in 6G-enabled Metaverse contexts,
providing an efficient and adaptive solution for IAB deployment in diverse
urban environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10822v1' target='_blank'>Rotated Bitboards in FUSc# and Reinforcement Learning in Computer Chess
  and Beyond</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Johannes Buchner</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 19:13:51</h6>
<p class='card-text'>There exist several techniques for representing the chess board inside the
computer. In the first part of this paper, the concepts of the
bitboard-representation and the advantages of (rotated) bitboards in move
generation are explained. In order to illustrate those ideas practice, the
concrete implementation of the move-generator in FUSc# is discussed and we
explain a technique how to verify the move-generator with the "perft"-command.
We show that the move-generator of FUSc# works 100% correct.
  The second part of this paper deals with reinforcement learning in computer
chess (and beyond). We exemplify the progress that has been made in this field
in the last 15-20 years by comparing the "state of the art" from 2002-2008,
when FUSc# was developed, with recent innovations connected to "AlphaZero". We
discuss how a "FUSc#-Zero" could be implemented and what would be necessary to
reduce the number of training games necessary to achieve a good performance.
This can be seen as a test case to the general prblem of improving "sample
effciency" in reinforcement learning.
  In the final part, we move beyond computer chess, as the importance of sample
effciency extends far beyond board games into a wide range of applications
where data is costly, diffcult to obtain, or time consuming to generate. We
review some application of the ideas developed in AlphaZero in other domains,
i.e. the "other Alphas" like AlphaFold, AlphaTensor, AlphaGeometry and
AlphaProof. We also discuss future research and the potential for such methods
for ecological economic planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10434v1' target='_blank'>Finetuning Generative Trajectory Model with Reinforcement Learning from
  Human Feedback</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 14:56:17</h6>
<p class='card-text'>Generating human-like and adaptive trajectories is essential for autonomous
driving in dynamic environments. While generative models have shown promise in
synthesizing feasible trajectories, they often fail to capture the nuanced
variability of human driving styles due to dataset biases and distributional
shifts. To address this, we introduce TrajHF, a human feedback-driven
finetuning framework for generative trajectory models, designed to align motion
planning with diverse driving preferences. TrajHF incorporates
multi-conditional denoiser and reinforcement learning with human feedback to
refine multi-modal trajectory generation beyond conventional imitation
learning. This enables better alignment with human driving preferences while
maintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95
on NavSim benchmark, significantly exceeding other methods. TrajHF sets a new
paradigm for personalized and adaptable trajectory generation in autonomous
driving.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.10419v1' target='_blank'>A nonlinear real time capable motion cueing algorithm based on deep
  reinforcement learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hendrik Scheidel, Camilo Gonzalez, Houshyar Asadi, Tobias Bellmann, Andreas Seefried, Shady Mohamed, Saeid Nahavandi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-13 14:39:19</h6>
<p class='card-text'>In motion simulation, motion cueing algorithms are used for the trajectory
planning of the motion simulator platform, where workspace limitations prevent
direct reproduction of reference trajectories. Strategies such as motion
washout, which return the platform to its center, are crucial in these
settings. For serial robotic MSPs with highly nonlinear workspaces, it is
essential to maximize the efficient utilization of the MSPs kinematic and
dynamic capabilities. Traditional approaches, including classical washout
filtering and linear model predictive control, fail to consider
platform-specific, nonlinear properties, while nonlinear model predictive
control, though comprehensive, imposes high computational demands that hinder
real-time, pilot-in-the-loop application without further simplification. To
overcome these limitations, we introduce a novel approach using deep
reinforcement learning for motion cueing, demonstrated here for the first time
in a 6-degree-of-freedom setting with full consideration of the MSPs kinematic
nonlinearities. Previous work by the authors successfully demonstrated the
application of DRL to a simplified 2-DOF setup, which did not consider
kinematic or dynamic constraints. This approach has been extended to all 6 DOF
by incorporating a complete kinematic model of the MSP into the algorithm, a
crucial step for enabling its application on a real motion simulator. The
training of the DRL-MCA is based on Proximal Policy Optimization in an
actor-critic implementation combined with an automated hyperparameter
optimization. After detailing the necessary training framework and the
algorithm itself, we provide a comprehensive validation, demonstrating that the
DRL MCA achieves competitive performance against established algorithms.
Moreover, it generates feasible trajectories by respecting all system
constraints and meets all real-time requirements with low...</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09758v1' target='_blank'>Multi-Agent LLM Actor-Critic Framework for Social Robot Navigation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Weizheng Wang, Ike Obi, Byung-Cheol Min</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 18:59:53</h6>
<p class='card-text'>Recent advances in robotics and large language models (LLMs) have sparked
growing interest in human-robot collaboration and embodied intelligence. To
enable the broader deployment of robots in human-populated environments,
socially-aware robot navigation (SAN) has become a key research area. While
deep reinforcement learning approaches that integrate human-robot interaction
(HRI) with path planning have demonstrated strong benchmark performance, they
often struggle to adapt to new scenarios and environments. LLMs offer a
promising avenue for zero-shot navigation through commonsense inference.
However, most existing LLM-based frameworks rely on centralized
decision-making, lack robust verification mechanisms, and face inconsistencies
in translating macro-actions into precise low-level control signals. To address
these challenges, we propose SAMALM, a decentralized multi-agent LLM
actor-critic framework for multi-robot social navigation. In this framework, a
set of parallel LLM actors, each reflecting distinct robot personalities or
configurations, directly generate control signals. These actions undergo a
two-tier verification process via a global critic that evaluates group-level
behaviors and individual critics that assess each robot's context. An
entropy-based score fusion mechanism further enhances self-verification and
re-query, improving both robustness and coordination. Experimental results
confirm that SAMALM effectively balances local autonomy with global oversight,
yielding socially compliant behaviors and strong adaptability across diverse
multi-robot scenarios. More details and videos about this work are available
at: https://sites.google.com/view/SAMALM.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09730v1' target='_blank'>Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem
  Proving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sara Rajaee, Kumar Pratik, Gabriele Cesa, Arash Behboodi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 18:20:47</h6>
<p class='card-text'>The most promising recent methods for AI reasoning require applying variants
of reinforcement learning (RL) either on rolled out trajectories from the
model, even for the step-wise rewards, or large quantities of human annotated
trajectory data. The reliance on the rolled-out trajectory renders the compute
cost and time prohibitively high. In particular, the correctness of a reasoning
trajectory can typically only be judged at its completion, leading to sparse
rewards in RL or requiring expensive synthetic data generation in expert
iteration-like methods. In this work, we focus on the Automatic Theorem Proving
(ATP) task and propose a novel verifier-in-the-loop design, which unlike
existing approaches that leverage feedback on the entire reasoning trajectory,
employs an automated verifier to give intermediate feedback at each step of the
reasoning process. Using Lean as the verifier, we empirically show that the
step-by-step local verification produces a global improvement in the model's
reasoning accuracy and efficiency.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.09501v2' target='_blank'>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ziyu Wan, Yunxiang Li, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-12 16:05:31</h6>
<p class='card-text'>Recent research on Reasoning of Large Language Models (LLMs) has sought to
further enhance their performance by integrating meta-thinking -- enabling
models to monitor, evaluate, and control their reasoning processes for more
adaptive and effective problem-solving. However, current single-agent work
lacks a specialized design for acquiring meta-thinking, resulting in low
efficacy. To address this challenge, we introduce Reinforced Meta-thinking
Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement
Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think
about thinking. ReMA decouples the reasoning process into two hierarchical
agents: a high-level meta-thinking agent responsible for generating strategic
oversight and plans, and a low-level reasoning agent for detailed executions.
Through iterative reinforcement learning with aligned objectives, these agents
explore and learn collaboration, leading to improved generalization and
robustness. Experimental results demonstrate that ReMA outperforms single-agent
RL baselines on complex reasoning tasks, including competitive-level
mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation
studies further illustrate the evolving dynamics of each distinct agent,
providing valuable insights into how the meta-thinking reasoning process
enhances the reasoning capabilities of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.08338v1' target='_blank'>Trinity: A Modular Humanoid Robot AI System</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingkai Sun, Qiang Zhang, Gang Han, Wen Zhao, Zhe Yong, Yan He, Jiaxu Wang, Jiahang Cao, Yijie Guo, Renjing Xu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-11 11:50:36</h6>
<p class='card-text'>In recent years, research on humanoid robots has garnered increasing
attention. With breakthroughs in various types of artificial intelligence
algorithms, embodied intelligence, exemplified by humanoid robots, has been
highly anticipated. The advancements in reinforcement learning (RL) algorithms
have significantly improved the motion control and generalization capabilities
of humanoid robots. Simultaneously, the groundbreaking progress in large
language models (LLM) and visual language models (VLM) has brought more
possibilities and imagination to humanoid robots. LLM enables humanoid robots
to understand complex tasks from language instructions and perform long-term
task planning, while VLM greatly enhances the robots' understanding and
interaction with their environment. This paper introduces
\textcolor{magenta}{Trinity}, a novel AI system for humanoid robots that
integrates RL, LLM, and VLM. By combining these technologies, Trinity enables
efficient control of humanoid robots in complex environments. This innovative
approach not only enhances the capabilities but also opens new avenues for
future research and applications of humanoid robotics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07608v1' target='_blank'>AlphaDrive: Unleashing the Power of VLMs in Autonomous Driving via
  Reinforcement Learning and Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu, Xinggang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 17:59:42</h6>
<p class='card-text'>OpenAI o1 and DeepSeek R1 achieve or even surpass human expert-level
performance in complex domains like mathematics and science, with reinforcement
learning (RL) and reasoning playing a crucial role. In autonomous driving,
recent end-to-end models have greatly improved planning performance but still
struggle with long-tailed problems due to limited common sense and reasoning
abilities. Some studies integrate vision-language models (VLMs) into autonomous
driving, but they typically rely on pre-trained models with simple supervised
fine-tuning (SFT) on driving data, without further exploration of training
strategies or optimizations specifically tailored for planning. In this paper,
we propose AlphaDrive, a RL and reasoning framework for VLMs in autonomous
driving. AlphaDrive introduces four GRPO-based RL rewards tailored for planning
and employs a two-stage planning reasoning training strategy that combines SFT
with RL. As a result, AlphaDrive significantly improves both planning
performance and training efficiency compared to using only SFT or without
reasoning. Moreover, we are also excited to discover that, following RL
training, AlphaDrive exhibits some emergent multimodal planning capabilities,
which is critical for improving driving safety and efficiency. To the best of
our knowledge, AlphaDrive is the first to integrate GRPO-based RL with planning
reasoning into autonomous driving. Code will be released to facilitate future
research.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.07411v1' target='_blank'>PER-DPP Sampling Framework and Its Application in Path Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junzhe Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 14:58:16</h6>
<p class='card-text'>Autonomous navigation in intelligent mobile systems represents a core
research focus within artificial intelligence-driven robotics. Contemporary
path planning approaches face constraints in dynamic environmental
responsiveness and multi-objective task scalability, limiting their capacity to
address growing intelligent operation requirements. Decision-centric
reinforcement learning frameworks, capitalizing on their unique strengths in
adaptive environmental interaction and self-optimization, have gained
prominence in advanced control system research. This investigation introduces
methodological improvements to address sample homogeneity challenges in
reinforcement learning experience replay mechanisms. By incorporating
determinant point processes (DPP) for diversity assessment, we develop a
dual-criteria sampling framework with adaptive selection protocols. This
approach resolves representation bias in conventional prioritized experience
replay (PER) systems while preserving algorithmic interoperability, offering
improved decision optimization for dynamic operational scenarios. Key
contributions comprise: Develop a hybrid sampling paradigm (PER-DPP) combining
priority sequencing with diversity maximization.Based on this,create an
integrated optimization scheme (PER-DPP-Elastic DQN) merging diversity-aware
sampling with adaptive step-size regulation. Comparative simulations in 2D
navigation scenarios demonstrate that the elastic step-size component
temporarily delays initial convergence speed but synergistically enhances
final-stage optimization with PER-DPP integration. The synthesized method
generates navigation paths with optimized length efficiency and directional
stability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06985v1' target='_blank'>Learning Decision Trees as Amortized Structure Inference</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mohammed Mahfoud, Ghait Boukachab, Michał Koziarski, Alex Hernandez-Garcia, Stefan Bauer, Yoshua Bengio, Nikolay Malkin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-10 07:05:07</h6>
<p class='card-text'>Building predictive models for tabular data presents fundamental challenges,
notably in scaling consistently, i.e., more resources translating to better
performance, and generalizing systematically beyond the training data
distribution. Designing decision tree models remains especially challenging
given the intractably large search space, and most existing methods rely on
greedy heuristics, while deep learning inductive biases expect a temporal or
spatial structure not naturally present in tabular data. We propose a hybrid
amortized structure inference approach to learn predictive decision tree
ensembles given data, formulating decision tree construction as a sequential
planning problem. We train a deep reinforcement learning (GFlowNet) policy to
solve this problem, yielding a generative model that samples decision trees
from the Bayesian posterior. We show that our approach, DT-GFN, outperforms
state-of-the-art decision tree and deep learning methods on standard
classification benchmarks derived from real-world data, robustness to
distribution shifts, and anomaly detection, all while yielding interpretable
models with shorter description lengths. Samples from the trained DT-GFN model
can be ensembled to construct a random forest, and we further show that the
performance of scales consistently in ensemble size, yielding ensembles of
predictors that continue to generalize systematically.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06578v1' target='_blank'>Non-Equilibrium MAV-Capture-MAV via Time-Optimal Planning and
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Canlun Zheng, Zhanyu Guo, Zikang Yin, Chunyu Wang, Zhikun Wang, Shiyu Zhao</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 12:16:30</h6>
<p class='card-text'>The capture of flying MAVs (micro aerial vehicles) has garnered increasing
research attention due to its intriguing challenges and promising applications.
Despite recent advancements, a key limitation of existing work is that capture
strategies are often relatively simple and constrained by platform performance.
This paper addresses control strategies capable of capturing
high-maneuverability targets. The unique challenge of achieving target capture
under unstable conditions distinguishes this task from traditional
pursuit-evasion and guidance problems. In this study, we transition from larger
MAV platforms to a specially designed, compact capture MAV equipped with a
custom launching device while maintaining high maneuverability. We explore both
time-optimal planning (TOP) and reinforcement learning (RL) methods.
Simulations demonstrate that TOP offers highly maneuverable and shorter
trajectories, while RL excels in real-time adaptability and stability.
Moreover, the RL method has been tested in real-world scenarios, successfully
achieving target capture even in unstable states.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.06514v1' target='_blank'>GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with
  Generative Flow Networks</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Haoqiang Kang, Enna Sachdeva, Piyush Gupta, Sangjae Bae, Kwonjoon Lee</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-09 08:38:10</h6>
<p class='card-text'>Vision-Language Models (VLMs) have recently shown promising advancements in
sequential decision-making tasks through task-specific fine-tuning. However,
common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO),
present notable limitations: SFT assumes Independent and Identically
Distributed (IID) data, while PPO focuses on maximizing cumulative rewards.
These limitations often restrict solution diversity and hinder generalization
in multi-step reasoning tasks. To address these challenges, we introduce a
novel framework, GFlowVLM, a framework that fine-tune VLMs using Generative
Flow Networks (GFlowNets) to promote generation of diverse solutions for
complex reasoning tasks. GFlowVLM models the environment as a non-Markovian
decision process, allowing it to capture long-term dependencies essential for
real-world applications. It takes observations and task descriptions as inputs
to prompt chain-of-thought (CoT) reasoning which subsequently guides action
selection. We use task based rewards to fine-tune VLM with GFlowNets. This
approach enables VLMs to outperform prior fine-tuning methods, including SFT
and RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex
tasks such as card games (NumberLine, BlackJack) and embodied planning tasks
(ALFWorld), showing enhanced training efficiency, solution diversity, and
stronger generalization capabilities across both in-distribution and
out-of-distribution scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05905v1' target='_blank'>Performance Comparisons of Reinforcement Learning Algorithms for
  Sequential Experimental Design</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yasir Zubayr Barlas, Kizito Salako</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 19:57:39</h6>
<p class='card-text'>Recent developments in sequential experimental design look to construct a
policy that can efficiently navigate the design space, in a way that maximises
the expected information gain. Whilst there is work on achieving tractable
policies for experimental design problems, there is significantly less work on
obtaining policies that are able to generalise well - i.e. able to give good
performance despite a change in the underlying statistical properties of the
experiments. Conducting experiments sequentially has recently brought about the
use of reinforcement learning, where an agent is trained to navigate the design
space to select the most informative designs for experimentation. However,
there is still a lack of understanding about the benefits and drawbacks of
using certain reinforcement learning algorithms to train these agents. In our
work, we investigate several reinforcement learning algorithms and their
efficacy in producing agents that take maximally informative design decisions
in sequential experimental design scenarios. We find that agent performance is
impacted depending on the algorithm used for training, and that particular
algorithms, using dropout or ensemble approaches, empirically showcase
attractive generalisation properties.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.05276v1' target='_blank'>Constrained Reinforcement Learning for the Dynamic Inventory Routing
  Problem under Stochastic Supply and Demand</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Umur Hasturk, Albert H. Schrotenboer, Kees Jan Roodbergen, Evrim Ursavas</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-07 09:47:15</h6>
<p class='card-text'>Green hydrogen has multiple use cases and is produced from renewable energy,
such as solar or wind energy. It can be stored in large quantities, decoupling
renewable energy generation from its use, and is therefore considered essential
for achieving a climate-neutral economy. The intermittency of renewable energy
generation and the stochastic nature of demand are, however, challenging
factors for the dynamic planning of hydrogen storage and transportation. This
holds particularly in the early-adoption phase when hydrogen distribution
occurs through vehicle-based networks. We therefore address the Dynamic
Inventory Routing Problem (DIRP) under stochastic supply and demand with direct
deliveries for the vehicle-based distribution of hydrogen. To solve this
problem, we propose a Constrained Reinforcement Learning (CRL) framework that
integrates constraints into the learning process and incorporates parameterized
post-decision state value predictions. Additionally, we introduce
Lookahead-based CRL (LCRL), which improves decision-making over a multi-period
horizon to enhance short-term planning while maintaining the value predictions.
Our computational experiments demonstrate the efficacy of CRL and LCRL across
diverse instances. Our learning methods provide near-optimal solutions on small
scale instances that are solved via value iteration. Furthermore, both methods
outperform typical deep learning approaches such as Proximal Policy
Optimization, as well as classical inventory heuristics, such as
(s,S)-policy-based and Power-of-Two-based heuristics. Furthermore, LCRL
achieves a 10% improvement over CRL on average, albeit with higher
computational requirements. Analyses of optimal replenishment policies reveal
that accounting for stochastic supply and demand influences these policies,
showing the importance of our addition to the DIRP.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2503.04280v2' target='_blank'>Towards Autonomous Reinforcement Learning for Real-World Robotic
  Manipulation with Large Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-03-06 10:08:44</h6>
<p class='card-text'>Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>