<!DOCTYPE html>
<html>
<head>
<title>arXiv Papers - 2025-02-24</title>
</head>
<body>
<h1>üìö arXiv Papers on 2025-02-24</h1>
<h2><a href='http://arxiv.org/abs/1901.01492v1' target='_blank'>What Should I Do Now? Marrying Reinforcement Learning and Symbolic
  Planning</a></h2>
<p><strong>Authors:</strong> Daniel Gordon, Dieter Fox, Ali Farhadi</p>
<p><strong>Summary:</strong> Long-term planning poses a major difficulty to many reinforcement learning
algorithms. This problem becomes even more pronounced in dynamic visual
environments. In this work we propose Hierarchical Planning and Reinforcement
Learning (HIP-RL), a method for merging the benefits and capabilities of
Symbolic Planning with the learning abilities of Deep Reinforcement Learning.
We apply HIPRL to the complex visual tasks of interactive question answering
and visual semantic planning and achieve state-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.12079v4' target='_blank'>Trajectory Planning for Autonomous Vehicle Using Iterative Reward
  Prediction in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hyunwoo Park</p>
<p><strong>Summary:</strong> Traditional trajectory planning methods for autonomous vehicles have several
limitations. For example, heuristic and explicit simple rules limit
generalizability and hinder complex motions. These limitations can be addressed
using reinforcement learning-based trajectory planning. However, reinforcement
learning suffers from unstable learning, and existing reinforcement
learning-based trajectory planning methods do not consider the uncertainties.
Thus, this paper, proposes a reinforcement learnin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.07779v3' target='_blank'>PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement
  Learning for Robust Decision-Making</a></h2>
<p><strong>Authors:</strong> Fangkai Yang, Daoming Lyu, Bo Liu, Steven Gustafson</p>
<p><strong>Summary:</strong> Reinforcement learning and symbolic planning have both been used to build
intelligent autonomous agents. Reinforcement learning relies on learning from
interactions with real world, which often requires an unfeasibly large amount
of experience. Symbolic planning relies on manually crafted symbolic knowledge,
which may not be robust to domain uncertainties and changes. In this paper we
present a unified framework {\em PEORL} that integrates symbolic planning with
hierarchical reinforcement learni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.09683v2' target='_blank'>From semantics to execution: Integrating action planning with
  reinforcement learning for robotic causal problem-solving</a></h2>
<p><strong>Authors:</strong> Manfred Eppe, Phuong D. H. Nguyen, Stefan Wermter</p>
<p><strong>Summary:</strong> Reinforcement learning is an appropriate and successful method to robustly
perform low-level robot control under noisy conditions. Symbolic action
planning is useful to resolve causal dependencies and to break a causally
complex problem down into a sequence of simpler high-level actions. A problem
with the integration of both approaches is that action planning is based on
discrete high-level action- and state spaces, whereas reinforcement learning is
usually driven by a continuous reward functio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.09260v1' target='_blank'>Deep Reinforcement Learning for Motion Planning of Mobile Robots</a></h2>
<p><strong>Authors:</strong> Leonid Butyrev, Thorsten Edelh√§u√üer, Christopher Mutschler</p>
<p><strong>Summary:</strong> This paper presents a novel motion and trajectory planning algorithm for
nonholonomic mobile robots that uses recent advances in deep reinforcement
learning. Starting from a random initial state, i.e., position, velocity and
orientation, the robot reaches an arbitrary target state while taking both
kinematic and dynamic constraints into account. Our deep reinforcement learning
agent not only processes a continuous state space it also executes continuous
actions, i.e., the acceleration of wheels ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.15902v1' target='_blank'>Can flocking aid the path planning of microswimmers in turbulent flows?</a></h2>
<p><strong>Authors:</strong> Akanksha Gupta, Jaya Kumar Alageshan, Kolluru Venkata Kiran, Rahul Pandit</p>
<p><strong>Summary:</strong> We show that flocking of microswimmers in a turbulent flow can enhance the
efficacy of reinforcement-learning-based path-planning of microswimmers in
turbulent flows. In particular, we develop a machine-learning strategy that
incorporates Vicsek-model-type flocking in microswimmer assemblies in a
statistically homogeneous and isotropic turbulent flow in two dimensions (2D).
We build on the adversarial-reinforcement-learning of
Ref.~\cite{alageshan2020machine} for non-interacting microswimmers in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.01112v1' target='_blank'>The Dreaming Variational Autoencoder for Reinforcement Learning
  Environments</a></h2>
<p><strong>Authors:</strong> Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo</p>
<p><strong>Summary:</strong> Reinforcement learning has shown great potential in generalizing over raw
sensory data using only a single neural network for value optimization. There
are several challenges in the current state-of-the-art reinforcement learning
algorithms that prevent them from converging towards the global optima. It is
likely that the solution to these problems lies in short- and long-term
planning, exploration and memory management for reinforcement learning
algorithms. Games are often used to benchmark rei...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.00477v2' target='_blank'>Posterior Sampling for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Remo Sasso, Michelangelo Conserva, Paulo Rauber</p>
<p><strong>Summary:</strong> Despite remarkable successes, deep reinforcement learning algorithms remain
sample inefficient: they require an enormous amount of trial and error to find
good policies. Model-based algorithms promise sample efficiency by building an
environment model that can be used for planning. Posterior Sampling for
Reinforcement Learning is such a model-based algorithm that has attracted
significant interest due to its performance in the tabular setting. This paper
introduces Posterior Sampling for Deep Re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.08554v1' target='_blank'>Adjust Planning Strategies to Accommodate Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Xuerun Chen</p>
<p><strong>Summary:</strong> In agent control issues, the idea of combining reinforcement learning and
planning has attracted much attention. Two methods focus on micro and macro
action respectively. Their advantages would show together if there is a good
cooperation between them. An essential for the cooperation is to find an
appropriate boundary, assigning different functions to each method. Such
boundary could be represented by parameters in a planning algorithm. In this
paper, we create an optimization strategy for plan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1705.10432v1' target='_blank'>Fine-grained acceleration control for autonomous intersection management
  using deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Hamid Mirzaei, Tony Givargis</p>
<p><strong>Summary:</strong> Recent advances in combining deep learning and Reinforcement Learning have
shown a promising path for designing new control agents that can learn optimal
policies for challenging control tasks. These new methods address the main
limitations of conventional Reinforcement Learning methods such as customized
feature engineering and small action/state space dimension requirements. In
this paper, we leverage one of the state-of-the-art Reinforcement Learning
methods, known as Trust Region Policy Opti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.15009v4' target='_blank'>A Unifying Framework for Reinforcement Learning and Planning</a></h2>
<p><strong>Authors:</strong> Thomas M. Moerland, Joost Broekens, Aske Plaat, Catholijn M. Jonker</p>
<p><strong>Summary:</strong> Sequential decision making, commonly formalized as optimization of a Markov
Decision Process, is a key challenge in artificial intelligence. Two successful
approaches to MDP optimization are reinforcement learning and planning, which
both largely have their own research communities. However, if both research
fields solve the same problem, then we might be able to disentangle the common
factors in their solution approaches. Therefore, this paper presents a unifying
algorithmic framework for reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.08280v1' target='_blank'>Integration of Reinforcement Learning Based Behavior Planning With
  Sampling Based Motion Planning for Automated Driving</a></h2>
<p><strong>Authors:</strong> Marvin Klimke, Benjamin V√∂lz, Michael Buchholz</p>
<p><strong>Summary:</strong> Reinforcement learning has received high research interest for developing
planning approaches in automated driving. Most prior works consider the
end-to-end planning task that yields direct control commands and rarely deploy
their algorithm to real vehicles. In this work, we propose a method to employ a
trained deep reinforcement learning policy for dedicated high-level behavior
planning. By populating an abstract objective interface, established motion
planning algorithms can be leveraged, whic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.02874v1' target='_blank'>Assessing Policy, Loss and Planning Combinations in Reinforcement
  Learning using a New Modular Architecture</a></h2>
<p><strong>Authors:</strong> Tiago Gaspar Oliveira, Arlindo L. Oliveira</p>
<p><strong>Summary:</strong> The model-based reinforcement learning paradigm, which uses planning
algorithms and neural network models, has recently achieved unprecedented
results in diverse applications, leading to what is now known as deep
reinforcement learning. These agents are quite complex and involve multiple
components, factors that can create challenges for research. In this work, we
propose a new modular software architecture suited for these types of agents,
and a set of building blocks that can be easily reused ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.12003v1' target='_blank'>Embracing advanced AI/ML to help investors achieve success: Vanguard
  Reinforcement Learning for Financial Goal Planning</a></h2>
<p><strong>Authors:</strong> Shareefuddin Mohammed, Rusty Bealer, Jason Cohen</p>
<p><strong>Summary:</strong> In the world of advice and financial planning, there is seldom one right
answer. While traditional algorithms have been successful in solving linear
problems, its success often depends on choosing the right features from a
dataset, which can be a challenge for nuanced financial planning scenarios.
Reinforcement learning is a machine learning approach that can be employed with
complex data sets where picking the right features can be nearly impossible. In
this paper, we will explore the use of ma...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1206.3281v1' target='_blank'>Model-Based Bayesian Reinforcement Learning in Large Structured Domains</a></h2>
<p><strong>Authors:</strong> Stephane Ross, Joelle Pineau</p>
<p><strong>Summary:</strong> Model-based Bayesian reinforcement learning has generated significant
interest in the AI community as it provides an elegant solution to the optimal
exploration-exploitation tradeoff in classical reinforcement learning.
Unfortunately, the applicability of this type of approach has been limited to
small domains due to the high complexity of reasoning about the joint posterior
over model parameters. In this paper, we consider the use of factored
representations combined with online planning techni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.08606v2' target='_blank'>Single-step Options for Adversary Driving</a></h2>
<p><strong>Authors:</strong> Nazmus Sakib, Hengshuai Yao, Hong Zhang, Shangling Jui</p>
<p><strong>Summary:</strong> In this paper, we use reinforcement learning for safety driving in adversary
settings. In our work, the knowledge in state-of-art planning methods is reused
by single-step options whose action suggestions are compared in parallel with
primitive actions. We show two advantages by doing so. First, training this
reinforcement learning agent is easier and faster than training the
primitive-action agent. Second, our new agent outperforms the primitive-action
reinforcement learning agent, human tester...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.08163v1' target='_blank'>Fast deep reinforcement learning using online adjustments from the past</a></h2>
<p><strong>Authors:</strong> Steven Hansen, Pablo Sprechmann, Alexander Pritzel, Andr√© Barreto, Charles Blundell</p>
<p><strong>Summary:</strong> We propose Ephemeral Value Adjusments (EVA): a means of allowing deep
reinforcement learning agents to rapidly adapt to experience in their replay
buffer. EVA shifts the value predicted by a neural network with an estimate of
the value function found by planning over experience tuples from the replay
buffer near the current state. EVA combines a number of recent ideas around
combining episodic memory-like structures into reinforcement learning agents:
slot-based storage, content-based retrieval,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.13489v2' target='_blank'>Boosting Reinforcement Learning and Planning with Demonstrations: A
  Survey</a></h2>
<p><strong>Authors:</strong> Tongzhou Mu, Hao Su</p>
<p><strong>Summary:</strong> Although reinforcement learning has seen tremendous success recently, this
kind of trial-and-error learning can be impractical or inefficient in complex
environments. The use of demonstrations, on the other hand, enables agents to
benefit from expert knowledge rather than having to discover the best action to
take through exploration. In this survey, we discuss the advantages of using
demonstrations in sequential decision making, various ways to apply
demonstrations in learning-based decision ma...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.02376v2' target='_blank'>A review of motion planning algorithms for intelligent robotics</a></h2>
<p><strong>Authors:</strong> Chengmin Zhou, Bingding Huang, Pasi Fr√§nti</p>
<p><strong>Summary:</strong> We investigate and analyze principles of typical motion planning algorithms.
These include traditional planning algorithms, supervised learning, optimal
value reinforcement learning, policy gradient reinforcement learning.
Traditional planning algorithms we investigated include graph search
algorithms, sampling-based algorithms, and interpolating curve algorithms.
Supervised learning algorithms include MSVM, LSTM, MCTS and CNN. Optimal value
reinforcement learning algorithms include Q learning, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.03616v2' target='_blank'>Arena-Rosnav: Towards Deployment of Deep-Reinforcement-Learning-Based
  Obstacle Avoidance into Conventional Autonomous Navigation Systems</a></h2>
<p><strong>Authors:</strong> Linh K√§stner, Teham Buiyan, Xinlin Zhao, Lei Jiao, Zhengcheng Shen, Jens Lambrecht</p>
<p><strong>Summary:</strong> Recently, mobile robots have become important tools in various industries,
especially in logistics. Deep reinforcement learning emerged as an alternative
planning method to replace overly conservative approaches and promises more
efficient and flexible navigation. However, deep reinforcement learning
approaches are not suitable for long-range navigation due to their proneness to
local minima and lack of long term memory, which hinders its widespread
integration into industrial applications of mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.00766v1' target='_blank'>Tracking the Race Between Deep Reinforcement Learning and Imitation
  Learning -- Extended Version</a></h2>
<p><strong>Authors:</strong> Timo P. Gros, Daniel H√∂ller, J√∂rg Hoffmann, Verena Wolf</p>
<p><strong>Summary:</strong> Learning-based approaches for solving large sequential decision making
problems have become popular in recent years. The resulting agents perform
differently and their characteristics depend on those of the underlying
learning approach. Here, we consider a benchmark planning problem from the
reinforcement learning domain, the Racetrack, to investigate the properties of
agents derived from different deep (reinforcement) learning approaches. We
compare the performance of deep supervised learning, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.02140v2' target='_blank'>Integration of Imitation Learning using GAIL and Reinforcement Learning
  using Task-achievement Rewards via Probabilistic Graphical Model</a></h2>
<p><strong>Authors:</strong> Akira Kinose, Tadahiro Taniguchi</p>
<p><strong>Summary:</strong> Integration of reinforcement learning and imitation learning is an important
problem that has been studied for a long time in the field of intelligent
robotics. Reinforcement learning optimizes policies to maximize the cumulative
reward, whereas imitation learning attempts to extract general knowledge about
the trajectories demonstrated by experts, i.e., demonstrators. Because each of
them has their own drawbacks, methods combining them and compensating for each
set of drawbacks have been explor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.10792v1' target='_blank'>Hierarchical Reinforcement Learning with Abductive Planning</a></h2>
<p><strong>Authors:</strong> Kazeto Yamamoto, Takashi Onishi, Yoshimasa Tsuruoka</p>
<p><strong>Summary:</strong> One of the key challenges in applying reinforcement learning to real-life
problems is that the amount of train-and-error required to learn a good policy
increases drastically as the task becomes complex. One potential solution to
this problem is to combine reinforcement learning with automated symbol
planning and utilize prior knowledge on the domain. However, existing methods
have limitations in their applicability and expressiveness. In this paper we
propose a hierarchical reinforcement learni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.13906v3' target='_blank'>Compositional Reinforcement Learning from Logical Specifications</a></h2>
<p><strong>Authors:</strong> Kishor Jothimurugan, Suguman Bansal, Osbert Bastani, Rajeev Alur</p>
<p><strong>Summary:</strong> We study the problem of learning control policies for complex tasks given by
logical specifications. Recent approaches automatically generate a reward
function from a given specification and use a suitable reinforcement learning
algorithm to learn a policy that maximizes the expected reward. These
approaches, however, scale poorly to complex tasks that require high-level
planning. In this work, we develop a compositional learning approach, called
DiRL, that interleaves high-level planning and re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.08312v1' target='_blank'>Calibrated Model-Based Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, Stefano Ermon</p>
<p><strong>Summary:</strong> Estimates of predictive uncertainty are important for accurate model-based
planning and reinforcement learning. However, predictive
uncertainties---especially ones derived from modern deep learning systems---can
be inaccurate and impose a bottleneck on performance. This paper explores which
uncertainties are needed for model-based reinforcement learning and argues that
good uncertainties must be calibrated, i.e. their probabilities should match
empirical frequencies of predicted events. We descr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.05479v1' target='_blank'>Spatially and Seamlessly Hierarchical Reinforcement Learning for State
  Space and Policy space in Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Jaehyun Kim, Jaeseung Jeong</p>
<p><strong>Summary:</strong> Despite advances in hierarchical reinforcement learning, its applications to
path planning in autonomous driving on highways are challenging. One reason is
that conventional hierarchical reinforcement learning approaches are not
amenable to autonomous driving due to its riskiness: the agent must move
avoiding multiple obstacles such as other agents that are highly unpredictable,
thus safe regions are small, scattered, and changeable over time. To overcome
this challenge, we propose a spatially h...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.14264v1' target='_blank'>SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game
  Dynamics</a></h2>
<p><strong>Authors:</strong> Fernando Martinez-Lopez, Juntao Chen, Yingdong Lu</p>
<p><strong>Summary:</strong> Deep reinforcement learning agents often face challenges to effectively
coordinate perception and decision-making components, particularly in
environments with high-dimensional sensory inputs where feature relevance
varies. This work introduces SPRIG (Stackelberg Perception-Reinforcement
learning with Internal Game dynamics), a framework that models the internal
perception-policy interaction within a single agent as a cooperative
Stackelberg game. In SPRIG, the perception module acts as a leader...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.10342v2' target='_blank'>A Reinforcement Learning based Path Planning Approach in 3D Environment</a></h2>
<p><strong>Authors:</strong> Geesara Kulathunga</p>
<p><strong>Summary:</strong> Optimal motion planning involves obstacles avoidance where path planning is
the key to success in optimal motion planning. Due to the computational
demands, most of the path planning algorithms can not be employed for real-time
based applications. Model-based reinforcement learning approaches for path
planning have received certain success in the recent past. Yet, most of such
approaches do not have deterministic output due to the randomness. We analyzed
several types of reinforcement learning-b...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.05530v1' target='_blank'>Model-based Reinforcement Learning with Multi-step Plan Value Estimation</a></h2>
<p><strong>Authors:</strong> Haoxin Lin, Yihao Sun, Jiaji Zhang, Yang Yu</p>
<p><strong>Summary:</strong> A promising way to improve the sample efficiency of reinforcement learning is
model-based methods, in which many explorations and evaluations can happen in
the learned models to save real-world samples. However, when the learned model
has a non-negligible model error, sequential steps in the model are hard to be
accurately evaluated, limiting the model's utilization. This paper proposes to
alleviate this issue by introducing multi-step plans to replace multi-step
actions for model-based RL. We e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01562v1' target='_blank'>A New View on Planning in Online Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kevin Roice, Parham Mohammad Panahi, Scott M. Jordan, Adam White, Martha White</p>
<p><strong>Summary:</strong> This paper investigates a new approach to model-based reinforcement learning
using background planning: mixing (approximate) dynamic programming updates and
model-free updates, similar to the Dyna architecture. Background planning with
learned models is often worse than model-free alternatives, such as Double DQN,
even though the former uses significantly more memory and computation. The
fundamental problem is that learned models can be inaccurate and often generate
invalid states, especially wh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.13911v1' target='_blank'>Predictive Control Using Learned State Space Models via Rolling Horizon
  Evolution</a></h2>
<p><strong>Authors:</strong> Alvaro Ovalle, Simon M. Lucas</p>
<p><strong>Summary:</strong> A large part of the interest in model-based reinforcement learning derives
from the potential utility to acquire a forward model capable of strategic long
term decision making. Assuming that an agent succeeds in learning a useful
predictive model, it still requires a mechanism to harness it to generate and
select among competing simulated plans. In this paper, we explore this theme
combining evolutionary algorithmic planning techniques with models learned via
deep learning and variational infere...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.05146v1' target='_blank'>Entropy Regularized Motion Planning via Stein Variational Inference</a></h2>
<p><strong>Authors:</strong> Alexander Lambert, Byron Boots</p>
<p><strong>Summary:</strong> Many Imitation and Reinforcement Learning approaches rely on the availability
of expert-generated demonstrations for learning policies or value functions
from data. Obtaining a reliable distribution of trajectories from motion
planners is non-trivial, since it must broadly cover the space of states likely
to be encountered during execution while also satisfying task-based
constraints. We propose a sampling strategy based on variational inference to
generate distributions of feasible, low-cost tr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.10119v2' target='_blank'>Minimal Value-Equivalent Partial Models for Scalable and Robust Planning
  in Lifelong Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Safa Alver, Doina Precup</p>
<p><strong>Summary:</strong> Learning models of the environment from pure interaction is often considered
an essential component of building lifelong reinforcement learning agents.
However, the common practice in model-based reinforcement learning is to learn
models that model every aspect of the agent's environment, regardless of
whether they are important in coming up with optimal decisions or not. In this
paper, we argue that such models are not particularly well-suited for
performing scalable and robust planning in life...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.06752v1' target='_blank'>Critic PI2: Master Continuous Planning via Policy Improvement with Path
  Integrals and Deep Actor-Critic Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jiajun Fan, He Ba, Xian Guo, Jianye Hao</p>
<p><strong>Summary:</strong> Constructing agents with planning capabilities has long been one of the main
challenges in the pursuit of artificial intelligence. Tree-based planning
methods from AlphaGo to Muzero have enjoyed huge success in discrete domains,
such as chess and Go. Unfortunately, in real-world applications like robot
control and inverted pendulum, whose action space is normally continuous, those
tree-based planning techniques will be struggling. To address those
limitations, in this paper, we present a novel m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.08226v1' target='_blank'>Can Euclidean Symmetry be Leveraged in Reinforcement Learning and
  Planning?</a></h2>
<p><strong>Authors:</strong> Linfeng Zhao, Owen Howell, Jung Yeon Park, Xupeng Zhu, Robin Walters, Lawson L. S. Wong</p>
<p><strong>Summary:</strong> In robotic tasks, changes in reference frames typically do not influence the
underlying physical properties of the system, which has been known as
invariance of physical laws.These changes, which preserve distance, encompass
isometric transformations such as translations, rotations, and reflections,
collectively known as the Euclidean group. In this work, we delve into the
design of improved learning algorithms for reinforcement learning and planning
tasks that possess Euclidean group symmetry. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.14063v1' target='_blank'>Bridging the gap between Learning-to-plan, Motion Primitives and Safe
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Piotr Kicki, Davide Tateo, Puze Liu, Jonas Guenster, Jan Peters, Krzysztof Walas</p>
<p><strong>Summary:</strong> Trajectory planning under kinodynamic constraints is fundamental for advanced
robotics applications that require dexterous, reactive, and rapid skills in
complex environments. These constraints, which may represent task, safety, or
actuator limitations, are essential for ensuring the proper functioning of
robotic platforms and preventing unexpected behaviors. Recent advances in
kinodynamic planning demonstrate that learning-to-plan techniques can generate
complex and reactive motions under intri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1807.05037v1' target='_blank'>Exploring Hierarchy-Aware Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Chris Cundy, Daniel Filan</p>
<p><strong>Summary:</strong> We introduce a new generative model for human planning under the Bayesian
Inverse Reinforcement Learning (BIRL) framework which takes into account the
fact that humans often plan using hierarchical strategies. We describe the
Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of
hierarchical planners, and use an illustrative toy model to show that BIHRL
retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to
accurately predict the goals of `Wikispeedia'...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.02078v3' target='_blank'>Multi-UAV Mobile Edge Computing and Path Planning Platform based on
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Huan Chang, Yicheng Chen, Baochang Zhang, David Doermann</p>
<p><strong>Summary:</strong> Unmanned Aerial vehicles (UAVs) are widely used as network processors in
mobile networks, but more recently, UAVs have been used in Mobile Edge
Computing as mobile servers. However, there are significant challenges to use
UAVs in complex environments with obstacles and cooperation between UAVs. We
introduce a new multi-UAV Mobile Edge Computing platform, which aims to provide
better Quality-of-Service and path planning based on reinforcement learning to
address these issues. The contributions of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.06415v1' target='_blank'>Deep reinforcement learning uncovers processes for separating azeotropic
  mixtures without prior knowledge</a></h2>
<p><strong>Authors:</strong> Quirin G√∂ttl, Jonathan Pirnay, Jakob Burger, Dominik G. Grimm</p>
<p><strong>Summary:</strong> Process synthesis in chemical engineering is a complex planning problem due
to vast search spaces, continuous parameters and the need for generalization.
Deep reinforcement learning agents, trained without prior knowledge, have shown
to outperform humans in various complex planning problems in recent years.
Existing work on reinforcement learning for flowsheet synthesis shows promising
concepts, but focuses on narrow problems in a single chemical system, limiting
its practicality. We present a g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.07789v1' target='_blank'>Safe Reinforcement Learning by Imagining the Near Future</a></h2>
<p><strong>Authors:</strong> Garrett Thomas, Yuping Luo, Tengyu Ma</p>
<p><strong>Summary:</strong> Safe reinforcement learning is a promising path toward applying reinforcement
learning algorithms to real-world problems, where suboptimal behaviors may lead
to actual negative consequences. In this work, we focus on the setting where
unsafe states can be avoided by planning ahead a short time into the future. In
this setting, a model-based agent with a sufficiently accurate model can avoid
unsafe states. We devise a model-based algorithm that heavily penalizes unsafe
trajectories, and derive gu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.02562v1' target='_blank'>Project proposal: A modular reinforcement learning based automated
  theorem prover</a></h2>
<p><strong>Authors:</strong> Boris Shminke</p>
<p><strong>Summary:</strong> We propose to build a reinforcement learning prover of independent
components: a deductive system (an environment), the proof state representation
(how an agent sees the environment), and an agent training algorithm. To that
purpose, we contribute an additional Vampire-based environment to
$\texttt{gym-saturation}$ package of OpenAI Gym environments for saturation
provers. We demonstrate a prototype of using $\texttt{gym-saturation}$ together
with a popular reinforcement learning framework (Ray ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.10369v1' target='_blank'>Reinforcement learning</a></h2>
<p><strong>Authors:</strong> Sarod Yatawatta</p>
<p><strong>Summary:</strong> Observing celestial objects and advancing our scientific knowledge about them
involves tedious planning, scheduling, data collection and data
post-processing. Many of these operational aspects of astronomy are guided and
executed by expert astronomers. Reinforcement learning is a mechanism where we
(as humans and astronomers) can teach agents of artificial intelligence to
perform some of these tedious tasks. In this paper, we will present a state of
the art overview of reinforcement learning and...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.09013v1' target='_blank'>Self-Inspection Method of Unmanned Aerial Vehicles in Power Plants Using
  Deep Q-Network Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haoran Guan</p>
<p><strong>Summary:</strong> For the purpose of inspecting power plants, autonomous robots can be built
using reinforcement learning techniques. The method replicates the environment
and employs a simple reinforcement learning (RL) algorithm. This strategy might
be applied in several sectors, including the electricity generation sector. A
pre-trained model with perception, planning, and action is suggested by the
research. To address optimization problems, such as the Unmanned Aerial Vehicle
(UAV) navigation problem, Deep Q...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.02305v1' target='_blank'>Generalized Planning With Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Or Rivlin, Tamir Hazan, Erez Karpas</p>
<p><strong>Summary:</strong> A hallmark of intelligence is the ability to deduce general principles from
examples, which are correct beyond the range of those observed. Generalized
Planning deals with finding such principles for a class of planning problems,
so that principles discovered using small instances of a domain can be used to
solve much larger instances of the same domain. In this work we study the use
of Deep Reinforcement Learning and Graph Neural Networks to learn such
generalized policies and demonstrate that ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.00340v1' target='_blank'>Deep Reinforcement Learning in Autonomous Car Path Planning and Control:
  A Survey</a></h2>
<p><strong>Authors:</strong> Yiyang Chen, Chao Ji, Yunrui Cai, Tong Yan, Bo Su</p>
<p><strong>Summary:</strong> Combining data-driven applications with control systems plays a key role in
recent Autonomous Car research. This thesis offers a structured review of the
latest literature on Deep Reinforcement Learning (DRL) within the realm of
autonomous vehicle Path Planning and Control. It collects a series of DRL
methodologies and algorithms and their applications in the field, focusing
notably on their roles in trajectory planning and dynamic control. In this
review, we delve into the application outcomes ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.05253v1' target='_blank'>Search on the Replay Buffer: Bridging Planning and Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine</p>
<p><strong>Summary:</strong> The history of learning for control has been an exciting back and forth
between two broad classes of algorithms: planning and reinforcement learning.
Planning algorithms effectively reason over long horizons, but assume access to
a local policy and distance metric over collision-free paths. Reinforcement
learning excels at learning policies and the relative values of states, but
fails to plan over long horizons. Despite the successes of each method in
various domains, tasks that require reasonin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1701.07274v6' target='_blank'>Deep Reinforcement Learning: An Overview</a></h2>
<p><strong>Authors:</strong> Yuxi Li</p>
<p><strong>Summary:</strong> We give an overview of recent exciting achievements of deep reinforcement
learning (RL). We discuss six core elements, six important mechanisms, and
twelve applications. We start with background of machine learning, deep
learning and reinforcement learning. Next we discuss core RL elements,
including value function, in particular, Deep Q-Network (DQN), policy, reward,
model, planning, and exploration. After that, we discuss important mechanisms
for RL, including attention and memory, unsupervise...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.03663v1' target='_blank'>Connecting Deep-Reinforcement-Learning-based Obstacle Avoidance with
  Conventional Global Planners using Waypoint Generators</a></h2>
<p><strong>Authors:</strong> Linh K√§stner, Teham Buiyan, Xinlin Zhao, Zhengcheng Shen, Cornelius Marx, Jens Lambrecht</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning has emerged as an efficient dynamic obstacle
avoidance method in highly dynamic environments. It has the potential to
replace overly conservative or inefficient navigation approaches. However, the
integration of Deep Reinforcement Learning into existing navigation systems is
still an open frontier due to the myopic nature of
Deep-Reinforcement-Learning-based navigation, which hinders its widespread
integration into current navigation systems. In this paper, we propose...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.03022v1' target='_blank'>Deceptive Reinforcement Learning for Privacy-Preserving Planning</a></h2>
<p><strong>Authors:</strong> Zhengshang Liu, Yue Yang, Tim Miller, Peta Masters</p>
<p><strong>Summary:</strong> In this paper, we study the problem of deceptive reinforcement learning to
preserve the privacy of a reward function. Reinforcement learning is the
problem of finding a behaviour policy based on rewards received from
exploratory behaviour. A key ingredient in reinforcement learning is a reward
function, which determines how much reward (negative or positive) is given and
when. However, in some situations, we may want to keep a reward function
private; that is, to make it difficult for an observe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02025v1' target='_blank'>Between Rate-Distortion Theory & Value Equivalence in Model-Based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dilip Arumugam, Benjamin Van Roy</p>
<p><strong>Summary:</strong> The quintessential model-based reinforcement-learning agent iteratively
refines its estimates or prior beliefs about the true underlying model of the
environment. Recent empirical successes in model-based reinforcement learning
with function approximation, however, eschew the true model in favor of a
surrogate that, while ignoring various facets of the environment, still
facilitates effective planning over behaviors. Recently formalized as the value
equivalence principle, this algorithmic techni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.04361v1' target='_blank'>State Dropout-Based Curriculum Reinforcement Learning for Self-Driving
  at Unsignalized Intersections</a></h2>
<p><strong>Authors:</strong> Shivesh Khaitan, John M. Dolan</p>
<p><strong>Summary:</strong> Traversing intersections is a challenging problem for autonomous vehicles,
especially when the intersections do not have traffic control. Recently deep
reinforcement learning has received massive attention due to its success in
dealing with autonomous driving tasks. In this work, we address the problem of
traversing unsignalized intersections using a novel curriculum for deep
reinforcement learning. The proposed curriculum leads to: 1) A faster training
process for the reinforcement learning age...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.14457v2' target='_blank'>How to ensure a safe control strategy? Towards a SRL for urban transit
  autonomous operation</a></h2>
<p><strong>Authors:</strong> Zicong Zhao</p>
<p><strong>Summary:</strong> Deep reinforcement learning has gradually shown its latent decision-making
ability in urban rail transit autonomous operation. However, since
reinforcement learning can not neither guarantee safety during learning nor
execution, this is still one of the major obstacles to the practical
application of reinforcement learning. Given this drawback, reinforcement
learning applied in the safety-critical autonomous operation domain remains
challenging without generating a safe control command sequence ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.17490v2' target='_blank'>BricksRL: A Platform for Democratizing Robotics and Reinforcement
  Learning Research and Education with LEGO</a></h2>
<p><strong>Authors:</strong> Sebastian Dittert, Vincent Moens, Gianni De Fabritiis</p>
<p><strong>Summary:</strong> We present BricksRL, a platform designed to democratize access to robotics
for reinforcement learning research and education. BricksRL facilitates the
creation, design, and training of custom LEGO robots in the real world by
interfacing them with the TorchRL library for reinforcement learning agents.
The integration of TorchRL with the LEGO hubs, via Bluetooth bidirectional
communication, enables state-of-the-art reinforcement learning training on GPUs
for a wide variety of LEGO builds. This off...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.05777v1' target='_blank'>Strategizing Equitable Transit Evacuations: A Data-Driven Reinforcement
  Learning Approach</a></h2>
<p><strong>Authors:</strong> Fang Tang, Han Wang, Maria Laura Delle Monache</p>
<p><strong>Summary:</strong> As natural disasters become increasingly frequent, the need for efficient and
equitable evacuation planning has become more critical. This paper proposes a
data-driven, reinforcement learning-based framework to optimize bus-based
evacuations with an emphasis on improving both efficiency and equity. We model
the evacuation problem as a Markov Decision Process solved by reinforcement
learning, using real-time transit data from General Transit Feed Specification
and transportation networks extracte...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.11240v1' target='_blank'>Truncated Horizon Policy Search: Combining Reinforcement Learning &
  Imitation Learning</a></h2>
<p><strong>Authors:</strong> Wen Sun, J. Andrew Bagnell, Byron Boots</p>
<p><strong>Summary:</strong> In this paper, we propose to combine imitation and reinforcement learning via
the idea of reward shaping using an oracle. We study the effectiveness of the
near-optimal cost-to-go oracle on the planning horizon and demonstrate that the
cost-to-go oracle shortens the learner's planning horizon as function of its
accuracy: a globally optimal oracle can shorten the planning horizon to one,
leading to a one-step greedy Markov Decision Process which is much easier to
optimize, while an oracle that is...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.11667v1' target='_blank'>PBCS : Efficient Exploration and Exploitation Using a Synergy between
  Reinforcement Learning and Motion Planning</a></h2>
<p><strong>Authors:</strong> Guillaume Matheron, Nicolas Perrin, Olivier Sigaud</p>
<p><strong>Summary:</strong> The exploration-exploitation trade-off is at the heart of reinforcement
learning (RL). However, most continuous control benchmarks used in recent RL
research only require local exploration. This led to the development of
algorithms that have basic exploration capabilities, and behave poorly in
benchmarks that require more versatile exploration. For instance, as
demonstrated in our empirical study, state-of-the-art RL algorithms such as
DDPG and TD3 are unable to steer a point mass in even small ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.08374v1' target='_blank'>Online Multimodal Transportation Planning using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Amirreza Farahani, Laura Genga, Remco Dijkman</p>
<p><strong>Summary:</strong> In this paper we propose a Deep Reinforcement Learning approach to solve a
multimodal transportation planning problem, in which containers must be
assigned to a truck or to trains that will transport them to their destination.
While traditional planning methods work "offline" (i.e., they take decisions
for a batch of containers before the transportation starts), the proposed
approach is "online", in that it can take decisions for individual containers,
while transportation is being executed. Pla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.09836v2' target='_blank'>Creativity of AI: Hierarchical Planning Model Learning for Facilitating
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hankz Hankui Zhuo, Shuting Deng, Mu Jin, Zhihao Ma, Kebing Jin, Chen Chen, Chao Yu</p>
<p><strong>Summary:</strong> Despite of achieving great success in real-world applications, Deep
Reinforcement Learning (DRL) is still suffering from three critical issues,
i.e., data efficiency, lack of the interpretability and transferability. Recent
research shows that embedding symbolic knowledge into DRL is promising in
addressing those challenges. Inspired by this, we introduce a novel deep
reinforcement learning framework with symbolic options. Our framework features
a loop training procedure, which enables guiding t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.17234v2' target='_blank'>Speeding Up Path Planning via Reinforcement Learning in MCTS for
  Automated Parking</a></h2>
<p><strong>Authors:</strong> Xinlong Zheng, Xiaozhou Zhang, Donghao Xu</p>
<p><strong>Summary:</strong> In this paper, we address a method that integrates reinforcement learning
into the Monte Carlo tree search to boost online path planning under fully
observable environments for automated parking tasks. Sampling-based planning
methods under high-dimensional space can be computationally expensive and
time-consuming. State evaluation methods are useful by leveraging the prior
knowledge into the search steps, making the process faster in a real-time
system. Given the fact that automated parking task...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.07404v1' target='_blank'>Think Too Fast Nor Too Slow: The Computational Trade-off Between
  Planning And Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Thomas M. Moerland, Anna Deichler, Simone Baldi, Joost Broekens, Catholijn M. Jonker</p>
<p><strong>Summary:</strong> Planning and reinforcement learning are two key approaches to sequential
decision making. Multi-step approximate real-time dynamic programming, a
recently successful algorithm class of which AlphaZero [Silver et al., 2018] is
an example, combines both by nesting planning within a learning loop. However,
the combination of planning and learning introduces a new question: how should
we balance time spend on planning, learning and acting? The importance of this
trade-off has not been explicitly stu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.01956v1' target='_blank'>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement
  Learning Agents</a></h2>
<p><strong>Authors:</strong> Shashank Sharma, Janina Hoffmann, Vinay Namboodiri</p>
<p><strong>Summary:</strong> In this paper, we address the challenge of long-horizon visual planning tasks
using Hierarchical Reinforcement Learning (HRL). Our key contribution is a
Discrete Hierarchical Planning (DHP) method, an alternative to traditional
distance-based approaches. We provide theoretical foundations for the method
and demonstrate its effectiveness through extensive empirical evaluations.
  Our agent recursively predicts subgoals in the context of a long-term goal
and receives discrete rewards for construct...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.03860v1' target='_blank'>Reinforcement and Deep Reinforcement Learning-based Solutions for
  Machine Maintenance Planning, Scheduling Policies, and Optimization</a></h2>
<p><strong>Authors:</strong> Oluwaseyi Ogunfowora, Homayoun Najjaran</p>
<p><strong>Summary:</strong> Systems and machines undergo various failure modes that result in machine
health degradation, so maintenance actions are required to restore them back to
a state where they can perform their expected functions. Since maintenance
tasks are inevitable, maintenance planning is essential to ensure the smooth
operations of the production system and other industries at large. Maintenance
planning is a decision-making problem that aims at developing optimum
maintenance policies and plans that help redu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.12263v1' target='_blank'>Plan-Guided Reinforcement Learning for Whole-Body Manipulation</a></h2>
<p><strong>Authors:</strong> Mengchao Zhang, Jose Barreiros, Aykut Ozgun Onol</p>
<p><strong>Summary:</strong> Synthesizing complex whole-body manipulation behaviors has fundamental
challenges due to the rapidly growing combinatorics inherent to contact
interaction planning. While model-based methods have shown promising results in
solving long-horizon manipulation tasks, they often work under strict
assumptions, such as known model parameters, oracular observation of the
environment state, and simplified dynamics, resulting in plans that cannot
easily transfer to hardware. Learning-based approaches, suc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.11576v1' target='_blank'>Automating proton PBS treatment planning for head and neck cancers using
  policy gradient-based deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Qingqing Wang, Chang Chang</p>
<p><strong>Summary:</strong> Proton pencil beam scanning (PBS) treatment planning for head and neck (H&N)
cancers is a time-consuming and experience-demanding task where a large number
of planning objectives are involved. Deep reinforcement learning (DRL) has
recently been introduced to the planning processes of intensity-modulated
radiation therapy and brachytherapy for prostate, lung, and cervical cancers.
However, existing approaches are built upon the Q-learning framework and
weighted linear combinations of clinical met...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.16772v2' target='_blank'>AI Agent as Urban Planner: Steering Stakeholder Dynamics in Urban
  Planning via Consensus-based Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kejiang Qian, Lingjun Mao, Xin Liang, Yimin Ding, Jin Gao, Xinran Wei, Ziyi Guo, Jiajie Li</p>
<p><strong>Summary:</strong> In urban planning, land use readjustment plays a pivotal role in aligning
land use configurations with the current demands for sustainable urban
development. However, present-day urban planning practices face two main
issues. Firstly, land use decisions are predominantly dependent on human
experts. Besides, while resident engagement in urban planning can promote urban
sustainability and livability, it is challenging to reconcile the diverse
interests of stakeholders. To address these challenges,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.08191v3' target='_blank'>Hierarchically Structured Reinforcement Learning for Topically Coherent
  Visual Story Generation</a></h2>
<p><strong>Authors:</strong> Qiuyuan Huang, Zhe Gan, Asli Celikyilmaz, Dapeng Wu, Jianfeng Wang, Xiaodong He</p>
<p><strong>Summary:</strong> We propose a hierarchically structured reinforcement learning approach to
address the challenges of planning for generating coherent multi-sentence
stories for the visual storytelling task. Within our framework, the task of
generating a story given a sequence of images is divided across a two-level
hierarchical decoder. The high-level decoder constructs a plan by generating a
semantic concept (i.e., topic) for each image in sequence. The low-level
decoder generates a sentence for each image usin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1908.04436v1' target='_blank'>Superstition in the Network: Deep Reinforcement Learning Plays Deceptive
  Games</a></h2>
<p><strong>Authors:</strong> Philip Bontrager, Ahmed Khalifa, Damien Anderson, Matthew Stephenson, Christoph Salge, Julian Togelius</p>
<p><strong>Summary:</strong> Deep reinforcement learning has learned to play many games well, but failed
on others. To better characterize the modes and reasons of failure of deep
reinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C)
algorithm on four deceptive games, which are specially designed to provide
challenges to game-playing agents. These games are implemented in the General
Video Game AI framework, which allows us to compare the behavior of
reinforcement learning-based agents with plannin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.12142v1' target='_blank'>Bridging Imagination and Reality for Model-Based Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Guangxiang Zhu, Minghao Zhang, Honglak Lee, Chongjie Zhang</p>
<p><strong>Summary:</strong> Sample efficiency has been one of the major challenges for deep reinforcement
learning. Recently, model-based reinforcement learning has been proposed to
address this challenge by performing planning on imaginary trajectories with a
learned world model. However, world model learning may suffer from overfitting
to training trajectories, and thus model-based value estimation and policy
search will be pone to be sucked in an inferior local policy. In this paper, we
propose a novel model-based reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.14325v1' target='_blank'>Improving Safety in Deep Reinforcement Learning using Unsupervised
  Action Planning</a></h2>
<p><strong>Authors:</strong> Hao-Lun Hsu, Qiuhua Huang, Sehoon Ha</p>
<p><strong>Summary:</strong> One of the key challenges to deep reinforcement learning (deep RL) is to
ensure safety at both training and testing phases. In this work, we propose a
novel technique of unsupervised action planning to improve the safety of
on-policy reinforcement learning algorithms, such as trust region policy
optimization (TRPO) or proximal policy optimization (PPO). We design our
safety-aware reinforcement learning by storing all the history of "recovery"
actions that rescue the agent from dangerous situatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.07729v2' target='_blank'>Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement
  Learning for Planned-Ahead Vision-and-Language Navigation</a></h2>
<p><strong>Authors:</strong> Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang</p>
<p><strong>Summary:</strong> Existing research studies on vision and language grounding for robot
navigation focus on improving model-free deep reinforcement learning (DRL)
models in synthetic environments. However, model-free DRL models do not
consider the dynamics in the real-world environments, and they often fail to
generalize to new scenes. In this paper, we take a radical approach to bridge
the gap between synthetic studies and real-world practices---We propose a
novel, planned-ahead hybrid reinforcement learning mode...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.11799v1' target='_blank'>Motion Planning by Reinforcement Learning for an Unmanned Aerial Vehicle
  in Virtual Open Space with Static Obstacles</a></h2>
<p><strong>Authors:</strong> Sanghyun Kim, Jongmin Park, Jae-Kwan Yun, Jiwon Seo</p>
<p><strong>Summary:</strong> In this study, we applied reinforcement learning based on the proximal policy
optimization algorithm to perform motion planning for an unmanned aerial
vehicle (UAV) in an open space with static obstacles. The application of
reinforcement learning through a real UAV has several limitations such as time
and cost; thus, we used the Gazebo simulator to train a virtual quadrotor UAV
in a virtual environment. As the reinforcement learning progressed, the mean
reward and goal rate of the model were inc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.08349v2' target='_blank'>Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic
  Approach</a></h2>
<p><strong>Authors:</strong> Rishi Hazra, Luc De Raedt</p>
<p><strong>Summary:</strong> Despite numerous successes in Deep Reinforcement Learning (DRL), the learned
policies are not interpretable. Moreover, since DRL does not exploit symbolic
relational representations, it has difficulties in coping with structural
changes in its environment (such as increasing the number of objects).
Relational Reinforcement Learning, on the other hand, inherits the relational
representations from symbolic planning to learn reusable policies. However, it
has so far been unable to scale up and expl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.16659v2' target='_blank'>Multi-Agent Reinforcement Learning-Based UAV Pathfinding for Obstacle
  Avoidance in Stochastic Environment</a></h2>
<p><strong>Authors:</strong> Qizhen Wu, Kexin Liu, Lei Chen, Jinhu L√º</p>
<p><strong>Summary:</strong> Traditional methods plan feasible paths for multiple agents in the stochastic
environment. However, the methods' iterations with the changes in the
environment result in computation complexities, especially for the
decentralized agents without a centralized planner. Although reinforcement
learning provides a plausible solution because of the generalization for
different environments, it struggles with enormous agent-environment
interactions in training. Here, we propose a novel centralized train...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.12244v3' target='_blank'>Provable Representation with Efficient Planning for Partial Observable
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hongming Zhang, Tongzheng Ren, Chenjun Xiao, Dale Schuurmans, Bo Dai</p>
<p><strong>Summary:</strong> In most real-world reinforcement learning applications, state information is
only partially observable, which breaks the Markov decision process assumption
and leads to inferior performance for algorithms that conflate observations
with state. Partially Observable Markov Decision Processes (POMDPs), on the
other hand, provide a general framework that allows for partial observability
to be accounted for in learning, exploration and planning, but presents
significant computational and statistical ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.12800v2' target='_blank'>Deep Reinforcement Learning for Time-Critical Wilderness Search And
  Rescue Using Drones</a></h2>
<p><strong>Authors:</strong> Jan-Hendrik Ewers, David Anderson, Douglas Thomson</p>
<p><strong>Summary:</strong> Traditional search and rescue methods in wilderness areas can be
time-consuming and have limited coverage. Drones offer a faster and more
flexible solution, but optimizing their search paths is crucial. This paper
explores the use of deep reinforcement learning to create efficient search
missions for drones in wilderness environments. Our approach leverages a priori
data about the search area and the missing person in the form of a probability
distribution map. This allows the deep reinforcement...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.02903v1' target='_blank'>Open Grounded Planning: Challenges and Benchmark Construction</a></h2>
<p><strong>Authors:</strong> Shiguang Guo, Ziliang Deng, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun</p>
<p><strong>Summary:</strong> The emergence of large language models (LLMs) has increasingly drawn
attention to the use of LLMs for human-like planning. Existing work on
LLM-based planning either focuses on leveraging the inherent language
generation capabilities of LLMs to produce free-style plans, or employs
reinforcement learning approaches to learn decision-making for a limited set of
actions within restricted environments. However, both approaches exhibit
significant discrepancies from the open and executable requiremen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.00527v2' target='_blank'>Is Long Horizon Reinforcement Learning More Difficult Than Short Horizon
  Reinforcement Learning?</a></h2>
<p><strong>Authors:</strong> Ruosong Wang, Simon S. Du, Lin F. Yang, Sham M. Kakade</p>
<p><strong>Summary:</strong> Learning to plan for long horizons is a central challenge in episodic
reinforcement learning problems. A fundamental question is to understand how
the difficulty of the problem scales as the horizon increases. Here the natural
measure of sample complexity is a normalized one: we are interested in the
number of episodes it takes to provably discover a policy whose value is
$\varepsilon$ near to that of the optimal value, where the value is measured by
the normalized cumulative reward in each epis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.06250v1' target='_blank'>Robust and Decentralized Reinforcement Learning for UAV Path Planning in
  IoT Networks</a></h2>
<p><strong>Authors:</strong> Xueyuan Wang, M. Cenk Gursoy</p>
<p><strong>Summary:</strong> Unmanned aerial vehicle (UAV)-based networks and Internet of Things (IoT) are
being considered as integral components of current and next-generation wireless
networks.
  In particular, UAVs can provide IoT devices with seamless connectivity and
high coverage and this can be accomplished with effective UAV path planning.
  In this article, we study robust and decentralized UAV path planning for data
collection in IoT networks in the presence of other noncooperative UAVs and
adversarial jamming at...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.08910v1' target='_blank'>Meta-operators for Enabling Parallel Planning Using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> √Ångel Aso-Mollar, Eva Onaindia</p>
<p><strong>Summary:</strong> There is a growing interest in the application of Reinforcement Learning (RL)
techniques to AI planning with the aim to come up with general policies.
Typically, the mapping of the transition model of AI planning to the state
transition system of a Markov Decision Process is established by assuming a
one-to-one correspondence of the respective action spaces. In this paper, we
introduce the concept of meta-operator as the result of simultaneously applying
multiple planning operators, and we show ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.00090v4' target='_blank'>SDRL: Interpretable and Data-efficient Deep Reinforcement Learning
  Leveraging Symbolic Planning</a></h2>
<p><strong>Authors:</strong> Daoming Lyu, Fangkai Yang, Bo Liu, Steven Gustafson</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) has gained great success by learning
directly from high-dimensional sensory inputs, yet is notorious for the lack of
interpretability. Interpretability of the subtasks is critical in hierarchical
decision-making as it increases the transparency of black-box-style DRL
approach and helps the RL practitioners to understand the high-level behavior
of the system better. In this paper, we introduce symbolic planning into DRL
and propose a framework of Symbolic Deep Re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.03127v1' target='_blank'>Experience-Based Heuristic Search: Robust Motion Planning with Deep
  Q-Learning</a></h2>
<p><strong>Authors:</strong> Julian Bernhard, Robert Gieselmann, Klemens Esterle, Alois Knoll</p>
<p><strong>Summary:</strong> Interaction-aware planning for autonomous driving requires an exploration of
a combinatorial solution space when using conventional search- or
optimization-based motion planners. With Deep Reinforcement Learning, optimal
driving strategies for such problems can be derived also for higher-dimensional
problems. However, these methods guarantee optimality of the resulting policy
only in a statistical sense, which impedes their usage in safety critical
systems, such as autonomous vehicles. Thus, we ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.09996v1' target='_blank'>Robust Route Planning with Distributional Reinforcement Learning in a
  Stochastic Road Network Environment</a></h2>
<p><strong>Authors:</strong> Xi Lin, Paul Szenher, John D. Martin, Brendan Englot</p>
<p><strong>Summary:</strong> Route planning is essential to mobile robot navigation problems. In recent
years, deep reinforcement learning (DRL) has been applied to learning optimal
planning policies in stochastic environments without prior knowledge. However,
existing works focus on learning policies that maximize the expected return,
the performance of which can vary greatly when the level of stochasticity in
the environment is high. In this work, we propose a distributional
reinforcement learning based framework that lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.01708v2' target='_blank'>Distributional Model Equivalence for Risk-Sensitive Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Tyler Kastner, Murat A. Erdogdu, Amir-massoud Farahmand</p>
<p><strong>Summary:</strong> We consider the problem of learning models for risk-sensitive reinforcement
learning. We theoretically demonstrate that proper value equivalence, a method
of learning models which can be used to plan optimally in the risk-neutral
setting, is not sufficient to plan optimally in the risk-sensitive setting. We
leverage distributional reinforcement learning to introduce two new notions of
model equivalence, one which is general and can be used to plan for any risk
measure, but is intractable; and a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.00063v1' target='_blank'>Safe multi-agent motion planning under uncertainty for drones using
  filtered reinforcement learning</a></h2>
<p><strong>Authors:</strong> Sleiman Safaoui, Abraham P. Vinod, Ankush Chakrabarty, Rien Quirynen, Nobuyuki Yoshikawa, Stefano Di Cairano</p>
<p><strong>Summary:</strong> We consider the problem of safe multi-agent motion planning for drones in
uncertain, cluttered workspaces. For this problem, we present a tractable
motion planner that builds upon the strengths of reinforcement learning and
constrained-control-based trajectory planning. First, we use single-agent
reinforcement learning to learn motion plans from data that reach the target
but may not be collision-free. Next, we use a convex optimization, chance
constraints, and set-based methods for constrained ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.01465v1' target='_blank'>A Reinforcement Learning-Boosted Motion Planning Framework:
  Comprehensive Generalization Performance in Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Rainer Trauth, Alexander Hobmeier, Johannes Betz</p>
<p><strong>Summary:</strong> This study introduces a novel approach to autonomous motion planning,
informing an analytical algorithm with a reinforcement learning (RL) agent
within a Frenet coordinate system. The combination directly addresses the
challenges of adaptability and safety in autonomous driving. Motion planning
algorithms are essential for navigating dynamic and complex scenarios.
Traditional methods, however, lack the flexibility required for unpredictable
environments, whereas machine learning techniques, part...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.16882v1' target='_blank'>Revisiting Space Mission Planning: A Reinforcement Learning-Guided
  Approach for Multi-Debris Rendezvous</a></h2>
<p><strong>Authors:</strong> Agni Bandyopadhyay, Guenther Waxenegger-Wilfing</p>
<p><strong>Summary:</strong> This research introduces a novel application of a masked Proximal Policy
Optimization (PPO) algorithm from the field of deep reinforcement learning
(RL), for determining the most efficient sequence of space debris visitation,
utilizing the Lambert solver as per Izzo's adaptation for individual
rendezvous. The aim is to optimize the sequence in which all the given debris
should be visited to get the least total time for rendezvous for the entire
mission. A neural network (NN) policy is developed,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.08574v1' target='_blank'>Learning Sketch Decompositions in Planning via Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Michael Aichm√ºller, Hector Geffner</p>
<p><strong>Summary:</strong> In planning and reinforcement learning, the identification of common subgoal
structures across problems is important when goals are to be achieved over long
horizons. Recently, it has been shown that such structures can be expressed as
feature-based rules, called sketches, over a number of classical planning
domains. These sketches split problems into subproblems which then become
solvable in low polynomial time by a greedy sequence of IW$(k)$ searches.
Methods for learning sketches using featur...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.16712v4' target='_blank'>Model-based Reinforcement Learning: A Survey</a></h2>
<p><strong>Authors:</strong> Thomas M. Moerland, Joost Broekens, Aske Plaat, Catholijn M. Jonker</p>
<p><strong>Summary:</strong> Sequential decision making, commonly formalized as Markov Decision Process
(MDP) optimization, is a important challenge in artificial intelligence. Two
key approaches to this problem are reinforcement learning (RL) and planning.
This paper presents a survey of the integration of both fields, better known as
model-based reinforcement learning. Model-based RL has two main steps. First,
we systematically cover approaches to dynamics model learning, including
challenges like dealing with stochastici...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.04021v2' target='_blank'>On the role of planning in model-based deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Jessica B. Hamrick, Abram L. Friesen, Feryal Behbahani, Arthur Guez, Fabio Viola, Sims Witherspoon, Thomas Anthony, Lars Buesing, Petar Veliƒçkoviƒá, Th√©ophane Weber</p>
<p><strong>Summary:</strong> Model-based planning is often thought to be necessary for deep, careful
reasoning and generalization in artificial agents. While recent successes of
model-based reinforcement learning (MBRL) with deep function approximation have
strengthened this hypothesis, the resulting diversity of model-based methods
has also made it difficult to track which components drive success and why. In
this paper, we seek to disentangle the contributions of recent methods by
focusing on three questions: (1) How does...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.08955v1' target='_blank'>Integrating Task-Motion Planning with Reinforcement Learning for Robust
  Decision Making in Mobile Robots</a></h2>
<p><strong>Authors:</strong> Yuqian Jiang, Fangkai Yang, Shiqi Zhang, Peter Stone</p>
<p><strong>Summary:</strong> Task-motion planning (TMP) addresses the problem of efficiently generating
executable and low-cost task plans in a discrete space such that the (initially
unknown) action costs are determined by motion plans in a corresponding
continuous space. However, a task-motion plan can be sensitive to unexpected
domain uncertainty and changes, leading to suboptimal behaviors or execution
failures. In this paper, we propose a novel framework, TMP-RL, which is an
integration of TMP and reinforcement learnin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.03359v1' target='_blank'>Deep Interactive Reinforcement Learning for Path Following of Autonomous
  Underwater Vehicle</a></h2>
<p><strong>Authors:</strong> Qilei Zhang, Jinying Lin, Qixin Sha, Bo He, Guangliang Li</p>
<p><strong>Summary:</strong> Autonomous underwater vehicle (AUV) plays an increasingly important role in
ocean exploration. Existing AUVs are usually not fully autonomous and generally
limited to pre-planning or pre-programming tasks. Reinforcement learning (RL)
and deep reinforcement learning have been introduced into the AUV design and
research to improve its autonomy. However, these methods are still difficult to
apply directly to the actual AUV system because of the sparse rewards and low
learning efficiency. In this pa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.14383v1' target='_blank'>MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based
  Inter-Robot Negotiation</a></h2>
<p><strong>Authors:</strong> Toby Godfrey, William Hunt, Mohammad D. Soorati</p>
<p><strong>Summary:</strong> Multi-agent reinforcement learning is a key method for training multi-robot
systems over a series of episodes in which robots are rewarded or punished
according to their performance; only once the system is trained to a suitable
standard is it deployed in the real world. If the system is not trained enough,
the task will likely not be completed and could pose a risk to the surrounding
environment. Therefore, reaching high performance in a shorter training period
can lead to significant reduction...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.13181v2' target='_blank'>Planning and Learning: Path-Planning for Autonomous Vehicles, a Review
  of the Literature</a></h2>
<p><strong>Authors:</strong> Kevin Osanlou, Christophe Guettier, Tristan Cazenave, Eric Jacopin</p>
<p><strong>Summary:</strong> This short review aims to make the reader familiar with state-of-the-art
works relating to planning, scheduling and learning. First, we study
state-of-the-art planning algorithms. We give a brief introduction of neural
networks. Then we explore in more detail graph neural networks, a recent
variant of neural networks suited for processing graph-structured inputs. We
describe briefly the concept of reinforcement learning algorithms and some
approaches designed to date. Next, we study some success...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1807.03515v1' target='_blank'>A Reinforcement Learning Approach to Jointly Adapt Vehicular
  Communications and Planning for Optimized Driving</a></h2>
<p><strong>Authors:</strong> Mayank K. Pal, Rupali Bhati, Anil Sharma, Sanjit K. Kaul, Saket Anand, P. B. Sujit</p>
<p><strong>Summary:</strong> Our premise is that autonomous vehicles must optimize communications and
motion planning jointly. Specifically, a vehicle must adapt its motion plan
staying cognizant of communications rate related constraints and adapt the use
of communications while being cognizant of motion planning related restrictions
that may be imposed by the on-road environment. To this end, we formulate a
reinforcement learning problem wherein an autonomous vehicle jointly chooses
(a) a motion planning action that execu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.01771v1' target='_blank'>Deep Reinforcement Learning Based Dynamic Route Planning for Minimizing
  Travel Time</a></h2>
<p><strong>Authors:</strong> Yuanzhe Geng, Erwu Liu, Rui Wang, Yiming Liu</p>
<p><strong>Summary:</strong> Route planning is important in transportation. Existing works focus on
finding the shortest path solution or using metrics such as safety and energy
consumption to determine the planning. It is noted that most of these studies
rely on prior knowledge of road network, which may be not available in certain
situations. In this paper, we design a route planning algorithm based on deep
reinforcement learning (DRL) for pedestrians. We use travel time consumption as
the metric, and plan the route by pr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.12802v3' target='_blank'>Planning Multiple Epidemic Interventions with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Anh Mai, Nikunj Gupta, Azza Abouzied, Dennis Shasha</p>
<p><strong>Summary:</strong> Combating an epidemic entails finding a plan that describes when and how to
apply different interventions, such as mask-wearing mandates, vaccinations,
school or workplace closures. An optimal plan will curb an epidemic with
minimal loss of life, disease burden, and economic cost. Finding an optimal
plan is an intractable computational problem in realistic settings.
Policy-makers, however, would greatly benefit from tools that can efficiently
search for plans that minimize disease and economic c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.04145v1' target='_blank'>Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep
  Reinforcement Learning Method for Global Path Planning</a></h2>
<p><strong>Authors:</strong> Guoming Huang, Mingxin Hou, Xiaofang Yuan, Shuqiao Huang, Yaonan Wang</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) methods have recently shown promise in path
planning tasks. However, when dealing with global planning tasks, these methods
face serious challenges such as poor convergence and generalization. To this
end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan
Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems
from the perspective of DRL's observation, revealing that the traditional
design causes DRL to be interfered ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.09513v1' target='_blank'>Planning Transformer: Long-Horizon Offline Reinforcement Learning with
  Planning Tokens</a></h2>
<p><strong>Authors:</strong> Joseph Clinton, Robert Lieck</p>
<p><strong>Summary:</strong> Supervised learning approaches to offline reinforcement learning,
particularly those utilizing the Decision Transformer, have shown effectiveness
in continuous environments and for sparse rewards. However, they often struggle
with long-horizon tasks due to the high compounding error of auto-regressive
models. To overcome this limitation, we go beyond next-token prediction and
introduce Planning Tokens, which contain high-level, long time-scale
information about the agent's future. Predicting dua...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.22459v1' target='_blank'>Predicting Future Actions of Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Stephen Chung, Scott Niekum, David Krueger</p>
<p><strong>Summary:</strong> As reinforcement learning agents become increasingly deployed in real-world
scenarios, predicting future agent actions and events during deployment is
important for facilitating better human-agent interaction and preventing
catastrophic outcomes. This paper experimentally evaluates and compares the
effectiveness of future action and event prediction for three types of RL
agents: explicitly planning, implicitly planning, and non-planning. We employ
two approaches: the inner state approach, which ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.01727v1' target='_blank'>Proposing Hierarchical Goal-Conditioned Policy Planning in Multi-Goal
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Gavin B. Rens</p>
<p><strong>Summary:</strong> Humanoid robots must master numerous tasks with sparse rewards, posing a
challenge for reinforcement learning (RL). We propose a method combining RL and
automated planning to address this. Our approach uses short goal-conditioned
policies (GCPs) organized hierarchically, with Monte Carlo Tree Search (MCTS)
planning using high-level actions (HLAs). Instead of primitive actions, the
planning process generates HLAs. A single plan-tree, maintained during the
agent's lifetime, holds knowledge about g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.04410v2' target='_blank'>Optimizing Query Evaluations using Reinforcement Learning for Web Search</a></h2>
<p><strong>Authors:</strong> Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, Saurabh Tiwary</p>
<p><strong>Summary:</strong> In web search, typically a candidate generation step selects a small set of
documents---from collections containing as many as billions of web pages---that
are subsequently ranked and pruned before being presented to the user. In Bing,
the candidate generation involves scanning the index using statically designed
match plans that prescribe sequences of different match criteria and stopping
conditions. In this work, we pose match planning as a reinforcement learning
task and observe up to 20% red...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.03240v3' target='_blank'>Multi-task Reinforcement Learning with a Planning Quasi-Metric</a></h2>
<p><strong>Authors:</strong> Vincent Micheli, Karthigan Sinnathamby, Fran√ßois Fleuret</p>
<p><strong>Summary:</strong> We introduce a new reinforcement learning approach combining a planning
quasi-metric (PQM) that estimates the number of steps required to go from any
state to another, with task-specific "aimers" that compute a target state to
reach a given goal. This decomposition allows the sharing across tasks of a
task-agnostic model of the quasi-metric that captures the environment's
dynamics and can be learned in a dense and unsupervised manner. We achieve
multiple-fold training speed-up compared to recent...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.10628v2' target='_blank'>Independent Control and Path Planning of Microswimmers with a Uniform
  Magnetic Field</a></h2>
<p><strong>Authors:</strong> Lucas Amoudruz, Petros Koumoutsakos</p>
<p><strong>Summary:</strong> Artificial bacteria flagella (ABFs) are magnetic helical micro-swimmers that
can be remotely controlled via a uniform, rotating magnetic field. Previous
studies have used the heterogeneous response of microswimmers to external
magnetic fields for achieving independent control. Here we introduce analytical
and reinforcement learning control strategies for path planning to a target by
multiple swimmers using a uniform magnetic field. The comparison of the two
algorithms shows the superiority of re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.09986v1' target='_blank'>Indoor Path Planning for an Unmanned Aerial Vehicle via Curriculum
  Learning</a></h2>
<p><strong>Authors:</strong> Jongmin Park, Sooyoung Jang, Younghoon Shin</p>
<p><strong>Summary:</strong> In this study, reinforcement learning was applied to learning two-dimensional
path planning including obstacle avoidance by unmanned aerial vehicle (UAV) in
an indoor environment. The task assigned to the UAV was to reach the goal
position in the shortest amount of time without colliding with any obstacles.
Reinforcement learning was performed in a virtual environment created using
Gazebo, a virtual environment simulator, to reduce the learning time and cost.
Curriculum learning, which consists ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.13630v1' target='_blank'>Offline Skill Graph (OSG): A Framework for Learning and Planning using
  Offline Reinforcement Learning Skills</a></h2>
<p><strong>Authors:</strong> Ben-ya Halevy, Yehudit Aperstein, Dotan Di Castro</p>
<p><strong>Summary:</strong> Reinforcement Learning has received wide interest due to its success in
competitive games. Yet, its adoption in everyday applications is limited (e.g.
industrial, home, healthcare, etc.). In this paper, we address this limitation
by presenting a framework for planning over offline skills and solving complex
tasks in real-world environments. Our framework is comprised of three modules
that together enable the agent to learn from previously collected data and
generalize over it to solve long-horiz...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.00313v1' target='_blank'>Control in Stochastic Environment with Delays: A Model-based
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Zhiyuan Yao, Ionut Florescu, Chihoon Lee</p>
<p><strong>Summary:</strong> In this paper we are introducing a new reinforcement learning method for
control problems in environments with delayed feedback. Specifically, our
method employs stochastic planning, versus previous methods that used
deterministic planning. This allows us to embed risk preference in the policy
optimization problem. We show that this formulation can recover the optimal
policy for problems with deterministic transitions. We contrast our policy with
two prior methods from literature. We apply the m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.03037v3' target='_blank'>Model-based Reinforcement Learning for Parameterized Action Spaces</a></h2>
<p><strong>Authors:</strong> Renhao Zhang, Haotian Fu, Yilin Miao, George Konidaris</p>
<p><strong>Summary:</strong> We propose a novel model-based reinforcement learning algorithm -- Dynamics
Learning and predictive control with Parameterized Actions (DLPA) -- for
Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a
parameterized-action-conditioned dynamics model and plans with a modified Model
Predictive Path Integral control. We theoretically quantify the difference
between the generated trajectory and the optimal trajectory during planning in
terms of the value they achieved through ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.00757v1' target='_blank'>Collaborative motion planning for multi-manipulator systems through
  Reinforcement Learning and Dynamic Movement Primitives</a></h2>
<p><strong>Authors:</strong> Siddharth Singh, Tian Xu, Qing Chang</p>
<p><strong>Summary:</strong> Robotic tasks often require multiple manipulators to enhance task efficiency
and speed, but this increases complexity in terms of collaboration, collision
avoidance, and the expanded state-action space. To address these challenges, we
propose a multi-level approach combining Reinforcement Learning (RL) and
Dynamic Movement Primitives (DMP) to generate adaptive, real-time trajectories
for new tasks in dynamic environments using a demonstration library. This
method ensures collision-free trajector...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.13033v1' target='_blank'>Robust Hierarchical Planning with Policy Delegation</a></h2>
<p><strong>Authors:</strong> Tin Lai, Philippe Morere</p>
<p><strong>Summary:</strong> We propose a novel framework and algorithm for hierarchical planning based on
the principle of delegation. This framework, the Markov Intent Process,
features a collection of skills which are each specialised to perform a single
task well. Skills are aware of their intended effects and are able to analyse
planning goals to delegate planning to the best-suited skill. This principle
dynamically creates a hierarchy of plans, in which each skill plans for
sub-goals for which it is specialised. The p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.09772v1' target='_blank'>Robotic Test Tube Rearrangement Using Combined Reinforcement Learning
  and Motion Planning</a></h2>
<p><strong>Authors:</strong> Hao Chen, Weiwei Wan, Masaki Matsushita, Takeyuki Kotaka, Kensuke Harada</p>
<p><strong>Summary:</strong> A combined task-level reinforcement learning and motion planning framework is
proposed in this paper to address a multi-class in-rack test tube rearrangement
problem. At the task level, the framework uses reinforcement learning to infer
a sequence of swap actions while ignoring robotic motion details. At the motion
level, the framework accepts the swapping action sequences inferred by
task-level agents and plans the detailed robotic pick-and-place motion. The
task and motion-level planning form ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.20579v2' target='_blank'>HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for
  Diverse Parking Scenarios</a></h2>
<p><strong>Authors:</strong> Mingyang Jiang, Yueyuan Li, Songan Zhang, Siyuan Chen, Chunxiang Wang, Ming Yang</p>
<p><strong>Summary:</strong> Automated parking stands as a highly anticipated application of autonomous
driving technology. However, existing path planning methodologies fall short of
addressing this need due to their incapability to handle the diverse and
complex parking scenarios in reality. While non-learning methods provide
reliable planning results, they are vulnerable to intricate occasions, whereas
learning-based ones are good at exploration but unstable in converging to
feasible solutions. To leverage the strengths ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.18065v1' target='_blank'>SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for
  Long-Horizon Manipulation</a></h2>
<p><strong>Authors:</strong> Zihan Zhou, Animesh Garg, Dieter Fox, Caelan Garrett, Ajay Mandlekar</p>
<p><strong>Summary:</strong> Robot learning has proven to be a general and effective technique for
programming manipulators. Imitation learning is able to teach robots solely
from human demonstrations but is bottlenecked by the capabilities of the
demonstrations. Reinforcement learning uses exploration to discover better
behaviors; however, the space of possible improvements can be too large to
start from scratch. And for both techniques, the learning difficulty increases
proportional to the length of the manipulation task....</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.10102v1' target='_blank'>Intelligent Inverse Treatment Planning via Deep Reinforcement Learning,
  a Proof-of-Principle Study in High Dose-rate Brachytherapy for Cervical
  Cancer</a></h2>
<p><strong>Authors:</strong> Chenyang Shen, Yesenia Gonzalez, Peter Klages, Nan Qin, Hyunuk Jung, Liyuan Chen, Dan Nguyen, Steve B. Jiang, Xun Jia</p>
<p><strong>Summary:</strong> Inverse treatment planning in radiation therapy is formulated as optimization
problems. The objective function and constraints consist of multiple terms
designed for different clinical and practical considerations. Weighting factors
of these terms are needed to define the optimization problem. While a treatment
planning system can solve the optimization problem with given weights,
adjusting the weights for high plan quality is performed by human. The weight
tuning task is labor intensive, time c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/cs/0212025v1' target='_blank'>Searching for Plannable Domains can Speed up Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Istvan Szita, Balint Takacs, Andras Lorincz</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) involves sequential decision making in uncertain
environments. The aim of the decision-making agent is to maximize the benefit
of acting in its environment over an extended period of time. Finding an
optimal policy in RL may be very slow. To speed up learning, one often used
solution is the integration of planning, for example, Sutton's Dyna algorithm,
or various other methods using macro-actions.
  Here we suggest to separate plannable, i.e., close to deterministic p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.00175v2' target='_blank'>Fast Exploration with Simplified Models and Approximately Optimistic
  Planning in Model Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ramtin Keramati, Jay Whang, Patrick Cho, Emma Brunskill</p>
<p><strong>Summary:</strong> Humans learn to play video games significantly faster than the
state-of-the-art reinforcement learning (RL) algorithms. People seem to build
simple models that are easy to learn to support planning and strategic
exploration. Inspired by this, we investigate two issues in leveraging
model-based RL for sample efficiency. First we investigate how to perform
strategic exploration when exact planning is not feasible and empirically show
that optimistic Monte Carlo Tree Search outperforms posterior sa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.12651v1' target='_blank'>PEARL: PrEference Appraisal Reinforcement Learning for Motion Planning</a></h2>
<p><strong>Authors:</strong> Aleksandra Faust, Hao-Tien Lewis Chiang, Lydia Tapia</p>
<p><strong>Summary:</strong> Robot motion planning often requires finding trajectories that balance
different user intents, or preferences. One of these preferences is usually
arrival at the goal, while another might be obstacle avoidance. Here, we
formalize these, and similar, tasks as preference balancing tasks (PBTs) on
acceleration controlled robots, and propose a motion planning solution,
PrEference Appraisal Reinforcement Learning (PEARL). PEARL uses reinforcement
learning on a restricted training domain, combined wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.01599v2' target='_blank'>Learning Dynamics Model in Reinforcement Learning by Incorporating the
  Long Term Future</a></h2>
<p><strong>Authors:</strong> Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh, Dhruv Batra</p>
<p><strong>Summary:</strong> In model-based reinforcement learning, the agent interleaves between model
learning and planning. These two components are inextricably intertwined. If
the model is not able to provide sensible long-term prediction, the executed
planner would exploit model flaws, which can yield catastrophic failures. This
paper focuses on building a model that reasons about the long-term future and
demonstrates how to use this for efficient planning and exploration. To this
end, we build a latent-variable autor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.07630v1' target='_blank'>Value-Added Chemical Discovery Using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Peihong Jiang, Hieu Doan, Sandeep Madireddy, Rajeev Surendran Assary, Prasanna Balaprakash</p>
<p><strong>Summary:</strong> Computer-assisted synthesis planning aims to help chemists find better
reaction pathways faster. Finding viable and short pathways from sugar
molecules to value-added chemicals can be modeled as a retrosynthesis planning
problem with a catalyst allowed. This is a crucial step in efficient biomass
conversion. The traditional computational chemistry approach to identifying
possible reaction pathways involves computing the reaction energies of hundreds
of intermediates, which is a critical bottlene...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.07890v1' target='_blank'>Informative Path Planning for Mobile Sensing with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yongyong Wei, Rong Zheng</p>
<p><strong>Summary:</strong> Large-scale spatial data such as air quality, thermal conditions and location
signatures play a vital role in a variety of applications. Collecting such data
manually can be tedious and labour intensive. With the advancement of robotic
technologies, it is feasible to automate such tasks using mobile robots with
sensing and navigation capabilities. However, due to limited battery lifetime
and scarcity of charging stations, it is important to plan paths for the robots
that maximize the utility of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.15638v2' target='_blank'>Abstract Value Iteration for Hierarchical Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kishor Jothimurugan, Osbert Bastani, Rajeev Alur</p>
<p><strong>Summary:</strong> We propose a novel hierarchical reinforcement learning framework for control
with continuous state and action spaces. In our framework, the user specifies
subgoal regions which are subsets of states; then, we (i) learn options that
serve as transitions between these subgoal regions, and (ii) construct a
high-level plan in the resulting abstract decision process (ADP). A key
challenge is that the ADP may not be Markov, which we address by proposing two
algorithms for planning in the ADP. Our firs...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00650v1' target='_blank'>Multi-lane Cruising Using Hierarchical Planning and Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Kasra Rezaee, Peyman Yadmellat, Masoud S. Nosrati, Elmira Amirloo Abolfathi, Mohammed Elmahgiubi, Jun Luo</p>
<p><strong>Summary:</strong> Competent multi-lane cruising requires using lane changes and within-lane
maneuvers to achieve good speed and maintain safety. This paper proposes a
design for autonomous multi-lane cruising by combining a hierarchical
reinforcement learning framework with a novel state-action space abstraction.
While the proposed solution follows the classical hierarchy of behavior
decision, motion planning and control, it introduces a key intermediate
abstraction within the motion planner to discretize the sta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.09018v1' target='_blank'>Reinforcement Learning-Based Coverage Path Planning with Implicit
  Cellular Decomposition</a></h2>
<p><strong>Authors:</strong> Javad Heydari, Olimpiya Saha, Viswanath Ganapathy</p>
<p><strong>Summary:</strong> Coverage path planning in a generic known environment is shown to be NP-hard.
When the environment is unknown, it becomes more challenging as the robot is
required to rely on its online map information built during coverage for
planning its path. A significant research effort focuses on designing heuristic
or approximate algorithms that achieve reasonable performance. Such algorithms
have sub-optimal performance in terms of covering the area or the cost of
coverage, e.g., coverage time or energy...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.04972v1' target='_blank'>Risk Sensitive Model-Based Reinforcement Learning using Uncertainty
  Guided Planning</a></h2>
<p><strong>Authors:</strong> Stefan Radic Webster, Peter Flach</p>
<p><strong>Summary:</strong> Identifying uncertainty and taking mitigating actions is crucial for safe and
trustworthy reinforcement learning agents, especially when deployed in
high-risk environments. In this paper, risk sensitivity is promoted in a
model-based reinforcement learning algorithm by exploiting the ability of a
bootstrap ensemble of dynamics models to estimate environment epistemic
uncertainty. We propose uncertainty guided cross-entropy method planning, which
penalises action sequences that result in high var...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.01160v1' target='_blank'>Hierarchical Reinforcement Learning for Precise Soccer Shooting Skills
  using a Quadrupedal Robot</a></h2>
<p><strong>Authors:</strong> Yandong Ji, Zhongyu Li, Yinan Sun, Xue Bin Peng, Sergey Levine, Glen Berseth, Koushil Sreenath</p>
<p><strong>Summary:</strong> We address the problem of enabling quadrupedal robots to perform precise
shooting skills in the real world using reinforcement learning. Developing
algorithms to enable a legged robot to shoot a soccer ball to a given target is
a challenging problem that combines robot motion control and planning into one
task. To solve this problem, we need to consider the dynamics limitation and
motion stability during the control of a dynamic legged robot. Moreover, we
need to consider motion planning to shoo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.05980v1' target='_blank'>Deep-Reinforcement-Learning-based Path Planning for Industrial Robots
  using Distance Sensors as Observation</a></h2>
<p><strong>Authors:</strong> Teham Bhuiyan, Linh K√§stner, Yifan Hu, Benno Kutschank, Jens Lambrecht</p>
<p><strong>Summary:</strong> Industrial robots are widely used in various manufacturing environments due
to their efficiency in doing repetitive tasks such as assembly or welding. A
common problem for these applications is to reach a destination without
colliding with obstacles or other robot arms. Commonly used sampling-based path
planning approaches such as RRT require long computation times, especially in
complex environments. Furthermore, the environment in which they are employed
needs to be known beforehand. When util...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.04376v1' target='_blank'>Efficient Planning in Combinatorial Action Spaces with Applications to
  Cooperative Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Volodymyr Tkachuk, Seyed Alireza Bakhtiari, Johannes Kirschner, Matej Jusup, Ilija Bogunovic, Csaba Szepesv√°ri</p>
<p><strong>Summary:</strong> A practical challenge in reinforcement learning are combinatorial action
spaces that make planning computationally demanding. For example, in
cooperative multi-agent reinforcement learning, a potentially large number of
agents jointly optimize a global reward function, which leads to a
combinatorial blow-up in the action space by the number of agents. As a minimal
requirement, we assume access to an argmax oracle that allows to efficiently
compute the greedy policy for any Q-function in the mode...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.14971v1' target='_blank'>Distributed multi-agent target search and tracking with Gaussian process
  and reinforcement learning</a></h2>
<p><strong>Authors:</strong> Jigang Kim, Dohyun Jang, H. Jin Kim</p>
<p><strong>Summary:</strong> Deploying multiple robots for target search and tracking has many practical
applications, yet the challenge of planning over unknown or partially known
targets remains difficult to address. With recent advances in deep learning,
intelligent control techniques such as reinforcement learning have enabled
agents to learn autonomously from environment interactions with little to no
prior knowledge. Such methods can address the exploration-exploitation tradeoff
of planning over unknown targets in a d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.03701v1' target='_blank'>Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement
  Learning Adaptation</a></h2>
<p><strong>Authors:</strong> Maxwell Joseph Jacobson, Yexiang Xue</p>
<p><strong>Summary:</strong> Meta Reinforcement Learning (Meta RL) trains agents that adapt to
fast-changing environments and tasks. Current strategies often lose adaption
efficiency due to the passive nature of model exploration, causing delayed
understanding of new transition dynamics. This results in particularly
fast-evolving tasks being impossible to solve. We propose a novel approach,
Hypothesis Network Planned Exploration (HyPE), that integrates an active and
planned exploration process via the hypothesis network to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.18051v1' target='_blank'>Inverse Reinforcement Learning with Multiple Planning Horizons</a></h2>
<p><strong>Authors:</strong> Jiayu Yao, Weiwei Pan, Finale Doshi-Velez, Barbara E Engelhardt</p>
<p><strong>Summary:</strong> In this work, we study an inverse reinforcement learning (IRL) problem where
the experts are planning under a shared reward function but with different,
unknown planning horizons. Without the knowledge of discount factors, the
reward function has a larger feasible solution set, which makes it harder for
existing IRL approaches to identify a reward function. To overcome this
challenge, we develop algorithms that can learn a global multi-agent reward
function with agent-specific discount factors t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1301.2343v1' target='_blank'>Planning by Prioritized Sweeping with Small Backups</a></h2>
<p><strong>Authors:</strong> Harm van Seijen, Richard S. Sutton</p>
<p><strong>Summary:</strong> Efficient planning plays a crucial role in model-based reinforcement
learning. Traditionally, the main planning operation is a full backup based on
the current estimates of the successor states. Consequently, its computation
time is proportional to the number of successor states. In this paper, we
introduce a new planning backup that uses only the current value of a single
successor state and has a computation time independent of the number of
successor states. This new backup, which we call a s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.08453v1' target='_blank'>Planning with Goal-Conditioned Policies</a></h2>
<p><strong>Authors:</strong> Soroush Nasiriany, Vitchyr H. Pong, Steven Lin, Sergey Levine</p>
<p><strong>Summary:</strong> Planning methods can solve temporally extended sequential decision making
problems by composing simple behaviors. However, planning requires suitable
abstractions for the states and transitions, which typically need to be
designed by hand. In contrast, model-free reinforcement learning (RL) can
acquire behaviors from low-level inputs directly, but often struggles with
temporally extended tasks. Can we utilize reinforcement learning to
automatically form the abstractions needed for planning, thus...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.05420v2' target='_blank'>Mobile Robot Path Planning in Dynamic Environments through Globally
  Guided Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Binyu Wang, Zhe Liu, Qingbiao Li, Amanda Prorok</p>
<p><strong>Summary:</strong> Path planning for mobile robots in large dynamic environments is a
challenging problem, as the robots are required to efficiently reach their
given goals while simultaneously avoiding potential conflicts with other robots
or dynamic objects. In the presence of dynamic obstacles, traditional solutions
usually employ re-planning strategies, which re-call a planning algorithm to
search for an alternative path whenever the robot encounters a conflict.
However, such re-planning strategies often cause...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.13037v1' target='_blank'>SPOTTER: Extending Symbolic Planning Operators through Targeted
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vasanth Sarathy, Daniel Kasenberg, Shivam Goel, Jivko Sinapov, Matthias Scheutz</p>
<p><strong>Summary:</strong> Symbolic planning models allow decision-making agents to sequence actions in
arbitrary ways to achieve a variety of goals in dynamic domains. However, they
are typically handcrafted and tend to require precise formulations that are not
robust to human error. Reinforcement learning (RL) approaches do not require
such models, and instead learn domain dynamics by exploring the environment and
collecting rewards. However, RL approaches tend to require millions of episodes
of experience and often lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.13338v2' target='_blank'>Solving Challenging Control Problems Using Two-Staged Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Nitish Sontakke, Sehoon Ha</p>
<p><strong>Summary:</strong> We present a deep reinforcement learning (deep RL) algorithm that consists of
learning-based motion planning and imitation to tackle challenging control
problems. Deep RL has been an effective tool for solving many high-dimensional
continuous control problems, but it cannot effectively solve challenging
problems with certain properties, such as sparse reward functions or sensitive
dynamics. In this work, we propose an approach that decomposes the given
problem into two deep RL stages: motion pla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.13834v2' target='_blank'>Self-Imitation Learning by Planning</a></h2>
<p><strong>Authors:</strong> Sha Luo, Hamidreza Kasaei, Lambert Schomaker</p>
<p><strong>Summary:</strong> Imitation learning (IL) enables robots to acquire skills quickly by
transferring expert knowledge, which is widely adopted in reinforcement
learning (RL) to initialize exploration. However, in long-horizon motion
planning tasks, a challenging problem in deploying IL and RL methods is how to
generate and collect massive, broadly distributed data such that these methods
can generalize effectively. In this work, we solve this problem using our
proposed approach called {self-imitation learning by pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.06754v1' target='_blank'>Reinforcement Learning in Robotic Motion Planning by Combined
  Experience-based Planning and Self-Imitation Learning</a></h2>
<p><strong>Authors:</strong> Sha Luo, Lambert Schomaker</p>
<p><strong>Summary:</strong> High-quality and representative data is essential for both Imitation Learning
(IL)- and Reinforcement Learning (RL)-based motion planning tasks. For real
robots, it is challenging to collect enough qualified data either as
demonstrations for IL or experiences for RL due to safety considerations in
environments with obstacles. We target this challenge by proposing the
self-imitation learning by planning plus (SILP+) algorithm, which efficiently
embeds experience-based planning into the learning a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.15607v1' target='_blank'>Reinforced Imitative Trajectory Planning for Urban Automated Driving</a></h2>
<p><strong>Authors:</strong> Di Zeng, Ling Zheng, Xiantong Yang, Yinong Li</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) faces challenges in trajectory planning for urban
automated driving due to the poor convergence of RL and the difficulty in
designing reward functions. The convergence problem is alleviated by combining
RL with supervised learning. However, most existing approaches only reason one
step ahead and lack the capability to plan for multiple future steps. Besides,
although inverse reinforcement learning holds promise for solving the reward
function design issue, existing me...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.03111v1' target='_blank'>Experience-driven discovery of planning strategies</a></h2>
<p><strong>Authors:</strong> Ruiqi He, Falk Lieder</p>
<p><strong>Summary:</strong> One explanation for how people can plan efficiently despite limited cognitive
resources is that we possess a set of adaptive planning strategies and know
when and how to use them. But how are these strategies acquired? While previous
research has studied how individuals learn to choose among existing strategies,
little is known about the process of forming new planning strategies. In this
work, we propose that new planning strategies are discovered through
metacognitive reinforcement learning. T...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1801.02805v2' target='_blank'>DeepTraffic: Crowdsourced Hyperparameter Tuning of Deep Reinforcement
  Learning Systems for Multi-Agent Dense Traffic Navigation</a></h2>
<p><strong>Authors:</strong> Lex Fridman, Jack Terwilliger, Benedikt Jenik</p>
<p><strong>Summary:</strong> We present a traffic simulation named DeepTraffic where the planning systems
for a subset of the vehicles are handled by a neural network as part of a
model-free, off-policy reinforcement learning process. The primary goal of
DeepTraffic is to make the hands-on study of deep reinforcement learning
accessible to thousands of students, educators, and researchers in order to
inspire and fuel the exploration and evaluation of deep Q-learning network
variants and hyperparameter configurations through...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.01265v2' target='_blank'>Equivalence Between Wasserstein and Value-Aware Loss for Model-based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman</p>
<p><strong>Summary:</strong> Learning a generative model is a key component of model-based reinforcement
learning. Though learning a good model in the tabular setting is a simple task,
learning a useful model in the approximate setting is challenging. In this
context, an important question is the loss function used for model learning as
varying the loss function can have a remarkable impact on effectiveness of
planning. Recently Farahmand et al. (2017) proposed a value-aware model
learning (VAML) objective that captures the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.05310v1' target='_blank'>Deep Reinforcement Learning for Dynamic Urban Transportation Problems</a></h2>
<p><strong>Authors:</strong> Laura Schultz, Vadim Sokolov</p>
<p><strong>Summary:</strong> We explore the use of deep learning and deep reinforcement learning for
optimization problems in transportation. Many transportation system analysis
tasks are formulated as an optimization problem - such as optimal control
problems in intelligent transportation systems and long term urban planning.
Often transportation models used to represent dynamics of a transportation
system involve large data sets with complex input-output interactions and are
difficult to use in the context of optimization...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.05542v1' target='_blank'>Unsupervised Visuomotor Control through Distributional Planning Networks</a></h2>
<p><strong>Authors:</strong> Tianhe Yu, Gleb Shevchuk, Dorsa Sadigh, Chelsea Finn</p>
<p><strong>Summary:</strong> While reinforcement learning (RL) has the potential to enable robots to
autonomously acquire a wide range of skills, in practice, RL usually requires
manual, per-task engineering of reward functions, especially in real world
settings where aspects of the environment needed to compute progress are not
directly accessible. To enable robots to autonomously learn skills, we instead
consider the problem of reinforcement learning without access to rewards. We
aim to learn an unsupervised embedding spa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.00531v1' target='_blank'>Context-Aware Safe Reinforcement Learning for Non-Stationary
  Environments</a></h2>
<p><strong>Authors:</strong> Baiming Chen, Zuxin Liu, Jiacheng Zhu, Mengdi Xu, Wenhao Ding, Ding Zhao</p>
<p><strong>Summary:</strong> Safety is a critical concern when deploying reinforcement learning agents for
realistic tasks. Recently, safe reinforcement learning algorithms have been
developed to optimize the agent's performance while avoiding violations of
safety constraints. However, few studies have addressed the non-stationary
disturbances in the environments, which may cause catastrophic outcomes. In
this paper, we propose the context-aware safe reinforcement learning (CASRL)
method, a meta-learning framework to realiz...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.07462v1' target='_blank'>Deep Reinforcement Learning for Producing Furniture Layout in Indoor
  Scenes</a></h2>
<p><strong>Authors:</strong> Xinhan Di, Pengqian Yu</p>
<p><strong>Summary:</strong> In the industrial interior design process, professional designers plan the
size and position of furniture in a room to achieve a satisfactory design for
selling. In this paper, we explore the interior scene design task as a Markov
decision process (MDP), which is solved by deep reinforcement learning. The
goal is to produce an accurate position and size of the furniture
simultaneously for the indoor layout task. In particular, we first formulate
the furniture layout task as a MDP problem by defi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.05246v2' target='_blank'>Deep Reinforcement-Learning-based Driving Policy for Autonomous Road
  Vehicles</a></h2>
<p><strong>Authors:</strong> Konstantinos Makantasis, Maria Kontorinaki, Ioannis Nikolos</p>
<p><strong>Summary:</strong> In this work the problem of path planning for an autonomous vehicle that
moves on a freeway is considered. The most common approaches that are used to
address this problem are based on optimal control methods, which make
assumptions about the model of the environment and the system dynamics. On the
contrary, this work proposes the development of a driving policy based on
reinforcement learning. In this way, the proposed driving policy makes minimal
or no assumptions about the environment, since ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.00397v1' target='_blank'>A Theory of Abstraction in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> David Abel</p>
<p><strong>Summary:</strong> Reinforcement learning defines the problem facing agents that learn to make
good decisions through action and observation alone. To be effective problem
solvers, such agents must efficiently explore vast worlds, assign credit from
delayed feedback, and generalize to new experiences, all while making use of
limited data, computational resources, and perceptual bandwidth. Abstraction is
essential to all of these endeavors. Through abstraction, agents can form
concise models of their environment th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.15073v2' target='_blank'>A Benchmark Comparison of Imitation Learning-based Control Policies for
  Autonomous Racing</a></h2>
<p><strong>Authors:</strong> Xiatao Sun, Mingyan Zhou, Zhijun Zhuang, Shuo Yang, Johannes Betz, Rahul Mangharam</p>
<p><strong>Summary:</strong> Autonomous racing with scaled race cars has gained increasing attention as an
effective approach for developing perception, planning and control algorithms
for safe autonomous driving at the limits of the vehicle's handling. To train
agile control policies for autonomous racing, learning-based approaches largely
utilize reinforcement learning, albeit with mixed results. In this study, we
benchmark a variety of imitation learning policies for racing vehicles that are
applied directly or for boots...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.12876v2' target='_blank'>Guiding Online Reinforcement Learning with Action-Free Offline
  Pretraining</a></h2>
<p><strong>Authors:</strong> Deyao Zhu, Yuhui Wang, J√ºrgen Schmidhuber, Mohamed Elhoseiny</p>
<p><strong>Summary:</strong> Offline RL methods have been shown to reduce the need for environment
interaction by training agents using offline collected episodes. However, these
methods typically require action information to be logged during data
collection, which can be difficult or even impossible in some practical cases.
In this paper, we investigate the potential of using action-free offline
datasets to improve online reinforcement learning, name this problem
Reinforcement Learning with Action-Free Offline Pretraining...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.02921v1' target='_blank'>Holistic Deep-Reinforcement-Learning-based Training of Autonomous
  Navigation Systems</a></h2>
<p><strong>Authors:</strong> Linh K√§stner, Marvin Meusel, Teham Bhuiyan, Jens Lambrecht</p>
<p><strong>Summary:</strong> In recent years, Deep Reinforcement Learning emerged as a promising approach
for autonomous navigation of ground vehicles and has been utilized in various
areas of navigation such as cruise control, lane changing, or obstacle
avoidance. However, most research works either focus on providing an end-to-end
solution training the whole system using Deep Reinforcement Learning or focus
on one specific aspect such as local motion planning. This however, comes along
with a number of problems such as ca...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.07535v1' target='_blank'>Path Planning using Reinforcement Learning: A Policy Iteration Approach</a></h2>
<p><strong>Authors:</strong> Saumil Shivdikar, Jagannath Nirmal</p>
<p><strong>Summary:</strong> With the impact of real-time processing being realized in the recent past,
the need for efficient implementations of reinforcement learning algorithms has
been on the rise. Albeit the numerous advantages of Bellman equations utilized
in RL algorithms, they are not without the large search space of design
parameters.
  This research aims to shed light on the design space exploration associated
with reinforcement learning parameters, specifically that of Policy Iteration.
Given the large computati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.06796v2' target='_blank'>Towards Theoretical Understanding of Data-Driven Policy Refinement</a></h2>
<p><strong>Authors:</strong> Ali Baheri</p>
<p><strong>Summary:</strong> This paper presents an approach for data-driven policy refinement in
reinforcement learning, specifically designed for safety-critical applications.
Our methodology leverages the strengths of data-driven optimization and
reinforcement learning to enhance policy safety and optimality through
iterative refinement. Our principal contribution lies in the mathematical
formulation of this data-driven policy refinement concept. This framework
systematically improves reinforcement learning policies by l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.17250v1' target='_blank'>Self-Supervised Reinforcement Learning that Transfers using Random
  Features</a></h2>
<p><strong>Authors:</strong> Boyuan Chen, Chuning Zhu, Pulkit Agrawal, Kaiqing Zhang, Abhishek Gupta</p>
<p><strong>Summary:</strong> Model-free reinforcement learning algorithms have exhibited great potential
in solving single-task sequential decision-making problems with
high-dimensional observations and long horizons, but are known to be hard to
generalize across tasks. Model-based RL, on the other hand, learns
task-agnostic models of the world that naturally enables transfer across
different reward functions, but struggles to scale to complex environments due
to the compounding error. To get the best of both worlds, we pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.04182v1' target='_blank'>Reinforcement Learning with Ensemble Model Predictive Safety
  Certification</a></h2>
<p><strong>Authors:</strong> Sven Gronauer, Tom Haider, Felippe Schmoeller da Roza, Klaus Diepold</p>
<p><strong>Summary:</strong> Reinforcement learning algorithms need exploration to learn. However,
unsupervised exploration prevents the deployment of such algorithms on
safety-critical tasks and limits real-world deployment. In this paper, we
propose a new algorithm called Ensemble Model Predictive Safety Certification
that combines model-based deep reinforcement learning with tube-based model
predictive control to correct the actions taken by a learning agent, keeping
safety constraint violations at a minimum through plan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.08220v1' target='_blank'>Optimization of Link Configuration for Satellite Communication Using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tobias Rohe, Michael K√∂lle, Jan Matheis, R√ºdiger H√∂pfl, Leo S√ºnkel, Claudia Linnhoff-Popien</p>
<p><strong>Summary:</strong> Satellite communication is a key technology in our modern connected world.
With increasingly complex hardware, one challenge is to efficiently configure
links (connections) on a satellite transponder. Planning an optimal link
configuration is extremely complex and depends on many parameters and metrics.
The optimal use of the limited resources, bandwidth and power of the
transponder is crucial. Such an optimization problem can be approximated using
metaheuristic methods such as simulated anneali...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.05901v2' target='_blank'>Nearly Minimax Optimal Reward-free Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Simon S. Du, Xiangyang Ji</p>
<p><strong>Summary:</strong> We study the reward-free reinforcement learning framework, which is
particularly suitable for batch reinforcement learning and scenarios where one
needs policies for multiple reward functions. This framework has two phases. In
the exploration phase, the agent collects trajectories by interacting with the
environment without using any reward signal. In the planning phase, the agent
needs to return a near-optimal policy for arbitrary reward functions. We give a
new efficient algorithm, \textbf{S}t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.05018v3' target='_blank'>Learning Temporally Extended Skills in Continuous Domains as Symbolic
  Actions for Planning</a></h2>
<p><strong>Authors:</strong> Jan Achterhold, Markus Krimmel, Joerg Stueckler</p>
<p><strong>Summary:</strong> Problems which require both long-horizon planning and continuous control
capabilities pose significant challenges to existing reinforcement learning
agents. In this paper we introduce a novel hierarchical reinforcement learning
agent which links temporally extended skills for continuous control with a
forward model in a symbolic discrete abstraction of the environment's state for
planning. We term our agent SEADS for Symbolic Effect-Aware Diverse Skills. We
formulate an objective and correspondi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1404.1140v2' target='_blank'>Scalable Planning and Learning for Multiagent POMDPs: Extended Version</a></h2>
<p><strong>Authors:</strong> Christopher Amato, Frans A. Oliehoek</p>
<p><strong>Summary:</strong> Online, sample-based planning algorithms for POMDPs have shown great promise
in scaling to problems with large state spaces, but they become intractable for
large action and observation spaces. This is particularly problematic in
multiagent POMDPs where the action and observation space grows exponentially
with the number of agents. To combat this intractability, we propose a novel
scalable approach based on sample-based planning and factored value functions
that exploits structure present in man...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.13599v2' target='_blank'>End-to-End Motion Planning of Quadrotors Using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Efe Camci, Erdal Kayacan</p>
<p><strong>Summary:</strong> In this work, a novel, end-to-end motion planning method is proposed for
quadrotor navigation in cluttered environments. The proposed method circumvents
the explicit sensing-reconstructing-planning in contrast to conventional
navigation algorithms. It uses raw depth images obtained from a front-facing
camera and directly generates local motion plans in the form of smooth motion
primitives that move a quadrotor to a goal by avoiding obstacles. Promising
training and testing results are presented ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.04866v1' target='_blank'>Planning for Novelty: Width-Based Algorithms for Common Problems in
  Control, Planning and Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Nir Lipovetzky</p>
<p><strong>Summary:</strong> Width-based algorithms search for solutions through a general definition of
state novelty. These algorithms have been shown to result in state-of-the-art
performance in classical planning, and have been successfully applied to
model-based and model-free settings where the dynamics of the problem are given
through simulation engines. Width-based algorithms performance is understood
theoretically through the notion of planning width, providing polynomial
guarantees on their runtime and memory cons...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.05598v2' target='_blank'>Deep Model-Based Reinforcement Learning for High-Dimensional Problems, a
  Survey</a></h2>
<p><strong>Authors:</strong> Aske Plaat, Walter Kosters, Mike Preuss</p>
<p><strong>Summary:</strong> Deep reinforcement learning has shown remarkable success in the past few
years. Highly complex sequential decision making problems have been solved in
tasks such as game playing and robotics. Unfortunately, the sample complexity
of most deep reinforcement learning methods is high, precluding their use in
some important applications. Model-based reinforcement learning creates an
explicit model of the environment dynamics to reduce the need for environment
samples. Current deep learning methods us...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.13503v2' target='_blank'>Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal
  Algorithm Escaping the Curse of Horizon</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Xiangyang Ji, Simon S. Du</p>
<p><strong>Summary:</strong> Episodic reinforcement learning and contextual bandits are two widely studied
sequential decision-making problems. Episodic reinforcement learning
generalizes contextual bandits and is often perceived to be more difficult due
to long planning horizon and unknown state-dependent transitions. The current
paper shows that the long planning horizon and the unknown state-dependent
transitions (at most) pose little additional difficulty on sample complexity.
  We consider the episodic reinforcement le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.19538v1' target='_blank'>Scalable Hierarchical Reinforcement Learning for Hyper Scale Multi-Robot
  Task Planning</a></h2>
<p><strong>Authors:</strong> Xuan Zhou, Xiang Shi, Lele Zhang, Chen Chen, Hongbo Li, Lin Ma, Fang Deng, Jie Chen</p>
<p><strong>Summary:</strong> To improve the efficiency of warehousing system and meet huge customer
orders, we aim to solve the challenges of dimension disaster and dynamic
properties in hyper scale multi-robot task planning (MRTP) for robotic mobile
fulfillment system (RMFS). Existing research indicates that hierarchical
reinforcement learning (HRL) is an effective method to reduce these challenges.
Based on that, we construct an efficient multi-stage HRL-based multi-robot task
planner for hyper scale MRTP in RMFS, and the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.02689v1' target='_blank'>Solving Hard AI Planning Instances Using Curriculum-Driven Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dieqiao Feng, Carla P. Gomes, Bart Selman</p>
<p><strong>Summary:</strong> Despite significant progress in general AI planning, certain domains remain
out of reach of current AI planning systems. Sokoban is a PSPACE-complete
planning task and represents one of the hardest domains for current AI
planners. Even domain-specific specialized search methods fail quickly due to
the exponential search complexity on hard instances. Our approach based on deep
reinforcement learning augmented with a curriculum-driven method is the first
one to solve hard instances within one day ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.11456v2' target='_blank'>Guiding Robot Exploration in Reinforcement Learning via Automated
  Planning</a></h2>
<p><strong>Authors:</strong> Yohei Hayamizu, Saeid Amiri, Kishan Chandan, Keiki Takadama, Shiqi Zhang</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) enables an agent to learn from trial-and-error
experiences toward achieving long-term goals; automated planning aims to
compute plans for accomplishing tasks using action knowledge. Despite their
shared goal of completing complex tasks, the development of RL and automated
planning has been largely isolated due to their different computational
modalities. Focusing on improving RL agents' learning efficiency, we develop
Guided Dyna-Q (GDQ) to enable RL agents to reason ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.04697v1' target='_blank'>Behavior Planning at Urban Intersections through Hierarchical
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhiqian Qiao, Jeff Schneider, John M. Dolan</p>
<p><strong>Summary:</strong> For autonomous vehicles, effective behavior planning is crucial to ensure
safety of the ego car. In many urban scenarios, it is hard to create
sufficiently general heuristic rules, especially for challenging scenarios that
some new human drivers find difficult. In this work, we propose a behavior
planning structure based on reinforcement learning (RL) which is capable of
performing autonomous vehicle behavior planning with a hierarchical structure
in simulated urban environments. Application of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.01434v1' target='_blank'>Reinforcement Learning with Prior Policy Guidance for Motion Planning of
  Dual-Arm Free-Floating Space Robot</a></h2>
<p><strong>Authors:</strong> Yuxue Cao, Shengjie Wang, Xiang Zheng, Wenke Ma, Xinru Xie, Lei Liu</p>
<p><strong>Summary:</strong> Reinforcement learning methods as a promising technique have achieved
superior results in the motion planning of free-floating space robots. However,
due to the increase in planning dimension and the intensification of system
dynamics coupling, the motion planning of dual-arm free-floating space robots
remains an open challenge. In particular, the current study cannot handle the
task of capturing a non-cooperative object due to the lack of the pose
constraint of the end-effectors. To address the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.16872v1' target='_blank'>Planning to the Information Horizon of BAMDPs via Epistemic State
  Abstraction</a></h2>
<p><strong>Authors:</strong> Dilip Arumugam, Satinder Singh</p>
<p><strong>Summary:</strong> The Bayes-Adaptive Markov Decision Process (BAMDP) formalism pursues the
Bayes-optimal solution to the exploration-exploitation trade-off in
reinforcement learning. As the computation of exact solutions to Bayesian
reinforcement-learning problems is intractable, much of the literature has
focused on developing suitable approximation algorithms. In this work, before
diving into algorithm design, we first define, under mild structural
assumptions, a complexity measure for BAMDP planning. As effici...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.12617v1' target='_blank'>Leveraging Jumpy Models for Planning and Fast Learning in Robotic
  Domains</a></h2>
<p><strong>Authors:</strong> Jingwei Zhang, Jost Tobias Springenberg, Arunkumar Byravan, Leonard Hasenclever, Abbas Abdolmaleki, Dushyant Rao, Nicolas Heess, Martin Riedmiller</p>
<p><strong>Summary:</strong> In this paper we study the problem of learning multi-step dynamics prediction
models (jumpy models) from unlabeled experience and their utility for fast
inference of (high-level) plans in downstream tasks. In particular we propose
to learn a jumpy model alongside a skill embedding space offline, from
previously collected experience for which no labels or reward annotations are
required. We then investigate several options of harnessing those learned
components in combination with model-based pla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.01150v1' target='_blank'>Multi-UAV Adaptive Path Planning Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jonas Westheider, Julius R√ºckin, Marija Popoviƒá</p>
<p><strong>Summary:</strong> Efficient aerial data collection is important in many remote sensing
applications. In large-scale monitoring scenarios, deploying a team of unmanned
aerial vehicles (UAVs) offers improved spatial coverage and robustness against
individual failures. However, a key challenge is cooperative path planning for
the UAVs to efficiently achieve a joint mission goal. We propose a novel
multi-agent informative path planning approach based on deep reinforcement
learning for adaptive terrain monitoring scen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.14085v1' target='_blank'>Sim-to-Real Surgical Robot Learning and Autonomous Planning for Internal
  Tissue Points Manipulation using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yafei Ou, Mahdi Tavakoli</p>
<p><strong>Summary:</strong> Indirect simultaneous positioning (ISP), where internal tissue points are
placed at desired locations indirectly through the manipulation of boundary
points, is a type of subtask frequently performed in robotic surgeries.
Although challenging due to complex tissue dynamics, automating the task can
potentially reduce the workload of surgeons. This paper presents a sim-to-real
framework for learning to automate the task without interacting with a real
environment, and for planning preoperatively t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.16062v2' target='_blank'>Using Implicit Behavior Cloning and Dynamic Movement Primitive to
  Facilitate Reinforcement Learning for Robot Motion Planning</a></h2>
<p><strong>Authors:</strong> Zengjie Zhang, Jayden Hong, Amir Soufi Enayati, Homayoun Najjaran</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) for motion planning of multi-degree-of-freedom
robots still suffers from low efficiency in terms of slow training speed and
poor generalizability. In this paper, we propose a novel RL-based robot motion
planning framework that uses implicit behavior cloning (IBC) and dynamic
movement primitive (DMP) to improve the training speed and generalizability of
an off-policy RL agent. IBC utilizes human demonstration data to leverage the
training speed of RL, and DMP serves as...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.14237v2' target='_blank'>Hierarchical Reinforcement Learning Based on Planning Operators</a></h2>
<p><strong>Authors:</strong> Jing Zhang, Emmanuel Dean, Karinne Ramirez-Amaro</p>
<p><strong>Summary:</strong> Long-horizon manipulation tasks such as stacking represent a longstanding
challenge in the field of robotic manipulation, particularly when using
reinforcement learning (RL) methods which often struggle to learn the correct
sequence of actions for achieving these complex goals. To learn this sequence,
symbolic planning methods offer a good solution based on high-level reasoning,
however, planners often fall short in addressing the low-level control
specificity needed for precise execution. This ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.06974v1' target='_blank'>Deep Reinforcement Learning for Mobile Robot Path Planning</a></h2>
<p><strong>Authors:</strong> Hao Liu, Yi Shen, Shuangjiang Yu, Zijun Gao, Tong Wu</p>
<p><strong>Summary:</strong> Path planning is an important problem with the the applications in many
aspects, such as video games, robotics etc. This paper proposes a novel method
to address the problem of Deep Reinforcement Learning (DRL) based path planning
for a mobile robot. We design DRL-based algorithms, including reward functions,
and parameter optimization, to avoid time-consuming work in a 2D environment.
We also designed an Two-way search hybrid A* algorithm to improve the quality
of local path planning. We transf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.15410v1' target='_blank'>Planning the path with Reinforcement Learning: Optimal Robot Motion
  Planning in RoboCup Small Size League Environments</a></h2>
<p><strong>Authors:</strong> Mateus G. Machado, Jo√£o G. Melo, Cleber Zanchettin, Pedro H. M. Braga, Pedro V. Cunha, Edna N. S. Barros, Hansenclever F. Bassani</p>
<p><strong>Summary:</strong> This work investigates the potential of Reinforcement Learning (RL) to tackle
robot motion planning challenges in the dynamic RoboCup Small Size League
(SSL). Using a heuristic control approach, we evaluate RL's effectiveness in
obstacle-free and single-obstacle path-planning environments. Ablation studies
reveal significant performance improvements. Our method achieved a 60% time
gain in obstacle-free environments compared to baseline algorithms.
Additionally, our findings demonstrated dynamic ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.10990v1' target='_blank'>G-Learner and GIRL: Goal Based Wealth Management with Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Matthew Dixon, Igor Halperin</p>
<p><strong>Summary:</strong> We present a reinforcement learning approach to goal based wealth management
problems such as optimization of retirement plans or target dated funds. In
such problems, an investor seeks to achieve a financial goal by making periodic
investments in the portfolio while being employed, and periodically draws from
the account when in retirement, in addition to the ability to re-balance the
portfolio by selling and buying different assets (e.g. stocks). Instead of
relying on a utility of consumption,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.09624v1' target='_blank'>Batch-Augmented Multi-Agent Reinforcement Learning for Efficient Traffic
  Signal Optimization</a></h2>
<p><strong>Authors:</strong> Yueh-Hua Wu, I-Hau Yeh, David Hu, Hong-Yuan Mark Liao</p>
<p><strong>Summary:</strong> The goal of this work is to provide a viable solution based on reinforcement
learning for traffic signal control problems. Although the state-of-the-art
reinforcement learning approaches have yielded great success in a variety of
domains, directly applying it to alleviate traffic congestion can be
challenging, considering the requirement of high sample efficiency and how
training data is gathered. In this work, we address several challenges that we
encountered when we attempted to mitigate serio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.06975v2' target='_blank'>Precise atom manipulation through deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> I-Ju Chen, Markus Aapro, Abraham Kipnis, Alexander Ilin, Peter Liljeroth, Adam S. Foster</p>
<p><strong>Summary:</strong> Atomic-scale manipulation in scanning tunneling microscopy has enabled the
creation of quantum states of matter based on artificial structures and extreme
miniaturization of computational circuitry based on individual atoms. The
ability to autonomously arrange atomic structures with precision will enable
the scaling up of nanoscale fabrication and expand the range of artificial
structures hosting exotic quantum states. However, the \textit{a priori}
unknown manipulation parameters, the possibili...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.12136v2' target='_blank'>Using Reinforcement Learning for the Three-Dimensional Loading
  Capacitated Vehicle Routing Problem</a></h2>
<p><strong>Authors:</strong> Stefan Schoepf, Stephen Mak, Julian Senoner, Liming Xu, Netland Torbj√∂rn, Alexandra Brintrup</p>
<p><strong>Summary:</strong> Heavy goods vehicles are vital backbones of the supply chain delivery system
but also contribute significantly to carbon emissions with only 60% loading
efficiency in the United Kingdom. Collaborative vehicle routing has been
proposed as a solution to increase efficiency, but challenges remain to make
this a possibility. One key challenge is the efficient computation of viable
solutions for co-loading and routing. Current operations research methods
suffer from non-linear scaling with increasing...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.00669v2' target='_blank'>Hierarchical Reinforcement Learning with AI Planning Models</a></h2>
<p><strong>Authors:</strong> Junkyu Lee, Michael Katz, Don Joven Agravante, Miao Liu, Geraud Nangue Tasse, Tim Klinger, Shirin Sohrabi</p>
<p><strong>Summary:</strong> Two common approaches to sequential decision-making are AI planning (AIP) and
reinforcement learning (RL). Each has strengths and weaknesses. AIP is
interpretable, easy to integrate with symbolic knowledge, and often efficient,
but requires an up-front logical domain specification and is sensitive to
noise; RL only requires specification of rewards and is robust to noise but is
sample inefficient and not easily supplied with external knowledge. We propose
an integrative approach that combines hi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.09055v1' target='_blank'>Predictive Maneuver Planning with Deep Reinforcement Learning (PMP-DRL)
  for comfortable and safe autonomous driving</a></h2>
<p><strong>Authors:</strong> Jayabrata Chowdhury, Vishruth Veerendranath, Suresh Sundaram, Narasimhan Sundararajan</p>
<p><strong>Summary:</strong> This paper presents a Predictive Maneuver Planning with Deep Reinforcement
Learning (PMP-DRL) model for maneuver planning. Traditional rule-based maneuver
planning approaches often have to improve their abilities to handle the
variabilities of real-world driving scenarios. By learning from its experience,
a Reinforcement Learning (RL)-based driving agent can adapt to changing driving
conditions and improve its performance over time. Our proposed approach
combines a predictive model and an RL age...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.08152v1' target='_blank'>Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor
  Re-planning</a></h2>
<p><strong>Authors:</strong> Gilhyun Ryou, Geoffrey Wang, Sertac Karaman</p>
<p><strong>Summary:</strong> High-speed online trajectory planning for UAVs poses a significant challenge
due to the need for precise modeling of complex dynamics while also being
constrained by computational limitations. This paper presents a multi-fidelity
reinforcement learning method (MFRL) that aims to effectively create a
realistic dynamics model and simultaneously train a planning policy that can be
readily deployed in real-time applications. The proposed method involves the
co-training of a planning policy and a rew...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.14077v1' target='_blank'>Research on Robot Path Planning Based on Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Wang Ruiqi</p>
<p><strong>Summary:</strong> This project has conducted research on robot path planning based on Visual
SLAM. The main work of this project is as follows: (1) Construction of Visual
SLAM system. Research has been conducted on the basic architecture of Visual
SLAM. A Visual SLAM system is developed based on ORB-SLAM3 system, which can
conduct dense point cloud mapping. (2) The map suitable for two-dimensional
path planning is obtained through map conversion. This part converts the dense
point cloud map obtained by Visual SLA...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1105.1749v2' target='_blank'>A Real-Time Model-Based Reinforcement Learning Architecture for Robot
  Control</a></h2>
<p><strong>Authors:</strong> Todd Hester, Michael Quinlan, Peter Stone</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) is a method for learning decision-making tasks
that could enable robots to learn and adapt to their situation on-line. For an
RL algorithm to be practical for robotic control tasks, it must learn in very
few actions, while continually taking those actions in real-time. Existing
model-based RL methods learn in relatively few actions, but typically take too
much time between each action for practical on-line learning. In this paper, we
present a novel parallel architect...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1301.0601v1' target='_blank'>Reinforcement Learning with Partially Known World Dynamics</a></h2>
<p><strong>Authors:</strong> Christian R. Shelton</p>
<p><strong>Summary:</strong> Reinforcement learning would enjoy better success on real-world problems if
domain knowledge could be imparted to the algorithm by the modelers. Most
problems have both hidden state and unknown dynamics. Partially observable
Markov decision processes (POMDPs) allow for the modeling of both.
Unfortunately, they do not provide a natural framework in which to specify
knowledge about the domain dynamics. The designer must either admit to knowing
nothing about the dynamics or completely specify the d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1612.05533v3' target='_blank'>Deep Reinforcement Learning with Successor Features for Navigation
  across Similar Environments</a></h2>
<p><strong>Authors:</strong> Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, Wolfram Burgard</p>
<p><strong>Summary:</strong> In this paper we consider the problem of robot navigation in simple maze-like
environments where the robot has to rely on its onboard sensors to perform the
navigation task. In particular, we are interested in solutions to this problem
that do not require localization, mapping or planning. Additionally, we require
that our solution can quickly adapt to new situations (e.g., changing
navigation goals and environments). To meet these criteria we frame this
problem as a sequence of related reinforc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1701.02392v1' target='_blank'>Reinforcement Learning via Recurrent Convolutional Neural Networks</a></h2>
<p><strong>Authors:</strong> Tanmay Shankar, Santosha K. Dwivedy, Prithwijit Guha</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning has enabled the learning of policies for complex
tasks in partially observable environments, without explicitly learning the
underlying model of the tasks. While such model-free methods achieve
considerable performance, they often ignore the structure of task. We present a
natural representation of to Reinforcement Learning (RL) problems using
Recurrent Convolutional Neural Networks (RCNNs), to better exploit this
inherent structure. We define 3 such RCNNs, whose forw...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1707.06203v2' target='_blank'>Imagination-Augmented Agents for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Th√©ophane Weber, S√©bastien Racani√®re, David P. Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdom√®nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, Razvan Pascanu, Peter Battaglia, Demis Hassabis, David Silver, Daan Wierstra</p>
<p><strong>Summary:</strong> We introduce Imagination-Augmented Agents (I2As), a novel architecture for
deep reinforcement learning combining model-free and model-based aspects. In
contrast to most existing model-based reinforcement learning and planning
methods, which prescribe how a model should be used to arrive at a policy, I2As
learn to interpret predictions from a learned environment model to construct
implicit plans in arbitrary ways, by using the predictions as additional
context in deep policy networks. I2As show i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.13418v2' target='_blank'>Intelligent Trajectory Planning in UAV-mounted Wireless Networks: A
  Quantum-Inspired Reinforcement Learning Perspective</a></h2>
<p><strong>Authors:</strong> Yuanjian Li, A. Hamid Aghvami, Daoyi Dong</p>
<p><strong>Summary:</strong> In this paper, we consider a wireless uplink transmission scenario in which
an unmanned aerial vehicle (UAV) serves as an aerial base station collecting
data from ground users. To optimize the expected sum uplink transmit rate
without any prior knowledge of ground users (e.g., locations, channel state
information and transmit power), the trajectory planning problem is optimized
via the quantum-inspired reinforcement learning (QiRL) approach. Specifically,
the QiRL method adopts novel probabilist...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.12974v1' target='_blank'>Improving the Exploration of Deep Reinforcement Learning in Continuous
  Domains using Planning for Policy Search</a></h2>
<p><strong>Authors:</strong> Jakob J. Hollenstein, Erwan Renaudo, Matteo Saveriano, Justus Piater</p>
<p><strong>Summary:</strong> Local policy search is performed by most Deep Reinforcement Learning (D-RL)
methods, which increases the risk of getting trapped in a local minimum.
Furthermore, the availability of a simulation model is not fully exploited in
D-RL even in simulation-based training, which potentially decreases efficiency.
To better exploit simulation models in policy search, we propose to integrate a
kinodynamic planner in the exploration strategy and to learn a control policy
in an offline fashion from the gene...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.06778v2' target='_blank'>A Safe Hierarchical Planning Framework for Complex Driving Scenarios
  based on Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jinning Li, Liting Sun, Jianyu Chen, Masayoshi Tomizuka, Wei Zhan</p>
<p><strong>Summary:</strong> Autonomous vehicles need to handle various traffic conditions and make safe
and efficient decisions and maneuvers. However, on the one hand, a single
optimization/sampling-based motion planner cannot efficiently generate safe
trajectories in real time, particularly when there are many interactive
vehicles near by. On the other hand, end-to-end learning methods cannot assure
the safety of the outcomes. To address this challenge, we propose a
hierarchical behavior planning framework with a set of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.13280v2' target='_blank'>Hierarchically Integrated Models: Learning to Navigate from
  Heterogeneous Robots</a></h2>
<p><strong>Authors:</strong> Katie Kang, Gregory Kahn, Sergey Levine</p>
<p><strong>Summary:</strong> Deep reinforcement learning algorithms require large and diverse datasets in
order to learn successful policies for perception-based mobile navigation.
However, gathering such datasets with a single robot can be prohibitively
expensive. Collecting data with multiple different robotic platforms with
possibly different dynamics is a more scalable approach to large-scale data
collection. But how can deep reinforcement learning algorithms leverage such
heterogeneous datasets? In this work, we propos...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.13377v1' target='_blank'>Model-Free Reinforcement Learning for Optimal Control of MarkovDecision
  Processes Under Signal Temporal Logic Specifications</a></h2>
<p><strong>Authors:</strong> Krishna C. Kalagarla, Rahul Jain, Pierluigi Nuzzo</p>
<p><strong>Summary:</strong> We present a model-free reinforcement learning algorithm to find an optimal
policy for a finite-horizon Markov decision process while guaranteeing a
desired lower bound on the probability of satisfying a signal temporal logic
(STL) specification. We propose a method to effectively augment the MDP state
space to capture the required state history and express the STL objective as a
reachability objective. The planning problem can then be formulated as a
finite-horizon constrained Markov decision p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.14811v2' target='_blank'>Surveillance Evasion Through Bayesian Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dongping Qi, David Bindel, Alexander Vladimirsky</p>
<p><strong>Summary:</strong> We consider a task of surveillance-evading path-planning in a continuous
setting. An Evader strives to escape from a 2D domain while minimizing the risk
of detection (and immediate capture). The probability of detection is
path-dependent and determined by the spatially inhomogeneous surveillance
intensity, which is fixed but a priori unknown and gradually learned in the
multi-episodic setting. We introduce a Bayesian reinforcement learning
algorithm that relies on a Gaussian Process regression (...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.13320v1' target='_blank'>Combating the Compounding-Error Problem with a Multi-step Model</a></h2>
<p><strong>Authors:</strong> Kavosh Asadi, Dipendra Misra, Seungchan Kim, Michel L. Littman</p>
<p><strong>Summary:</strong> Model-based reinforcement learning is an appealing framework for creating
agents that learn, plan, and act in sequential environments. Model-based
algorithms typically involve learning a transition model that takes a state and
an action and outputs the next state---a one-step model. This model can be
composed with itself to enable predicting multiple steps into the future, but
one-step prediction errors can get magnified, leading to unacceptable
inaccuracy. This compounding-error problem plagues...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.04702v1' target='_blank'>Safe Trajectory Planning Using Reinforcement Learning for Self Driving</a></h2>
<p><strong>Authors:</strong> Josiah Coad, Zhiqian Qiao, John M. Dolan</p>
<p><strong>Summary:</strong> Self-driving vehicles must be able to act intelligently in diverse and
difficult environments, marked by high-dimensional state spaces, a myriad of
optimization objectives and complex behaviors. Traditionally, classical
optimization and search techniques have been applied to the problem of
self-driving; but they do not fully address operations in environments with
high-dimensional states and complex behaviors. Recently, imitation learning has
been proposed for the task of self-driving; but it is...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.08577v1' target='_blank'>Structured World Belief for Reinforcement Learning in POMDP</a></h2>
<p><strong>Authors:</strong> Gautam Singh, Skand Peri, Junghyun Kim, Hyunseok Kim, Sungjin Ahn</p>
<p><strong>Summary:</strong> Object-centric world models provide structured representation of the scene
and can be an important backbone in reinforcement learning and planning.
However, existing approaches suffer in partially-observable environments due to
the lack of belief states. In this paper, we propose Structured World Belief, a
model for learning and inference of object-centric belief states. Inferred by
Sequential Monte Carlo (SMC), our belief states provide multiple object-centric
scene hypotheses. To synergize the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.06443v3' target='_blank'>Learning Reward Models for Cooperative Trajectory Planning with Inverse
  Reinforcement Learning and Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Karl Kurzer, Matthias Bitzer, J. Marius Z√∂llner</p>
<p><strong>Summary:</strong> Cooperative trajectory planning methods for automated vehicles can solve
traffic scenarios that require a high degree of cooperation between traffic
participants. However, for cooperative systems to integrate into human-centered
traffic, the automated systems must behave human-like so that humans can
anticipate the system's decisions. While Reinforcement Learning has made
remarkable progress in solving the decision-making part, it is non-trivial to
parameterize a reward model that yields predict...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.07709v2' target='_blank'>Adaptive Environment Modeling Based Reinforcement Learning for Collision
  Avoidance in Complex Scenes</a></h2>
<p><strong>Authors:</strong> Shuaijun Wang, Rui Gao, Ruihua Han, Shengduo Chen, Chengyang Li, Qi Hao</p>
<p><strong>Summary:</strong> The major challenges of collision avoidance for robot navigation in crowded
scenes lie in accurate environment modeling, fast perceptions, and trustworthy
motion planning policies. This paper presents a novel adaptive environment
model based collision avoidance reinforcement learning (i.e., AEMCARL)
framework for an unmanned robot to achieve collision-free motions in
challenging navigation scenarios. The novelty of this work is threefold: (1)
developing a hierarchical network of gated-recurrent-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.06608v1' target='_blank'>Visuo-Tactile Manipulation Planning Using Reinforcement Learning with
  Affordance Representation</a></h2>
<p><strong>Authors:</strong> Wenyu Liang, Fen Fang, Cihan Acar, Wei Qi Toh, Ying Sun, Qianli Xu, Yan Wu</p>
<p><strong>Summary:</strong> Robots are increasingly expected to manipulate objects in ever more
unstructured environments where the object properties have high perceptual
uncertainty from any single sensory modality. This directly impacts successful
object manipulation. In this work, we propose a reinforcement learning-based
motion planning framework for object manipulation which makes use of both
on-the-fly multisensory feedback and a learned attention-guided deep affordance
model as perceptual states. The affordance mode...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.09588v1' target='_blank'>New Auction Algorithms for Path Planning, Network Transport, and
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dimitri Bertsekas</p>
<p><strong>Summary:</strong> We consider some classical optimization problems in path planning and network
transport, and we introduce new auction-based algorithms for their optimal and
suboptimal solution. The algorithms are based on mathematical ideas that are
related to competitive bidding by persons for objects and the attendant market
equilibrium, which underlie auction processes. However, the starting point of
our algorithms is different, namely weighted and unweighted path construction
in directed graphs, rather than...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.02064v1' target='_blank'>E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel
  Program Guidance</a></h2>
<p><strong>Authors:</strong> Can Chang, Ni Mu, Jiajun Wu, Ling Pan, Huazhe Xu</p>
<p><strong>Summary:</strong> A critical challenge in multi-agent reinforcement learning(MARL) is for
multiple agents to efficiently accomplish complex, long-horizon tasks. The
agents often have difficulties in cooperating on common goals, dividing complex
tasks, and planning through several stages to make progress. We propose to
address these challenges by guiding agents with programs designed for
parallelization, since programs as a representation contain rich structural and
semantic information, and are widely used as abs...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.09120v2' target='_blank'>Robot path planning using deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Miguel Quinones-Ramirez, Jorge Rios-Martinez, Victor Uc-Cetina</p>
<p><strong>Summary:</strong> Autonomous navigation is challenging for mobile robots, especially in an
unknown environment. Commonly, the robot requires multiple sensors to map the
environment, locate itself, and make a plan to reach the target. However,
reinforcement learning methods offer an alternative to map-free navigation
tasks by learning the optimal actions to take. In this article, deep
reinforcement learning agents are implemented using variants of the deep Q
networks method, the D3QN and rainbow algorithms, for bo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.10669v2' target='_blank'>UAV Path Planning Employing MPC- Reinforcement Learning Method
  Considering Collision Avoidance</a></h2>
<p><strong>Authors:</strong> Mahya Ramezani, Hamed Habibi, Jose luis Sanchez Lopez, Holger Voos</p>
<p><strong>Summary:</strong> In this paper, we tackle the problem of Unmanned Aerial (UA V) path planning
in complex and uncertain environments by designing a Model Predictive Control
(MPC), based on a Long-Short-Term Memory (LSTM) network integrated into the
Deep Deterministic Policy Gradient algorithm. In the proposed solution,
LSTM-MPC operates as a deterministic policy within the DDPG network, and it
leverages a predicting pool to store predicted future states and actions for
improved robustness and efficiency. The use ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.10838v1' target='_blank'>Deceptive Reinforcement Learning in Model-Free Domains</a></h2>
<p><strong>Authors:</strong> Alan Lewis, Tim Miller</p>
<p><strong>Summary:</strong> This paper investigates deceptive reinforcement learning for privacy
preservation in model-free and continuous action space domains. In
reinforcement learning, the reward function defines the agent's objective. In
adversarial scenarios, an agent may need to both maximise rewards and keep its
reward function private from observers. Recent research presented the ambiguity
model (AM), which selects actions that are ambiguous over a set of possible
reward functions, via pre-trained $Q$-functions. De...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.14713v1' target='_blank'>Enhancing High-Speed Cruising Performance of Autonomous Vehicles through
  Integrated Deep Reinforcement Learning Framework</a></h2>
<p><strong>Authors:</strong> Jinhao Liang, Kaidi Yang, Chaopeng Tan, Jinxiang Wang, Guodong Yin</p>
<p><strong>Summary:</strong> High-speed cruising scenarios with mixed traffic greatly challenge the road
safety of autonomous vehicles (AVs). Unlike existing works that only look at
fundamental modules in isolation, this work enhances AV safety in mixed-traffic
high-speed cruising scenarios by proposing an integrated framework that
synthesizes three fundamental modules, i.e., behavioral decision-making,
path-planning, and motion-control modules. Considering that the integrated
framework would increase the system complexity,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.16006v1' target='_blank'>Bounding-Box Inference for Error-Aware Model-Based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Erin J. Talvitie, Zilei Shao, Huiying Li, Jinghan Hu, Jacob Boerma, Rory Zhao, Xintong Wang</p>
<p><strong>Summary:</strong> In model-based reinforcement learning, simulated experiences from the learned
model are often treated as equivalent to experience from the real environment.
However, when the model is inaccurate, it can catastrophically interfere with
policy learning. Alternatively, the agent might learn about the model's
accuracy and selectively use it only when it can provide reliable predictions.
We empirically explore model uncertainty measures for selective planning and
show that best results require distri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.01923v1' target='_blank'>Scalable Signal Temporal Logic Guided Reinforcement Learning via Value
  Function Space Optimization</a></h2>
<p><strong>Authors:</strong> Yiting He, Peiran Liu, Yiding Ji</p>
<p><strong>Summary:</strong> The integration of reinforcement learning (RL) and formal methods has emerged
as a promising framework for solving long-horizon planning problems.
Conventional approaches typically involve abstraction of the state and action
spaces and manually created labeling functions or predicates. However, the
efficiency of these approaches deteriorates as the tasks become increasingly
complex, which results in exponential growth in the size of labeling functions
or predicates. To address these issues, we p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.00214v1' target='_blank'>Harnessing Reinforcement Learning for Neural Motion Planning</a></h2>
<p><strong>Authors:</strong> Tom Jurgenson, Aviv Tamar</p>
<p><strong>Summary:</strong> Motion planning is an essential component in most of today's robotic
applications. In this work, we consider the learning setting, where a set of
solved motion planning problems is used to improve the efficiency of motion
planning on different, yet similar problems. This setting is important in
applications with rapidly changing environments such as in e-commerce, among
others. We investigate a general deep learning based approach, where a neural
network is trained to map an image of the domain,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.10876v2' target='_blank'>Flexible and Efficient Long-Range Planning Through Curious Exploration</a></h2>
<p><strong>Authors:</strong> Aidan Curtis, Minjian Xin, Dilip Arumugam, Kevin Feigelis, Daniel Yamins</p>
<p><strong>Summary:</strong> Identifying algorithms that flexibly and efficiently discover
temporally-extended multi-phase plans is an essential step for the advancement
of robotics and model-based reinforcement learning. The core problem of
long-range planning is finding an efficient way to search through the tree of
possible action sequences. Existing non-learned planning solutions from the
Task and Motion Planning (TAMP) literature rely on the existence of logical
descriptions for the effects and preconditions for action...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.07371v1' target='_blank'>Learning to Plan Hierarchically from Curriculum</a></h2>
<p><strong>Authors:</strong> Philippe Morere, Lionel Ott, Fabio Ramos</p>
<p><strong>Summary:</strong> We present a framework for learning to plan hierarchically in domains with
unknown dynamics. We enhance planning performance by exploiting problem
structure in several ways: (i) We simplify the search over plans by leveraging
knowledge of skill objectives, (ii) Shorter plans are generated by enforcing
aggressively hierarchical planning, (iii) We learn transition dynamics with
sparse local models for better generalisation. Our framework decomposes
transition dynamics into skill effects and succes...</p>
<hr>
<h2><a href='http://arxiv.org/abs/0912.5029v1' target='_blank'>Complexity of stochastic branch and bound methods for belief tree search
  in Bayesian reinforcement learning</a></h2>
<p><strong>Authors:</strong> Christos Dimitrakakis</p>
<p><strong>Summary:</strong> There has been a lot of recent work on Bayesian methods for reinforcement
learning exhibiting near-optimal online performance. The main obstacle facing
such methods is that in most problems of interest, the optimal solution
involves planning in an infinitely large tree. However, it is possible to
obtain stochastic lower and upper bounds on the value of each tree node. This
enables us to use stochastic branch and bound algorithms to search the tree
efficiently. This paper proposes two such algori...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1712.07887v1' target='_blank'>Multiagent-based Participatory Urban Simulation through Inverse
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Soma Suzuki</p>
<p><strong>Summary:</strong> The multiagent-based participatory simulation features prominently in urban
planning as the acquired model is considered as the hybrid system of the domain
and the local knowledge. However, the key problem of generating realistic
agents for particular social phenomena invariably remains. The existing models
have attempted to dictate the factors involving human behavior, which appeared
to be intractable. In this paper, Inverse Reinforcement Learning (IRL) is
introduced to address this problem. IR...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.00128v1' target='_blank'>Towards a Simple Approach to Multi-step Model-based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman</p>
<p><strong>Summary:</strong> When environmental interaction is expensive, model-based reinforcement
learning offers a solution by planning ahead and avoiding costly mistakes.
Model-based agents typically learn a single-step transition model. In this
paper, we propose a multi-step model that predicts the outcome of an action
sequence with variable length. We show that this model is easy to learn, and
that the model can make policy-conditional predictions. We report preliminary
results that show a clear advantage for the mult...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.04411v3' target='_blank'>Learning to Paint With Model-based Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhewei Huang, Wen Heng, Shuchang Zhou</p>
<p><strong>Summary:</strong> We show how to teach machines to paint like human painters, who can use a
small number of strokes to create fantastic paintings. By employing a neural
renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to
determine the position and color of each stroke and make long-term plans to
decompose texture-rich images into strokes. Experiments demonstrate that
excellent visual effects can be achieved using hundreds of strokes. The
training process does not require the experience ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.09885v1' target='_blank'>Temporal Logic Guided Safe Reinforcement Learning Using Control Barrier
  Functions</a></h2>
<p><strong>Authors:</strong> Xiao Li, Calin Belta</p>
<p><strong>Summary:</strong> Using reinforcement learning to learn control policies is a challenge when
the task is complex with potentially long horizons. Ensuring adequate but safe
exploration is also crucial for controlling physical systems. In this paper, we
use temporal logic to facilitate specification and learning of complex tasks.
We combine temporal logic with control Lyapunov functions to improve
exploration. We incorporate control barrier functions to safeguard the
exploration and deployment process. We develop a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.09950v2' target='_blank'>Delta Schema Network in Model-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Andrey Gorodetskiy, Alexandra Shlychkova, Aleksandr I. Panov</p>
<p><strong>Summary:</strong> This work is devoted to unresolved problems of Artificial General
Intelligence - the inefficiency of transfer learning. One of the mechanisms
that are used to solve this problem in the area of reinforcement learning is a
model-based approach. In the paper we are expanding the schema networks method
which allows to extract the logical relationships between objects and actions
from the environment data. We present algorithms for training a Delta Schema
Network (DSN), predicting future states of th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.10960v1' target='_blank'>Adaptive Traffic Control with Deep Reinforcement Learning: Towards
  State-of-the-art and Beyond</a></h2>
<p><strong>Authors:</strong> Siavash Alemzadeh, Ramin Moslemi, Ratnesh Sharma, Mehran Mesbahi</p>
<p><strong>Summary:</strong> In this work, we study adaptive data-guided traffic planning and control
using Reinforcement Learning (RL). We shift from the plain use of classic
methods towards state-of-the-art in deep RL community. We embed several recent
techniques in our algorithm that improve the original Deep Q-Networks (DQN) for
discrete control and discuss the traffic-related interpretations that follow.
We propose a novel DQN-based algorithm for Traffic Control (called TC-DQN+) as
a tool for fast and more reliable tra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.16361v2' target='_blank'>Towards Preference Learning for Autonomous Ground Robot Navigation Tasks</a></h2>
<p><strong>Authors:</strong> Cory Hayes, Matthew Marge</p>
<p><strong>Summary:</strong> We are interested in the design of autonomous robot behaviors that learn the
preferences of users over continued interactions, with the goal of efficiently
executing navigation behaviors in a way that the user expects. In this paper,
we discuss our work in progress to modify a general model for robot navigation
behaviors in an exploration task on a per-user basis using preference-based
reinforcement learning. The novel contribution of this approach is that it
combines reinforcement learning, mot...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.01072v3' target='_blank'>Deep Residual Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson</p>
<p><strong>Summary:</strong> We revisit residual algorithms in both model-free and model-based
reinforcement learning settings. We propose the bidirectional target network
technique to stabilize residual algorithms, yielding a residual version of DDPG
that significantly outperforms vanilla DDPG in the DeepMind Control Suite
benchmark. Moreover, we find the residual algorithm an effective approach to
the distribution mismatch problem in model-based planning. Compared with the
existing TD($k$) method, our residual-based metho...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.09046v1' target='_blank'>A Deep Reinforcement Learning Driving Policy for Autonomous Road
  Vehicles</a></h2>
<p><strong>Authors:</strong> Konstantinos Makantasis, Maria Kontorinaki, Ioannis Nikolos</p>
<p><strong>Summary:</strong> This work regards our preliminary investigation on the problem of path
planning for autonomous vehicles that move on a freeway. We approach this
problem by proposing a driving policy based on Reinforcement Learning. The
proposed policy makes minimal or no assumptions about the environment, since no
a priori knowledge about the system dynamics is required. We compare the
performance of the proposed policy against an optimal policy derived via
Dynamic Programming and against manual driving simulat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.13966v1' target='_blank'>Energy-Efficient Autonomous Driving Using Cognitive Driver Behavioral
  Models and Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Huayi Li, Nan Li, Ilya Kolmanovsky, Anouck Girard</p>
<p><strong>Summary:</strong> Autonomous driving technologies are expected to not only improve mobility and
road safety but also bring energy efficiency benefits. In the foreseeable
future, autonomous vehicles (AVs) will operate on roads shared with
human-driven vehicles. To maintain safety and liveness while simultaneously
minimizing energy consumption, the AV planning and decision-making process
should account for interactions between the autonomous ego vehicle and
surrounding human-driven vehicles. In this chapter, we des...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.05433v1' target='_blank'>Comparing Model-free and Model-based Algorithms for Offline
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Phillip Swazinna, Steffen Udluft, Daniel Hein, Thomas Runkler</p>
<p><strong>Summary:</strong> Offline reinforcement learning (RL) Algorithms are often designed with
environments such as MuJoCo in mind, in which the planning horizon is extremely
long and no noise exists. We compare model-free, model-based, as well as hybrid
offline RL approaches on various industrial benchmark (IB) datasets to test the
algorithms in settings closer to real world problems, including complex noise
and partially observable states. We find that on the IB, hybrid approaches face
severe difficulties and that si...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.01965v1' target='_blank'>State Representation Learning for Goal-Conditioned Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Lorenzo Steccanella, Anders Jonsson</p>
<p><strong>Summary:</strong> This paper presents a novel state representation for reward-free Markov
decision processes. The idea is to learn, in a self-supervised manner, an
embedding space where distances between pairs of embedded states correspond to
the minimum number of actions needed to transition between them. Compared to
previous methods, our approach does not require any domain knowledge, learning
from offline and unlabeled data. We show how this representation can be
leveraged to learn goal-conditioned policies, p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.02764v3' target='_blank'>Mathematical Models and Reinforcement Learning based Evolutionary
  Algorithm Framework for Satellite Scheduling Problem</a></h2>
<p><strong>Authors:</strong> Yanjie Song</p>
<p><strong>Summary:</strong> For complex combinatorial optimization problems, models and algorithms are at
the heart of the solution. The complexity of many types of satellite mission
planning problems is NP-hard and places high demands on the solution. In this
paper, two types of satellite scheduling problem models are introduced and a
reinforcement learning based evolutionary algorithm framework based is
proposed....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.02171v1' target='_blank'>Path planning of magnetic microswimmers in high-fidelity simulations of
  capillaries with deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Lucas Amoudruz, Sergey Litvinov, Petros Koumoutsakos</p>
<p><strong>Summary:</strong> Biomedical applications such as targeted drug delivery, microsurgery or
sensing rely on reaching precise areas within the body in a minimally invasive
way. Artificial bacterial flagella (ABFs) have emerged as potential tools for
this task by navigating through the circulatory system. While the control and
swimming characteristics of ABFs is understood in simple scenarios, their
behavior within the bloodstream remains unclear. We conduct simulations of ABFs
evolving in the complex capillary netwo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.09249v2' target='_blank'>Graph Neural Networks with Model-based Reinforcement Learning for
  Multi-agent Systems</a></h2>
<p><strong>Authors:</strong> Hanxiao Chen</p>
<p><strong>Summary:</strong> Multi-agent systems (MAS) constitute a significant role in exploring machine
intelligence and advanced applications. In order to deeply investigate
complicated interactions within MAS scenarios, we originally propose "GNN for
MBRL" model, which utilizes a state-spaced Graph Neural Networks with
Model-based Reinforcement Learning to address specific MAS missions (e.g.,
Billiard-Avoidance, Autonomous Driving Cars). In detail, we firstly used GNN
model to predict future states and trajectories of m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.18962v1' target='_blank'>Autonomous Navigation of Unmanned Vehicle Through Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Letian Xu, Jiabei Liu, Haopeng Zhao, Tianyao Zheng, Tongzhou Jiang, Lipeng Liu</p>
<p><strong>Summary:</strong> This paper explores the method of achieving autonomous navigation of unmanned
vehicles through Deep Reinforcement Learning (DRL). The focus is on using the
Deep Deterministic Policy Gradient (DDPG) algorithm to address issues in
high-dimensional continuous action spaces. The paper details the model of a
Ackermann robot and the structure and application of the DDPG algorithm.
Experiments were conducted in a simulation environment to verify the
feasibility of the improved algorithm. The results de...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.09856v1' target='_blank'>Action Categorization for Computationally Improved Task Learning and
  Planning</a></h2>
<p><strong>Authors:</strong> Lakshmi Nair, Sonia Chernova</p>
<p><strong>Summary:</strong> This paper explores the problem of task learning and planning, contributing
the Action-Category Representation (ACR) to improve computational performance
of both Planning and Reinforcement Learning (RL). ACR is an algorithm-agnostic,
abstract data representation that maps objects to action categories (groups of
actions), inspired by the psychological concept of action codes. We validate
our approach in StarCraft and Lightworld domains; our results demonstrate
several benefits of ACR relating to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.10097v1' target='_blank'>Planning in Dynamic Environments with Conditional Autoregressive Models</a></h2>
<p><strong>Authors:</strong> Johanna Hansen, Kyle Kastner, Aaron Courville, Gregory Dudek</p>
<p><strong>Summary:</strong> We demonstrate the use of conditional autoregressive generative models (van
den Oord et al., 2016a) over a discrete latent space (van den Oord et al.,
2017b) for forward planning with MCTS. In order to test this method, we
introduce a new environment featuring varying difficulty levels, along with
moving goals and obstacles. The combination of high-quality frame generation
and classical planning approaches nearly matches true environment performance
for our task, demonstrating the usefulness of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.09949v1' target='_blank'>Scene Induced Multi-Modal Trajectory Forecasting via Planning</a></h2>
<p><strong>Authors:</strong> Nachiket Deo, Mohan M. Trivedi</p>
<p><strong>Summary:</strong> We address multi-modal trajectory forecasting of agents in unknown scenes by
formulating it as a planning problem. We present an approach consisting of
three models; a goal prediction model to identify potential goals of the agent,
an inverse reinforcement learning model to plan optimal paths to each goal, and
a trajectory generator to obtain future trajectories along the planned paths.
Analysis of predictions on the Stanford drone dataset, shows generalizability
of our approach to novel scenes....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.14824v1' target='_blank'>An investigation of belief-free DRL and MCTS for inspection and
  maintenance planning</a></h2>
<p><strong>Authors:</strong> Daniel Koutas, Elizabeth Bismut, Daniel Straub</p>
<p><strong>Summary:</strong> We propose a novel Deep Reinforcement Learning (DRL) architecture for
sequential decision processes under uncertainty, as encountered in inspection
and maintenance (I&M) planning. Unlike other DRL algorithms for (I&M) planning,
the proposed +RQN architecture dispenses with computing the belief state and
directly handles erroneous observations instead. We apply the algorithm to a
basic I&M planning problem for a one-component system subject to deterioration.
In addition, we investigate the perfor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.09108v1' target='_blank'>Bridging the gap between Markowitz planning and deep reinforcement
  learning</a></h2>
<p><strong>Authors:</strong> Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay</p>
<p><strong>Summary:</strong> While researchers in the asset management industry have mostly focused on
techniques based on financial and risk planning techniques like Markowitz
efficient frontier, minimum variance, maximum diversification or equal risk
parity, in parallel, another community in machine learning has started working
on reinforcement learning and more particularly deep reinforcement learning to
solve other decision making problems for challenging task like autonomous
driving, robot learning, and on a more conce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.10249v1' target='_blank'>Using Deep Reinforcement Learning Methods for Autonomous Vessels in 2D
  Environments</a></h2>
<p><strong>Authors:</strong> Mohammad Etemad, Nader Zare, Mahtab Sarvmaili, Amilcar Soares, Bruno Brandoli Machado, Stan Matwin</p>
<p><strong>Summary:</strong> Unmanned Surface Vehicles technology (USVs) is an exciting topic that
essentially deploys an algorithm to safely and efficiently performs a mission.
Although reinforcement learning is a well-known approach to modeling such a
task, instability and divergence may occur when combining off-policy and
function approximation. In this work, we used deep reinforcement learning
combining Q-learning with a neural representation to avoid instability. Our
methodology uses deep q-learning and combines it wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.05205v2' target='_blank'>Real-time scheduling of renewable power systems through planning-based
  reinforcement learning</a></h2>
<p><strong>Authors:</strong> Shaohuai Liu, Jinbo Liu, Weirui Ye, Nan Yang, Guanglun Zhang, Haiwang Zhong, Chongqing Kang, Qirong Jiang, Xuri Song, Fangchun Di, Yang Gao</p>
<p><strong>Summary:</strong> The growing renewable energy sources have posed significant challenges to
traditional power scheduling. It is difficult for operators to obtain accurate
day-ahead forecasts of renewable generation, thereby requiring the future
scheduling system to make real-time scheduling decisions aligning with
ultra-short-term forecasts. Restricted by the computation speed, traditional
optimization-based methods can not solve this problem. Recent developments in
reinforcement learning (RL) have demonstrated t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.03824v1' target='_blank'>Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function
  for Real-Time Strategy Tasks</a></h2>
<p><strong>Authors:</strong> Weilong Yang, Jie Zhang, Xunyun Liu, Yanqing Ye</p>
<p><strong>Summary:</strong> Effective evaluation of real-time strategy tasks requires adaptive mechanisms
to cope with dynamic and unpredictable environments. This study proposes a
method to improve evaluation functions for real-time responsiveness to
battle-field situation changes, utilizing an online reinforcement
learning-based dynam-ic weight adjustment mechanism within the real-time
strategy game. Building on traditional static evaluation functions, the method
employs gradient descent in online reinforcement learning ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.11178v2' target='_blank'>Hierarchies of Planning and Reinforcement Learning for Robot Navigation</a></h2>
<p><strong>Authors:</strong> Jan W√∂hlke, Felix Schmitt, Herke van Hoof</p>
<p><strong>Summary:</strong> Solving robotic navigation tasks via reinforcement learning (RL) is
challenging due to their sparse reward and long decision horizon nature.
However, in many navigation tasks, high-level (HL) task representations, like a
rough floor plan, are available. Previous work has demonstrated efficient
learning by hierarchal approaches consisting of path planning in the HL
representation and using sub-goals derived from the plan to guide the RL policy
in the source task. However, these approaches usually...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.00521v1' target='_blank'>Improving Human Decision-Making by Discovering Efficient Strategies for
  Hierarchical Planning</a></h2>
<p><strong>Authors:</strong> Saksham Consul, Lovis Heindrich, Jugoslav Stojcheski, Falk Lieder</p>
<p><strong>Summary:</strong> To make good decisions in the real world people need efficient planning
strategies because their computational resources are limited. Knowing which
planning strategies would work best for people in different situations would be
very useful for understanding and improving human decision-making. But our
ability to compute those strategies used to be limited to very small and very
simple planning tasks. To overcome this computational bottleneck, we introduce
a cognitively-inspired reinforcement lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.13619v4' target='_blank'>A review of mobile robot motion planning methods: from classical motion
  planning workflows to reinforcement learning-based architectures</a></h2>
<p><strong>Authors:</strong> Lu Dong, Zichen He, Chunwei Song, Changyin Sun</p>
<p><strong>Summary:</strong> Motion planning is critical to realize the autonomous operation of mobile
robots. As the complexity and randomness of robot application scenarios
increase, the planning capability of the classical hierarchical motion planners
is challenged. With the development of machine learning, deep reinforcement
learning (DRL)-based motion planner has gradually become a research hotspot due
to its several advantageous features. DRL-based motion planner is model-free
and does not rely on the prior structured...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.09765v2' target='_blank'>Generative Planning for Temporally Coordinated Exploration in
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haichao Zhang, Wei Xu, Haonan Yu</p>
<p><strong>Summary:</strong> Standard model-free reinforcement learning algorithms optimize a policy that
generates the action to be taken in the current time step in order to maximize
expected future return. While flexible, it faces difficulties arising from the
inefficient exploration due to its single step nature. In this work, we present
Generative Planning method (GPM), which can generate actions not only for the
current step, but also for a number of future steps (thus termed as generative
planning). This brings sever...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.00492v1' target='_blank'>Reinforcement Learning Based User-Guided Motion Planning for Human-Robot
  Collaboration</a></h2>
<p><strong>Authors:</strong> Tian Yu, Qing Chang</p>
<p><strong>Summary:</strong> Robots are good at performing repetitive tasks in modern manufacturing
industries. However, robot motions are mostly planned and preprogrammed with a
notable lack of adaptivity to task changes. Even for slightly changed tasks,
the whole system must be reprogrammed by robotics experts. Therefore, it is
highly desirable to have a flexible motion planning method, with which robots
can adapt to specific task changes in unstructured environments, such as
production systems or warehouses, with little ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1203.3518v1' target='_blank'>Variance-Based Rewards for Approximate Bayesian Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jonathan Sorg, Satinder Singh, Richard L. Lewis</p>
<p><strong>Summary:</strong> The explore{exploit dilemma is one of the central challenges in Reinforcement
Learning (RL). Bayesian RL solves the dilemma by providing the agent with
information in the form of a prior distribution over environments; however,
full Bayesian planning is intractable. Planning with the mean MDP is a common
myopic approximation of Bayesian planning. We derive a novel reward bonus that
is a function of the posterior distribution over environments, which, when
added to the reward in planning with the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1304.7168v1' target='_blank'>Non Deterministic Logic Programs</a></h2>
<p><strong>Authors:</strong> Emad Saad</p>
<p><strong>Summary:</strong> Non deterministic applications arise in many domains, including, stochastic
optimization, multi-objectives optimization, stochastic planning, contingent
stochastic planning, reinforcement learning, reinforcement learning in
partially observable Markov decision processes, and conditional planning. We
present a logic programming framework called non deterministic logic programs,
along with a declarative semantics and fixpoint semantics, to allow
representing and reasoning about inherently non dete...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.05898v1' target='_blank'>Improving width-based planning with compact policies</a></h2>
<p><strong>Authors:</strong> Miquel Junyent, Anders Jonsson, Vicen√ß G√≥mez</p>
<p><strong>Summary:</strong> Optimal action selection in decision problems characterized by sparse,
delayed rewards is still an open challenge. For these problems, current deep
reinforcement learning methods require enormous amounts of data to learn
controllers that reach human-level performance. In this work, we propose a
method that interleaves planning and learning to address this issue. The
planning step hinges on the Iterated-Width (IW) planner, a state of the art
planner that makes explicit use of the state representa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.12255v3' target='_blank'>Harnessing Structures for Value-Based Planning and Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Yuzhe Yang, Guo Zhang, Zhi Xu, Dina Katabi</p>
<p><strong>Summary:</strong> Value-based methods constitute a fundamental methodology in planning and deep
reinforcement learning (RL). In this paper, we propose to exploit the
underlying structures of the state-action value function, i.e., Q function, for
both planning and deep RL. In particular, if the underlying system dynamics
lead to some global structures of the Q function, one should be capable of
inferring the function better by leveraging such structures. Specifically, we
investigate the low-rank structure, which w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.11206v1' target='_blank'>Learning to Combat Compounding-Error in Model-Based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Chenjun Xiao, Yifan Wu, Chen Ma, Dale Schuurmans, Martin M√ºller</p>
<p><strong>Summary:</strong> Despite its potential to improve sample complexity versus model-free
approaches, model-based reinforcement learning can fail catastrophically if the
model is inaccurate. An algorithm should ideally be able to trust an imperfect
model over a reasonably long planning horizon, and only rely on model-free
updates when the model errors get infeasibly large. In this paper, we
investigate techniques for choosing the planning horizon on a state-dependent
basis, where a state's planning horizon is determ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.03327v2' target='_blank'>Reward Tweaking: Maximizing the Total Reward While Planning for Short
  Horizons</a></h2>
<p><strong>Authors:</strong> Chen Tessler, Shie Mannor</p>
<p><strong>Summary:</strong> In reinforcement learning, the discount factor $\gamma$ controls the agent's
effective planning horizon. Traditionally, this parameter was considered part
of the MDP; however, as deep reinforcement learning algorithms tend to become
unstable when the effective planning horizon is long, recent works refer to
$\gamma$ as a hyper-parameter -- thus changing the underlying MDP and
potentially leading the agent towards sub-optimal behavior on the original
task. In this work, we introduce \emph{reward ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.02097v3' target='_blank'>A Consciousness-Inspired Planning Agent for Model-Based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Mingde Zhao, Zhen Liu, Sitao Luan, Shuyuan Zhang, Doina Precup, Yoshua Bengio</p>
<p><strong>Summary:</strong> We present an end-to-end, model-based deep reinforcement learning agent which
dynamically attends to relevant parts of its state during planning. The agent
uses a bottleneck mechanism over a set-based representation to force the number
of entities to which the agent attends at each planning step to be small. In
experiments, we investigate the bottleneck mechanism with several sets of
customized environments featuring different challenges. We consistently observe
that the design allows the planni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.11527v2' target='_blank'>Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy
  Policies</a></h2>
<p><strong>Authors:</strong> Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, Shie Mannor</p>
<p><strong>Summary:</strong> State-of-the-art efficient model-based Reinforcement Learning (RL) algorithms
typically act by iteratively solving empirical models, i.e., by performing
\emph{full-planning} on Markov Decision Processes (MDPs) built by the gathered
experience. In this paper, we focus on model-based RL in the finite-state
finite-horizon MDP setting and establish that exploring with \emph{greedy
policies} -- act by \emph{1-step planning} -- can achieve tight minimax
performance in terms of regret, $\tilde{\mathcal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.01882v2' target='_blank'>Secure Planning Against Stealthy Attacks via Model-Free Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Alper Kamil Bozkurt, Yu Wang, Miroslav Pajic</p>
<p><strong>Summary:</strong> We consider the problem of security-aware planning in an unknown stochastic
environment, in the presence of attacks on control signals (i.e., actuators) of
the robot. We model the attacker as an agent who has the full knowledge of the
controller as well as the employed intrusion-detection system and who wants to
prevent the controller from performing tasks while staying stealthy. We
formulate the problem as a stochastic game between the attacker and the
controller and present an approach to expr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.12999v2' target='_blank'>Discriminator Augmented Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Behzad Haghgoo, Allan Zhou, Archit Sharma, Chelsea Finn</p>
<p><strong>Summary:</strong> By planning through a learned dynamics model, model-based reinforcement
learning (MBRL) offers the prospect of good performance with little environment
interaction. However, it is common in practice for the learned model to be
inaccurate, impairing planning and leading to poor performance. This paper aims
to improve planning with an importance sampling framework that accounts and
corrects for discrepancy between the true and learned dynamics. This framework
also motivates an alternative objectiv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00044v2' target='_blank'>Trajectory Planning with Deep Reinforcement Learning in High-Level
  Action Spaces</a></h2>
<p><strong>Authors:</strong> Kyle R. Williams, Rachel Schlossman, Daniel Whitten, Joe Ingram, Srideep Musuvathy, Anirudh Patel, James Pagan, Kyle A. Williams, Sam Green, Anirban Mazumdar, Julie Parish</p>
<p><strong>Summary:</strong> This paper presents a technique for trajectory planning based on continuously
parameterized high-level actions (motion primitives) of variable duration. This
technique leverages deep reinforcement learning (Deep RL) to formulate a policy
which is suitable for real-time implementation. There is no separation of
motion primitive generation and trajectory planning: each individual
short-horizon motion is formed during the Deep RL training to achieve the
full-horizon objective. Effectiveness of the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.01460v1' target='_blank'>Multi-Agent Path Planning Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mert √áetinkaya</p>
<p><strong>Summary:</strong> In this paper a deep reinforcement based multi-agent path planning approach
is introduced. The experiments are realized in a simulation environment and in
this environment different multi-agent path planning problems are produced. The
produced problems are actually similar to a vehicle routing problem and they
are solved using multi-agent deep reinforcement learning. In the simulation
environment, the model is trained on different consecutive problems in this way
and, as the time passes, it is o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.09967v1' target='_blank'>A Fully Controllable Agent in the Path Planning using Goal-Conditioned
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> GyeongTaek Lee</p>
<p><strong>Summary:</strong> The aim of path planning is to reach the goal from starting point by
searching for the route of an agent. In the path planning, the routes may vary
depending on the number of variables such that it is important for the agent to
reach various goals. Numerous studies, however, have dealt with a single goal
that is predefined by the user. In the present study, I propose a novel
reinforcement learning framework for a fully controllable agent in the path
planning. To do this, I propose a bi-direction...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.11790v1' target='_blank'>Hierarchical Planning Through Goal-Conditioned Offline Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Jinning Li, Chen Tang, Masayoshi Tomizuka, Wei Zhan</p>
<p><strong>Summary:</strong> Offline Reinforcement learning (RL) has shown potent in many safe-critical
tasks in robotics where exploration is risky and expensive. However, it still
struggles to acquire skills in temporally extended tasks. In this paper, we
study the problem of offline RL for temporally extended tasks. We propose a
hierarchical planning framework, consisting of a low-level goal-conditioned RL
policy and a high-level goal planner. The low-level policy is trained via
offline RL. We improve the offline trainin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.01963v1' target='_blank'>Multi-Start Team Orienteering Problem for UAS Mission Re-Planning with
  Data-Efficient Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dong Ho Lee, Jaemyung Ahn</p>
<p><strong>Summary:</strong> In this paper, we study the Multi-Start Team Orienteering Problem (MSTOP), a
mission re-planning problem where vehicles are initially located away from the
depot and have different amounts of fuel. We consider/assume the goal of
multiple vehicles is to travel to maximize the sum of collected profits under
resource (e.g., time, fuel) consumption constraints. Such re-planning problems
occur in a wide range of intelligent UAS applications where changes in the
mission environment force the operation...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.12794v1' target='_blank'>Learned Graph Rewriting with Equality Saturation: A New Paradigm in
  Relational Query Rewrite and Beyond</a></h2>
<p><strong>Authors:</strong> George-Octavian BƒÉrbulescu, Taiyi Wang, Zak Singh, Eiko Yoneki</p>
<p><strong>Summary:</strong> Query rewrite systems perform graph substitutions using rewrite rules to
generate optimal SQL query plans. Rewriting logical and physical relational
query plans is proven to be an NP-hard sequential decision-making problem with
a search space exponential in the number of rewrite rules. In this paper, we
address the query rewrite problem by interleaving Equality Saturation and Graph
Reinforcement Learning (RL). The proposed system, Aurora, rewrites relational
queries by guiding Equality Saturatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.00275v4' target='_blank'>RESC: A Reinforcement Learning Based Search-to-Control Framework for
  Quadrotor Local Planning in Dense Environments</a></h2>
<p><strong>Authors:</strong> Zhaohong Liu, Wenxuan Gao, Yinshuai Sun, Peng Dong</p>
<p><strong>Summary:</strong> Agile flight in complex environments poses significant challenges to current
motion planning methods, as they often fail to fully leverage the quadrotor
dynamic potential, leading to performance failures and reduced efficiency
during aggressive maneuvers.Existing approaches frequently decouple trajectory
optimization from control generation and neglect the dynamics, further limiting
their ability to generate aggressive and feasible motions.To address these
challenges, we introduce an enhanced Se...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06953v2' target='_blank'>Off-road Autonomous Vehicles Traversability Analysis and Trajectory
  Planning Based on Deep Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zeyu Zhu, Nan Li, Ruoyu Sun, Huijing Zhao, Donghao Xu</p>
<p><strong>Summary:</strong> Terrain traversability analysis is a fundamental issue to achieve the
autonomy of a robot at off-road environments. Geometry-based and
appearance-based methods have been studied in decades, while behavior-based
methods exploiting learning from demonstration (LfD) are new trends.
Behavior-based methods learn cost functions that guide trajectory planning in
compliance with experts' demonstrations, which can be more scalable to various
scenes and driving behaviors. This research proposes a method o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.15724v1' target='_blank'>MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement
  Learning in Mixed Dynamic Environments</a></h2>
<p><strong>Authors:</strong> Zuxin Liu, Baiming Chen, Hongyi Zhou, Guru Koushik, Martial Hebert, Ding Zhao</p>
<p><strong>Summary:</strong> Multi-agent navigation in dynamic environments is of great industrial value
when deploying a large scale fleet of robot to real-world applications. This
paper proposes a decentralized partially observable multi-agent path planning
with evolutionary reinforcement learning (MAPPER) method to learn an effective
local planning policy in mixed dynamic environments. Reinforcement
learning-based methods usually suffer performance degradation on long-horizon
tasks with goal-conditioned sparse rewards, s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.06906v2' target='_blank'>Model-based Reinforcement Learning for Decentralized Multiagent
  Rendezvous</a></h2>
<p><strong>Authors:</strong> Rose E. Wang, J. Chase Kew, Dennis Lee, Tsang-Wei Edward Lee, Tingnan Zhang, Brian Ichter, Jie Tan, Aleksandra Faust</p>
<p><strong>Summary:</strong> Collaboration requires agents to align their goals on the fly. Underlying the
human ability to align goals with other agents is their ability to predict the
intentions of others and actively update their own plans. We propose
hierarchical predictive planning (HPP), a model-based reinforcement learning
method for decentralized multiagent rendezvous. Starting with pretrained,
single-agent point to point navigation policies and using noisy,
high-dimensional sensor inputs like lidar, we first learn ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.14567v1' target='_blank'>Plan-Space State Embeddings for Improved Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Max Pflueger, Gaurav S. Sukhatme</p>
<p><strong>Summary:</strong> Robot control problems are often structured with a policy function that maps
state values into control values, but in many dynamic problems the observed
state can have a difficult to characterize relationship with useful policy
actions. In this paper we present a new method for learning state embeddings
from plans or other forms of demonstrations such that the embedding space has a
specified geometric relationship with the demonstrations. We present a novel
variational framework for learning the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.12544v1' target='_blank'>Human-Level Reinforcement Learning through Theory-Based Modeling,
  Exploration, and Planning</a></h2>
<p><strong>Authors:</strong> Pedro A. Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy, Samuel J. Gershman, Joshua B. Tenenbaum</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) studies how an agent comes to achieve reward in
an environment through interactions over time. Recent advances in machine RL
have surpassed human expertise at the world's oldest board games and many
classic video games, but they require vast quantities of experience to learn
successfully -- none of today's algorithms account for the human ability to
learn so many different tasks, so quickly. Here we propose a new approach to
this challenge based on a particularly stro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.11097v3' target='_blank'>UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning
  Leveraging Planning</a></h2>
<p><strong>Authors:</strong> Christopher Diehl, Timo Sievernich, Martin Kr√ºger, Frank Hoffmann, Torsten Bertram</p>
<p><strong>Summary:</strong> Offline reinforcement learning (RL) provides a framework for learning
decision-making from offline data and therefore constitutes a promising
approach for real-world applications as automated driving. Self-driving
vehicles (SDV) learn a policy, which potentially even outperforms the behavior
in the sub-optimal data set. Especially in safety-critical applications as
automated driving, explainability and transferability are key to success. This
motivates the use of model-based offline RL approache...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.15968v1' target='_blank'>Action and Trajectory Planning for Urban Autonomous Driving with
  Hierarchical Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xinyang Lu, Flint Xiaofeng Fan, Tianying Wang</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) has made promising progress in planning and
decision-making for Autonomous Vehicles (AVs) in simple driving scenarios.
However, existing RL algorithms for AVs fail to learn critical driving skills
in complex urban scenarios. First, urban driving scenarios require AVs to
handle multiple driving tasks of which conventional RL algorithms are
incapable. Second, the presence of other vehicles in urban scenarios results in
a dynamically changing environment, which challenge...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.13863v2' target='_blank'>Dynamic Model Predictive Shielding for Provably Safe Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Arko Banerjee, Kia Rahmani, Joydeep Biswas, Isil Dillig</p>
<p><strong>Summary:</strong> Among approaches for provably safe reinforcement learning, Model Predictive
Shielding (MPS) has proven effective at complex tasks in continuous,
high-dimensional state spaces, by leveraging a backup policy to ensure safety
when the learned policy attempts to take risky actions. However, while MPS can
ensure safety both during and after training, it often hinders task progress
due to the conservative and task-oblivious nature of backup policies. This
paper introduces Dynamic Model Predictive Shie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.12756v2' target='_blank'>Navigating Demand Uncertainty in Container Shipping: Deep Reinforcement
  Learning for Enabling Adaptive and Feasible Master Stowage Planning</a></h2>
<p><strong>Authors:</strong> Jaike van Twiller, Yossiri Adulyasak, Erick Delage, Djordje Grbic, Rune M√∏ller Jensen</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has shown promise in solving various
combinatorial optimization problems. However, conventional RL faces challenges
when dealing with real-world constraints, especially when action space
feasibility is explicit and dependent on the corresponding state or trajectory.
In this work, we focus on using RL in container shipping, often considered the
cornerstone of global trade, by dealing with the critical challenge of master
stowage planning. The main objective is to maxim...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.06311v2' target='_blank'>Leveraging Statistical Multi-Agent Online Planning with Emergent Value
  Function Approximation</a></h2>
<p><strong>Authors:</strong> Thomy Phan, Lenz Belzner, Thomas Gabor, Kyrill Schmid</p>
<p><strong>Summary:</strong> Making decisions is a great challenge in distributed autonomous environments
due to enormous state spaces and uncertainty. Many online planning algorithms
rely on statistical sampling to avoid searching the whole state space, while
still being able to make acceptable decisions. However, planning often has to
be performed under strict computational constraints making online planning in
multi-agent systems highly limited, which could lead to poor system
performance, especially in stochastic domain...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.09232v4' target='_blank'>Using reinforcement learning to autonomously identify sources of error
  for agents in group missions</a></h2>
<p><strong>Authors:</strong> Keishu Utimula, Ken-taro Hayaschi, Trevor J. Bihl, Kenta Hongo, Ryo Maezono</p>
<p><strong>Summary:</strong> When agents swarm to execute a mission, some of them frequently exhibit
sudden failure, as observed from the command base. It is generally difficult to
determine whether a failure is caused by actuators (hypothesis, $h_a$) or
sensors (hypothesis, $h_s$) by solely relying on the communication between the
command base and concerning agent. However, by instigating collusion between
the agents, the cause of failure can be identified; in other words, we expect
to detect corresponding displacements fo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1705.08439v4' target='_blank'>Thinking Fast and Slow with Deep Learning and Tree Search</a></h2>
<p><strong>Authors:</strong> Thomas Anthony, Zheng Tian, David Barber</p>
<p><strong>Summary:</strong> Sequential decision making problems, such as structured prediction, robotic
control, and game playing, require a combination of planning policies and
generalisation of those plans. In this paper, we present Expert Iteration
(ExIt), a novel reinforcement learning algorithm which decomposes the problem
into separate planning and generalisation tasks. Planning new policies is
performed by tree search, while a deep neural network generalises those plans.
Subsequently, tree search is improved by usin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.14830v2' target='_blank'>Reinforcement Learning for Classical Planning: Viewing Heuristics as
  Dense Reward Generators</a></h2>
<p><strong>Authors:</strong> Clement Gehring, Masataro Asai, Rohan Chitnis, Tom Silver, Leslie Pack Kaelbling, Shirin Sohrabi, Michael Katz</p>
<p><strong>Summary:</strong> Recent advances in reinforcement learning (RL) have led to a growing interest
in applying RL to classical planning domains or applying classical planning
methods to some complex RL domains. However, the long-horizon goal-based
problems found in classical planning lead to sparse rewards for RL, making
direct application inefficient. In this paper, we propose to leverage
domain-independent heuristic functions commonly used in the classical planning
literature to improve the sample efficiency of RL...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.04813v2' target='_blank'>Bimanual Regrasping for Suture Needles using Reinforcement Learning for
  Rapid Motion Planning</a></h2>
<p><strong>Authors:</strong> Zih-Yun Chiu, Florian Richter, Emily K. Funk, Ryan K. Orosco, Michael C. Yip</p>
<p><strong>Summary:</strong> Regrasping a suture needle is an important yet time-consuming process in
suturing. To bring efficiency into regrasping, prior work either designs a
task-specific mechanism or guides the gripper toward some specific pick-up
point for proper grasping of a needle. Yet, these methods are usually not
deployable when the working space is changed. Therefore, in this work, we
present rapid trajectory generation for bimanual needle regrasping via
reinforcement learning (RL). Demonstrations from a samplin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.12491v3' target='_blank'>World Model as a Graph: Learning Latent Landmarks for Planning</a></h2>
<p><strong>Authors:</strong> Lunjun Zhang, Ge Yang, Bradly C. Stadie</p>
<p><strong>Summary:</strong> Planning - the ability to analyze the structure of a problem in the large and
decompose it into interrelated subproblems - is a hallmark of human
intelligence. While deep reinforcement learning (RL) has shown great promise
for solving relatively straightforward control tasks, it remains an open
problem how to best incorporate planning into existing deep RL paradigms to
handle increasingly complex environments. One prominent framework, Model-Based
RL, learns a world model and plans using step-by-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.07560v2' target='_blank'>Skill-based Model-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Lucy Xiaoyang Shi, Joseph J. Lim, Youngwoon Lee</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL) is a sample-efficient way of learning
complex behaviors by leveraging a learned single-step dynamics model to plan
actions in imagination. However, planning every action for long-horizon tasks
is not practical, akin to a human planning out every muscle movement. Instead,
humans efficiently plan with high-level skills to solve complex tasks. From
this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that
enables planning in the skill spa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.13060v3' target='_blank'>Road Planning for Slums via Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yu Zheng, Hongyuan Su, Jingtao Ding, Depeng Jin, Yong Li</p>
<p><strong>Summary:</strong> Millions of slum dwellers suffer from poor accessibility to urban services
due to inadequate road infrastructure within slums, and road planning for slums
is critical to the sustainable development of cities. Existing re-blocking or
heuristic methods are either time-consuming which cannot generalize to
different slums, or yield sub-optimal road plans in terms of accessibility and
construction costs. In this paper, we present a deep reinforcement learning
based approach to automatically layout ro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.13783v2' target='_blank'>Deep Reinforcement Learning-based Multi-objective Path Planning on the
  Off-road Terrain Environment for Ground Vehicles</a></h2>
<p><strong>Authors:</strong> Shuqiao Huang, Xiru Wu, Guoming Huang</p>
<p><strong>Summary:</strong> Due to the vastly different energy consumption between up-slope and
down-slope, a path with the shortest length on a complex off-road terrain
environment (2.5D map) is not always the path with the least energy
consumption. For any energy-sensitive vehicle, realizing a good trade-off
between distance and energy consumption in 2.5D path planning is significantly
meaningful. In this paper, we propose a deep reinforcement learning-based 2.5D
multi-objective path planning method (DMOP). The DMOP can ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.10675v1' target='_blank'>Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End
  LLM Plan Generation</a></h2>
<p><strong>Authors:</strong> Sukai Huang, Trevor Cohn, Nir Lipovetzky</p>
<p><strong>Summary:</strong> The capability of Large Language Models (LLMs) to plan remains a topic of
debate. Some critics argue that strategies to boost LLMs' reasoning skills are
ineffective in planning tasks, while others report strong outcomes merely from
training models on a planning corpus. This study reassesses recent strategies
by developing an end-to-end LLM planner and employing diverse metrics for a
thorough evaluation. We find that merely fine-tuning LLMs on a corpus of
planning instances does not lead to robus...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1709.09761v1' target='_blank'>The detour problem in a stochastic environment: Tolman revisited</a></h2>
<p><strong>Authors:</strong> Pegah Fakhari, Arash Khodadadi, Jerome Busemeyer</p>
<p><strong>Summary:</strong> We designed a grid world task to study human planning and re-planning
behavior in an unknown stochastic environment. In our grid world, participants
were asked to travel from a random starting point to a random goal position
while maximizing their reward. Because they were not familiar with the
environment, they needed to learn its characteristics from experience to plan
optimally. Later in the task, we randomly blocked the optimal path to
investigate whether and how people adjust their original...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.00645v2' target='_blank'>Universal Planning Networks</a></h2>
<p><strong>Authors:</strong> Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea Finn</p>
<p><strong>Summary:</strong> A key challenge in complex visuomotor control is learning abstract
representations that are effective for specifying goals, planning, and
generalization. To this end, we introduce universal planning networks (UPN).
UPNs embed differentiable planning within a goal-directed policy. This planning
computation unrolls a forward model in a latent space and infers an optimal
action plan through gradient descent trajectory optimization. The
plan-by-gradient-descent process and its underlying representat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.14195v2' target='_blank'>Mobile Robot Path Planning in Dynamic Environments: A Survey</a></h2>
<p><strong>Authors:</strong> Kuanqi Cai, Chaoqun Wang, Jiyu Cheng, Clarence W De Silva, Max Q. -H. Meng</p>
<p><strong>Summary:</strong> There are many challenges for robot navigation in densely populated dynamic
environments. This paper presents a survey of the path planning methods for
robot navigation in dense environments. Particularly, the path planning in the
navigation framework of mobile robots is composed of global path planning and
local path planning, with regard to the planning scope and the executability.
Within this framework, the recent progress of the path planning methods is
presented in the paper, while examinin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.11240v2' target='_blank'>Dynamic Planning Networks</a></h2>
<p><strong>Authors:</strong> Norman Tasfi, Miriam Capretz</p>
<p><strong>Summary:</strong> We introduce Dynamic Planning Networks (DPN), a novel architecture for deep
reinforcement learning, that combines model-based and model-free aspects for
online planning. Our architecture learns to dynamically construct plans using a
learned state-transition model by selecting and traversing between simulated
states and actions to maximize information before acting. In contrast to
model-free methods, model-based planning lets the agent efficiently test action
hypotheses without performing costly ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.14993v2' target='_blank'>Thinker: Learning to Plan and Act</a></h2>
<p><strong>Authors:</strong> Stephen Chung, Ivan Anokhin, David Krueger</p>
<p><strong>Summary:</strong> We propose the Thinker algorithm, a novel approach that enables reinforcement
learning agents to autonomously interact with and utilize a learned world
model. The Thinker algorithm wraps the environment with a world model and
introduces new actions designed for interacting with the world model. These
model-interaction actions enable agents to perform planning by proposing
alternative plans to the world model before selecting a final action to execute
in the environment. This approach eliminates ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.01698v1' target='_blank'>Large language model empowered participatory urban planning</a></h2>
<p><strong>Authors:</strong> Zhilun Zhou, Yuming Lin, Yong Li</p>
<p><strong>Summary:</strong> Participatory urban planning is the mainstream of modern urban planning and
involves the active engagement of different stakeholders. However, the
traditional participatory paradigm encounters challenges in time and manpower,
while the generative planning tools fail to provide adjustable and inclusive
solutions. This research introduces an innovative urban planning approach
integrating Large Language Models (LLMs) within the participatory process. The
framework, based on the crafted LLM agent, c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1304.1819v1' target='_blank'>Model-based Bayesian Reinforcement Learning for Dialogue Management</a></h2>
<p><strong>Authors:</strong> Pierre Lison</p>
<p><strong>Summary:</strong> Reinforcement learning methods are increasingly used to optimise dialogue
policies from experience. Most current techniques are model-free: they directly
estimate the utility of various actions, without explicit model of the
interaction dynamics. In this paper, we investigate an alternative strategy
grounded in model-based Bayesian reinforcement learning. Bayesian inference is
used to maintain a posterior distribution over the model parameters, reflecting
the model uncertainty. This parameter di...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1612.04318v1' target='_blank'>Incorporating Human Domain Knowledge into Large Scale Cost Function
  Learning</a></h2>
<p><strong>Authors:</strong> Markus Wulfmeier, Dushyant Rao, Ingmar Posner</p>
<p><strong>Summary:</strong> Recent advances have shown the capability of Fully Convolutional Neural
Networks (FCN) to model cost functions for motion planning in the context of
learning driving preferences purely based on demonstration data from human
drivers. While pure learning from demonstrations in the framework of Inverse
Reinforcement Learning (IRL) is a promising approach, we can benefit from well
informed human priors and incorporate them into the learning process. Our work
achieves this by pretraining a model to r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1704.08795v2' target='_blank'>Mapping Instructions and Visual Observations to Actions with
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dipendra Misra, John Langford, Yoav Artzi</p>
<p><strong>Summary:</strong> We propose to directly map raw visual observations and text input to actions
for instruction execution. While existing approaches assume access to
structured environment representations or use a pipeline of separately trained
models, we learn a single model to jointly reason about linguistic and visual
input. We use reinforcement learning in a contextual bandit setting to train a
neural network agent. To guide the agent's exploration, we use reward shaping
with different forms of supervision. Ou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1705.00470v2' target='_blank'>Learning Multimodal Transition Dynamics for Model-Based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker</p>
<p><strong>Summary:</strong> In this paper we study how to learn stochastic, multimodal transition
dynamics in reinforcement learning (RL) tasks. We focus on evaluating
transition function estimation, while we defer planning over this model to
future work. Stochasticity is a fundamental property of many task environments.
However, discriminative function approximators have difficulty estimating
multimodal stochasticity. In contrast, deep generative models do capture
complex high-dimensional outcome distributions. First we d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1706.04546v2' target='_blank'>Reinforcement Learning with Budget-Constrained Nonparametric Function
  Approximation for Opportunistic Spectrum Access</a></h2>
<p><strong>Authors:</strong> Theodoros Tsiligkaridis, David Romero</p>
<p><strong>Summary:</strong> Opportunistic spectrum access is one of the emerging techniques for
maximizing throughput in congested bands and is enabled by predicting idle
slots in spectrum. We propose a kernel-based reinforcement learning approach
coupled with a novel budget-constrained sparsification technique that
efficiently captures the environment to find the best channel access actions.
This approach allows learning and planning over the intrinsic state-action
space and extends well to large state spaces. We apply ou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1707.05878v1' target='_blank'>On-line Building Energy Optimization using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Elena Mocanu, Decebal Constantin Mocanu, Phuong H. Nguyen, Antonio Liotta, Michael E. Webber, Madeleine Gibescu, J. G. Slootweg</p>
<p><strong>Summary:</strong> Unprecedented high volumes of data are becoming available with the growth of
the advanced metering infrastructure. These are expected to benefit planning
and operation of the future power system, and to help the customers transition
from a passive to an active role. In this paper, we explore for the first time
in the smart grid context the benefits of using Deep Reinforcement Learning, a
hybrid type of methods that combines Reinforcement Learning with Deep Learning,
to perform on-line optimizati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.02448v1' target='_blank'>Deep Reinforcement Learning for General Video Game AI</a></h2>
<p><strong>Authors:</strong> Ruben Rodriguez Torrado, Philip Bontrager, Julian Togelius, Jialin Liu, Diego Perez-Liebana</p>
<p><strong>Summary:</strong> The General Video Game AI (GVGAI) competition and its associated software
framework provides a way of benchmarking AI algorithms on a large number of
games written in a domain-specific description language. While the competition
has seen plenty of interest, it has so far focused on online planning,
providing a forward model that allows the use of algorithms such as Monte Carlo
Tree Search.
  In this paper, we describe how we interface GVGAI to the OpenAI Gym
environment, a widely used way of con...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.03555v1' target='_blank'>Modular Architecture for StarCraft II with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dennis Lee, Haoran Tang, Jeffrey O Zhang, Huazhe Xu, Trevor Darrell, Pieter Abbeel</p>
<p><strong>Summary:</strong> We present a novel modular architecture for StarCraft II AI. The architecture
splits responsibilities between multiple modules that each control one aspect
of the game, such as build-order selection or tactics. A centralized scheduler
reviews macros suggested by all modules and decides their order of execution.
An updater keeps track of environment changes and instantiates macros into
series of executable actions. Modules in this framework can be optimized
independently or jointly via human desi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.01536v3' target='_blank'>Exploring applications of deep reinforcement learning for real-world
  autonomous driving systems</a></h2>
<p><strong>Authors:</strong> Victor Talpaert, Ibrahim Sobh, B Ravi Kiran, Patrick Mannion, Senthil Yogamani, Ahmad El-Sallab, Patrick Perez</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning (DRL) has become increasingly powerful in recent
years, with notable achievements such as Deepmind's AlphaGo. It has been
successfully deployed in commercial vehicles like Mobileye's path planning
system. However, a vast majority of work on DRL is focused on toy examples in
controlled synthetic car simulator environments such as TORCS and CARLA. In
general, DRL is still at its infancy in terms of usability in real-world
applications. Our goal in this paper is to encou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.11021v1' target='_blank'>Cooperation-Aware Reinforcement Learning for Merging in Dense Traffic</a></h2>
<p><strong>Authors:</strong> Maxime Bouton, Alireza Nakhaei, Kikuo Fujimura, Mykel J. Kochenderfer</p>
<p><strong>Summary:</strong> Decision making in dense traffic can be challenging for autonomous vehicles.
An autonomous system only relying on predefined road priorities and considering
other drivers as moving objects will cause the vehicle to freeze and fail the
maneuver. Human drivers leverage the cooperation of other drivers to avoid such
deadlock situations and convince others to change their behavior. Decision
making algorithms must reason about the interaction with other drivers and
anticipate a broad range of driver ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1908.06012v1' target='_blank'>Model-based Lookahead Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhang-Wei Hong, Joni Pajarinen, Jan Peters</p>
<p><strong>Summary:</strong> Model-based Reinforcement Learning (MBRL) allows data-efficient learning
which is required in real world applications such as robotics. However, despite
the impressive data-efficiency, MBRL does not achieve the final performance of
state-of-the-art Model-free Reinforcement Learning (MFRL) methods. We leverage
the strengths of both realms and propose an approach that obtains high
performance with a small amount of data. In particular, we combine MFRL and
Model Predictive Control (MPC). While MFRL...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.01562v1' target='_blank'>DeepRacer: Educational Autonomous Racing Platform for Experimentation
  with Sim2Real Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, Eddie Calleja, Sunil Muralidhara, Dhanasekar Karuppasamy</p>
<p><strong>Summary:</strong> DeepRacer is a platform for end-to-end experimentation with RL and can be
used to systematically investigate the key challenges in developing intelligent
control systems. Using the platform, we demonstrate how a 1/18th scale car can
learn to drive autonomously using RL with a monocular camera. It is trained in
simulation with no additional tuning in physical world and demonstrates: 1)
formulation and solution of a robust reinforcement learning algorithm, 2)
narrowing the reality gap through join...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.06940v1' target='_blank'>Reinforcement Learning with Probabilistically Complete Exploration</a></h2>
<p><strong>Authors:</strong> Philippe Morere, Gilad Francis, Tom Blau, Fabio Ramos</p>
<p><strong>Summary:</strong> Balancing exploration and exploitation remains a key challenge in
reinforcement learning (RL). State-of-the-art RL algorithms suffer from high
sample complexity, particularly in the sparse reward case, where they can do no
better than to explore in all directions until the first positive rewards are
found. To mitigate this, we propose Rapidly Randomly-exploring Reinforcement
Learning (R3L). We formulate exploration as a search problem and leverage
widely-used planning algorithms such as Rapidly-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.02579v1' target='_blank'>Causality and Batch Reinforcement Learning: Complementary Approaches To
  Planning In Unknown Domains</a></h2>
<p><strong>Authors:</strong> James Bannon, Brad Windsor, Wenbo Song, Tao Li</p>
<p><strong>Summary:</strong> Reinforcement learning algorithms have had tremendous successes in online
learning settings. However, these successes have relied on low-stakes
interactions between the algorithmic agent and its environment. In many
settings where RL could be of use, such as health care and autonomous driving,
the mistakes made by most online RL algorithms during early training come with
unacceptable costs. These settings require developing reinforcement learning
algorithms that can operate in the so-called batc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.05579v1' target='_blank'>Deep reinforcement learning for optical systems: A case study of
  mode-locked lasers</a></h2>
<p><strong>Authors:</strong> Chang Sun, Eurika Kaiser, Steven L. Brunton, J. Nathan Kutz</p>
<p><strong>Summary:</strong> We demonstrate that deep reinforcement learning (deep RL) provides a highly
effective strategy for the control and self-tuning of optical systems. Deep RL
integrates the two leading machine learning architectures of deep neural
networks and reinforcement learning to produce robust and stable learning for
control. Deep RL is ideally suited for optical systems as the tuning and
control relies on interactions with its environment with a goal-oriented
objective to achieve optimal immediate or delaye...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.00450v2' target='_blank'>Supervised Learning and Reinforcement Learning of Feedback Models for
  Reactive Behaviors: Tactile Feedback Testbed</a></h2>
<p><strong>Authors:</strong> Giovanni Sutanto, Katharina Rombach, Yevgen Chebotar, Zhe Su, Stefan Schaal, Gaurav S. Sukhatme, Franziska Meier</p>
<p><strong>Summary:</strong> Robots need to be able to adapt to unexpected changes in the environment such
that they can autonomously succeed in their tasks. However, hand-designing
feedback models for adaptation is tedious, if at all possible, making
data-driven methods a promising alternative. In this paper we introduce a full
framework for learning feedback models for reactive motion planning. Our
pipeline starts by segmenting demonstrations of a complete task into motion
primitives via a semi-automated segmentation algo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.03749v1' target='_blank'>Sharp Analysis of Smoothed Bellman Error Embedding</a></h2>
<p><strong>Authors:</strong> Ahmed Touati, Pascal Vincent</p>
<p><strong>Summary:</strong> The \textit{Smoothed Bellman Error Embedding} algorithm~\citep{dai2018sbeed},
known as SBEED, was proposed as a provably convergent reinforcement learning
algorithm with general nonlinear function approximation. It has been
successfully implemented with neural networks and achieved strong empirical
results. In this work, we study the theoretical behavior of SBEED in batch-mode
reinforcement learning. We prove a near-optimal performance guarantee that
depends on the representation power of the us...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.02868v4' target='_blank'>Reinforcement Learning in Deep Structured Teams: Initial Results with
  Finite and Infinite Valued Features</a></h2>
<p><strong>Authors:</strong> Jalal Arabneydi, Masoud Roudneshin, Amir G. Aghdam</p>
<p><strong>Summary:</strong> In this paper, we consider Markov chain and linear quadratic models for deep
structured teams with discounted and time-average cost functions under two
non-classical information structures, namely, deep state sharing and no
sharing. In deep structured teams, agents are coupled in dynamics and cost
functions through deep state, where deep state refers to a set of orthogonal
linear regressions of the states. In this article, we consider a homogeneous
linear regression for Markov chain models (i.e....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.06266v1' target='_blank'>Model-Based Reinforcement Learning for Type 1Diabetes Blood Glucose
  Control</a></h2>
<p><strong>Authors:</strong> Taku Yamagata, Aisling O'Kane, Amid Ayobi, Dmitri Katz, Katarzyna Stawarz, Paul Marshall, Peter Flach, Ra√∫l Santos-Rodr√≠guez</p>
<p><strong>Summary:</strong> In this paper we investigate the use of model-based reinforcement learning to
assist people with Type 1 Diabetes with insulin dose decisions. The proposed
architecture consists of multiple Echo State Networks to predict blood glucose
levels combined with Model Predictive Controller for planning. Echo State
Network is a version of recurrent neural networks which allows us to learn long
term dependencies in the input of time series data in an online manner.
Additionally, we address the quantificat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.12914v3' target='_blank'>Planning with Exploration: Addressing Dynamics Bottleneck in Model-based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xiyao Wang, Junge Zhang, Wenzhen Huang, Qiyue Yin</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) is believed to have higher sample
efficiency compared with model-free reinforcement learning (MFRL). However,
MBRL is plagued by dynamics bottleneck dilemma. Dynamics bottleneck dilemma is
the phenomenon that the performance of the algorithm falls into the local
optimum instead of increasing when the interaction step with the environment
increases, which means more data can not bring better performance. In this
paper, we find that the trajectory reward e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.05970v1' target='_blank'>Affordance-based Reinforcement Learning for Urban Driving</a></h2>
<p><strong>Authors:</strong> Tanmay Agarwal, Hitesh Arora, Jeff Schneider</p>
<p><strong>Summary:</strong> Traditional autonomous vehicle pipelines that follow a modular approach have
been very successful in the past both in academia and industry, which has led
to autonomy deployed on road. Though this approach provides ease of
interpretation, its generalizability to unseen environments is limited and
hand-engineering of numerous parameters is required, especially in the
prediction and planning systems. Recently, deep reinforcement learning has been
shown to learn complex strategic games and perform ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.06294v1' target='_blank'>Online and Offline Reinforcement Learning by Planning with a Learned
  Model</a></h2>
<p><strong>Authors:</strong> Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, David Silver</p>
<p><strong>Summary:</strong> Learning efficiently from small amounts of data has long been the focus of
model-based reinforcement learning, both for the online case when interacting
with the environment and the offline case when learning from a fixed dataset.
However, to date no single unified algorithm could demonstrate state-of-the-art
results in both settings. In this work, we describe the Reanalyse algorithm
which uses model-based policy and value improvement operators to compute new
improved training targets on existin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.11288v1' target='_blank'>Enhancing Navigational Safety in Crowded Environments using
  Semantic-Deep-Reinforcement-Learning-based Navigation</a></h2>
<p><strong>Authors:</strong> Linh K√§stner, Junhui Li, Zhengcheng Shen, Jens Lambrecht</p>
<p><strong>Summary:</strong> Intelligent navigation among social crowds is an essential aspect of mobile
robotics for applications such as delivery, health care, or assistance. Deep
Reinforcement Learning emerged as an alternative planning method to
conservative approaches and promises more efficient and flexible navigation.
However, in highly dynamic environments employing different kinds of obstacle
classes, safe navigation still presents a grand challenge. In this paper, we
propose a semantic Deep-reinforcement-learning-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1509.06841v3' target='_blank'>One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation
  and Neural Network Priors</a></h2>
<p><strong>Authors:</strong> Justin Fu, Sergey Levine, Pieter Abbeel</p>
<p><strong>Summary:</strong> One of the key challenges in applying reinforcement learning to complex
robotic control tasks is the need to gather large amounts of experience in
order to find an effective policy for the task at hand. Model-based
reinforcement learning can achieve good sample efficiency, but requires the
ability to learn a model of the dynamics that is good enough to learn an
effective policy. In this work, we develop a model-based reinforcement learning
algorithm that combines prior knowledge from previous ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.04674v1' target='_blank'>Hierarchical Reinforcement Learning: Approximating Optimal Discounted
  TSP Using Local Policies</a></h2>
<p><strong>Authors:</strong> Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour</p>
<p><strong>Summary:</strong> In this work, we provide theoretical guarantees for reward decomposition in
deterministic MDPs. Reward decomposition is a special case of Hierarchical
Reinforcement Learning, that allows one to learn many policies in parallel and
combine them into a composite solution. Our approach builds on mapping this
problem into a Reward Discounted Traveling Salesman Problem, and then deriving
approximate solutions for it. In particular, we focus on approximate solutions
that are local, i.e., solutions that...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.08456v1' target='_blank'>Deep Reinforcement Learning with Model Learning and Monte Carlo Tree
  Search in Minecraft</a></h2>
<p><strong>Authors:</strong> Stephan Alaniz</p>
<p><strong>Summary:</strong> Deep reinforcement learning has been successfully applied to several
visual-input tasks using model-free methods. In this paper, we propose a
model-based approach that combines learning a DNN-based transition model with
Monte Carlo tree search to solve a block-placing task in Minecraft. Our learned
transition model predicts the next frame and the rewards one step ahead given
the last four frames of the agent's first-person-view image and the current
action. Then a Monte Carlo tree search algorit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.05027v1' target='_blank'>Learning with Training Wheels: Speeding up Training with a Simple
  Controller for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Linhai Xie, Sen Wang, Stefano Rosa, Andrew Markham, Niki Trigoni</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning (DRL) has been applied successfully to many
robotic applications. However, the large number of trials needed for training
is a key issue. Most of existing techniques developed to improve training
efficiency (e.g. imitation) target on general tasks rather than being tailored
for robot applications, which have their specific context to benefit from. We
propose a novel framework, Assisted Reinforcement Learning, where a classical
controller (e.g. a PID controller) is use...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.05759v1' target='_blank'>Safer Deep RL with Shallow MCTS: A Case Study in Pommerman</a></h2>
<p><strong>Authors:</strong> Bilal Kartal, Pablo Hernandez-Leal, Chao Gao, Matthew E. Taylor</p>
<p><strong>Summary:</strong> Safe reinforcement learning has many variants and it is still an open
research problem. Here, we focus on how to use action guidance by means of a
non-expert demonstrator to avoid catastrophic events in a domain with sparse,
delayed, and deceptive rewards: the recently-proposed multi-agent benchmark of
Pommerman. This domain is very challenging for reinforcement learning (RL) ---
past work has shown that model-free RL algorithms fail to achieve significant
learning. In this paper, we shed light ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.10681v2' target='_blank'>Composing Task-Agnostic Policies with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ahmed H. Qureshi, Jacob J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, Michael C. Yip</p>
<p><strong>Summary:</strong> The composition of elementary behaviors to solve challenging transfer
learning problems is one of the key elements in building intelligent machines.
To date, there has been plenty of work on learning task-specific policies or
skills but almost no focus on composing necessary, task-agnostic skills to find
a solution to new problems. In this paper, we propose a novel deep
reinforcement learning-based skill transfer and composition method that takes
the agent's primitive policies to solve unseen ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.03876v1' target='_blank'>Deep Active Inference as Variational Policy Gradients</a></h2>
<p><strong>Authors:</strong> Beren Millidge</p>
<p><strong>Summary:</strong> Active Inference is a theory of action arising from neuroscience which casts
action and planning as a bayesian inference problem to be solved by minimizing
a single quantity - the variational free energy. Active Inference promises a
unifying account of action and perception coupled with a biologically plausible
process theory. Despite these potential advantages, current implementations of
Active Inference can only handle small, discrete policy and state-spaces and
typically require the environme...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.11703v1' target='_blank'>Action Guidance with MCTS for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Bilal Kartal, Pablo Hernandez-Leal, Matthew E. Taylor</p>
<p><strong>Summary:</strong> Deep reinforcement learning has achieved great successes in recent years,
however, one main challenge is the sample inefficiency. In this paper, we focus
on how to use action guidance by means of a non-expert demonstrator to improve
sample efficiency in a domain with sparse, delayed, and possibly deceptive
rewards: the recently-proposed multi-agent benchmark of Pommerman. We propose a
new framework where even a non-expert simulated demonstrator, e.g., planning
algorithms such as Monte Carlo tree...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.04069v1' target='_blank'>Zooming for Efficient Model-Free Reinforcement Learning in Metric Spaces</a></h2>
<p><strong>Authors:</strong> Ahmed Touati, Adrien Ali Taiga, Marc G. Bellemare</p>
<p><strong>Summary:</strong> Despite the wealth of research into provably efficient reinforcement learning
algorithms, most works focus on tabular representation and thus struggle to
handle exponentially or infinitely large state-action spaces. In this paper, we
consider episodic reinforcement learning with a continuous state-action space
which is assumed to be equipped with a natural metric that characterizes the
proximity between different states and actions. We propose ZoomRL, an online
algorithm that leverages ideas fro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.01226v1' target='_blank'>Sample-efficient reinforcement learning using deep Gaussian processes</a></h2>
<p><strong>Authors:</strong> Charles Gadd, Markus Heinonen, Harri L√§hdesm√§ki, Samuel Kaski</p>
<p><strong>Summary:</strong> Reinforcement learning provides a framework for learning to control which
actions to take towards completing a task through trial-and-error. In many
applications observing interactions is costly, necessitating sample-efficient
learning. In model-based reinforcement learning efficiency is improved by
learning to simulate the world dynamics. The challenge is that model
inaccuracies rapidly accumulate over planned trajectories. We introduce deep
Gaussian processes where the depth of the composition...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.10824v1' target='_blank'>Policy Teaching in Reinforcement Learning via Environment Poisoning
  Attacks</a></h2>
<p><strong>Authors:</strong> Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, Adish Singla</p>
<p><strong>Summary:</strong> We study a security threat to reinforcement learning where an attacker
poisons the learning environment to force the agent into executing a target
policy chosen by the attacker. As a victim, we consider RL agents whose
objective is to find a policy that maximizes reward in infinite-horizon problem
settings. The attacker can manipulate the rewards and the transition dynamics
in the learning environment at training-time, and is interested in doing so in
a stealthy manner. We propose an optimizatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.09137v1' target='_blank'>Multi-Agent Reinforcement Learning of 3D Furniture Layout Simulation in
  Indoor Graphics Scenes</a></h2>
<p><strong>Authors:</strong> Xinhan Di, Pengqian Yu</p>
<p><strong>Summary:</strong> In the industrial interior design process, professional designers plan the
furniture layout to achieve a satisfactory 3D design for selling. In this
paper, we explore the interior graphics scenes design task as a Markov decision
process (MDP) in 3D simulation, which is solved by multi-agent reinforcement
learning. The goal is to produce furniture layout in the 3D simulation of the
indoor graphics scenes. In particular, we firstly transform the 3D interior
graphic scenes into two 2D simulated sce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.09560v3' target='_blank'>Objective-aware Traffic Simulation via Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Guanjie Zheng, Hanyang Liu, Kai Xu, Zhenhui Li</p>
<p><strong>Summary:</strong> Traffic simulators act as an essential component in the operating and
planning of transportation systems. Conventional traffic simulators usually
employ a calibrated physical car-following model to describe vehicles'
behaviors and their interactions with traffic environment. However, there is no
universal physical model that can accurately predict the pattern of vehicle's
behaviors in different situations. A fixed physical model tends to be less
effective in a complicated environment given the n...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.08241v1' target='_blank'>High-Accuracy Model-Based Reinforcement Learning, a Survey</a></h2>
<p><strong>Authors:</strong> Aske Plaat, Walter Kosters, Mike Preuss</p>
<p><strong>Summary:</strong> Deep reinforcement learning has shown remarkable success in the past few
years. Highly complex sequential decision making problems from game playing and
robotics have been solved with deep model-free methods. Unfortunately, the
sample complexity of model-free methods is often high. To reduce the number of
environment samples, model-based reinforcement learning creates an explicit
model of the environment dynamics. Achieving high model accuracy is a challenge
in high-dimensional problems. In rece...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.02717v2' target='_blank'>Beyond No Regret: Instance-Dependent PAC Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Andrew Wagenmaker, Max Simchowitz, Kevin Jamieson</p>
<p><strong>Summary:</strong> The theory of reinforcement learning has focused on two fundamental problems:
achieving low regret, and identifying $\epsilon$-optimal policies. While a
simple reduction allows one to apply a low-regret algorithm to obtain an
$\epsilon$-optimal policy and achieve the worst-case optimal rate, it is
unknown whether low-regret algorithms can obtain the instance-optimal rate for
policy identification. We show this is not possible -- there exists a
fundamental tradeoff between achieving low regret an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.06594v1' target='_blank'>Offline-Online Reinforcement Learning for Energy Pricing in Office
  Demand Response: Lowering Energy and Data Costs</a></h2>
<p><strong>Authors:</strong> Doseok Jang, Lucas Spangher, Manan Khattar, Utkarsha Agwan, Selvaprabuh Nadarajah, Costas Spanos</p>
<p><strong>Summary:</strong> Our team is proposing to run a full-scale energy demand response experiment
in an office building. Although this is an exciting endeavor which will provide
value to the community, collecting training data for the reinforcement learning
agent is costly and will be limited. In this work, we examine how offline
training can be leveraged to minimize data costs (accelerate convergence) and
program implementation costs. We present two approaches to doing so:
pretraining our model to warm start the exp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.01926v1' target='_blank'>Improved Reinforcement Learning Coordinated Control of a Mobile
  Manipulator using Joint Clamping</a></h2>
<p><strong>Authors:</strong> Denis Hadjivelichkov, Kostas Vlachos, Dimitrios Kanoulas</p>
<p><strong>Summary:</strong> Many robotic path planning problems are continuous, stochastic, and
high-dimensional. The ability of a mobile manipulator to coordinate its base
and manipulator in order to control its whole-body online is particularly
challenging when self and environment collision avoidance is required.
Reinforcement Learning techniques have the potential to solve such problems
through their ability to generalise over environments. We study joint penalties
and joint limits of a state-of-the-art mobile manipula...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.07729v1' target='_blank'>An Independent Study of Reinforcement Learning and Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Hanzhi Yang</p>
<p><strong>Summary:</strong> Reinforcement learning has become one of the most trending subjects in the
recent decade. It has seen applications in various fields such as robot
manipulations, autonomous driving, path planning, computer gaming, etc. We
accomplished three tasks during the course of this project. Firstly, we studied
the Q-learning algorithm for tabular environments and applied it successfully
to an OpenAi Gym environment, Taxi. Secondly, we gained an understanding of and
implemented the deep Q-network algorithm...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.09653v1' target='_blank'>The Paradox of Choice: Using Attention in Hierarchical Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Andrei Nica, Khimya Khetarpal, Doina Precup</p>
<p><strong>Summary:</strong> Decision-making AI agents are often faced with two important challenges: the
depth of the planning horizon, and the branching factor due to having many
choices. Hierarchical reinforcement learning methods aim to solve the first
problem, by providing shortcuts that skip over multiple time steps. To cope
with the breadth, it is desirable to restrict the agent's attention at each
step to a reasonable number of possible choices. The concept of affordances
(Gibson, 1977) suggests that only certain ac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.11296v2' target='_blank'>Reinforcement Learning in Practice: Opportunities and Challenges</a></h2>
<p><strong>Authors:</strong> Yuxi Li</p>
<p><strong>Summary:</strong> This article is a gentle discussion about the field of reinforcement learning
in practice, about opportunities and challenges, touching a broad range of
topics, with perspectives and without technical details. The article is based
on both historical and recent research papers, surveys, tutorials, talks,
blogs, books, (panel) discussions, and workshops/conferences. Various groups of
readers, like researchers, engineers, students, managers, investors, officers,
and people wanting to know more abou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.07676v2' target='_blank'>An Introduction to Multi-Agent Reinforcement Learning and Review of its
  Application to Autonomous Mobility</a></h2>
<p><strong>Authors:</strong> Lukas M. Schmidt, Johanna Brosig, Axel Plinge, Bjoern M. Eskofier, Christopher Mutschler</p>
<p><strong>Summary:</strong> Many scenarios in mobility and traffic involve multiple different agents that
need to cooperate to find a joint solution. Recent advances in behavioral
planning use Reinforcement Learning to find effective and performant behavior
strategies. However, as autonomous vehicles and vehicle-to-X communications
become more mature, solutions that only utilize single, independent agents
leave potential performance gains on the road. Multi-Agent Reinforcement
Learning (MARL) is a research field that aims ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.14033v1' target='_blank'>Aggressive Quadrotor Flight Using Curiosity-Driven Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Qiyu Sun, Jinbao Fang, Wei Xing Zheng, Yang Tang</p>
<p><strong>Summary:</strong> The ability to perform aggressive movements, which are called aggressive
flights, is important for quadrotors during navigation. However, aggressive
quadrotor flights are still a great challenge to practical applications. The
existing solutions to aggressive flights heavily rely on a predefined
trajectory, which is a time-consuming preprocessing step. To avoid such path
planning, we propose a curiosity-driven reinforcement learning method for
aggressive flight missions and a similarity-based cur...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.08349v3' target='_blank'>Unguided Self-exploration in Narrow Spaces with Safety Region Enhanced
  Reinforcement Learning for Ackermann-steering Robots</a></h2>
<p><strong>Authors:</strong> Zhaofeng Tian, Zichuan Liu, Xingyu Zhou, Weisong Shi</p>
<p><strong>Summary:</strong> In narrow spaces, motion planning based on the traditional hierarchical
autonomous system could cause collisions due to mapping, localization, and
control noises, especially for car-like Ackermann-steering robots which suffer
from non-convex and non-holonomic kinematics. To tackle these problems, we
leverage deep reinforcement learning which is verified to be effective in
self-decision-making, to self-explore in narrow spaces without a given map and
destination while avoiding collisions. Specifi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.03136v1' target='_blank'>Design Process is a Reinforcement Learning Problem</a></h2>
<p><strong>Authors:</strong> Reza kakooee, Benjamin Dillunberger</p>
<p><strong>Summary:</strong> While reinforcement learning has been used widely in research during the past
few years, it found fewer real-world applications than supervised learning due
to some weaknesses that the RL algorithms suffer from, such as performance
degradation in transitioning from the simulator to the real world. Here, we
argue the design process is a reinforcement learning problem and can
potentially be a proper application for RL algorithms as it is an offline
process and conventionally is done in CAD softwar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.10660v2' target='_blank'>Evaluating the Perceived Safety of Urban City via Maximum Entropy Deep
  Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yaxuan Wang, Zhixin Zeng, Qijun Zhao</p>
<p><strong>Summary:</strong> Inspired by expert evaluation policy for urban perception, we proposed a
novel inverse reinforcement learning (IRL) based framework for predicting urban
safety and recovering the corresponding reward function. We also presented a
scalable state representation method to model the prediction problem as a
Markov decision process (MDP) and use reinforcement learning (RL) to solve the
problem. Additionally, we built a dataset called SmallCity based on the
crowdsourcing method to conduct the research....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.02179v2' target='_blank'>Developing Driving Strategies Efficiently: A Skill-Based Hierarchical
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Yigit Gurses, Kaan Buyukdemirci, Yildiray Yildiz</p>
<p><strong>Summary:</strong> Driving in dense traffic with human and autonomous drivers is a challenging
task that requires high-level planning and reasoning. Human drivers can achieve
this task comfortably, and there has been many efforts to model human driver
strategies. These strategies can be used as inspirations for developing
autonomous driving algorithms or to create high-fidelity simulators.
Reinforcement learning is a common tool to model driver policies, but
conventional training of these models can be computation...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.14451v1' target='_blank'>Hierarchical Reinforcement Learning in Complex 3D Environments</a></h2>
<p><strong>Authors:</strong> Bernardo Avila Pires, Feryal Behbahani, Hubert Soyer, Kyriacos Nikiforou, Thomas Keck, Satinder Singh</p>
<p><strong>Summary:</strong> Hierarchical Reinforcement Learning (HRL) agents have the potential to
demonstrate appealing capabilities such as planning and exploration with
abstraction, transfer, and skill reuse. Recent successes with HRL across
different domains provide evidence that practical, effective HRL agents are
possible, even if existing agents do not yet fully realize the potential of
HRL. Despite these successes, visually complex partially observable 3D
environments remained a challenge for HRL agents. We address...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.00576v1' target='_blank'>Joint Learning of Policy with Unknown Temporal Constraints for Safe
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Lunet Yifru, Ali Baheri</p>
<p><strong>Summary:</strong> In many real-world applications, safety constraints for reinforcement
learning (RL) algorithms are either unknown or not explicitly defined. We
propose a framework that concurrently learns safety constraints and optimal RL
policies in such environments, supported by theoretical guarantees. Our
approach merges a logically-constrained RL algorithm with an evolutionary
algorithm to synthesize signal temporal logic (STL) specifications. The
framework is underpinned by theorems that establish the con...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.06541v3' target='_blank'>On the Effective Horizon of Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yiqing Xu, Finale Doshi-Velez, David Hsu</p>
<p><strong>Summary:</strong> Inverse reinforcement learning (IRL) algorithms often rely on (forward)
reinforcement learning or planning, over a given time horizon, to compute an
approximately optimal policy for a hypothesized reward function; they then
match this policy with expert demonstrations. The time horizon plays a critical
role in determining both the accuracy of reward estimates and the computational
efficiency of IRL algorithms. Interestingly, an *effective time horizon*
shorter than the ground-truth value often p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.07176v3' target='_blank'>SafeDreamer: Safe Reinforcement Learning with World Models</a></h2>
<p><strong>Authors:</strong> Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang, Yaodong Yang</p>
<p><strong>Summary:</strong> The deployment of Reinforcement Learning (RL) in real-world applications is
constrained by its failure to satisfy safety criteria. Existing Safe
Reinforcement Learning (SafeRL) methods, which rely on cost functions to
enforce safety, often fail to achieve zero-cost performance in complex
scenarios, especially vision-only tasks. These limitations are primarily due to
model inaccuracies and inadequate sample efficiency. The integration of the
world model has proven effective in mitigating these sh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.01797v1' target='_blank'>Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to
  Sequence approach</a></h2>
<p><strong>Authors:</strong> Giovanni Bonetta, Davide Zago, Rossella Cancelliere, Andrea Grosso</p>
<p><strong>Summary:</strong> Job scheduling is a well-known Combinatorial Optimization problem with
endless applications. Well planned schedules bring many benefits in the context
of automated systems: among others, they limit production costs and waste.
Nevertheless, the NP-hardness of this problem makes it essential to use
heuristics whose design is difficult, requires specialized knowledge and often
produces methods tailored to the specific task. This paper presents an original
end-to-end Deep Reinforcement Learning appr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.14139v1' target='_blank'>Reinforcement Learning-based Optimal Control and Software Rejuvenation
  for Safe and Efficient UAV Navigation</a></h2>
<p><strong>Authors:</strong> Angela Chen, Konstantinos Mitsopoulos, Raffaele Romagnoli</p>
<p><strong>Summary:</strong> Unmanned autonomous vehicles (UAVs) rely on effective path planning and
tracking control to accomplish complex tasks in various domains. Reinforcement
Learning (RL) methods are becoming increasingly popular in control
applications, as they can learn from data and deal with unmodelled dynamics.
Cyber-physical systems (CPSs), such as UAVs, integrate sensing, network
communication, control, and computation to solve challenging problems. In this
context, Software Rejuvenation (SR) is a protection me...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.14156v2' target='_blank'>Designing and evaluating an online reinforcement learning agent for
  physical exercise recommendations in N-of-1 trials</a></h2>
<p><strong>Authors:</strong> Dominik Meier, Ipek Ensari, Stefan Konigorski</p>
<p><strong>Summary:</strong> Personalized adaptive interventions offer the opportunity to increase patient
benefits, however, there are challenges in their planning and implementation.
Once implemented, it is an important question whether personalized adaptive
interventions are indeed clinically more effective compared to a fixed gold
standard intervention. In this paper, we present an innovative N-of-1 trial
study design testing whether implementing a personalized intervention by an
online reinforcement learning agent is f...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.16164v1' target='_blank'>Learning to Terminate in Object Navigation</a></h2>
<p><strong>Authors:</strong> Yuhang Song, Anh Nguyen, Chun-Yi Lee</p>
<p><strong>Summary:</strong> This paper tackles the critical challenge of object navigation in autonomous
navigation systems, particularly focusing on the problem of target approach and
episode termination in environments with long optimal episode length in Deep
Reinforcement Learning (DRL) based methods. While effective in environment
exploration and object localization, conventional DRL methods often struggle
with optimal path planning and termination recognition due to a lack of depth
information. To overcome these limit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.09873v1' target='_blank'>Reinforcement Learning for Reduced-order Models of Legged Robots</a></h2>
<p><strong>Authors:</strong> Yu-Ming Chen, Hien Bui, Michael Posa</p>
<p><strong>Summary:</strong> Model-based approaches for planning and control for bipedal locomotion have a
long history of success. It can provide stability and safety guarantees while
being effective in accomplishing many locomotion tasks. Model-free
reinforcement learning, on the other hand, has gained much popularity in recent
years due to computational advancements. It can achieve high performance in
specific tasks, but it lacks physical interpretability and flexibility in
re-purposing the policy for a different set of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.14328v1' target='_blank'>Offline Skill Generalization via Task and Motion Planning</a></h2>
<p><strong>Authors:</strong> Shin Watanabe, Geir Horn, Jim T√∏rresen, Kai Olav Ellefsen</p>
<p><strong>Summary:</strong> This paper presents a novel approach to generalizing robot manipulation
skills by combining a sampling-based task-and-motion planner with an offline
reinforcement learning algorithm. Starting with a small library of scripted
primitive skills (e.g. Push) and object-centric symbolic predicates (e.g.
On(block, plate)), the planner autonomously generates a demonstration dataset
of manipulation skills in the context of a long-horizon task. An offline
reinforcement learning algorithm then extracts a p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.17587v1' target='_blank'>Deep Reinforcement Learning Graphs: Feedback Motion Planning via Neural
  Lyapunov Verification</a></h2>
<p><strong>Authors:</strong> Armin Ghanbarzadeh, Esmaeil Najafi</p>
<p><strong>Summary:</strong> Recent advancements in model-free deep reinforcement learning have enabled
efficient agent training. However, challenges arise when determining the region
of attraction for these controllers, especially if the region does not fully
cover the desired area. This paper addresses this issue by introducing a
feedback motion control algorithm that utilizes data-driven techniques and
neural networks. The algorithm constructs a graph of connected
reinforcement-learning based controllers, each with its o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.07710v1' target='_blank'>Go-Explore for Residential Energy Management</a></h2>
<p><strong>Authors:</strong> Junlin Lu, Patrick Mannion, Karl Mason</p>
<p><strong>Summary:</strong> Reinforcement learning is commonly applied in residential energy management,
particularly for optimizing energy costs. However, RL agents often face
challenges when dealing with deceptive and sparse rewards in the energy control
domain, especially with stochastic rewards. In such situations, thorough
exploration becomes crucial for learning an optimal policy. Unfortunately, the
exploration mechanism can be misled by deceptive reward signals, making
thorough exploration difficult. Go-Explore is a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.16974v1' target='_blank'>CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Andreas W. M. Sauter, Nicol√≤ Botteghi, Erman Acar, Aske Plaat</p>
<p><strong>Summary:</strong> Causal discovery is the challenging task of inferring causal structure from
data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive
observations alone are not enough to distinguish correlation from causation,
there has been a recent push to incorporate interventions into machine learning
research. Reinforcement learning provides a convenient framework for such an
active approach to learning. This paper presents CORE, a deep reinforcement
learning-based approach for causal ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.07182v3' target='_blank'>Divide and Conquer: Provably Unveiling the Pareto Front with
  Multi-Objective Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Willem R√∂pke, Mathieu Reymond, Patrick Mannion, Diederik M. Roijers, Ann Now√©, Roxana RƒÉdulescu</p>
<p><strong>Summary:</strong> An important challenge in multi-objective reinforcement learning is obtaining
a Pareto front of policies to attain optimal performance under different
preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), which
decomposes finding the Pareto front into a sequence of constrained
single-objective problems. This enables us to guarantee convergence while
providing an upper bound on the distance to undiscovered Pareto optimal
solutions at each step. We evaluate IPRO using utility-bas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.08421v2' target='_blank'>Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Eslam Eldeeb, Houssem Sifaou, Osvaldo Simeone, Mohammad Shehab, Hirley Alves</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has been widely adopted for controlling and
optimizing complex engineering systems such as next-generation wireless
networks. An important challenge in adopting RL is the need for direct access
to the physical environment. This limitation is particularly severe in
multi-agent systems, for which conventional multi-agent reinforcement learning
(MARL) requires a large number of coordinated online interactions with the
environment during training. When only offline data i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.11175v1' target='_blank'>Prior-dependent analysis of posterior sampling reinforcement learning
  with function approximation</a></h2>
<p><strong>Authors:</strong> Yingru Li, Zhi-Quan Luo</p>
<p><strong>Summary:</strong> This work advances randomized exploration in reinforcement learning (RL) with
function approximation modeled by linear mixture MDPs. We establish the first
prior-dependent Bayesian regret bound for RL with function approximation; and
refine the Bayesian regret analysis for posterior sampling reinforcement
learning (PSRL), presenting an upper bound of ${\mathcal{O}}(d\sqrt{H^3 T \log
T})$, where $d$ represents the dimensionality of the transition kernel, $H$ the
planning horizon, and $T$ the tota...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01440v1' target='_blank'>A Review of Reward Functions for Reinforcement Learning in the context
  of Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Ahmed Abouelazm, Jonas Michel, J. Marius Zoellner</p>
<p><strong>Summary:</strong> Reinforcement learning has emerged as an important approach for autonomous
driving. A reward function is used in reinforcement learning to establish the
learned skill objectives and guide the agent toward the optimal policy. Since
autonomous driving is a complex domain with partly conflicting objectives with
varying degrees of priority, developing a suitable reward function represents a
fundamental challenge. This paper aims to highlight the gap in such function
design by assessing different pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01061v1' target='_blank'>Satellites swarm cooperation for pursuit-attachment tasks with
  transformer-based reinforcement learning</a></h2>
<p><strong>Authors:</strong> yonghao Li</p>
<p><strong>Summary:</strong> The on-orbit intelligent planning of satellites swarm has attracted
increasing attention from scholars. Especially in tasks such as the pursuit and
attachment of non-cooperative satellites, satellites swarm must achieve
coordinated cooperation with limited resources. The study proposes a
reinforcement learning framework that integrates the transformer and expert
networks. Firstly, under the constraints of incomplete information about
non-cooperative satellites, an implicit multi-satellites coope...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.09287v1' target='_blank'>Instruction Following with Goal-Conditioned Reinforcement Learning in
  Virtual Environments</a></h2>
<p><strong>Authors:</strong> Zoya Volovikova, Alexey Skrynnik, Petr Kuderov, Aleksandr I. Panov</p>
<p><strong>Summary:</strong> In this study, we address the issue of enabling an artificial intelligence
agent to execute complex language instructions within virtual environments. In
our framework, we assume that these instructions involve intricate linguistic
structures and multiple interdependent tasks that must be navigated
successfully to achieve the desired outcomes. To effectively manage these
complexities, we propose a hierarchical framework that combines the deep
language comprehension of large language models with ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.06520v2' target='_blank'>Retrieval-Augmented Hierarchical in-Context Reinforcement Learning and
  Hindsight Modular Reflections for Task Planning with LLMs</a></h2>
<p><strong>Authors:</strong> Chuanneng Sun, Songjun Huang, Dario Pompili</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable abilities in
various language tasks, making them promising candidates for decision-making in
robotics. Inspired by Hierarchical Reinforcement Learning (HRL), we propose
Retrieval-Augmented in-context reinforcement Learning (RAHL), a novel framework
that decomposes complex tasks into sub-tasks using an LLM-based high-level
policy, in which a complex task is decomposed into sub-tasks by a high-level
policy on-the-fly. The sub-tasks, defined...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.17443v1' target='_blank'>Cat-and-Mouse Satellite Dynamics: Divergent Adversarial Reinforcement
  Learning for Contested Multi-Agent Space Operations</a></h2>
<p><strong>Authors:</strong> Cameron Mehlman, Joseph Abramov, Gregory Falco</p>
<p><strong>Summary:</strong> As space becomes increasingly crowded and contested, robust autonomous
capabilities for multi-agent environments are gaining critical importance.
Current autonomous systems in space primarily rely on optimization-based path
planning or long-range orbital maneuvers, which have not yet proven effective
in adversarial scenarios where one satellite is actively pursuing another. We
introduce Divergent Adversarial Reinforcement Learning (DARL), a two-stage
Multi-Agent Reinforcement Learning (MARL) app...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.08997v2' target='_blank'>Hierarchical Universal Value Function Approximators</a></h2>
<p><strong>Authors:</strong> Rushiv Arora</p>
<p><strong>Summary:</strong> There have been key advancements to building universal approximators for
multi-goal collections of reinforcement learning value functions -- key
elements in estimating long-term returns of states in a parameterized manner.
We extend this to hierarchical reinforcement learning, using the options
framework, by introducing hierarchical universal value function approximators
(H-UVFAs). This allows us to leverage the added benefits of scaling, planning,
and generalization expected in temporal abstrac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.16666v1' target='_blank'>QuasiNav: Asymmetric Cost-Aware Navigation Planning with Constrained
  Quasimetric Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jumman Hossain, Abu-Zaher Faridee, Derrik Asher, Jade Freeman, Theron Trout, Timothy Gregory, Nirmalya Roy</p>
<p><strong>Summary:</strong> Autonomous navigation in unstructured outdoor environments is inherently
challenging due to the presence of asymmetric traversal costs, such as varying
energy expenditures for uphill versus downhill movement. Traditional
reinforcement learning methods often assume symmetric costs, which can lead to
suboptimal navigation paths and increased safety risks in real-world scenarios.
In this paper, we introduce QuasiNav, a novel reinforcement learning framework
that integrates quasimetric embeddings to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.11457v1' target='_blank'>Upside-Down Reinforcement Learning for More Interpretable Optimal
  Control</a></h2>
<p><strong>Authors:</strong> Juan Cardenas-Cartagena, Massimiliano Falzari, Marco Zullich, Matthia Sabatelli</p>
<p><strong>Summary:</strong> Model-Free Reinforcement Learning (RL) algorithms either learn how to map
states to expected rewards or search for policies that can maximize a certain
performance function. Model-Based algorithms instead, aim to learn an
approximation of the underlying model of the RL environment and then use it in
combination with planning algorithms. Upside-Down Reinforcement Learning (UDRL)
is a novel learning paradigm that aims to learn how to predict actions from
states and desired commands. This task is f...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.05766v1' target='_blank'>Policy-shaped prediction: avoiding distractions in model-based
  reinforcement learning</a></h2>
<p><strong>Authors:</strong> Miles Hutson, Isaac Kauvar, Nick Haber</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) is a promising route to
sample-efficient policy optimization. However, a known vulnerability of
reconstruction-based MBRL consists of scenarios in which detailed aspects of
the world are highly predictable, but irrelevant to learning a good policy.
Such scenarios can lead the model to exhaust its capacity on meaningless
content, at the cost of neglecting important environment dynamics. While
existing approaches attempt to solve this problem, we highlight...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.09080v1' target='_blank'>Average-Reward Reinforcement Learning with Entropy Regularization</a></h2>
<p><strong>Authors:</strong> Jacob Adamczyk, Volodymyr Makarenko, Stas Tiomkin, Rahul V. Kulkarni</p>
<p><strong>Summary:</strong> The average-reward formulation of reinforcement learning (RL) has drawn
increased interest in recent years due to its ability to solve
temporally-extended problems without discounting. Independently, RL algorithms
have benefited from entropy-regularization: an approach used to make the
optimal policy stochastic, thereby more robust to noise. Despite the distinct
benefits of the two approaches, the combination of entropy regularization with
an average-reward objective is not well-studied in the l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.12542v1' target='_blank'>Reinforcement Learning Constrained Beam Search for Parameter
  Optimization of Paper Drying Under Flexible Constraints</a></h2>
<p><strong>Authors:</strong> Siyuan Chen, Hanshen Yu, Jamal Yagoobi, Chenhui Shao</p>
<p><strong>Summary:</strong> Existing approaches to enforcing design constraints in Reinforcement Learning
(RL) applications often rely on training-time penalties in the reward function
or training/inference-time invalid action masking, but these methods either
cannot be modified after training, or are limited in the types of constraints
that can be implemented. To address this limitation, we propose Reinforcement
Learning Constrained Beam Search (RLCBS) for inference-time refinement in
combinatorial optimization problems. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.13084v1' target='_blank'>Attention-Driven Hierarchical Reinforcement Learning with Particle
  Filtering for Source Localization in Dynamic Fields</a></h2>
<p><strong>Authors:</strong> Yiwei Shi, Mengyue Yang, Qi Zhang, Weinan Zhang, Cunjia Liu, Weiru Liu</p>
<p><strong>Summary:</strong> In many real-world scenarios, such as gas leak detection or environmental
pollutant tracking, solving the Inverse Source Localization and
Characterization problem involves navigating complex, dynamic fields with
sparse and noisy observations. Traditional methods face significant challenges,
including partial observability, temporal and spatial dynamics,
out-of-distribution generalization, and reward sparsity. To address these
issues, we propose a hierarchical framework that integrates Bayesian i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.05595v1' target='_blank'>Data efficient Robotic Object Throwing with Model-Based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Niccol√≤ Turcato, Giulio Giacomuzzo, Matteo Terreran, Davide Allegro, Ruggero Carli, Alberto Dalla Libera</p>
<p><strong>Summary:</strong> Pick-and-place (PnP) operations, featuring object grasping and trajectory
planning, are fundamental in industrial robotics applications. Despite many
advancements in the field, PnP is limited by workspace constraints, reducing
flexibility. Pick-and-throw (PnT) is a promising alternative where the robot
throws objects to target locations, leveraging extrinsic resources like gravity
to improve efficiency and expand the workspace. However, PnT execution is
complex, requiring precise coordination of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.07333v1' target='_blank'>Multi-agent Reinforcement Learning Embedded Game for the Optimization of
  Building Energy Control and Power System Planning</a></h2>
<p><strong>Authors:</strong> Jun Hao</p>
<p><strong>Summary:</strong> Most of the current game-theoretic demand-side management methods focus
primarily on the scheduling of home appliances, and the related numerical
experiments are analyzed under various scenarios to achieve the corresponding
Nash-equilibrium (NE) and optimal results. However, not much work is conducted
for academic or commercial buildings. The methods for optimizing
academic-buildings are distinct from the optimal methods for home appliances.
In my study, we address a novel methodology to control...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.10754v1' target='_blank'>Learning Q-network for Active Information Acquisition</a></h2>
<p><strong>Authors:</strong> Heejin Jeong, Brent Schlotfeldt, Hamed Hassani, Manfred Morari, Daniel D. Lee, George J. Pappas</p>
<p><strong>Summary:</strong> In this paper, we propose a novel Reinforcement Learning approach for solving
the Active Information Acquisition problem, which requires an agent to choose a
sequence of actions in order to acquire information about a process of interest
using on-board sensors. The classic challenges in the information acquisition
problem are the dependence of a planning algorithm on known models and the
difficulty of computing information-theoretic cost functions over arbitrary
distributions. In contrast, the p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.00527v1' target='_blank'>Deep Reinforcement Learning for Constrained Field Development
  Optimization in Subsurface Two-phase Flow</a></h2>
<p><strong>Authors:</strong> Yusuf Nasir, Jincong He, Chaoshun Hu, Shusei Tanaka, Kainan Wang, XianHuan Wen</p>
<p><strong>Summary:</strong> We present a deep reinforcement learning-based artificial intelligence agent
that could provide optimized development plans given a basic description of the
reservoir and rock/fluid properties with minimal computational cost. This
artificial intelligence agent, comprising of a convolutional neural network,
provides a mapping from a given state of the reservoir model, constraints, and
economic condition to the optimal decision (drill/do not drill and well
location) to be taken in the next stage o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.02610v2' target='_blank'>Continuous Motion Planning with Temporal Logic Specifications using Deep
  Neural Networks</a></h2>
<p><strong>Authors:</strong> Chuanzheng Wang, Yinan Li, Stephen L. Smith, Jun Liu</p>
<p><strong>Summary:</strong> In this paper, we propose a model-free reinforcement learning method to
synthesize control policies for motion planning problems with continuous states
and actions. The robot is modelled as a labeled discrete-time Markov decision
process (MDP) with continuous state and action spaces. Linear temporal logics
(LTL) are used to specify high-level tasks. We then train deep neural networks
to approximate the value function and policy using an actor-critic
reinforcement learning method. The LTL specifi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.13098v1' target='_blank'>An End-to-end Deep Reinforcement Learning Approach for the Long-term
  Short-term Planning on the Frenet Space</a></h2>
<p><strong>Authors:</strong> Majid Moghadam, Ali Alizadeh, Engin Tekin, Gabriel Hugh Elkaim</p>
<p><strong>Summary:</strong> Tactical decision making and strategic motion planning for autonomous highway
driving are challenging due to the complication of predicting other road users'
behaviors, diversity of environments, and complexity of the traffic
interactions. This paper presents a novel end-to-end continuous deep
reinforcement learning approach towards autonomous cars' decision-making and
motion planning. For the first time, we define both states and action spaces on
the Frenet space to make the driving behavior le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.02676v1' target='_blank'>Efficient UAV Trajectory-Planning using Economic Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alvi Ataur Khalil, Alexander J Byrne, Mohammad Ashiqur Rahman, Mohammad Hossein Manshaei</p>
<p><strong>Summary:</strong> Advances in unmanned aerial vehicle (UAV) design have opened up applications
as varied as surveillance, firefighting, cellular networks, and delivery
applications. Additionally, due to decreases in cost, systems employing fleets
of UAVs have become popular. The uniqueness of UAVs in systems creates a novel
set of trajectory or path planning and coordination problems. Environments
include many more points of interest (POIs) than UAVs, with obstacles and
no-fly zones. We introduce REPlanner, a nov...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.09568v1' target='_blank'>A Practical Guide to Multi-Objective Reinforcement Learning and Planning</a></h2>
<p><strong>Authors:</strong> Conor F. Hayes, Roxana RƒÉdulescu, Eugenio Bargiacchi, Johan K√§llstr√∂m, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten, Luisa M. Zintgraf, Richard Dazeley, Fredrik Heintz, Enda Howley, Athirai A. Irissappane, Patrick Mannion, Ann Now√©, Gabriel Ramos, Marcello Restelli, Peter Vamplew, Diederik M. Roijers</p>
<p><strong>Summary:</strong> Real-world decision-making tasks are generally complex, requiring trade-offs
between multiple, often conflicting, objectives. Despite this, the majority of
research in reinforcement learning and decision-theoretic planning either
assumes only a single objective, or that multiple objectives can be adequately
handled via a simple linear combination. Such approaches may oversimplify the
underlying problem and hence produce suboptimal results. This paper serves as a
guide to the application of multi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.13461v1' target='_blank'>Active Predictive Coding: A Unified Neural Framework for Learning
  Hierarchical World Models for Perception and Planning</a></h2>
<p><strong>Authors:</strong> Rajesh P. N. Rao, Dimitrios C. Gklezakos, Vishwas Sathish</p>
<p><strong>Summary:</strong> Predictive coding has emerged as a prominent model of how the brain learns
through predictions, anticipating the importance accorded to predictive
learning in recent AI architectures such as transformers. Here we propose a new
framework for predictive coding called active predictive coding which can learn
hierarchical world models and solve two radically different open problems in
AI: (1) how do we learn compositional representations, e.g., part-whole
hierarchies, for equivariant vision? and (2)...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.00573v1' target='_blank'>Risk-Sensitive and Robust Model-Based Reinforcement Learning and
  Planning</a></h2>
<p><strong>Authors:</strong> Marc Rigter</p>
<p><strong>Summary:</strong> Many sequential decision-making problems that are currently automated, such
as those in manufacturing or recommender systems, operate in an environment
where there is either little uncertainty, or zero risk of catastrophe. As
companies and researchers attempt to deploy autonomous systems in less
constrained environments, it is increasingly important that we endow sequential
decision-making algorithms with the ability to reason about uncertainty and
risk.
  In this thesis, we will address both pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.06567v1' target='_blank'>Deep reinforcement learning applied to an assembly sequence planning
  problem with user preferences</a></h2>
<p><strong>Authors:</strong> Miguel Neves, Pedro Neto</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) has demonstrated its potential in solving
complex manufacturing decision-making problems, especially in a context where
the system learns over time with actual operation in the absence of training
data. One interesting and challenging application for such methods is the
assembly sequence planning (ASP) problem. In this paper, we propose an approach
to the implementation of DRL methods in ASP. The proposed approach introduces
in the RL environment parametric acti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.10041v1' target='_blank'>Topological Guided Actor-Critic Modular Learning of Continuous Systems
  with Temporal Objectives</a></h2>
<p><strong>Authors:</strong> Lening Li, Zhentian Qian</p>
<p><strong>Summary:</strong> This work investigates the formal policy synthesis of continuous-state
stochastic dynamic systems given high-level specifications in linear temporal
logic. To learn an optimal policy that maximizes the satisfaction probability,
we take a product between a dynamic system and the translated automaton to
construct a product system on which we solve an optimal planning problem. Since
this product system has a hybrid product state space that results in reward
sparsity, we introduce a generalized opti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.01270v1' target='_blank'>Multi-Robot Path Planning Combining Heuristics and Multi-Agent
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Shaoming Peng</p>
<p><strong>Summary:</strong> Multi-robot path finding in dynamic environments is a highly challenging
classic problem. In the movement process, robots need to avoid collisions with
other moving robots while minimizing their travel distance. Previous methods
for this problem either continuously replan paths using heuristic search
methods to avoid conflicts or choose appropriate collision avoidance strategies
based on learning approaches. The former may result in long travel distances
due to frequent replanning, while the lat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.18487v1' target='_blank'>Human-Centric Aware UAV Trajectory Planning in Search and Rescue
  Missions Employing Multi-Objective Reinforcement Learning with AHP and
  Similarity-Based Experience Replay</a></h2>
<p><strong>Authors:</strong> Mahya Ramezani, Jose Luis Sanchez-Lopez</p>
<p><strong>Summary:</strong> The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue
(SAR) missions presents a promising avenue for enhancing operational efficiency
and effectiveness. However, the success of these missions is not solely
dependent on the technical capabilities of the drones but also on their
acceptance and interaction with humans on the ground. This paper explores the
effect of human-centric factor in UAV trajectory planning for SAR missions. We
introduce a novel approach based on the reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.11778v1' target='_blank'>Efficient Multi-agent Reinforcement Learning by Planning</a></h2>
<p><strong>Authors:</strong> Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, Chongjie Zhang</p>
<p><strong>Summary:</strong> Multi-agent reinforcement learning (MARL) algorithms have accomplished
remarkable breakthroughs in solving large-scale decision-making tasks.
Nonetheless, most existing MARL algorithms are model-free, limiting sample
efficiency and hindering their applicability in more challenging scenarios. In
contrast, model-based reinforcement learning (MBRL), particularly algorithms
integrating planning, such as MuZero, has demonstrated superhuman performance
with limited data in many tasks. Hence, we aim to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.16967v2' target='_blank'>Multi-Robot Informative Path Planning for Efficient Target Mapping using
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Apoorva Vashisth, Dipam Patel, Damon Conover, Aniket Bera</p>
<p><strong>Summary:</strong> Autonomous robots are being employed in several mapping and data collection
tasks due to their efficiency and low labor costs. In these tasks, the robots
are required to map targets-of-interest in an unknown environment while
constrained to a given resource budget such as path length or mission time.
This is a challenging problem as each robot has to not only detect and avoid
collisions from static obstacles in the environment but also has to model other
robots' trajectories to avoid inter-robot...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1706.05904v2' target='_blank'>Pedestrian Prediction by Planning using Deep Neural Networks</a></h2>
<p><strong>Authors:</strong> Eike Rehder, Florian Wirth, Martin Lauer, Christoph Stiller</p>
<p><strong>Summary:</strong> Accurate traffic participant prediction is the prerequisite for collision
avoidance of autonomous vehicles. In this work, we predict pedestrians by
emulating their own motion planning. From online observations, we infer a
mixture density function for possible destinations. We use this result as the
goal states of a planning stage that performs motion prediction based on common
behavior patterns. The entire system is modeled as one monolithic neural
network and trained via inverse reinforcement l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1808.10120v2' target='_blank'>ExIt-OOS: Towards Learning from Planning in Imperfect Information Games</a></h2>
<p><strong>Authors:</strong> Andy Kitchen, Michela Benedetti</p>
<p><strong>Summary:</strong> The current state of the art in playing many important perfect information
games, including Chess and Go, combines planning and deep reinforcement
learning with self-play. We extend this approach to imperfect information games
and present ExIt-OOS, a novel approach to playing imperfect information games
within the Expert Iteration framework and inspired by AlphaZero. We use Online
Outcome Sampling, an online search algorithm for imperfect information games in
place of MCTS. While training online...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.06389v1' target='_blank'>Sample-efficient Cross-Entropy Method for Real-time Planning</a></h2>
<p><strong>Authors:</strong> Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal Rolinek, Georg Martius</p>
<p><strong>Summary:</strong> Trajectory optimizers for model-based reinforcement learning, such as the
Cross-Entropy Method (CEM), can yield compelling results even in
high-dimensional control tasks and sparse-reward environments. However, their
sampling inefficiency prevents them from being used for real-time planning and
control. We propose an improved version of the CEM algorithm for fast planning,
with novel additions including temporally-correlated actions and memory,
requiring 2.7-22x less samples and yielding a perfo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.04988v1' target='_blank'>Simulating Coverage Path Planning with Roomba</a></h2>
<p><strong>Authors:</strong> Robert Chuchro</p>
<p><strong>Summary:</strong> Coverage Path Planning involves visiting every unoccupied state in an
environment with obstacles. In this paper, we explore this problem in
environments which are initially unknown to the agent, for purposes of
simulating the task of a vacuum cleaning robot. A survey of prior work reveals
sparse effort in applying learning to solve this problem. In this paper, we
explore modeling a Cover Path Planning problem using Deep Reinforcement
Learning, and compare it with the performance of the built-in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.17379v1' target='_blank'>Adaptive speed planning for Unmanned Vehicle Based on Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Hao Liu, Yi Shen, Wenjing Zhou, Yuelin Zou, Chang Zhou, Shuyao He</p>
<p><strong>Summary:</strong> In order to solve the problem of frequent deceleration of unmanned vehicles
when approaching obstacles, this article uses a Deep Q-Network (DQN) and its
extension, the Double Deep Q-Network (DDQN), to develop a local navigation
system that adapts to obstacles while maintaining optimal speed planning. By
integrating improved reward functions and obstacle angle determination methods,
the system demonstrates significant enhancements in maneuvering capabilities
without frequent decelerations. Experi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.15460v1' target='_blank'>TD3 Based Collision Free Motion Planning for Robot Navigation</a></h2>
<p><strong>Authors:</strong> Hao Liu, Yi Shen, Chang Zhou, Yuelin Zou, Zijun Gao, Qi Wang</p>
<p><strong>Summary:</strong> This paper addresses the challenge of collision-free motion planning in
automated navigation within complex environments. Utilizing advancements in
Deep Reinforcement Learning (DRL) and sensor technologies like LiDAR, we
propose the TD3-DWA algorithm, an innovative fusion of the traditional Dynamic
Window Approach (DWA) with the Twin Delayed Deep Deterministic Policy Gradient
(TD3). This hybrid algorithm enhances the efficiency of robotic path planning
by optimizing the sampling interval paramet...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.15820v2' target='_blank'>On shallow planning under partial observability</a></h2>
<p><strong>Authors:</strong> Randy Lefebvre, Audrey Durand</p>
<p><strong>Summary:</strong> Formulating a real-world problem under the Reinforcement Learning framework
involves non-trivial design choices, such as selecting a discount factor for
the learning objective (discounted cumulative rewards), which articulates the
planning horizon of the agent. This work investigates the impact of the
discount factor on the bias-variance trade-off given structural parameters of
the underlying Markov Decision Process. Our results support the idea that a
shorter planning horizon might be beneficia...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1712.05846v2' target='_blank'>Hierarchical Text Generation and Planning for Strategic Dialogue</a></h2>
<p><strong>Authors:</strong> Denis Yarats, Mike Lewis</p>
<p><strong>Summary:</strong> End-to-end models for goal-orientated dialogue are challenging to train,
because linguistic and strategic aspects are entangled in latent state vectors.
We introduce an approach to learning representations of messages in dialogues
by maximizing the likelihood of subsequent sentences and actions, which
decouples the semantics of the dialogue utterance from its linguistic
realization. We then use these latent sentence representations for hierarchical
language generation, planning and reinforcement...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.09088v2' target='_blank'>Flying through a narrow gap using neural network: an end-to-end planning
  and control approach</a></h2>
<p><strong>Authors:</strong> Jiarong Lin, Luqi Wang, Fei Gao, Shaojie Shen, Fu Zhang</p>
<p><strong>Summary:</strong> In this paper, we investigate the problem of enabling a drone to fly through
a tilted narrow gap, without a traditional planning and control pipeline. To
this end, we propose an end-to-end policy network, which imitates from the
traditional pipeline and is fine-tuned using reinforcement learning. Unlike
previous works which plan dynamical feasible trajectories using motion
primitives and track the generated trajectory by a geometric controller, our
proposed method is an end-to-end approach which...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.08399v2' target='_blank'>DeepGait: Planning and Control of Quadrupedal Gaits using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vassilios Tsounis, Mitja Alge, Joonho Lee, Farbod Farshidian, Marco Hutter</p>
<p><strong>Summary:</strong> This paper addresses the problem of legged locomotion in non-flat terrain. As
legged robots such as quadrupeds are to be deployed in terrains with geometries
which are difficult to model and predict, the need arises to equip them with
the capability to generalize well to unforeseen situations. In this work, we
propose a novel technique for training neural-network policies for
terrain-aware locomotion, which combines state-of-the-art methods for
model-based motion planning and reinforcement learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.02945v1' target='_blank'>A pedestrian path-planning model in accordance with obstacle's danger
  with reinforcement learning</a></h2>
<p><strong>Authors:</strong> Thanh-Trung Trinh, Dinh-Minh Vu, Masaomi Kimura</p>
<p><strong>Summary:</strong> Most microscopic pedestrian navigation models use the concept of "forces"
applied to the pedestrian agents to replicate the navigation environment. While
the approach could provide believable results in regular situations, it does
not always resemble natural pedestrian navigation behaviour in many typical
settings. In our research, we proposed a novel approach using reinforcement
learning for simulation of pedestrian agent path planning and collision
avoidance problem. The primary focus of this ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.08299v3' target='_blank'>GLIB: Efficient Exploration for Relational Model-Based Reinforcement
  Learning via Goal-Literal Babbling</a></h2>
<p><strong>Authors:</strong> Rohan Chitnis, Tom Silver, Joshua Tenenbaum, Leslie Pack Kaelbling, Tomas Lozano-Perez</p>
<p><strong>Summary:</strong> We address the problem of efficient exploration for transition model learning
in the relational model-based reinforcement learning setting without extrinsic
goals or rewards. Inspired by human curiosity, we propose goal-literal babbling
(GLIB), a simple and general method for exploration in such problems. GLIB
samples relational conjunctive goals that can be understood as specific,
targeted effects that the agent would like to achieve in the world, and plans
to achieve these goals using the tran...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.05694v1' target='_blank'>Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones</a></h2>
<p><strong>Authors:</strong> Ugurkan Ates</p>
<p><strong>Summary:</strong> In this paper, we study a long-term planning scenario that is based on drone
racing competitions held in real life. We conducted this experiment on a
framework created for "Game of Drones: Drone Racing Competition" at NeurIPS
2019. The racing environment was created using Microsoft's AirSim Drone Racing
Lab. A reinforcement learning agent, a simulated quadrotor in our case, has
trained with the Policy Proximal Optimization(PPO) algorithm was able to
successfully compete against another simulated...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.03094v3' target='_blank'>RLOC: Terrain-Aware Legged Locomotion using Reinforcement Learning and
  Optimal Control</a></h2>
<p><strong>Authors:</strong> Siddhant Gangapurwala, Mathieu Geisert, Romeo Orsolino, Maurice Fallon, Ioannis Havoutis</p>
<p><strong>Summary:</strong> We present a unified model-based and data-driven approach for quadrupedal
planning and control to achieve dynamic locomotion over uneven terrain. We
utilize on-board proprioceptive and exteroceptive feedback to map sensory
information and desired base velocity commands into footstep plans using a
reinforcement learning (RL) policy. This RL policy is trained in simulation
over a wide range of procedurally generated terrains. When ran online, the
system tracks the generated footstep plans using a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.08413v1' target='_blank'>Robust Reinforcement Learning Under Minimax Regret for Green Security</a></h2>
<p><strong>Authors:</strong> Lily Xu, Andrew Perrault, Fei Fang, Haipeng Chen, Milind Tambe</p>
<p><strong>Summary:</strong> Green security domains feature defenders who plan patrols in the face of
uncertainty about the adversarial behavior of poachers, illegal loggers, and
illegal fishers. Importantly, the deterrence effect of patrols on adversaries'
future behavior makes patrol planning a sequential decision-making problem.
Therefore, we focus on robust sequential patrol planning for green security
following the minimax regret criterion, which has not been considered in the
literature. We formulate the problem as a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.13229v2' target='_blank'>Model-Based Reinforcement Learning via Latent-Space Collocation</a></h2>
<p><strong>Authors:</strong> Oleh Rybkin, Chuning Zhu, Anusha Nagabandi, Kostas Daniilidis, Igor Mordatch, Sergey Levine</p>
<p><strong>Summary:</strong> The ability to plan into the future while utilizing only raw high-dimensional
observations, such as images, can provide autonomous agents with broad
capabilities. Visual model-based reinforcement learning (RL) methods that plan
future actions directly have shown impressive results on tasks that require
only short-horizon reasoning, however, these methods struggle on temporally
extended tasks. We argue that it is easier to solve long-horizon tasks by
planning sequences of states rather than just ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.13570v2' target='_blank'>Adaptive Informative Path Planning Using Deep Reinforcement Learning for
  UAV-based Active Sensing</a></h2>
<p><strong>Authors:</strong> Julius R√ºckin, Liren Jin, Marija Popoviƒá</p>
<p><strong>Summary:</strong> Aerial robots are increasingly being utilized for environmental monitoring
and exploration. However, a key challenge is efficiently planning paths to
maximize the information value of acquired data as an initially unknown
environment is explored. To address this, we propose a new approach for
informative path planning based on deep reinforcement learning (RL). Combining
recent advances in RL and robotic applications, our method combines tree search
with an offline-learned neural network predicti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.00055v2' target='_blank'>Deep Reinforcement Learning for Join Order Enumeration</a></h2>
<p><strong>Authors:</strong> Ryan Marcus, Olga Papaemmanouil</p>
<p><strong>Summary:</strong> Join order selection plays a significant role in query performance. However,
modern query optimizers typically employ static join enumeration algorithms
that do not receive any feedback about the quality of the resulting plan.
Hence, optimizers often repeatedly choose the same bad plan, as they do not
have a mechanism for "learning from their mistakes". In this paper, we argue
that existing deep reinforcement learning techniques can be applied to address
this challenge. These techniques, powered...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.02680v1' target='_blank'>Combining Planning and Deep Reinforcement Learning in Tactical Decision
  Making for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Carl-Johan Hoel, Katherine Driggs-Campbell, Krister Wolff, Leo Laine, Mykel J. Kochenderfer</p>
<p><strong>Summary:</strong> Tactical decision making for autonomous driving is challenging due to the
diversity of environments, the uncertainty in the sensor information, and the
complex interaction with other road users. This paper introduces a general
framework for tactical decision making, which combines the concepts of planning
and learning, in the form of Monte Carlo tree search and deep reinforcement
learning. The method is based on the AlphaGo Zero algorithm, which is extended
to a domain with a continuous state sp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.12604v1' target='_blank'>Graph neural induction of value iteration</a></h2>
<p><strong>Authors:</strong> Andreea Deac, Pierre-Luc Bacon, Jian Tang</p>
<p><strong>Summary:</strong> Many reinforcement learning tasks can benefit from explicit planning based on
an internal model of the environment. Previously, such planning components have
been incorporated through a neural network that partially aligns with the
computational graph of value iteration. Such network have so far been focused
on restrictive environments (e.g. grid-worlds), and modelled the planning
procedure only indirectly. We relax these constraints, proposing a graph neural
network (GNN) that executes the valu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.05079v1' target='_blank'>SAGE: Generating Symbolic Goals for Myopic Models in Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Andrew Chester, Michael Dann, Fabio Zambetta, John Thangarajah</p>
<p><strong>Summary:</strong> Model-based reinforcement learning algorithms are typically more sample
efficient than their model-free counterparts, especially in sparse reward
problems. Unfortunately, many interesting domains are too complex to specify
the complete models required by traditional model-based approaches. Learning a
model takes a large number of environment samples, and may not capture critical
information if the environment is hard to explore. If we could specify an
incomplete model and allow the agent to lear...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.10823v1' target='_blank'>Long Short-Term Memory for Spatial Encoding in Multi-Agent Path Planning</a></h2>
<p><strong>Authors:</strong> Marc R. Schlichting, Stefan Notter, Walter Fichter</p>
<p><strong>Summary:</strong> Reinforcement learning-based path planning for multi-agent systems of varying
size constitutes a research topic with increasing significance as progress in
domains such as urban air mobility and autonomous aerial vehicles continues.
Reinforcement learning with continuous state and action spaces is used to train
a policy network that accommodates desirable path planning behaviors and can be
used for time-critical applications. A Long Short-Term Memory module is
proposed to encode an unspecified n...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.03597v2' target='_blank'>Imitating, Fast and Slow: Robust learning from demonstrations via
  decision-time planning</a></h2>
<p><strong>Authors:</strong> Carl Qi, Pieter Abbeel, Aditya Grover</p>
<p><strong>Summary:</strong> The goal of imitation learning is to mimic expert behavior from
demonstrations, without access to an explicit reward signal. A popular class of
approach infers the (unknown) reward function via inverse reinforcement
learning (IRL) followed by maximizing this reward function via reinforcement
learning (RL). The policies learned via these approaches are however very
brittle in practice and deteriorate quickly even with small test-time
perturbations due to compounding errors. We propose Imitation w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.00270v1' target='_blank'>Provably Efficient Lifelong Reinforcement Learning with Linear Function
  Approximation</a></h2>
<p><strong>Authors:</strong> Sanae Amani, Lin F. Yang, Ching-An Cheng</p>
<p><strong>Summary:</strong> We study lifelong reinforcement learning (RL) in a regret minimization
setting of linear contextual Markov decision process (MDP), where the agent
needs to learn a multi-task policy while solving a streaming sequence of tasks.
We propose an algorithm, called UCB Lifelong Value Distillation (UCBlvd), that
provably achieves sublinear regret for any sequence of tasks, which may be
adaptively chosen based on the agent's past behaviors. Remarkably, our
algorithm uses only sublinear number of planning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.07514v1' target='_blank'>Decentralized Coverage Path Planning with Reinforcement Learning and
  Dual Guidance</a></h2>
<p><strong>Authors:</strong> Yongkai Liu, Jiawei Hu, Wei Dong</p>
<p><strong>Summary:</strong> Planning coverage path for multiple robots in a decentralized way enhances
robustness to coverage tasks handling uncertain malfunctions. To achieve high
efficiency in a distributed manner for each single robot, a comprehensive
understanding of both the complicated environments and cooperative agents
intent is crucial. Unfortunately, existing works commonly consider only part of
these factors, resulting in imbalanced subareas or unnecessary overlaps. To
tackle this issue, we introduce a Decentral...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.00460v1' target='_blank'>Multi-Arm Robot Task Planning for Fruit Harvesting Using Multi-Agent
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tao Li, Feng Xie, Ya Xiong, Qingchun Feng</p>
<p><strong>Summary:</strong> The emergence of harvesting robotics offers a promising solution to the issue
of limited agricultural labor resources and the increasing demand for fruits.
Despite notable advancements in the field of harvesting robotics, the
utilization of such technology in orchards is still limited. The key challenge
is to improve operational efficiency. Taking into account inner-arm conflicts,
couplings of DoFs, and dynamic tasks, we propose a task planning strategy for a
harvesting robot with four arms in t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.01346v3' target='_blank'>Co-learning Planning and Control Policies Constrained by Differentiable
  Logic Specifications</a></h2>
<p><strong>Authors:</strong> Zikang Xiong, Daniel Lawson, Joe Eappen, Ahmed H. Qureshi, Suresh Jagannathan</p>
<p><strong>Summary:</strong> Synthesizing planning and control policies in robotics is a fundamental task,
further complicated by factors such as complex logic specifications and
high-dimensional robot dynamics. This paper presents a novel reinforcement
learning approach to solving high-dimensional robot navigation tasks with
complex logic specifications by co-learning planning and control policies.
Notably, this approach significantly reduces the sample complexity in training,
allowing us to train high-quality policies wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.05022v1' target='_blank'>Learned Parameter Selection for Robotic Information Gathering</a></h2>
<p><strong>Authors:</strong> Christopher E. Denniston, Gautam Salhotra, Akseli Kangaslahti, David A. Caron, Gaurav S. Sukhatme</p>
<p><strong>Summary:</strong> When robots are deployed in the field for environmental monitoring they
typically execute pre-programmed motions, such as lawnmower paths, instead of
adaptive methods, such as informative path planning. One reason for this is
that adaptive methods are dependent on parameter choices that are both critical
to set correctly and difficult for the non-specialist to choose. Here, we show
how to automatically configure a planner for informative path planning by
training a reinforcement learning agent t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.03365v3' target='_blank'>Decision-Focused Model-based Reinforcement Learning for Reward Transfer</a></h2>
<p><strong>Authors:</strong> Abhishek Sharma, Sonali Parbhoo, Omer Gottesman, Finale Doshi-Velez</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) provides a way to learn a
transition model of the environment, which can then be used to plan
personalized policies for different patient cohorts and to understand the
dynamics involved in the decision-making process. However, standard MBRL
algorithms are either sensitive to changes in the reward function or achieve
suboptimal performance on the task when the transition model is restricted.
Motivated by the need to use simple and interpretable models in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.12828v1' target='_blank'>A optimization framework for herbal prescription planning based on deep
  reinforcement learning</a></h2>
<p><strong>Authors:</strong> Kuo Yang, Zecong Yu, Xin Su, Xiong He, Ning Wang, Qiguang Zheng, Feidie Yu, Zhuang Liu, Tiancai Wen, Xuezhong Zhou</p>
<p><strong>Summary:</strong> Treatment planning for chronic diseases is a critical task in medical
artificial intelligence, particularly in traditional Chinese medicine (TCM).
However, generating optimized sequential treatment strategies for patients with
chronic diseases in different clinical encounters remains a challenging issue
that requires further exploration. In this study, we proposed a TCM herbal
prescription planning framework based on deep reinforcement learning for
chronic disease treatment (PrescDRL). PrescDRL ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.09466v1' target='_blank'>Simplified Temporal Consistency Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yi Zhao, Wenshuai Zhao, Rinu Boney, Juho Kannala, Joni Pajarinen</p>
<p><strong>Summary:</strong> Reinforcement learning is able to solve complex sequential decision-making
tasks but is currently limited by sample efficiency and required computation.
To improve sample efficiency, recent work focuses on model-based RL which
interleaves model learning with planning. Recent methods further utilize policy
learning, value estimation, and, self-supervised learning as auxiliary
objectives. In this paper we show that, surprisingly, a simple representation
learning approach relying only on a latent d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.16978v4' target='_blank'>Learning Coverage Paths in Unknown Environments with Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Arvi Jonnarth, Jie Zhao, Michael Felsberg</p>
<p><strong>Summary:</strong> Coverage path planning (CPP) is the problem of finding a path that covers the
entire free space of a confined area, with applications ranging from robotic
lawn mowing to search-and-rescue. When the environment is unknown, the path
needs to be planned online while mapping the environment, which cannot be
addressed by offline planning methods that do not allow for a flexible path
space. We investigate how suitable reinforcement learning is for this
challenging problem, and analyze the involved com...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.05391v1' target='_blank'>Career Path Recommendations for Long-term Income Maximization: A
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Spyros Avlonitis, Dor Lavi, Masoud Mansoury, David Graus</p>
<p><strong>Summary:</strong> This study explores the potential of reinforcement learning algorithms to
enhance career planning processes. Leveraging data from Randstad The
Netherlands, the study simulates the Dutch job market and develops strategies
to optimize employees' long-term income. By formulating career planning as a
Markov Decision Process (MDP) and utilizing machine learning algorithms such as
Sarsa, Q-Learning, and A2C, we learn optimal policies that recommend career
paths with high-income occupations and industr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.13614v2' target='_blank'>Boosting Offline Reinforcement Learning for Autonomous Driving with
  Hierarchical Latent Skills</a></h2>
<p><strong>Authors:</strong> Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao</p>
<p><strong>Summary:</strong> Learning-based vehicle planning is receiving increasing attention with the
emergence of diverse driving simulators and large-scale driving datasets. While
offline reinforcement learning (RL) is well suited for these safety-critical
tasks, it still struggles to plan over extended periods. In this work, we
present a skill-based framework that enhances offline RL to overcome the
long-horizon vehicle planning challenge. Specifically, we design a variational
autoencoder (VAE) to learn skills from off...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.02167v1' target='_blank'>Towards a Unified Framework for Sequential Decision Making</a></h2>
<p><strong>Authors:</strong> Carlos N√∫√±ez-Molina, Pablo Mesejo, Juan Fern√°ndez-Olivares</p>
<p><strong>Summary:</strong> In recent years, the integration of Automated Planning (AP) and Reinforcement
Learning (RL) has seen a surge of interest. To perform this integration, a
general framework for Sequential Decision Making (SDM) would prove immensely
useful, as it would help us understand how AP and RL fit together. In this
preliminary work, we attempt to provide such a framework, suitable for any
method ranging from Classical Planning to Deep RL, by drawing on concepts from
Probability Theory and Bayesian inference...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.09459v3' target='_blank'>On Convex Optimal Value Functions For POSGs</a></h2>
<p><strong>Authors:</strong> Rafael F. Cunha, Jacopo Castellini, Johan Peralez, Jilles S. Dibangoye</p>
<p><strong>Summary:</strong> Multi-agent planning and reinforcement learning can be challenging when
agents cannot see the state of the world or communicate with each other due to
communication costs, latency, or noise. Partially Observable Stochastic Games
(POSGs) provide a mathematical framework for modelling such scenarios. This
paper aims to improve the efficiency of planning and reinforcement learning
algorithms for POSGs by identifying the underlying structure of optimal
state-value functions. The approach involves re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.01747v1' target='_blank'>Autonomous and Adaptive Role Selection for Multi-robot Collaborative
  Area Search Based on Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Lina Zhu, Jiyu Cheng, Hao Zhang, Zhichao Cui, Wei Zhang, Yuehu Liu</p>
<p><strong>Summary:</strong> In the tasks of multi-robot collaborative area search, we propose the unified
approach for simultaneous mapping for sensing more targets (exploration) while
searching and locating the targets (coverage). Specifically, we implement a
hierarchical multi-agent reinforcement learning algorithm to decouple task
planning from task execution. The role concept is integrated into the
upper-level task planning for role selection, which enables robots to learn the
role based on the state status from the up...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.01216v1' target='_blank'>Let Hybrid A* Path Planner Obey Traffic Rules: A Deep Reinforcement
  Learning-Based Planning Framework</a></h2>
<p><strong>Authors:</strong> Xibo Li, Shruti Patel, Christof B√ºskens</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) allows a system to interact with its
environment and take actions by training an efficient policy that maximizes
self-defined rewards. In autonomous driving, it can be used as a strategy for
high-level decision making, whereas low-level algorithms such as the hybrid A*
path planning have proven their ability to solve the local trajectory planning
problem. In this work, we combine these two methods where the DRL makes
high-level decisions such as lane change comm...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02217v1' target='_blank'>Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zakariae El Asri, Olivier Sigaud, Nicolas Thome</p>
<p><strong>Summary:</strong> Applying reinforcement learning (RL) to real-world applications requires
addressing a trade-off between asymptotic performance, sample efficiency, and
inference time. In this work, we demonstrate how to address this triple
challenge by leveraging partial physical knowledge about the system dynamics.
Our approach involves learning a physics-informed model to boost sample
efficiency and generating imaginary trajectories from this model to learn a
model-free policy and Q-function. Furthermore, we p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.05460v1' target='_blank'>Trajectory Planning for Teleoperated Space Manipulators Using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Bo Xia, Xianru Tian, Bo Yuan, Zhiheng Li, Bin Liang, Xueqian Wang</p>
<p><strong>Summary:</strong> Trajectory planning for teleoperated space manipulators involves challenges
such as accurately modeling system dynamics, particularly in free-floating
modes with non-holonomic constraints, and managing time delays that increase
model uncertainty and affect control precision. Traditional teleoperation
methods rely on precise dynamic models requiring complex parameter
identification and calibration, while data-driven methods do not require prior
knowledge but struggle with time delays. A novel fra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.13750v3' target='_blank'>Multi-Agent Target Assignment and Path Finding for Intelligent
  Warehouse: A Cooperative Multi-Agent Deep Reinforcement Learning Perspective</a></h2>
<p><strong>Authors:</strong> Qi Liu, Jianqi Gao, Dongjie Zhu, Zhongjian Qiao, Pengbin Chen, Jingxiang Guo, Yanjie Li</p>
<p><strong>Summary:</strong> Multi-agent target assignment and path planning (TAPF) are two key problems
in intelligent warehouse. However, most literature only addresses one of these
two problems separately. In this study, we propose a method to simultaneously
solve target assignment and path planning from a perspective of cooperative
multi-agent deep reinforcement learning (RL). To the best of our knowledge,
this is the first work to model the TAPF problem for intelligent warehouse to
cooperative multi-agent deep RL, and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.00968v1' target='_blank'>Solving Integrated Process Planning and Scheduling Problem via Graph
  Neural Network Based Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hongpei Li, Han Zhang, Ziyan He, Yunkai Jia, Bo Jiang, Xiang Huang, Dongdong Ge</p>
<p><strong>Summary:</strong> The Integrated Process Planning and Scheduling (IPPS) problem combines
process route planning and shop scheduling to achieve high efficiency in
manufacturing and maximize resource utilization, which is crucial for modern
manufacturing systems. Traditional methods using Mixed Integer Linear
Programming (MILP) and heuristic algorithms can not well balance solution
quality and speed when solving IPPS. In this paper, we propose a novel
end-to-end Deep Reinforcement Learning (DRL) method. We model th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.12237v1' target='_blank'>Equivariant Action Sampling for Reinforcement Learning and Planning</a></h2>
<p><strong>Authors:</strong> Linfeng Zhao, Owen Howell, Xupeng Zhu, Jung Yeon Park, Zhewen Zhang, Robin Walters, Lawson L. S. Wong</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) algorithms for continuous control tasks require
accurate sampling-based action selection. Many tasks, such as robotic
manipulation, contain inherent problem symmetries. However, correctly
incorporating symmetry into sampling-based approaches remains a challenge. This
work addresses the challenge of preserving symmetry in sampling-based planning
and control, a key component for enhancing decision-making efficiency in RL. We
introduce an action sampling approach that en...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.16395v1' target='_blank'>Autonomous Option Invention for Continual Hierarchical Reinforcement
  Learning and Planning</a></h2>
<p><strong>Authors:</strong> Rashmeet Kaur Nayyar, Siddharth Srivastava</p>
<p><strong>Summary:</strong> Abstraction is key to scaling up reinforcement learning (RL). However,
autonomously learning abstract state and action representations to enable
transfer and generalization remains a challenging open problem. This paper
presents a novel approach for inventing, representing, and utilizing options,
which represent temporally extended behaviors, in continual RL settings. Our
approach addresses streams of stochastic problems characterized by long
horizons, sparse rewards, and unknown transition and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.14488v1' target='_blank'>Breaking the Pre-Planning Barrier: Real-Time Adaptive Coordination of
  Mission and Charging UAVs Using Graph Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yuhan Hu, Yirong Sun, Yanjun Chen, Xinghao Chen</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicles (UAVs) are pivotal in applications such as search
and rescue and environmental monitoring, excelling in intelligent perception
tasks. However, their limited battery capacity hinders long-duration and
long-distance missions. Charging UAVs (CUAVs) offers a potential solution by
recharging mission UAVs (MUAVs), but existing methods rely on impractical
pre-planned routes, failing to enable organic cooperation and limiting mission
efficiency. We introduce a novel multi-agent ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.06813v1' target='_blank'>Automatic Inverse Treatment Planning for Gamma Knife Radiosurgery via
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yingzi Liu, Chenyang Shen, Tonghe Wang, Jiahan Zhang, Xiaofeng Yang, Tian Liu, Shannon Kahn, Hui-Kuo Shu, Zhen Tian</p>
<p><strong>Summary:</strong> Purpose: Several inverse planning algorithms have been developed for Gamma
Knife (GK) radiosurgery to determine a large number of plan parameters via
solving an optimization problem, which typically consists of multiple
objectives. The priorities among these objectives need to be repetitively
adjusted to achieve a clinically good plan for each patient. This study aimed
to achieve automatic and intelligent priority-tuning, by developing a deep
reinforcement learning (DRL) based method to model th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.11293v1' target='_blank'>Evolutionary Planning in Latent Space</a></h2>
<p><strong>Authors:</strong> Thor V. A. N. Olesen, Dennis T. T. Nguyen, Rasmus Berg Palm, Sebastian Risi</p>
<p><strong>Summary:</strong> Planning is a powerful approach to reinforcement learning with several
desirable properties. However, it requires a model of the world, which is not
readily available in many real-life problems. In this paper, we propose to
learn a world model that enables Evolutionary Planning in Latent Space (EPLS).
We use a Variational Auto Encoder (VAE) to learn a compressed latent
representation of individual observations and extend a Mixture Density
Recurrent Neural Network (MDRNN) to learn a stochastic, m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.08787v3' target='_blank'>Conditional Predictive Behavior Planning with Inverse Reinforcement
  Learning for Human-like Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Zhiyu Huang, Haochen Liu, Jingda Wu, Chen Lv</p>
<p><strong>Summary:</strong> Making safe and human-like decisions is an essential capability of autonomous
driving systems, and learning-based behavior planning presents a promising
pathway toward achieving this objective. Distinguished from existing
learning-based methods that directly output decisions, this work introduces a
predictive behavior planning framework that learns to predict and evaluate from
human driving data. This framework consists of three components: a behavior
generation module that produces a diverse se...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.18236v4' target='_blank'>Multi-AGV Path Planning Method via Reinforcement Learning and Particle
  Filters</a></h2>
<p><strong>Authors:</strong> Shao Shuo</p>
<p><strong>Summary:</strong> Thanks to its robust learning and search stabilities,the reinforcement
learning (RL) algorithm has garnered increasingly significant attention and
been exten-sively applied in Automated Guided Vehicle (AGV) path planning.
However, RL-based planning algorithms have been discovered to suffer from the
substantial variance of neural networks caused by environmental instability and
significant fluctua-tions in system structure. These challenges manifest in
slow convergence speed and low learning effi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.10658v2' target='_blank'>Trajectory Planning Using Reinforcement Learning for Interactive
  Overtaking Maneuvers in Autonomous Racing Scenarios</a></h2>
<p><strong>Authors:</strong> Levent √ñgretmen, Mo Chen, Phillip Pitschi, Boris Lohmann</p>
<p><strong>Summary:</strong> Conventional trajectory planning approaches for autonomous racing are based
on the sequential execution of prediction of the opposing vehicles and
subsequent trajectory planning for the ego vehicle. If the opposing vehicles do
not react to the ego vehicle, they can be predicted accurately. However, if
there is interaction between the vehicles, the prediction loses its validity.
For high interaction, instead of a planning approach that reacts exclusively to
the fixed prediction, a trajectory plan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.16313v1' target='_blank'>CATP-LLM: Empowering Large Language Models for Cost-Aware Tool Planning</a></h2>
<p><strong>Authors:</strong> Duo Wu, Jinghe Wang, Yuan Meng, Yanning Zhang, Le Sun, Zhi Wang</p>
<p><strong>Summary:</strong> Utilizing large language models (LLMs) for tool planning has emerged as a
promising avenue for developing general AI systems, where LLMs automatically
schedule external tools (e.g. vision models) to tackle complex tasks based on
task descriptions. To push this paradigm toward practical applications, it is
crucial for LLMs to consider tool execution costs (e.g. execution time) for
tool planning. Unfortunately, prior studies overlook the tool execution costs,
leading to the generation of expensive...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.05751v2' target='_blank'>Trajectory Optimization for Unknown Constrained Systems using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kei Ota, Devesh K. Jha, Tomoaki Oiki, Mamoru Miura, Takashi Nammoto, Daniel Nikovski, Toshisada Mariyama</p>
<p><strong>Summary:</strong> In this paper, we propose a reinforcement learning-based algorithm for
trajectory optimization for constrained dynamical systems. This problem is
motivated by the fact that for most robotic systems, the dynamics may not
always be known. Generating smooth, dynamically feasible trajectories could be
difficult for such systems. Using sampling-based algorithms for motion planning
may result in trajectories that are prone to undesirable control jumps.
However, they can usually provide a good referenc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.10521v1' target='_blank'>Which Channel to Ask My Question? Personalized Customer Service Request
  Stream Routing using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zining Liu, Chong Long, Xiaolu Lu, Zehong Hu, Jie Zhang, Yafang Wang</p>
<p><strong>Summary:</strong> Customer services are critical to all companies, as they may directly connect
to the brand reputation. Due to a great number of customers, e-commerce
companies often employ multiple communication channels to answer customers'
questions, for example, chatbot and hotline. On one hand, each channel has
limited capacity to respond to customers' requests, on the other hand,
customers have different preferences over these channels. The current
production systems are mainly built based on business rule...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.08684v3' target='_blank'>Efficient Model-Based Reinforcement Learning through Optimistic Policy
  Search and Planning</a></h2>
<p><strong>Authors:</strong> Sebastian Curi, Felix Berkenkamp, Andreas Krause</p>
<p><strong>Summary:</strong> Model-based reinforcement learning algorithms with probabilistic dynamical
models are amongst the most data-efficient learning methods. This is often
attributed to their ability to distinguish between epistemic and aleatoric
uncertainty. However, while most algorithms distinguish these two uncertainties
for learning the model, they ignore it when optimizing the policy, which leads
to greedy and insufficient exploration. At the same time, there are no
practical solvers for optimistic exploration ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.07792v2' target='_blank'>ReLMoGen: Leveraging Motion Generation in Reinforcement Learning for
  Mobile Manipulation</a></h2>
<p><strong>Authors:</strong> Fei Xia, Chengshu Li, Roberto Mart√≠n-Mart√≠n, Or Litany, Alexander Toshev, Silvio Savarese</p>
<p><strong>Summary:</strong> Many Reinforcement Learning (RL) approaches use joint control signals
(positions, velocities, torques) as action space for continuous control tasks.
We propose to lift the action space to a higher level in the form of subgoals
for a motion generator (a combination of motion planner and trajectory
executor). We argue that, by lifting the action space and by leveraging
sampling-based motion planners, we can efficiently use RL to solve complex,
long-horizon tasks that could not be solved with exist...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.11137v2' target='_blank'>Program Synthesis Guided Reinforcement Learning for Partially Observed
  Environments</a></h2>
<p><strong>Authors:</strong> Yichen David Yang, Jeevana Priya Inala, Osbert Bastani, Yewen Pu, Armando Solar-Lezama, Martin Rinard</p>
<p><strong>Summary:</strong> A key challenge for reinforcement learning is solving long-horizon planning
problems. Recent work has leveraged programs to guide reinforcement learning in
these settings. However, these approaches impose a high manual burden on the
user since they must provide a guiding program for every new task. Partially
observed environments further complicate the programming task because the
program must implement a strategy that correctly, and ideally optimally,
handles every possible configuration of the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.03450v2' target='_blank'>DeepFreight: Integrating Deep Reinforcement Learning and Mixed Integer
  Programming for Multi-transfer Truck Freight Delivery</a></h2>
<p><strong>Authors:</strong> Jiayu Chen, Abhishek K. Umrawal, Tian Lan, Vaneet Aggarwal</p>
<p><strong>Summary:</strong> With the freight delivery demands and shipping costs increasing rapidly,
intelligent control of fleets to enable efficient and cost-conscious solutions
becomes an important problem. In this paper, we propose DeepFreight, a
model-free deep-reinforcement-learning-based algorithm for multi-transfer
freight delivery, which includes two closely-collaborative components:
truck-dispatch and package-matching. Specifically, a deep multi-agent
reinforcement learning framework called QMIX is leveraged to l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.08981v2' target='_blank'>Hierarchical Reinforcement Learning Framework for Stochastic Spaceflight
  Campaign Design</a></h2>
<p><strong>Authors:</strong> Yuji Takubo, Hao Chen, Koki Ho</p>
<p><strong>Summary:</strong> This paper develops a hierarchical reinforcement learning architecture for
multimission spaceflight campaign design under uncertainty, including vehicle
design, infrastructure deployment planning, and space transportation
scheduling. This problem involves a high-dimensional design space and is
challenging especially with uncertainty present. To tackle this challenge, the
developed framework has a hierarchical structure with reinforcement learning
and network-based mixed-integer linear programmin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.12385v1' target='_blank'>A deep Q-learning method for optimizing visual search strategies in
  backgrounds of dynamic noise</a></h2>
<p><strong>Authors:</strong> Weimin Zhou, Miguel P. Eckstein</p>
<p><strong>Summary:</strong> Humans process visual information with varying resolution (foveated visual
system) and explore images by orienting through eye movements the
high-resolution fovea to points of interest. The Bayesian ideal searcher (IS)
that employs complete knowledge of task-relevant information optimizes eye
movement strategy and achieves the optimal search performance. The IS can be
employed as an important tool to evaluate the optimality of human eye
movements, and potentially provide guidance to improve huma...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.09603v3' target='_blank'>Comparing Deep Reinforcement Learning Algorithms in Two-Echelon Supply
  Chains</a></h2>
<p><strong>Authors:</strong> Francesco Stranieri, Fabio Stella</p>
<p><strong>Summary:</strong> In this study, we analyze and compare the performance of state-of-the-art
deep reinforcement learning algorithms for solving the supply chain inventory
management problem. This complex sequential decision-making problem consists of
determining the optimal quantity of products to be produced and shipped across
different warehouses over a given time horizon. In particular, we present a
mathematical formulation of a two-echelon supply chain environment with
stochastic and seasonal demand, which all...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02072v2' target='_blank'>Deciding What to Model: Value-Equivalent Sampling for Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Dilip Arumugam, Benjamin Van Roy</p>
<p><strong>Summary:</strong> The quintessential model-based reinforcement-learning agent iteratively
refines its estimates or prior beliefs about the true underlying model of the
environment. Recent empirical successes in model-based reinforcement learning
with function approximation, however, eschew the true model in favor of a
surrogate that, while ignoring various facets of the environment, still
facilitates effective planning over behaviors. Recently formalized as the value
equivalence principle, this algorithmic techni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.08959v1' target='_blank'>Latent Plans for Task-Agnostic Offline Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, Wolfram Burgard</p>
<p><strong>Summary:</strong> Everyday tasks of long-horizon and comprising a sequence of multiple implicit
subtasks still impose a major challenge in offline robot control. While a
number of prior methods aimed to address this setting with variants of
imitation and offline reinforcement learning, the learned behavior is typically
narrow and often struggles to reach configurable long-horizon goals. As both
paradigms have complementary strengths and weaknesses, we propose a novel
hierarchical approach that combines the streng...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.13032v2' target='_blank'>Monte Carlo Tree Search Algorithms for Risk-Aware and Multi-Objective
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Conor F. Hayes, Mathieu Reymond, Diederik M. Roijers, Enda Howley, Patrick Mannion</p>
<p><strong>Summary:</strong> In many risk-aware and multi-objective reinforcement learning settings, the
utility of the user is derived from a single execution of a policy. In these
settings, making decisions based on the average future returns is not suitable.
For example, in a medical setting a patient may only have one opportunity to
treat their illness. Making decisions using just the expected future returns --
known in reinforcement learning as the value -- cannot account for the
potential range of adverse or positive ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.14919v2' target='_blank'>On Reward Structures of Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Falcon Z. Dai</p>
<p><strong>Summary:</strong> A Markov decision process can be parameterized by a transition kernel and a
reward function. Both play essential roles in the study of reinforcement
learning as evidenced by their presence in the Bellman equations. In our
inquiry of various kinds of "costs" associated with reinforcement learning
inspired by the demands in robotic applications, rewards are central to
understanding the structure of a Markov decision process and reward-centric
notions can elucidate important concepts in reinforceme...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.02091v1' target='_blank'>The Problem of Social Cost in Multi-Agent General Reinforcement
  Learning: Survey and Synthesis</a></h2>
<p><strong>Authors:</strong> Kee Siong Ng, Samuel Yang-Zhao, Timothy Cadogan-Cowper</p>
<p><strong>Summary:</strong> The AI safety literature is full of examples of powerful AI agents that, in
blindly pursuing a specific and usually narrow objective, ends up with
unacceptable and even catastrophic collateral damage to others. In this paper,
we consider the problem of social harms that can result from actions taken by
learning and utility-maximising agents in a multi-agent environment. The
problem of measuring social harms or impacts in such multi-agent settings,
especially when the agents are artificial genera...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.07997v1' target='_blank'>An interpretable planning bot for pancreas stereotactic body radiation
  therapy</a></h2>
<p><strong>Authors:</strong> Jiahan Zhang, Chunhao Wang, Yang Sheng, Manisha Palta, Brian Czito, Christopher Willett, Jiang Zhang, P James Jensen, Fang-Fang Yin, Qiuwen Wu, Yaorong Ge, Q Jackie Wu</p>
<p><strong>Summary:</strong> Pancreas stereotactic body radiotherapy treatment planning requires planners
to make sequential, time consuming interactions with the treatment planning
system (TPS) to reach the optimal dose distribution. We seek to develop a
reinforcement learning (RL)-based planning bot to systematically address
complex tradeoffs and achieve high plan quality consistently and efficiently.
The focus of pancreas SBRT planning is finding a balance between organs-at-risk
sparing and planning target volume (PTV) c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.02418v3' target='_blank'>Selective Dyna-style Planning Under Limited Model Capacity</a></h2>
<p><strong>Authors:</strong> Zaheer Abbas, Samuel Sokota, Erin J. Talvitie, Martha White</p>
<p><strong>Summary:</strong> In model-based reinforcement learning, planning with an imperfect model of
the environment has the potential to harm learning progress. But even when a
model is imperfect, it may still contain information that is useful for
planning. In this paper, we investigate the idea of using an imperfect model
selectively. The agent should plan in parts of the state space where the model
would be helpful but refrain from using the model where it would be harmful. An
effective selective planning mechanism r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.00764v1' target='_blank'>Have I done enough planning or should I plan more?</a></h2>
<p><strong>Authors:</strong> Ruiqi He, Yash Raj Jain, Falk Lieder</p>
<p><strong>Summary:</strong> People's decisions about how to allocate their limited computational
resources are essential to human intelligence. An important component of this
metacognitive ability is deciding whether to continue thinking about what to do
and move on to the next decision. Here, we show that people acquire this
ability through learning and reverse-engineer the underlying learning
mechanisms. Using a process-tracing paradigm that externalises human planning,
we find that people quickly adapt how much planning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.04772v1' target='_blank'>GrASP: Gradient-Based Affordance Selection for Planning</a></h2>
<p><strong>Authors:</strong> Vivek Veeriah, Zeyu Zheng, Richard Lewis, Satinder Singh</p>
<p><strong>Summary:</strong> Planning with a learned model is arguably a key component of intelligence.
There are several challenges in realizing such a component in large-scale
reinforcement learning (RL) problems. One such challenge is dealing effectively
with continuous action spaces when using tree-search planning (e.g., it is not
feasible to consider every action even at just the root node of the tree). In
this paper we present a method for selecting affordances useful for planning --
for learning which small number of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.08442v3' target='_blank'>A Look at Value-Based Decision-Time vs. Background Planning Methods
  Across Different Settings</a></h2>
<p><strong>Authors:</strong> Safa Alver, Doina Precup</p>
<p><strong>Summary:</strong> In model-based reinforcement learning (RL), an agent can leverage a learned
model to improve its way of behaving in different ways. Two of the prevalent
ways to do this are through decision-time and background planning methods. In
this study, we are interested in understanding how the value-based versions of
these two planning methods will compare against each other across different
settings. Towards this goal, we first consider the simplest instantiations of
value-based decision-time and backgr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.03787v2' target='_blank'>Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method
  and Contrastive Learning</a></h2>
<p><strong>Authors:</strong> Mostafa Kotb, Cornelius Weber, Stefan Wermter</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) with real-time planning has shown
great potential in locomotion and manipulation control tasks. However, the
existing planning methods, such as the Cross-Entropy Method (CEM), do not scale
well to complex high-dimensional environments. One of the key reasons for
underperformance is the lack of exploration, as these planning methods only aim
to maximize the cumulative extrinsic reward over the planning horizon.
Furthermore, planning inside the compact lat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.00311v1' target='_blank'>Efficient Planning with Latent Diffusion</a></h2>
<p><strong>Authors:</strong> Wenhao Li</p>
<p><strong>Summary:</strong> Temporal abstraction and efficient planning pose significant challenges in
offline reinforcement learning, mainly when dealing with domains that involve
temporally extended tasks and delayed sparse rewards. Existing methods
typically plan in the raw action space and can be inefficient and inflexible.
Latent action spaces offer a more flexible paradigm, capturing only possible
actions within the behavior policy support and decoupling the temporal
structure between planning and modeling. However, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.02644v1' target='_blank'>Simple Hierarchical Planning with Diffusion</a></h2>
<p><strong>Authors:</strong> Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre, Sungjin Ahn</p>
<p><strong>Summary:</strong> Diffusion-based generative methods have proven effective in modeling
trajectories with offline datasets. However, they often face computational
challenges and can falter in generalization, especially in capturing temporal
abstractions for long-horizon tasks. To overcome this, we introduce the
Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning
method combining the advantages of hierarchical and diffusion-based planning.
Our model adopts a "jumpy" planning strategy at the h...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02090v2' target='_blank'>Universal Plans: One Action Sequence to Solve Them All!</a></h2>
<p><strong>Authors:</strong> Kalle G. Timperi, Alexander J. LaValle, Steven M. LaValle</p>
<p><strong>Summary:</strong> This paper introduces the notion of a universal plan, which when executed, is
guaranteed to solve all planning problems in a category, regardless of the
obstacles, initial state, and goal set. Such plans are specified as a
deterministic sequence of actions that are blindly applied without any sensor
feedback. Thus, they can be considered as pure exploration in a reinforcement
learning context, and we show that with basic memory requirements, they even
yield optimal plans. Building upon results i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.19886v1' target='_blank'>PDDLFuse: A Tool for Generating Diverse Planning Domains</a></h2>
<p><strong>Authors:</strong> Vedant Khandelwal, Amit Sheth, Forest Agostinelli</p>
<p><strong>Summary:</strong> Various real-world challenges require planning algorithms that can adapt to a
broad range of domains. Traditionally, the creation of planning domains has
relied heavily on human implementation, which limits the scale and diversity of
available domains. While recent advancements have leveraged generative AI
technologies such as large language models (LLMs) for domain creation, these
efforts have predominantly focused on translating existing domains from natural
language descriptions rather than g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.00300v1' target='_blank'>PlanCritic: Formal Planning with Human Feedback</a></h2>
<p><strong>Authors:</strong> Owen Burns, Dana Hughes, Katia Sycara</p>
<p><strong>Summary:</strong> Real world planning problems are often too complex to be effectively tackled
by a single unaided human. To alleviate this, some recent work has focused on
developing a collaborative planning system to assist humans in complex domains,
with bridging the gap between the system's problem representation and the real
world being a key consideration. Transferring the speed and correctness formal
planners provide to real-world planning problems is greatly complicated by the
dynamic and online nature of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.00346v1' target='_blank'>Actor Critic with Experience Replay-based automatic treatment planning
  for prostate cancer intensity modulated radiotherapy</a></h2>
<p><strong>Authors:</strong> Md Mainul Abrar, Parvat Sapkota, Damon Sprouts, Xun Jia, Yujie Chi</p>
<p><strong>Summary:</strong> Background: Real-time treatment planning in IMRT is challenging due to
complex beam interactions. AI has improved automation, but existing models
require large, high-quality datasets and lack universal applicability. Deep
reinforcement learning (DRL) offers a promising alternative by mimicking human
trial-and-error planning.
  Purpose: Develop a stochastic policy-based DRL agent for automatic treatment
planning with efficient training, broad applicability, and robustness against
adversarial atta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.01700v1' target='_blank'>Combining Subgoal Graphs with Reinforcement Learning to Build a Rational
  Pathfinder</a></h2>
<p><strong>Authors:</strong> Junjie Zeng, Long Qin, Yue Hu, Cong Hu, Quanjun Yin</p>
<p><strong>Summary:</strong> In this paper, we present a hierarchical path planning framework called SG-RL
(subgoal graphs-reinforcement learning), to plan rational paths for agents
maneuvering in continuous and uncertain environments. By "rational", we mean
(1) efficient path planning to eliminate first-move lags; (2) collision-free
and smooth for agents with kinematic constraints satisfied. SG-RL works in a
two-level manner. At the first level, SG-RL uses a geometric path-planning
method, i.e., Simple Subgoal Graphs (SSG)...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.04201v1' target='_blank'>Learning Latent State Spaces for Planning through Reward Prediction</a></h2>
<p><strong>Authors:</strong> Aaron Havens, Yi Ouyang, Prabhat Nagarajan, Yasuhiro Fujita</p>
<p><strong>Summary:</strong> Model-based reinforcement learning methods typically learn models for
high-dimensional state spaces by aiming to reconstruct and predict the original
observations. However, drawing inspiration from model-free reinforcement
learning, we propose learning a latent dynamics model directly from rewards. In
this work, we introduce a model-based planning framework which learns a latent
reward prediction model and then plans in the latent state-space. The latent
representation is learned exclusively fro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.06917v4' target='_blank'>UAV Path Planning using Global and Local Map Information with Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mirco Theile, Harald Bayerlein, Richard Nai, David Gesbert, Marco Caccamo</p>
<p><strong>Summary:</strong> Path planning methods for autonomous unmanned aerial vehicles (UAVs) are
typically designed for one specific type of mission. This work presents a
method for autonomous UAV path planning based on deep reinforcement learning
(DRL) that can be applied to a wide range of mission scenarios. Specifically,
we compare coverage path planning (CPP), where the UAV's goal is to survey an
area of interest to data harvesting (DH), where the UAV collects data from
distributed Internet of Things (IoT) sensor d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.15310v2' target='_blank'>Is Policy Learning Overrated?: Width-Based Planning and Active Learning
  for Atari</a></h2>
<p><strong>Authors:</strong> Benjamin Ayton, Masataro Asai</p>
<p><strong>Summary:</strong> Width-based planning has shown promising results on Atari 2600 games using
pixel input, while using substantially fewer environment interactions than
reinforcement learning. Recent width-based approaches have computed feature
vectors for each screen using a hand designed feature set or a variational
autoencoder trained on game screens (VAE-IW), and prune screens that do not
have novel features during the search. We propose Olive (Online-VAE-IW), which
updates the VAE features online using active...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.05716v6' target='_blank'>Acting upon Imagination: when to trust imagined trajectories in model
  based reinforcement learning</a></h2>
<p><strong>Authors:</strong> Adrian Remonda, Eduardo Veas, Granit Luzhnica</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) aims to learn model(s) of the
environment dynamics that can predict the outcome of its actions. Forward
application of the model yields so called imagined trajectories (sequences of
action, predicted state-reward) used to optimize the set of candidate actions
that maximize expected reward. The outcome, an ideal imagined trajectory or
plan, is imperfect and typically MBRL relies on model predictive control (MPC)
to overcome this by continuously re-plannin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.06618v1' target='_blank'>Adaptive Selection of Informative Path Planning Strategies via
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Taeyeong Choi, Grzegorz Cielniak</p>
<p><strong>Summary:</strong> In our previous work, we designed a systematic policy to prioritize sampling
locations to lead significant accuracy improvement in spatial interpolation by
using the prediction uncertainty of Gaussian Process Regression (GPR) as
"attraction force" to deployed robots in path planning. Although the
integration with Traveling Salesman Problem (TSP) solvers was also shown to
produce relatively short travel distance, we here hypothesise several factors
that could decrease the overall prediction preci...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.13386v1' target='_blank'>Reducing Planning Complexity of General Reinforcement Learning with
  Non-Markovian Abstractions</a></h2>
<p><strong>Authors:</strong> Sultan J. Majeed, Marcus Hutter</p>
<p><strong>Summary:</strong> The field of General Reinforcement Learning (GRL) formulates the problem of
sequential decision-making from ground up. The history of interaction
constitutes a "ground" state of the system, which never repeats. On the one
hand, this generality allows GRL to model almost every domain possible, e.g.\
Bandits, MDPs, POMDPs, PSRs, and history-based environments. On the other hand,
in general, the near-optimal policies in GRL are functions of complete history,
which hinders not only learning but also...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.09382v1' target='_blank'>Planning Irregular Object Packing via Hierarchical Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Sichao Huang, Ziwei Wang, Jie Zhou, Jiwen Lu</p>
<p><strong>Summary:</strong> Object packing by autonomous robots is an im-portant challenge in warehouses
and logistics industry. Most conventional data-driven packing planning
approaches focus on regular cuboid packing, which are usually heuristic and
limit the practical use in realistic applications with everyday objects. In
this paper, we propose a deep hierarchical reinforcement learning approach to
simultaneously plan packing sequence and placement for irregular object
packing. Specifically, the top manager network inf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.09631v1' target='_blank'>Integrated Ray-Tracing and Coverage Planning Control using Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Savvas Papaioannou, Panayiotis Kolios, Theocharis Theocharides, Christos G. Panayiotou, Marios M. Polycarpou</p>
<p><strong>Summary:</strong> In this work we propose a coverage planning control approach which allows a
mobile agent, equipped with a controllable sensor (i.e., a camera) with limited
sensing domain (i.e., finite sensing range and angle of view), to cover the
surface area of an object of interest. The proposed approach integrates
ray-tracing into the coverage planning process, thus allowing the agent to
identify which parts of the scene are visible at any point in time. The problem
of integrated ray-tracing and coverage pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.10846v1' target='_blank'>Goal-Conditioned Reinforcement Learning with Disentanglement-based
  Reachability Planning</a></h2>
<p><strong>Authors:</strong> Zhifeng Qian, Mingyu You, Hongjun Zhou, Xuanhui Xu, Bin He</p>
<p><strong>Summary:</strong> Goal-Conditioned Reinforcement Learning (GCRL) can enable agents to
spontaneously set diverse goals to learn a set of skills. Despite the excellent
works proposed in various fields, reaching distant goals in temporally extended
tasks remains a challenge for GCRL. Current works tackled this problem by
leveraging planning algorithms to plan intermediate subgoals to augment GCRL.
Their methods need two crucial requirements: (i) a state representation space
to search valid subgoals, and (ii) a dista...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.01207v1' target='_blank'>Learn to Follow: Decentralized Lifelong Multi-agent Pathfinding via
  Planning and Learning</a></h2>
<p><strong>Authors:</strong> Alexey Skrynnik, Anton Andreychuk, Maria Nesterova, Konstantin Yakovlev, Aleksandr Panov</p>
<p><strong>Summary:</strong> Multi-agent Pathfinding (MAPF) problem generally asks to find a set of
conflict-free paths for a set of agents confined to a graph and is typically
solved in a centralized fashion.
  Conversely, in this work, we investigate the decentralized MAPF setting, when
the central controller that posses all the information on the agents' locations
and goals is absent and the agents have to sequientially decide the actions on
their own without having access to a full state of the environment. We focus on
...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.11820v1' target='_blank'>Optimizing Job Shop Scheduling in the Furniture Industry: A
  Reinforcement Learning Approach Considering Machine Setup, Batch Variability,
  and Intralogistics</a></h2>
<p><strong>Authors:</strong> Malte Schneevogt, Karsten Binninger, Noah Klarmann</p>
<p><strong>Summary:</strong> This paper explores the potential application of Deep Reinforcement Learning
in the furniture industry. To offer a broad product portfolio, most furniture
manufacturers are organized as a job shop, which ultimately results in the Job
Shop Scheduling Problem (JSSP). The JSSP is addressed with a focus on extending
traditional models to better represent the complexities of real-world
production environments. Existing approaches frequently fail to consider
critical factors such as machine setup time...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1704.03084v3' target='_blank'>Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee, Kam-Fai Wong</p>
<p><strong>Summary:</strong> Building a dialogue agent to fulfill complex tasks, such as travel planning,
is challenging because the agent has to learn to collectively complete multiple
subtasks. For example, the agent needs to reserve a hotel and book a flight so
that there leaves enough time for commute between arrival and hotel check-in.
This paper addresses this challenge by formulating the task in the mathematical
framework of options over Markov Decision Processes (MDPs), and proposing a
hierarchical deep reinforcemen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.02813v1' target='_blank'>Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement
  Learning with Trajectory Embeddings</a></h2>
<p><strong>Authors:</strong> John D. Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach, Pieter Abbeel, Sergey Levine</p>
<p><strong>Summary:</strong> In this work, we take a representation learning perspective on hierarchical
reinforcement learning, where the problem of learning lower layers in a
hierarchy is transformed into the problem of learning trajectory-level
generative models. We show that we can learn continuous latent representations
of trajectories, which are effective in solving temporally extended and
multi-stage problems. Our proposed model, SeCTAR, draws inspiration from
variational autoencoders, and learns latent representatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.08549v2' target='_blank'>Reinforcement Learning and Inverse Reinforcement Learning with System 1
  and System 2</a></h2>
<p><strong>Authors:</strong> Alexander Peysakhovich</p>
<p><strong>Summary:</strong> Inferring a person's goal from their behavior is an important problem in
applications of AI (e.g. automated assistants, recommender systems). The
workhorse model for this task is the rational actor model - this amounts to
assuming that people have stable reward functions, discount the future
exponentially, and construct optimal plans. Under the rational actor assumption
techniques such as inverse reinforcement learning (IRL) can be used to infer a
person's goals from their actions. A competing m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.01356v1' target='_blank'>Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep
  Reinforcement Learning And Machine Teaching</a></h2>
<p><strong>Authors:</strong> Aleksei Gonnochenko, Aleksandr Semochkin, Dmitry Egorov, Dmitrii Statovoy, Seyedhassan Zabihifar, Aleksey Postnikov, Elena Seliverstova, Ali Zaidi, Jayson Stemmler, Kevin Limkrailassiri</p>
<p><strong>Summary:</strong> Given the laborious difficulty of moving heavy bags of physical currency in
the cash center of the bank, there is a large demand for training and deploying
safe autonomous systems capable of conducting such tasks in a collaborative
workspace. However, the deformable properties of the bag along with the large
quantity of rigid-body coins contained within it, significantly increases the
challenges of bag detection, grasping and manipulation by a robotic gripper and
arm. In this paper, we apply dee...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.01641v1' target='_blank'>Efficient Exploration in Constrained Environments with Goal-Oriented
  Reference Path</a></h2>
<p><strong>Authors:</strong> Kei Ota, Yoko Sasaki, Devesh K. Jha, Yusuke Yoshiyasu, Asako Kanezaki</p>
<p><strong>Summary:</strong> In this paper, we consider the problem of building learning agents that can
efficiently learn to navigate in constrained environments. The main goal is to
design agents that can efficiently learn to understand and generalize to
different environments using high-dimensional inputs (a 2D map), while
following feasible paths that avoid obstacles in obstacle-cluttered
environment. To achieve this, we make use of traditional path planning
algorithms, supervised learning, and reinforcement learning al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.11997v2' target='_blank'>Continual Model-Based Reinforcement Learning with Hypernetworks</a></h2>
<p><strong>Authors:</strong> Yizhou Huang, Kevin Xie, Homanga Bharadhwaj, Florian Shkurti</p>
<p><strong>Summary:</strong> Effective planning in model-based reinforcement learning (MBRL) and
model-predictive control (MPC) relies on the accuracy of the learned dynamics
model. In many instances of MBRL and MPC, this model is assumed to be
stationary and is periodically re-trained from scratch on state transition
experience collected from the beginning of environment interactions. This
implies that the time required to train the dynamics model - and the pause
required between plan executions - grows linearly with the s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.13265v1' target='_blank'>Robot Navigation in a Crowd by Integrating Deep Reinforcement Learning
  and Online Planning</a></h2>
<p><strong>Authors:</strong> Zhiqian Zhou, Pengming Zhu, Zhiwen Zeng, Junhao Xiao, Huimin Lu, Zongtan Zhou</p>
<p><strong>Summary:</strong> It is still an open and challenging problem for mobile robots navigating
along time-efficient and collision-free paths in a crowd. The main challenge
comes from the complex and sophisticated interaction mechanism, which requires
the robot to understand the crowd and perform proactive and foresighted
behaviors. Deep reinforcement learning is a promising solution to this problem.
However, most previous learning methods incur a tremendous computational
burden. To address these problems, we propose ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.05492v1' target='_blank'>Mastering the Game of No-Press Diplomacy via Human-Regularized
  Reinforcement Learning and Planning</a></h2>
<p><strong>Authors:</strong> Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown</p>
<p><strong>Summary:</strong> No-press Diplomacy is a complex strategy game involving both cooperation and
competition that has served as a benchmark for multi-agent AI research. While
self-play reinforcement learning has resulted in numerous successes in purely
adversarial games like chess, Go, and poker, self-play alone is insufficient
for achieving optimal performance in domains involving cooperation with humans.
We address this shortcoming by first introducing a planning algorithm we call
DiL-piKL that regularizes a rewa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.14188v2' target='_blank'>Exposure-Based Multi-Agent Inspection of a Tumbling Target Using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Joshua Aurand, Steven Cutlip, Henry Lei, Kendra Lang, Sean Phillips</p>
<p><strong>Summary:</strong> As space becomes more congested, on orbit inspection is an increasingly
relevant activity whether to observe a defunct satellite for planning repairs
or to de-orbit it. However, the task of on orbit inspection itself is
challenging, typically requiring the careful coordination of multiple observer
satellites. This is complicated by a highly nonlinear environment where the
target may be unknown or moving unpredictably without time for continuous
command and control from the ground. There is a nee...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.10071v1' target='_blank'>Joint Path planning and Power Allocation of a Cellular-Connected UAV
  using Apprenticeship Learning via Deep Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alireza Shamsoshoara, Fatemeh Lotfi, Sajad Mousavi, Fatemeh Afghah, Ismail Guvenc</p>
<p><strong>Summary:</strong> This paper investigates an interference-aware joint path planning and power
allocation mechanism for a cellular-connected unmanned aerial vehicle (UAV) in
a sparse suburban environment. The UAV's goal is to fly from an initial point
and reach a destination point by moving along the cells to guarantee the
required quality of service (QoS). In particular, the UAV aims to maximize its
uplink throughput and minimize the level of interference to the ground user
equipment (UEs) connected to the neighb...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.06184v2' target='_blank'>Deep Multi-Objective Reinforcement Learning for Utility-Based
  Infrastructural Maintenance Optimization</a></h2>
<p><strong>Authors:</strong> Jesse van Remmerden, Maurice Kenter, Diederik M. Roijers, Charalampos Andriotis, Yingqian Zhang, Zaharah Bukhsh</p>
<p><strong>Summary:</strong> In this paper, we introduce Multi-Objective Deep Centralized Multi-Agent
Actor-Critic (MO- DCMAC), a multi-objective reinforcement learning (MORL)
method for infrastructural maintenance optimization, an area traditionally
dominated by single-objective reinforcement learning (RL) approaches. Previous
single-objective RL methods combine multiple objectives, such as probability of
collapse and cost, into a singular reward signal through reward-shaping. In
contrast, MO-DCMAC can optimize a policy fo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.07877v2' target='_blank'>Hierarchical Reinforcement Learning for Swarm Confrontation with High
  Uncertainty</a></h2>
<p><strong>Authors:</strong> Qizhen Wu, Kexin Liu, Lei Chen, Jinhu L√º</p>
<p><strong>Summary:</strong> In swarm robotics, confrontation including the pursuit-evasion game is a key
scenario. High uncertainty caused by unknown opponents' strategies, dynamic
obstacles, and insufficient training complicates the action space into a hybrid
decision process. Although the deep reinforcement learning method is
significant for swarm confrontation since it can handle various sizes, as an
end-to-end implementation, it cannot deal with the hybrid process. Here, we
propose a novel hierarchical reinforcement le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02539v3' target='_blank'>Research on Autonomous Robots Navigation based on Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zixiang Wang, Hao Yan, Yining Wang, Zhengjia Xu, Zhuoyue Wang, Zhizhong Wu</p>
<p><strong>Summary:</strong> Reinforcement learning continuously optimizes decision-making based on
real-time feedback reward signals through continuous interaction with the
environment, demonstrating strong adaptive and self-learning capabilities. In
recent years, it has become one of the key methods to achieve autonomous
navigation of robots. In this work, an autonomous robot navigation method based
on reinforcement learning is introduced. We use the Deep Q Network (DQN) and
Proximal Policy Optimization (PPO) models to op...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.12185v1' target='_blank'>Satisficing Exploration for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dilip Arumugam, Saurabh Kumar, Ramki Gummadi, Benjamin Van Roy</p>
<p><strong>Summary:</strong> A default assumption in the design of reinforcement-learning algorithms is
that a decision-making agent always explores to learn optimal behavior. In
sufficiently complex environments that approach the vastness and scale of the
real world, however, attaining optimal performance may in fact be an entirely
intractable endeavor and an agent may seldom find itself in a position to
complete the requisite exploration for identifying an optimal policy. Recent
work has leveraged tools from information t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.15241v1' target='_blank'>Temporal Abstraction in Reinforcement Learning with Offline Data</a></h2>
<p><strong>Authors:</strong> Ranga Shaarad Ayyagari, Anurita Ghosh, Ambedkar Dukkipati</p>
<p><strong>Summary:</strong> Standard reinforcement learning algorithms with a single policy perform
poorly on tasks in complex environments involving sparse rewards, diverse
behaviors, or long-term planning. This led to the study of algorithms that
incorporate temporal abstraction by training a hierarchy of policies that plan
over different time scales. The options framework has been introduced to
implement such temporal abstraction by learning low-level options that act as
extended actions controlled by a high-level polic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.04054v2' target='_blank'>PLANRL: A Motion Planning and Imitation Learning Framework to Bootstrap
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Amisha Bhaskar, Zahiruddin Mahammad, Sachin R Jadhav, Pratap Tokekar</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) has shown remarkable progress in simulation
environments, yet its application to real-world robotic tasks remains limited
due to challenges in exploration and generalization. To address these issues,
we introduce PLANRL, a framework that chooses when the robot should use
classical motion planning and when it should learn a policy. To further improve
the efficiency in exploration, we use imitation data to bootstrap the
exploration. PLANRL dynamically switches between t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.08979v2' target='_blank'>Overcoming Slow Decision Frequencies in Continuous Control: Model-Based
  Sequence Reinforcement Learning for Model-Free Control</a></h2>
<p><strong>Authors:</strong> Devdhar Patel, Hava Siegelmann</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is rapidly reaching and surpassing human-level
control capabilities. However, state-of-the-art RL algorithms often require
timesteps and reaction times significantly faster than human capabilities,
which is impractical in real-world settings and typically necessitates
specialized hardware. Such speeds are difficult to achieve in the real world
and often requires specialized hardware. We introduce Sequence Reinforcement
Learning (SRL), an RL algorithm designed to produ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.11234v1' target='_blank'>Bayes Adaptive Monte Carlo Tree Search for Offline Model-based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jiayu Chen, Wentse Chen, Jeff Schneider</p>
<p><strong>Summary:</strong> Offline reinforcement learning (RL) is a powerful approach for data-driven
decision-making and control. Compared to model-free methods, offline
model-based reinforcement learning (MBRL) explicitly learns world models from a
static dataset and uses them as surrogate simulators, improving the data
efficiency and enabling the learned policy to potentially generalize beyond the
dataset support. However, there could be various MDPs that behave identically
on the offline dataset and so dealing with th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.07096v2' target='_blank'>Incremental learning abstract discrete planning domains and mappings to
  continuous perceptions</a></h2>
<p><strong>Authors:</strong> Luciano Serafini, Paolo Traverso</p>
<p><strong>Summary:</strong> Most of the works on planning and learning, e.g., planning by (model based)
reinforcement learning, are based on two main assumptions: (i) the set of
states of the planning domain is fixed; (ii) the mapping between the
observations from the real word and the states is implicitly assumed or learned
offline, and it is not part of the planning domain. Consequently, the focus is
on learning the transitions between states. In this paper, we drop such
assumptions. We provide a formal framework in whic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.11410v1' target='_blank'>Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning</a></h2>
<p><strong>Authors:</strong> Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica B. Hamrick, Nicolas Heess, Alexander Neitz, Theophane Weber</p>
<p><strong>Summary:</strong> Standard planners for sequential decision making (including Monte Carlo
planning, tree search, dynamic programming, etc.) are constrained by an
implicit sequential planning assumption: The order in which a plan is
constructed is the same in which it is executed. We consider alternatives to
this assumption for the class of goal-directed Reinforcement Learning (RL)
problems. Instead of an environment transition model, we assume an imperfect,
goal-directed policy. This low-level policy can be impro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.08321v3' target='_blank'>Simulated Mental Imagery for Robotic Task Planning</a></h2>
<p><strong>Authors:</strong> Shijia Li, Tomas Kulvicius, Minija Tamosiunaite, Florentin W√∂rg√∂tter</p>
<p><strong>Summary:</strong> Traditional AI-planning methods for task planning in robotics require a
symbolically encoded domain description. While powerful in well-defined
scenarios, as well as human-interpretable, setting this up requires substantial
effort. Different from this, most everyday planning tasks are solved by humans
intuitively, using mental imagery of the different planning steps. Here we
suggest that the same approach can be used for robots, too, in cases which
require only limited execution accuracy. In the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.04581v1' target='_blank'>PALMER: Perception-Action Loop with Memory for Long-Horizon Planning</a></h2>
<p><strong>Authors:</strong> Onur Beker, Mohammad Mohammadi, Amir Zamir</p>
<p><strong>Summary:</strong> To achieve autonomy in a priori unknown real-world scenarios, agents should
be able to: i) act from high-dimensional sensory observations (e.g., images),
ii) learn from past experience to adapt and improve, and iii) be capable of
long horizon planning. Classical planning algorithms (e.g. PRM, RRT) are
proficient at handling long-horizon planning. Deep learning based methods in
turn can provide the necessary representations to address the others, by
modeling statistical contingencies between obse...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1402.1958v1' target='_blank'>Better Optimism By Bayes: Adaptive Planning with Rich Models</a></h2>
<p><strong>Authors:</strong> Arthur Guez, David Silver, Peter Dayan</p>
<p><strong>Summary:</strong> The computational costs of inference and planning have confined Bayesian
model-based reinforcement learning to one of two dismal fates: powerful
Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian
non-parametric models but using simple, myopic planning strategies such as
Thompson sampling. We ask whether it is feasible and truly beneficial to
combine rich probabilistic models with a closer approximation to fully Bayesian
planning. First, we use a collection of counterex...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06878v2' target='_blank'>Model Based Planning with Energy Based Models</a></h2>
<p><strong>Authors:</strong> Yilun Du, Toru Lin, Igor Mordatch</p>
<p><strong>Summary:</strong> Model-based planning holds great promise for improving both sample efficiency
and generalization in reinforcement learning (RL). We show that energy-based
models (EBMs) are a promising class of models to use for model-based planning.
EBMs naturally support inference of intermediate states given start and goal
state distributions. We provide an online algorithm to train EBMs while
interacting with the environment, and show that EBMs allow for significantly
better online learning than correspondin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.08543v1' target='_blank'>Planning with Expectation Models for Control</a></h2>
<p><strong>Authors:</strong> Katya Kudashkina, Yi Wan, Abhishek Naik, Richard S. Sutton</p>
<p><strong>Summary:</strong> In model-based reinforcement learning (MBRL), Wan et al. (2019) showed
conditions under which the environment model could produce the expectation of
the next feature vector rather than the full distribution, or a sample thereof,
with no loss in planning performance. Such expectation models are of interest
when the environment is stochastic and non-stationary, and the model is
approximate, such as when it is learned using function approximation. In these
cases a full distribution model may be imp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.08100v2' target='_blank'>Planning with a Receding Horizon for Manipulation in Clutter using a
  Learned Value Function</a></h2>
<p><strong>Authors:</strong> Wissam Bejjani, Rafael Papallas, Matteo Leonetti, Mehmet R. Dogar</p>
<p><strong>Summary:</strong> Manipulation in clutter requires solving complex sequential decision making
problems in an environment rich with physical interactions. The transfer of
motion planning solutions from simulation to the real world, in open-loop,
suffers from the inherent uncertainty in modelling real world physics. We
propose interleaving planning and execution in real-time, in a closed-loop
setting, using a Receding Horizon Planner (RHP) for pushing manipulation in
clutter. In this context, we address the problem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.07908v1' target='_blank'>Learning to Execute: Efficient Learning of Universal Plan-Conditioned
  Policies in Robotics</a></h2>
<p><strong>Authors:</strong> Ingmar Schubert, Danny Driess, Ozgur S. Oguz, Marc Toussaint</p>
<p><strong>Summary:</strong> Applications of Reinforcement Learning (RL) in robotics are often limited by
high data demand. On the other hand, approximate models are readily available
in many robotics scenarios, making model-based approaches like planning a
data-efficient alternative. Still, the performance of these methods suffers if
the model is imprecise or wrong. In this sense, the respective strengths and
weaknesses of RL and model-based planners are. In the present work, we
investigate how both approaches can be integ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02902v5' target='_blank'>Goal-Space Planning with Subgoal Models</a></h2>
<p><strong>Authors:</strong> Chunlok Lo, Kevin Roice, Parham Mohammad Panahi, Scott Jordan, Adam White, Gabor Mihucz, Farzane Aminmansour, Martha White</p>
<p><strong>Summary:</strong> This paper investigates a new approach to model-based reinforcement learning
using background planning: mixing (approximate) dynamic programming updates and
model-free updates, similar to the Dyna architecture. Background planning with
learned models is often worse than model-free alternatives, such as Double DQN,
even though the former uses significantly more memory and computation. The
fundamental problem is that learned models can be inaccurate and often generate
invalid states, especially wh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.14530v1' target='_blank'>POMRL: No-Regret Learning-to-Plan with Increasing Horizons</a></h2>
<p><strong>Authors:</strong> Khimya Khetarpal, Claire Vernade, Brendan O'Donoghue, Satinder Singh, Tom Zahavy</p>
<p><strong>Summary:</strong> We study the problem of planning under model uncertainty in an online
meta-reinforcement learning (RL) setting where an agent is presented with a
sequence of related tasks with limited interactions per task. The agent can use
its experience in each task and across tasks to estimate both the transition
model and the distribution over tasks. We propose an algorithm to meta-learn
the underlying structure across tasks, utilize it to plan in each task, and
upper-bound the regret of the planning loss....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.16189v1' target='_blank'>Planning with Sequence Models through Iterative Energy Minimization</a></h2>
<p><strong>Authors:</strong> Hongyi Chen, Yilun Du, Yiye Chen, Joshua Tenenbaum, Patricio A. Vela</p>
<p><strong>Summary:</strong> Recent works have shown that sequence modeling can be effectively used to
train reinforcement learning (RL) policies. However, the success of applying
existing sequence models to planning, in which we wish to obtain a trajectory
of actions to reach some goal, is less straightforward. The typical
autoregressive generation procedures of sequence models preclude sequential
refinement of earlier steps, which limits the effectiveness of a predicted
plan. In this paper, we suggest an approach towards ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.12933v1' target='_blank'>Theoretically Guaranteed Policy Improvement Distilled from Model-Based
  Planning</a></h2>
<p><strong>Authors:</strong> Chuming Li, Ruonan Jia, Jie Liu, Yinmin Zhang, Yazhe Niu, Yaodong Yang, Yu Liu, Wanli Ouyang</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL) has demonstrated remarkable successes
on a range of continuous control tasks due to its high sample efficiency. To
save the computation cost of conducting planning online, recent practices tend
to distill optimized action sequences into an RL policy during the training
phase. Although the distillation can incorporate both the foresight of planning
and the exploration ability of RL policies, the theoretical understanding of
these methods is yet unclear. In ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.08404v1' target='_blank'>Scaling Value Iteration Networks to 5000 Layers for Extreme Long-Term
  Planning</a></h2>
<p><strong>Authors:</strong> Yuhui Wang, Qingyuan Wu, Weida Li, Dylan R. Ashley, Francesco Faccio, Chao Huang, J√ºrgen Schmidhuber</p>
<p><strong>Summary:</strong> The Value Iteration Network (VIN) is an end-to-end differentiable
architecture that performs value iteration on a latent MDP for planning in
reinforcement learning (RL). However, VINs struggle to scale to long-term and
large-scale planning tasks, such as navigating a $100\times 100$ maze -- a task
which typically requires thousands of planning steps to solve. We observe that
this deficiency is due to two issues: the representation capacity of the latent
MDP and the planning module's depth. We ad...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.18841v2' target='_blank'>QT-TDM: Planning With Transformer Dynamics Model and Autoregressive
  Q-Learning</a></h2>
<p><strong>Authors:</strong> Mostafa Kotb, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter</p>
<p><strong>Summary:</strong> Inspired by the success of the Transformer architecture in natural language
processing and computer vision, we investigate the use of Transformers in
Reinforcement Learning (RL), specifically in modeling the environment's
dynamics using Transformer Dynamics Models (TDMs). We evaluate the capabilities
of TDMs for continuous control in real-time planning scenarios with Model
Predictive Control (MPC). While Transformers excel in long-horizon prediction,
their tokenization mechanism and autoregressi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.17012v1' target='_blank'>AI-Driven Risk-Aware Scheduling for Active Debris Removal Missions</a></h2>
<p><strong>Authors:</strong> Antoine Poupon, Hugo de Rohan Willner, Pierre Nikitits, Adam Abdin</p>
<p><strong>Summary:</strong> The proliferation of debris in Low Earth Orbit (LEO) represents a significant
threat to space sustainability and spacecraft safety. Active Debris Removal
(ADR) has emerged as a promising approach to address this issue, utilising
Orbital Transfer Vehicles (OTVs) to facilitate debris deorbiting, thereby
reducing future collision risks. However, ADR missions are substantially
complex, necessitating accurate planning to make the missions economically
viable and technically effective. Moreover, these...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.05828v1' target='_blank'>Effort Allocation for Deadline-Aware Task and Motion Planning: A
  Metareasoning Approach</a></h2>
<p><strong>Authors:</strong> Yoonchang Sung, Shahaf S. Shperberg, Qi Wang, Peter Stone</p>
<p><strong>Summary:</strong> In robot planning, tasks can often be achieved through multiple options, each
consisting of several actions. This work specifically addresses deadline
constraints in task and motion planning, aiming to find a plan that can be
executed within the deadline despite uncertain planning and execution times. We
propose an effort allocation problem, formulated as a Markov decision process
(MDP), to find such a plan by leveraging metareasoning perspectives to allocate
computational resources among the gi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.07245v1' target='_blank'>AAAI Workshop on AI Planning for Cyber-Physical Systems -- CAIPI24</a></h2>
<p><strong>Authors:</strong> Oliver Niggemann, Gautam Biswas, Alexander Diedrich, Jonas Ehrhardt, Ren√© Heesch, Niklas Widulle</p>
<p><strong>Summary:</strong> The workshop 'AI-based Planning for Cyber-Physical Systems', which took place
on February 26, 2024, as part of the 38th Annual AAAI Conference on Artificial
Intelligence in Vancouver, Canada, brought together researchers to discuss
recent advances in AI planning methods for Cyber-Physical Systems (CPS). CPS
pose a major challenge due to their complexity and data-intensive nature, which
often exceeds the capabilities of traditional planning algorithms. The workshop
highlighted new approaches such...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1207.4708v2' target='_blank'>The Arcade Learning Environment: An Evaluation Platform for General
  Agents</a></h2>
<p><strong>Authors:</strong> Marc G. Bellemare, Yavar Naddaf, Joel Veness, Michael Bowling</p>
<p><strong>Summary:</strong> In this article we introduce the Arcade Learning Environment (ALE): both a
challenge problem and a platform and methodology for evaluating the development
of general, domain-independent AI technology. ALE provides an interface to
hundreds of Atari 2600 game environments, each one different, interesting, and
designed to be a challenge for human players. ALE presents significant research
challenges for reinforcement learning, model learning, model-based planning,
imitation learning, transfer learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1606.04686v1' target='_blank'>Natural Language Generation as Planning under Uncertainty Using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Verena Rieser, Oliver Lemon</p>
<p><strong>Summary:</strong> We present and evaluate a new model for Natural Language Generation (NLG) in
Spoken Dialogue Systems, based on statistical planning, given noisy feedback
from the current generation context (e.g. a user and a surface realiser). We
study its use in a standard NLG problem: how to present information (in this
case a set of search results) to users, given the complex trade- offs between
utterance length, amount of information conveyed, and cognitive load. We set
these trade-offs by analysing existin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1612.06018v2' target='_blank'>Self-Correcting Models for Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Erik Talvitie</p>
<p><strong>Summary:</strong> When an agent cannot represent a perfectly accurate model of its
environment's dynamics, model-based reinforcement learning (MBRL) can fail
catastrophically. Planning involves composing the predictions of the model;
when flawed predictions are composed, even minor errors can compound and render
the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the
model to "correct" itself when it produces errors, substantially improving MBRL
with flawed models. This paper theoretically ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1802.04325v2' target='_blank'>Efficient Model-Based Deep Reinforcement Learning with Variational State
  Tabulation</a></h2>
<p><strong>Authors:</strong> Dane Corneil, Wulfram Gerstner, Johanni Brea</p>
<p><strong>Summary:</strong> Modern reinforcement learning algorithms reach super-human performance on
many board and video games, but they are sample inefficient, i.e. they
typically require significantly more playing experience than humans to reach an
equal performance level. To improve sample efficiency, an agent may build a
model of the environment and use planning methods to update its policy. In this
article we introduce Variational State Tabulation (VaST), which maps an
environment with a high-dimensional state space...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.03584v1' target='_blank'>Learning Coordinated Tasks using Reinforcement Learning in Humanoids</a></h2>
<p><strong>Authors:</strong> S Phaniteja, Parijat Dewangan, Pooja Guhan, K Madhava Krishna, Abhishek Sarkar</p>
<p><strong>Summary:</strong> With the advent of artificial intelligence and machine learning, humanoid
robots are made to learn a variety of skills which humans possess. One of
fundamental skills which humans use in day-to-day activities is performing
tasks with coordination between both the hands. In case of humanoids, learning
such skills require optimal motion planning which includes avoiding collisions
with the surroundings. In this paper, we propose a framework to learn
coordinated tasks in cluttered environments based...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.01843v2' target='_blank'>How to Combine Tree-Search Methods in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yonathan Efroni, Gal Dalal, Bruno Scherrer, Shie Mannor</p>
<p><strong>Summary:</strong> Finite-horizon lookahead policies are abundantly used in Reinforcement
Learning and demonstrate impressive empirical success. Usually, the lookahead
policies are implemented with specific planning methods such as Monte Carlo
Tree Search (e.g. in AlphaZero). Referring to the planning problem as tree
search, a reasonable practice in these implementations is to back up the value
only at the leaves while the information obtained at the root is not leveraged
other than for updating the policy. Here, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.02926v1' target='_blank'>Probabilistic Prediction of Interactive Driving Behavior via
  Hierarchical Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Liting Sun, Wei Zhan, Masayoshi Tomizuka</p>
<p><strong>Summary:</strong> Autonomous vehicles (AVs) are on the road. To safely and efficiently interact
with other road participants, AVs have to accurately predict the behavior of
surrounding vehicles and plan accordingly. Such prediction should be
probabilistic, to address the uncertainties in human behavior. Such prediction
should also be interactive, since the distribution over all possible
trajectories of the predicted vehicle depends not only on historical
information, but also on future plans of other vehicles tha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.02661v4' target='_blank'>Bayesian Reinforcement Learning via Deep, Sparse Sampling</a></h2>
<p><strong>Authors:</strong> Divya Grover, Debabrota Basu, Christos Dimitrakakis</p>
<p><strong>Summary:</strong> We address the problem of Bayesian reinforcement learning using efficient
model-based online planning. We propose an optimism-free Bayes-adaptive
algorithm to induce deeper and sparser exploration with a theoretical bound on
its performance relative to the Bayes optimal policy, with a lower
computational complexity. The main novelty is the use of a candidate policy
generator, to generate long-term options in the planning tree (over beliefs),
which allows us to create much sparser and deeper tree...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.01669v1' target='_blank'>Deep Active Localization</a></h2>
<p><strong>Authors:</strong> Sai Krishna, Keehong Seo, Dhaivat Bhatt, Vincent Mai, Krishna Murthy, Liam Paull</p>
<p><strong>Summary:</strong> Active localization is the problem of generating robot actions that allow it
to maximally disambiguate its pose within a reference map. Traditional
approaches to this use an information-theoretic criterion for action selection
and hand-crafted perceptual models. In this work we propose an end-to-end
differentiable method for learning to take informative actions that is
trainable entirely in simulation and then transferable to real robot hardware
with zero refinement. The system is composed of tw...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.05757v1' target='_blank'>VRKitchen: an Interactive 3D Virtual Environment for Task-oriented
  Learning</a></h2>
<p><strong>Authors:</strong> Xiaofeng Gao, Ran Gong, Tianmin Shu, Xu Xie, Shu Wang, Song-Chun Zhu</p>
<p><strong>Summary:</strong> One of the main challenges of advancing task-oriented learning such as visual
task planning and reinforcement learning is the lack of realistic and
standardized environments for training and testing AI agents. Previously,
researchers often relied on ad-hoc lab environments. There have been recent
advances in virtual systems built with 3D physics engines and photo-realistic
rendering for indoor and outdoor environments, but the embodied agents in those
systems can only conduct simple interactions...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.05927v3' target='_blank'>On the Expressivity of Neural Networks for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kefan Dong, Yuping Luo, Tengyu Ma</p>
<p><strong>Summary:</strong> We compare the model-free reinforcement learning with the model-based
approaches through the lens of the expressive power of neural networks for
policies, $Q$-functions, and dynamics. We show, theoretically and empirically,
that even for one-dimensional continuous state space, there are many MDPs whose
optimal $Q$-functions and policies are much more complex than the dynamics. We
hypothesize many real-world MDPs also have a similar property. For these MDPs,
model-based planning is a favorable al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.03799v1' target='_blank'>Hierarchical Reinforcement Learning Method for Autonomous Vehicle
  Behavior Planning</a></h2>
<p><strong>Authors:</strong> Zhiqian Qiao, Zachariah Tyree, Priyantha Mudalige, Jeff Schneider, John M. Dolan</p>
<p><strong>Summary:</strong> In this work, we propose a hierarchical reinforcement learning (HRL)
structure which is capable of performing autonomous vehicle planning tasks in
simulated environments with multiple sub-goals. In this hierarchical structure,
the network is capable of 1) learning one task with multiple sub-goals
simultaneously; 2) extracting attentions of states according to changing
sub-goals during the learning process; 3) reusing the well-trained network of
sub-goals for other similar tasks with the same sub...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.02836v1' target='_blank'>Causally Correct Partial Models for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Danilo J. Rezende, Ivo Danihelka, George Papamakarios, Nan Rosemary Ke, Ray Jiang, Theophane Weber, Karol Gregor, Hamza Merzic, Fabio Viola, Jane Wang, Jovana Mitrovic, Frederic Besse, Ioannis Antonoglou, Lars Buesing</p>
<p><strong>Summary:</strong> In reinforcement learning, we can learn a model of future observations and
rewards, and use it to plan the agent's next actions. However, jointly modeling
future observations can be computationally expensive or even intractable if the
observations are high-dimensional (e.g. images). For this reason, previous
works have considered partial models, which model only part of the observation.
In this paper, we show that partial models can be causally incorrect: they are
confounded by the observations ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.06432v2' target='_blank'>PDDLGym: Gym Environments from PDDL Problems</a></h2>
<p><strong>Authors:</strong> Tom Silver, Rohan Chitnis</p>
<p><strong>Summary:</strong> We present PDDLGym, a framework that automatically constructs OpenAI Gym
environments from PDDL domains and problems. Observations and actions in
PDDLGym are relational, making the framework particularly well-suited for
research in relational reinforcement learning and relational sequential
decision-making. PDDLGym is also useful as a generic framework for rapidly
building numerous, diverse benchmarks from a concise and familiar specification
language. We discuss design decisions and implementat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.05960v2' target='_blank'>Planning to Explore via Self-Supervised World Models</a></h2>
<p><strong>Authors:</strong> Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak</p>
<p><strong>Summary:</strong> Reinforcement learning allows solving complex tasks, however, the learning
tends to be task-specific and the sample efficiency remains a challenge. We
present Plan2Explore, a self-supervised reinforcement learning agent that
tackles both these challenges through a new approach to self-supervised
exploration and fast adaptation to new tasks, which need not be known during
exploration. During exploration, unlike prior methods which retrospectively
compute the novelty of observations after the agen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.00900v1' target='_blank'>PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals</a></h2>
<p><strong>Authors:</strong> Henry Charlesworth, Giovanni Montana</p>
<p><strong>Summary:</strong> Learning with sparse rewards remains a significant challenge in reinforcement
learning (RL), especially when the aim is to train a policy capable of
achieving multiple different goals. To date, the most successful approaches for
dealing with multi-goal, sparse reward environments have been model-free RL
algorithms. In this work we propose PlanGAN, a model-based algorithm
specifically designed for solving multi-goal tasks in environments with sparse
rewards. Our method builds on the fact that any...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.15085v1' target='_blank'>What can I do here? A Theory of Affordances in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, Doina Precup</p>
<p><strong>Summary:</strong> Reinforcement learning algorithms usually assume that all actions are always
available to an agent. However, both people and animals understand the general
link between the features of their environment and the actions that are
feasible. Gibson (1977) coined the term "affordances" to describe the fact that
certain states enable an agent to do certain actions, in the context of
embodied agents. In this paper, we develop a theory of affordances for agents
who learn and plan in Markov Decision Proc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.03760v2' target='_blank'>Near-Optimal Provable Uniform Convergence in Offline Policy Evaluation
  for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ming Yin, Yu Bai, Yu-Xiang Wang</p>
<p><strong>Summary:</strong> The problem of Offline Policy Evaluation (OPE) in Reinforcement Learning (RL)
is a critical step towards applying RL in real-life applications. Existing work
on OPE mostly focus on evaluating a fixed target policy $\pi$, which does not
provide useful bounds for offline policy learning as $\pi$ will then be
data-dependent. We address this problem by simultaneously evaluating all
policies in a policy class $\Pi$ -- uniform convergence in OPE -- and obtain
nearly optimal error bounds for a number o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.04069v1' target='_blank'>Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for
  DNN Workloads</a></h2>
<p><strong>Authors:</strong> Siyu Wang, Yi Rong, Shiqing Fan, Zhen Zheng, LanSong Diao, Guoping Long, Jun Yang, Xiaoyong Liu, Wei Lin</p>
<p><strong>Summary:</strong> The last decade has witnessed growth in the computational requirements for
training deep neural networks. Current approaches (e.g., data/model
parallelism, pipeline parallelism) parallelize training tasks onto multiple
devices. However, these approaches always rely on specific deep learning
frameworks and requires elaborate manual design, which make it difficult to
maintain and share between different type of models. In this paper, we propose
Auto-MAP, a framework for exploring distributed execu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.13146v2' target='_blank'>XLVIN: eXecuted Latent Value Iteration Nets</a></h2>
<p><strong>Authors:</strong> Andreea Deac, Petar Veliƒçkoviƒá, Ognjen Milinkoviƒá, Pierre-Luc Bacon, Jian Tang, Mladen Nikoliƒá</p>
<p><strong>Summary:</strong> Value Iteration Networks (VINs) have emerged as a popular method to
incorporate planning algorithms within deep reinforcement learning, enabling
performance improvements on tasks requiring long-range reasoning and
understanding of environment dynamics. This came with several limitations,
however: the model is not incentivised in any way to perform meaningful
planning computations, the underlying state space is assumed to be discrete,
and the Markov decision process (MDP) is assumed fixed and kno...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.00186v1' target='_blank'>Inverse reinforcement learning for autonomous navigation via
  differentiable semantic mapping and planning</a></h2>
<p><strong>Authors:</strong> Tianyu Wang, Vikas Dhiman, Nikolay Atanasov</p>
<p><strong>Summary:</strong> This paper focuses on inverse reinforcement learning for autonomous
navigation using distance and semantic category observations. The objective is
to infer a cost function that explains demonstrated behavior while relying only
on the expert's observations and state-control trajectory. We develop a map
encoder, that infers semantic category probabilities from the observation
sequence, and a cost encoder, defined as a deep neural network over the
semantic features. Since the expert cost is not dir...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.09938v2' target='_blank'>Goal-Directed Planning by Reinforcement Learning and Active Inference</a></h2>
<p><strong>Authors:</strong> Dongqi Han, Kenji Doya, Jun Tani</p>
<p><strong>Summary:</strong> What is the difference between goal-directed and habitual behavior? We
propose a novel computational framework of decision making with Bayesian
inference, in which everything is integrated as an entire neural network model.
The model learns to predict environmental state transitions by self-exploration
and generating motor actions by sampling stochastic internal states ${z}$.
Habitual behavior, which is obtained from the prior distribution of ${z}$, is
acquired by reinforcement learning. Goal-di...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.11636v1' target='_blank'>All-in-One: A DRL-based Control Switch Combining State-of-the-art
  Navigation Planners</a></h2>
<p><strong>Authors:</strong> Linh K√§stner, Johannes Cox, Teham Buiyan, Jens Lambrecht</p>
<p><strong>Summary:</strong> Autonomous navigation of mobile robots is an essential aspect in use cases
such as delivery, assistance or logistics. Although traditional planning
methods are well integrated into existing navigation systems, they struggle in
highly dynamic environments. On the other hand,
Deep-Reinforcement-Learning-based methods show superior performance in dynamic
obstacle avoidance but are not suitable for long-range navigation and struggle
with local minima. In this paper, we propose a
Deep-Reinforcement-L...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.08621v1' target='_blank'>Improving Interactive Reinforcement Agent Planning with Human
  Demonstration</a></h2>
<p><strong>Authors:</strong> Guangliang Li, Randy Gomez, Keisuke Nakamura, Jinying Lin, Qilei Zhang, Bo He</p>
<p><strong>Summary:</strong> TAMER has proven to be a powerful interactive reinforcement learning method
for allowing ordinary people to teach and personalize autonomous agents'
behavior by providing evaluative feedback. However, a TAMER agent planning with
UCT---a Monte Carlo Tree Search strategy, can only update states along its path
and might induce high learning cost especially for a physical robot. In this
paper, we propose to drive the agent's exploration along the optimal path and
reduce the learning cost by initiali...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.10389v2' target='_blank'>Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and
  Regret Bound</a></h2>
<p><strong>Authors:</strong> Lin F. Yang, Mengdi Wang</p>
<p><strong>Summary:</strong> Exploration in reinforcement learning (RL) suffers from the curse of
dimensionality when the state-action space is large. A common practice is to
parameterize the high-dimensional value and policy functions using given
features. However existing methods either have no theoretical guarantee or
suffer a regret that is exponential in the planning horizon $H$. In this paper,
we propose an online RL algorithm, namely the MatrixRL, that leverages ideas
from linear bandit to learn a low-dimensional rep...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.09864v1' target='_blank'>Reinforcement Learning to Optimize the Logistics Distribution Routes of
  Unmanned Aerial Vehicle</a></h2>
<p><strong>Authors:</strong> Linfei Feng</p>
<p><strong>Summary:</strong> Path planning methods for the unmanned aerial vehicle (UAV) in goods delivery
have drawn great attention from industry and academics because of its
flexibility which is suitable for many situations in the "Last Kilometer"
between customer and delivery nodes. However, the complicated situation is
still a problem for traditional combinatorial optimization methods. Based on
the state-of-the-art Reinforcement Learning (RL), this paper proposed an
improved method to achieve path planning for UAVs in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.08430v1' target='_blank'>Multi-Stage Transmission Line Flow Control Using Centralized and
  Decentralized Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Xiumin Shang, Jinping Yang, Bingquan Zhu, Lin Ye, Jing Zhang, Jianping Xu, Qin Lyu, Ruisheng Diao</p>
<p><strong>Summary:</strong> Planning future operational scenarios of bulk power systems that meet
security and economic constraints typically requires intensive labor efforts in
performing massive simulations. To automate this process and relieve engineers'
burden, a novel multi-stage control approach is presented in this paper to
train centralized and decentralized reinforcement learning agents that can
automatically adjust grid controllers for regulating transmission line flows at
normal condition and under contingencies...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.13651v1' target='_blank'>On the Importance of Hyperparameter Optimization for Model-based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, Andr√© Biedenkapp, Kurtland Chua, Frank Hutter, Roberto Calandra</p>
<p><strong>Summary:</strong> Model-based Reinforcement Learning (MBRL) is a promising framework for
learning control in a data-efficient manner. MBRL algorithms can be fairly
complex due to the separate dynamics modeling and the subsequent planning
algorithm, and as a result, they often possess tens of hyperparameters and
architectural choices. For this reason, MBRL typically requires significant
human expertise before it can be applied to new problems and domains. To
alleviate this problem, we propose to use automatic hype...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.04555v3' target='_blank'>Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Yan Jiao, Xiaocheng Tang, Zhiwei Qin, Shuaiji Li, Fan Zhang, Hongtu Zhu, Jieping Ye</p>
<p><strong>Summary:</strong> We present a new practical framework based on deep reinforcement learning and
decision-time planning for real-world vehicle repositioning on ride-hailing (a
type of mobility-on-demand, MoD) platforms. Our approach learns the
spatiotemporal state-value function using a batch training algorithm with deep
value networks. The optimal repositioning action is generated on-demand through
value-based policy search, which combines planning and bootstrapping with the
value networks. For the large-fleet pr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.06469v3' target='_blank'>Generalizable Episodic Memory for Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hao Hu, Jianing Ye, Guangxiang Zhu, Zhizhou Ren, Chongjie Zhang</p>
<p><strong>Summary:</strong> Episodic memory-based methods can rapidly latch onto past successful
strategies by a non-parametric memory and improve sample efficiency of
traditional reinforcement learning. However, little effort is put into the
continuous domain, where a state is never visited twice, and previous episodic
methods fail to efficiently aggregate experience across trajectories. To
address this problem, we propose Generalizable Episodic Memory (GEM), which
effectively organizes the state-action values of episodic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.09264v1' target='_blank'>Robo-Advising: Enhancing Investment with Inverse Optimization and Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haoran Wang, Shi Yu</p>
<p><strong>Summary:</strong> Machine Learning (ML) has been embraced as a powerful tool by the financial
industry, with notable applications spreading in various domains including
investment management. In this work, we propose a full-cycle data-driven
investment robo-advising framework, consisting of two ML agents. The first
agent, an inverse portfolio optimization agent, infers an investor's risk
preference and expected return directly from historical allocation data using
online inverse optimization. The second agent, a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.11702v2' target='_blank'>Transfer Learning and Curriculum Learning in Sokoban</a></h2>
<p><strong>Authors:</strong> Zhao Yang, Mike Preuss, Aske Plaat</p>
<p><strong>Summary:</strong> Transfer learning can speed up training in machine learning and is regularly
used in classification tasks. It reuses prior knowledge from other tasks to
pre-train networks for new tasks. In reinforcement learning, learning actions
for a behavior policy that can be applied to new environments is still a
challenge, especially for tasks that involve much planning. Sokoban is a
challenging puzzle game. It has been used widely as a benchmark in
planning-based reinforcement learning. In this paper, we...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.14218v2' target='_blank'>A Survey of Deep Reinforcement Learning Algorithms for Motion Planning
  and Control of Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Fei Ye, Shen Zhang, Pin Wang, Ching-Yao Chan</p>
<p><strong>Summary:</strong> In this survey, we systematically summarize the current literature on studies
that apply reinforcement learning (RL) to the motion planning and control of
autonomous vehicles. Many existing contributions can be attributed to the
pipeline approach, which consists of many hand-crafted modules, each with a
functionality selected for the ease of human interpretation. However, this
approach does not automatically guarantee maximal performance due to the lack
of a system-level optimization. Therefore,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.03600v1' target='_blank'>Reinforcement Learning based Negotiation-aware Motion Planning of
  Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Zhitao Wang, Yuzheng Zhuang, Qiang Gu, Dong Chen, Hongbo Zhang, Wulong Liu</p>
<p><strong>Summary:</strong> For autonomous vehicles integrating onto roadways with human traffic
participants, it requires understanding and adapting to the participants'
intention and driving styles by responding in predictable ways without explicit
communication. This paper proposes a reinforcement learning based
negotiation-aware motion planning framework, which adopts RL to adjust the
driving style of the planner by dynamically modifying the prediction horizon
length of the motion planner in real time adaptively w.r.t ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.06477v1' target='_blank'>Feudal Reinforcement Learning by Reading Manuals</a></h2>
<p><strong>Authors:</strong> Kai Wang, Zhonghao Wang, Mo Yu, Humphrey Shi</p>
<p><strong>Summary:</strong> Reading to act is a prevalent but challenging task which requires the ability
to reason from a concise instruction. However, previous works face the semantic
mismatch between the low-level actions and the high-level language descriptions
and require the human-designed curriculum to work properly. In this paper, we
present a Feudal Reinforcement Learning (FRL) model consisting of a manager
agent and a worker agent. The manager agent is a multi-hop plan generator
dealing with high-level abstract i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.08028v1' target='_blank'>Improving Hyperparameter Optimization by Planning Ahead</a></h2>
<p><strong>Authors:</strong> Hadi S. Jomaa, Jonas Falkner, Lars Schmidt-Thieme</p>
<p><strong>Summary:</strong> Hyperparameter optimization (HPO) is generally treated as a bi-level
optimization problem that involves fitting a (probabilistic) surrogate model to
a set of observed hyperparameter responses, e.g. validation loss, and
consequently maximizing an acquisition function using a surrogate model to
identify good hyperparameter candidates for evaluation. The choice of a
surrogate and/or acquisition function can be further improved via knowledge
transfer across related tasks. In this paper, we propose a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.11292v1' target='_blank'>Excavation Reinforcement Learning Using Geometric Representation</a></h2>
<p><strong>Authors:</strong> Qingkai Lu, Yifan Zhu, Liangjun Zhang</p>
<p><strong>Summary:</strong> Excavation of irregular rigid objects in clutter, such as fragmented rocks
and wood blocks, is very challenging due to their complex interaction dynamics
and highly variable geometries. In this paper, we adopt reinforcement learning
(RL) to tackle this challenge and learn policies to plan for a sequence of
excavation trajectories for irregular rigid objects, given point clouds of
excavation scenes. Moreover, we separately learn a compact representation of
the point cloud on geometric tasks that ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.02650v1' target='_blank'>Vision-based Distributed Multi-UAV Collision Avoidance via Deep
  Reinforcement Learning for Navigation</a></h2>
<p><strong>Authors:</strong> Huaxing Huang, Guijie Zhu, Zhun Fan, Hao Zhai, Yuwei Cai, Ze Shi, Zhaohui Dong, Zhifeng Hao</p>
<p><strong>Summary:</strong> Online path planning for multiple unmanned aerial vehicle (multi-UAV) systems
is considered a challenging task. It needs to ensure collision-free path
planning in real-time, especially when the multi-UAV systems can become very
crowded on certain occasions. In this paper, we presented a vision-based
decentralized collision-avoidance policy for multi-UAV systems, which takes
depth images and inertial measurements as sensory inputs and outputs UAV's
steering commands. The policy is trained togethe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.03480v1' target='_blank'>Reinforcement Learning for Location-Aware Scheduling</a></h2>
<p><strong>Authors:</strong> Stelios Stavroulakis, Biswa Sengupta</p>
<p><strong>Summary:</strong> Recent techniques in dynamical scheduling and resource management have found
applications in warehouse environments due to their ability to organize and
prioritize tasks in a higher temporal resolution. The rise of deep
reinforcement learning, as a learning paradigm, has enabled decentralized agent
populations to discover complex coordination strategies. However, training
multiple agents simultaneously introduce many obstacles in training as
observation and action spaces become exponentially lar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.03196v3' target='_blank'>A Framework for Following Temporal Logic Instructions with Unknown
  Causal Dependencies</a></h2>
<p><strong>Authors:</strong> Duo Xu, Faramarz Fekri</p>
<p><strong>Summary:</strong> Teaching a deep reinforcement learning (RL) agent to follow instructions in
multi-task environments is a challenging problem. We consider that user defines
every task by a linear temporal logic (LTL) formula. However, some causal
dependencies in complex environments may be unknown to the user in advance.
Hence, when human user is specifying instructions, the robot cannot solve the
tasks by simply following the given instructions. In this work, we propose a
hierarchical reinforcement learning (HR...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.10736v1' target='_blank'>Should Models Be Accurate?</a></h2>
<p><strong>Authors:</strong> Esra'a Saleh, John D. Martin, Anna Koop, Arash Pourzarabi, Michael Bowling</p>
<p><strong>Summary:</strong> Model-based Reinforcement Learning (MBRL) holds promise for data-efficiency
by planning with model-generated experience in addition to learning with
experience from the environment. However, in complex or changing environments,
models in MBRL will inevitably be imperfect, and their detrimental effects on
learning can be difficult to mitigate. In this work, we question whether the
objective of these models should be the accurate simulation of environment
dynamics at all. We focus our investigatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.10865v1' target='_blank'>Robotic Table Wiping via Reinforcement Learning and Whole-body
  Trajectory Optimization</a></h2>
<p><strong>Authors:</strong> Thomas Lew, Sumeet Singh, Mario Prats, Jeffrey Bingham, Jonathan Weisz, Benjie Holson, Xiaohan Zhang, Vikas Sindhwani, Yao Lu, Fei Xia, Peng Xu, Tingnan Zhang, Jie Tan, Montserrat Gonzalez</p>
<p><strong>Summary:</strong> We propose a framework to enable multipurpose assistive mobile robots to
autonomously wipe tables to clean spills and crumbs. This problem is
challenging, as it requires planning wiping actions while reasoning over
uncertain latent dynamics of crumbs and spills captured via high-dimensional
visual observations. Simultaneously, we must guarantee constraints satisfaction
to enable safe deployment in unstructured cluttered environments. To tackle
this problem, we first propose a stochastic differen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.15901v1' target='_blank'>Multi-robot Social-aware Cooperative Planning in Pedestrian Environments
  Using Multi-agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zichen He, Chunwei Song, Lu Dong</p>
<p><strong>Summary:</strong> Safe and efficient co-planning of multiple robots in pedestrian participation
environments is promising for applications. In this work, a novel multi-robot
social-aware efficient cooperative planner that on the basis of off-policy
multi-agent reinforcement learning (MARL) under partial dimension-varying
observation and imperfect perception conditions is proposed. We adopt
temporal-spatial graph (TSG)-based social encoder to better extract the
importance of social relation between each robot and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.08214v1' target='_blank'>Reinforcement Learning for Agile Active Target Sensing with a UAV</a></h2>
<p><strong>Authors:</strong> Harsh Goel, Laura Jarin Lipschitz, Saurav Agarwal, Sandeep Manjanna, Vijay Kumar</p>
<p><strong>Summary:</strong> Active target sensing is the task of discovering and classifying an unknown
number of targets in an environment and is critical in search-and-rescue
missions. This paper develops a deep reinforcement learning approach to plan
informative trajectories that increase the likelihood for an uncrewed aerial
vehicle (UAV) to discover missing targets. Our approach efficiently (1)
explores the environment to discover new targets, (2) exploits its current
belief of the target states and incorporates inacc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.08765v2' target='_blank'>Latent Variable Representation for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, Sujay Sanghavi, Dale Schuurmans, Bo Dai</p>
<p><strong>Summary:</strong> Deep latent variable models have achieved significant empirical successes in
model-based reinforcement learning (RL) due to their expressiveness in modeling
complex transition dynamics. On the other hand, it remains unclear
theoretically and empirically how latent variable models may facilitate
learning, planning, and exploration to improve the sample efficiency of RL. In
this paper, we provide a representation view of the latent variable models for
state-action value functions, which allows bot...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.12579v2' target='_blank'>Sample Efficient Deep Reinforcement Learning via Local Planning</a></h2>
<p><strong>Authors:</strong> Dong Yin, Sridhar Thiagarajan, Nevena Lazic, Nived Rajaraman, Botao Hao, Csaba Szepesvari</p>
<p><strong>Summary:</strong> The focus of this work is sample-efficient deep reinforcement learning (RL)
with a simulator. One useful property of simulators is that it is typically
easy to reset the environment to a previously observed state. We propose an
algorithmic framework, named uncertainty-first local planning (UFLP), that
takes advantage of this property. Concretely, in each data collection
iteration, with some probability, our meta-algorithm resets the environment to
an observed state which has high uncertainty, in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.12717v2' target='_blank'>Automatic Intersection Management in Mixed Traffic Using Reinforcement
  Learning and Graph Neural Networks</a></h2>
<p><strong>Authors:</strong> Marvin Klimke, Benjamin V√∂lz, Michael Buchholz</p>
<p><strong>Summary:</strong> Connected automated driving has the potential to significantly improve urban
traffic efficiency, e.g., by alleviating issues due to occlusion. Cooperative
behavior planning can be employed to jointly optimize the motion of multiple
vehicles. Most existing approaches to automatic intersection management,
however, only consider fully automated traffic. In practice, mixed traffic,
i.e., the simultaneous road usage by automated and human-driven vehicles, will
be prevalent. The present work proposes ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.04994v1' target='_blank'>RIS-Assisted Jamming Rejection and Path Planning for UAV-Borne IoT
  Platform: A New Deep Reinforcement Learning Framework</a></h2>
<p><strong>Authors:</strong> Shuyan Hu, Xin Yuan, Wei Ni, Xin Wang, Abbas Jamalipour</p>
<p><strong>Summary:</strong> This paper presents a new deep reinforcement learning (DRL)-based approach to
the trajectory planning and jamming rejection of an unmanned aerial vehicle
(UAV) for the Internet-of-Things (IoT) applications. Jamming can prevent timely
delivery of sensing data and reception of operation instructions. With the
assistance of a reconfigurable intelligent surface (RIS), we propose to augment
the radio environment, suppress jamming signals, and enhance the desired
signals. The UAV is designed to learn ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.09273v1' target='_blank'>Reinforcement Learning in the Wild with Maximum Likelihood-based Model
  Transfer</a></h2>
<p><strong>Authors:</strong> Hannes Eriksson, Debabrota Basu, Tommy Tram, Mina Alibeigi, Christos Dimitrakakis</p>
<p><strong>Summary:</strong> In this paper, we study the problem of transferring the available Markov
Decision Process (MDP) models to learn and plan efficiently in an unknown but
similar MDP. We refer to it as \textit{Model Transfer Reinforcement Learning
(MTRL)} problem. First, we formulate MTRL for discrete MDPs and Linear
Quadratic Regulators (LQRs) with continuous state actions. Then, we propose a
generic two-stage algorithm, MLEMTRL, to address the MTRL problem in discrete
and continuous settings. In the first stage, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.06936v1' target='_blank'>An Option-Dependent Analysis of Regret Minimization Algorithms in
  Finite-Horizon Semi-Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Gianluca Drappo, Alberto Maria Metelli, Marcello Restelli</p>
<p><strong>Summary:</strong> A large variety of real-world Reinforcement Learning (RL) tasks is
characterized by a complex and heterogeneous structure that makes end-to-end
(or flat) approaches hardly applicable or even infeasible. Hierarchical
Reinforcement Learning (HRL) provides general solutions to address these
problems thanks to a convenient multi-level decomposition of the tasks, making
their solution accessible. Although often used in practice, few works provide
theoretical guarantees to justify this outcome effecti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.02029v2' target='_blank'>Model-aided Federated Reinforcement Learning for Multi-UAV Trajectory
  Planning in IoT Networks</a></h2>
<p><strong>Authors:</strong> Jichao Chen, Omid Esrafilian, Harald Bayerlein, David Gesbert, Marco Caccamo</p>
<p><strong>Summary:</strong> Deploying teams of unmanned aerial vehicles (UAVs) to harvest data from
distributed Internet of Things (IoT) devices requires efficient trajectory
planning and coordination algorithms. Multi-agent reinforcement learning (MARL)
has emerged as a solution, but requires extensive and costly real-world
training data. To tackle this challenge, we propose a novel model-aided
federated MARL algorithm to coordinate multiple UAVs on a data harvesting
mission with only limited knowledge about the environme...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.08400v1' target='_blank'>Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Evan Zheran Liu, Sahaana Suri, Tong Mu, Allan Zhou, Chelsea Finn</p>
<p><strong>Summary:</strong> Whereas machine learning models typically learn language by directly training
on language tasks (e.g., next-word prediction), language emerges in human
children as a byproduct of solving non-language tasks (e.g., acquiring food).
Motivated by this observation, we ask: can embodied reinforcement learning (RL)
agents also indirectly learn language from non-language tasks? Learning to
associate language with its meaning requires a dynamic environment with varied
language. Therefore, we investigate ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.05004v1' target='_blank'>Control as Probabilistic Inference as an Emergent Communication
  Mechanism in Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tomoaki Nakamura, Akira Taniguchi, Tadahiro Taniguchi</p>
<p><strong>Summary:</strong> This paper proposes a generative probabilistic model integrating emergent
communication and multi-agent reinforcement learning. The agents plan their
actions by probabilistic inference, called control as inference, and
communicate using messages that are latent variables and estimated based on the
planned actions. Through these messages, each agent can send information about
its actions and know information about the actions of another agent. Therefore,
the agents change their actions according ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.16240v1' target='_blank'>Robust Unmanned Surface Vehicle Navigation with Distributional
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xi Lin, John McConnell, Brendan Englot</p>
<p><strong>Summary:</strong> Autonomous navigation of Unmanned Surface Vehicles (USV) in marine
environments with current flows is challenging, and few prior works have
addressed the sensorbased navigation problem in such environments under no
prior knowledge of the current flow and obstacles. We propose a Distributional
Reinforcement Learning (RL) based local path planner that learns return
distributions which capture the uncertainty of action outcomes, and an adaptive
algorithm that automatically tunes the level of sensit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.03157v2' target='_blank'>Learning to Recharge: UAV Coverage Path Planning through Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mirco Theile, Harald Bayerlein, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</p>
<p><strong>Summary:</strong> Coverage path planning (CPP) is a critical problem in robotics, where the
goal is to find an efficient path that covers every point in an area of
interest. This work addresses the power-constrained CPP problem with recharge
for battery-limited unmanned aerial vehicles (UAVs). In this problem, a notable
challenge emerges from integrating recharge journeys into the overall coverage
strategy, highlighting the intricate task of making strategic, long-term
decisions. We propose a novel proximal polic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.00229v4' target='_blank'>Consciousness-Inspired Spatio-Temporal Abstractions for Better
  Generalization in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mingde Zhao, Safa Alver, Harm van Seijen, Romain Laroche, Doina Precup, Yoshua Bengio</p>
<p><strong>Summary:</strong> Inspired by human conscious planning, we propose Skipper, a model-based
reinforcement learning framework utilizing spatio-temporal abstractions to
generalize better in novel situations. It automatically decomposes the given
task into smaller, more manageable subtasks, and thus enables sparse
decision-making and focused computation on the relevant parts of the
environment. The decomposition relies on the extraction of an abstracted proxy
problem represented as a directed graph, in which vertices ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.07287v1' target='_blank'>Interactive Interior Design Recommendation via Coarse-to-fine Multimodal
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> He Zhang, Ying Sun, Weiyu Guo, Yafei Liu, Haonan Lu, Xiaodong Lin, Hui Xiong</p>
<p><strong>Summary:</strong> Personalized interior decoration design often incurs high labor costs. Recent
efforts in developing intelligent interior design systems have focused on
generating textual requirement-based decoration designs while neglecting the
problem of how to mine homeowner's hidden preferences and choose the proper
initial design. To fill this gap, we propose an Interactive Interior Design
Recommendation System (IIDRS) based on reinforcement learning (RL). IIDRS aims
to find an ideal plan by interacting wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.09353v1' target='_blank'>Flexible and Adaptive Manufacturing by Complementing Knowledge
  Representation, Reasoning and Planning with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Matthias Mayr, Faseeh Ahmad, Volker Krueger</p>
<p><strong>Summary:</strong> This paper describes a novel approach to adaptive manufacturing in the
context of small batch production and customization. It focuses on integrating
task-level planning and reasoning with reinforcement learning (RL) in the
SkiROS2 skill-based robot control platform. This integration enhances the
efficiency and adaptability of robotic systems in manufacturing, enabling them
to adjust to task variations and learn from interaction data. The paper
highlights the architecture of SkiROS2, particularl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.16996v1' target='_blank'>Goal-conditioned Offline Planning from Curious Exploration</a></h2>
<p><strong>Authors:</strong> Marco Bagatella, Georg Martius</p>
<p><strong>Summary:</strong> Curiosity has established itself as a powerful exploration strategy in deep
reinforcement learning. Notably, leveraging expected future novelty as
intrinsic motivation has been shown to efficiently generate exploratory
trajectories, as well as a robust dynamics model. We consider the challenge of
extracting goal-conditioned behavior from the products of such unsupervised
exploration techniques, without any additional environment interaction. We find
that conventional goal-conditioned reinforceme...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.04464v1' target='_blank'>Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement
  Learning with General Function Approximation</a></h2>
<p><strong>Authors:</strong> Jiayi Huang, Han Zhong, Liwei Wang, Lin F. Yang</p>
<p><strong>Summary:</strong> To tackle long planning horizon problems in reinforcement learning with
general function approximation, we propose the first algorithm, termed as
UCRL-WVTR, that achieves both \emph{horizon-free} and
\emph{instance-dependent}, since it eliminates the polynomial dependency on the
planning horizon. The derived regret bound is deemed \emph{sharp}, as it
matches the minimax lower bound when specialized to linear mixture MDPs up to
logarithmic factors. Furthermore, UCRL-WVTR is \emph{computationally ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.14923v1' target='_blank'>Reinforcement Learning Interventions on Boundedly Rational Human Agents
  in Frictionful Tasks</a></h2>
<p><strong>Authors:</strong> Eura Nofshin, Siddharth Swaroop, Weiwei Pan, Susan Murphy, Finale Doshi-Velez</p>
<p><strong>Summary:</strong> Many important behavior changes are frictionful; they require individuals to
expend effort over a long period with little immediate gratification. Here, an
artificial intelligence (AI) agent can provide personalized interventions to
help individuals stick to their goals. In these settings, the AI agent must
personalize rapidly (before the individual disengages) and interpretably, to
help us understand the behavioral interventions. In this paper, we introduce
Behavior Model Reinforcement Learning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.12856v3' target='_blank'>Equivariant Ensembles and Regularization for Reinforcement Learning in
  Map-based Path Planning</a></h2>
<p><strong>Authors:</strong> Mirco Theile, Hongpeng Cao, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</p>
<p><strong>Summary:</strong> In reinforcement learning (RL), exploiting environmental symmetries can
significantly enhance efficiency, robustness, and performance. However,
ensuring that the deep RL policy and value networks are respectively
equivariant and invariant to exploit these symmetries is a substantial
challenge. Related works try to design networks that are equivariant and
invariant by construction, limiting them to a very restricted library of
components, which in turn hampers the expressiveness of the networks. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.13245v2' target='_blank'>Federated reinforcement learning for robot motion planning with
  zero-shot generalization</a></h2>
<p><strong>Authors:</strong> Zhenyuan Yuan, Siyuan Xu, Minghui Zhu</p>
<p><strong>Summary:</strong> This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the correspond...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.04772v1' target='_blank'>Efficient Reinforcement Learning of Task Planners for Robotic
  Palletization through Iterative Action Masking Learning</a></h2>
<p><strong>Authors:</strong> Zheng Wu, Yichuan Li, Wei Zhan, Changliu Liu, Yun-Hui Liu, Masayoshi Tomizuka</p>
<p><strong>Summary:</strong> The development of robotic systems for palletization in logistics scenarios
is of paramount importance, addressing critical efficiency and precision
demands in supply chain management. This paper investigates the application of
Reinforcement Learning (RL) in enhancing task planning for such robotic
systems. Confronted with the substantial challenge of a vast action space,
which is a significant impediment to efficiently apply out-of-the-shelf RL
methods, our study introduces a novel method of ut...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.04159v1' target='_blank'>MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Demetros Aschu, Robinroy Peter, Sausar Karaf, Aleksey Fedoseev, Dzmitry Tsetserukou</p>
<p><strong>Summary:</strong> Achieving safe and precise landings for a swarm of drones poses a significant
challenge, primarily attributed to conventional control and planning methods.
This paper presents the implementation of multi-agent deep reinforcement
learning (MADRL) techniques for the precise landing of a drone swarm at
relocated target locations. The system is trained in a realistic simulated
environment with a maximum velocity of 3 m/s in training spaces of 4 x 4 x 4 m
and deployed utilizing Crazyflie drones with ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.02662v1' target='_blank'>Integrating Model-Based Footstep Planning with Model-Free Reinforcement
  Learning for Dynamic Legged Locomotion</a></h2>
<p><strong>Authors:</strong> Ho Jae Lee, Seungwoo Hong, Sangbae Kim</p>
<p><strong>Summary:</strong> In this work, we introduce a control framework that combines model-based
footstep planning with Reinforcement Learning (RL), leveraging desired footstep
patterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing
the LIP model, our method forward predicts robot states and determines the
desired foot placement given the velocity commands. We then train an RL policy
to track the foot placements without following the full reference motions
derived from the LIP model. This partial g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.02383v2' target='_blank'>Reinforcement Learning for Wheeled Mobility on Vertically Challenging
  Terrain</a></h2>
<p><strong>Authors:</strong> Tong Xu, Chenhui Pan, Xuesu Xiao</p>
<p><strong>Summary:</strong> Off-road navigation on vertically challenging terrain, involving steep slopes
and rugged boulders, presents significant challenges for wheeled robots both at
the planning level to achieve smooth collision-free trajectories and at the
control level to avoid rolling over or getting stuck. Considering the complex
model of wheel-terrain interactions, we develop an end-to-end Reinforcement
Learning (RL) system for an autonomous vehicle to learn wheeled mobility
through simulated trial-and-error exper...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.13620v1' target='_blank'>Subassembly to Full Assembly: Effective Assembly Sequence Planning
  through Graph-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Chang Shu, Anton Kim, Shinkyu Park</p>
<p><strong>Summary:</strong> This paper proposes an assembly sequence planning framework, named
Subassembly to Assembly (S2A). The framework is designed to enable a robotic
manipulator to assemble multiple parts in a prespecified structure by
leveraging object manipulation actions. The primary technical challenge lies in
the exponentially increasing complexity of identifying a feasible assembly
sequence as the number of parts grows. To address this, we introduce a
graph-based reinforcement learning approach, where a graph a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.16720v1' target='_blank'>Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning
  with Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xian Wang, Jin Zhou, Yuanli Feng, Jiahao Mei, Jiming Chen, Shuo Li</p>
<p><strong>Summary:</strong> Recent innovations in autonomous drones have facilitated time-optimal flight
in single-drone configurations and enhanced maneuverability in multi-drone
systems through the application of optimal control and learning-based methods.
However, few studies have achieved time-optimal motion planning for multi-drone
systems, particularly during highly agile maneuvers or in dynamic scenarios.
This paper presents a decentralized policy network for time-optimal multi-drone
flight using multi-agent reinfor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.16830v1' target='_blank'>OffRIPP: Offline RL-based Informative Path Planning</a></h2>
<p><strong>Authors:</strong> Srikar Babu Gadipudi, Srujan Deolasee, Siva Kailas, Wenhao Luo, Katia Sycara, Woojun Kim</p>
<p><strong>Summary:</strong> Informative path planning (IPP) is a crucial task in robotics, where agents
must design paths to gather valuable information about a target environment
while adhering to resource constraints. Reinforcement learning (RL) has been
shown to be effective for IPP, however, it requires environment interactions,
which are risky and expensive in practice. To address this problem, we propose
an offline RL-based IPP framework that optimizes information gain without
requiring real-time interaction during t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.03997v1' target='_blank'>YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yuan Zhuang, Yi Shen, Zhili Zhang, Yuxiao Chen, Fei Miao</p>
<p><strong>Summary:</strong> Advancements in deep multi-agent reinforcement learning (MARL) have
positioned it as a promising approach for decision-making in cooperative games.
However, it still remains challenging for MARL agents to learn cooperative
strategies for some game environments. Recently, large language models (LLMs)
have demonstrated emergent reasoning capabilities, making them promising
candidates for enhancing coordination among the agents. However, due to the
model size of LLMs, it can be expensive to frequen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.13501v1' target='_blank'>Integrating Large Language Models and Reinforcement Learning for
  Non-Linear Reasoning</a></h2>
<p><strong>Authors:</strong> Yoav Alon, Cristina David</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) were shown to struggle with long-term planning,
which may be caused by the limited way in which they explore the space of
possible solutions. We propose an architecture where a Reinforcement Learning
(RL) Agent guides an LLM's space exploration: (1) the Agent has access to
domain-specific information, and can therefore make decisions about the quality
of candidate solutions based on specific and relevant metrics, which were not
explicitly considered by the LLM's trai...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.03412v1' target='_blank'>Deep Reinforcement Learning-Based Optimization of Second-Life Battery
  Utilization in Electric Vehicles Charging Stations</a></h2>
<p><strong>Authors:</strong> Rouzbeh Haghighi, Ali Hassan, Van-Hai Bui, Akhtar Hussain, Wencong Su</p>
<p><strong>Summary:</strong> The rapid rise in electric vehicle (EV) adoption presents significant
challenges in managing the vast number of retired EV batteries. Research
indicates that second-life batteries (SLBs) from EVs typically retain
considerable residual capacity, offering extended utility. These batteries can
be effectively repurposed for use in EV charging stations (EVCS), providing a
cost-effective alternative to new batteries and reducing overall planning
costs. Integrating battery energy storage systems (BESS)...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.02239v2' target='_blank'>Functions that Emerge through End-to-End Reinforcement Learning - The
  Direction for Artificial General Intelligence -</a></h2>
<p><strong>Authors:</strong> Katsunari Shibata</p>
<p><strong>Summary:</strong> Recently, triggered by the impressive results in TV-games or game of Go by
Google DeepMind, end-to-end reinforcement learning (RL) is collecting
attentions. Although little is known, the author's group has propounded this
framework for around 20 years and already has shown various functions that
emerge in a neural network (NN) through RL. In this paper, they are introduced
again at this timing.
  "Function Modularization" approach is deeply penetrated subconsciously. The
inputs and outputs for a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.08809v1' target='_blank'>A Deep Reinforcement Learning Approach for Global Routing</a></h2>
<p><strong>Authors:</strong> Haiguang Liao, Wentai Zhang, Xuliang Dong, Barnabas Poczos, Kenji Shimada, Levent Burak Kara</p>
<p><strong>Summary:</strong> Global routing has been a historically challenging problem in electronic
circuit design, where the challenge is to connect a large and arbitrary number
of circuit components with wires without violating the design rules for the
printed circuit boards or integrated circuits. Similar routing problems also
exist in the design of complex hydraulic systems, pipe systems and logistic
networks. Existing solutions typically consist of greedy algorithms and
hard-coded heuristics. As such, existing approa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.09673v2' target='_blank'>Deep Q-Learning with Q-Matrix Transfer Learning for Novel Fire
  Evacuation Environment</a></h2>
<p><strong>Authors:</strong> Jivitesh Sharma, Per-Arne Andersen, Ole-Chrisoffer Granmo, Morten Goodwin</p>
<p><strong>Summary:</strong> We focus on the important problem of emergency evacuation, which clearly
could benefit from reinforcement learning that has been largely unaddressed.
Emergency evacuation is a complex task which is difficult to solve with
reinforcement learning, since an emergency situation is highly dynamic, with a
lot of changing variables and complex constraints that makes it difficult to
train on. In this paper, we propose the first fire evacuation environment to
train reinforcement learning agents for evacu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.03836v2' target='_blank'>Graph neural networks-based Scheduler for Production planning problems
  using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mohammed Sharafath Abdul Hameed, Andreas Schwung</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is increasingly adopted in job shop scheduling
problems (JSSP). But RL for JSSP is usually done using a vectorized
representation of machine features as the state space. It has three major
problems: (1) the relationship between the machine units and the job sequence
is not fully captured, (2) exponential increase in the size of the state space
with increasing machines/jobs, and (3) the generalization of the agent to
unseen scenarios. We present a novel framework - Gra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.09878v1' target='_blank'>Safety Aware Autonomous Path Planning Using Model Predictive
  Reinforcement Learning for Inland Waterways</a></h2>
<p><strong>Authors:</strong> Astrid Vanneste, Simon Vanneste, Olivier Vasseur, Robin Janssens, Mattias Billast, Ali Anwar, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx</p>
<p><strong>Summary:</strong> In recent years, interest in autonomous shipping in urban waterways has
increased significantly due to the trend of keeping cars and trucks out of city
centers. Classical approaches such as Frenet frame based planning and potential
field navigation often require tuning of many configuration parameters and
sometimes even require a different configuration depending on the situation. In
this paper, we propose a novel path planning approach based on reinforcement
learning called Model Predictive Rei...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.12591v1' target='_blank'>Improving Efficiency of Training a Virtual Treatment Planner Network via
  Knowledge-guided Deep Reinforcement Learning for Intelligent Automatic
  Treatment Planning of Radiotherapy</a></h2>
<p><strong>Authors:</strong> Chenyang Shen, Liyuan Chen, Yesenia Gonzalez, Xun Jia</p>
<p><strong>Summary:</strong> We previously proposed an intelligent automatic treatment planning framework
for radiotherapy, in which a virtual treatment planner network (VTPN) was built
using deep reinforcement learning (DRL) to operate a treatment planning system
(TPS). Despite the success, the training of VTPN via DRL was time consuming.
Also the training time is expected to grow with the complexity of the treatment
planning problem, preventing the development of VTPN for more complicated but
clinically relevant scenarios...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.10525v1' target='_blank'>Inverse Optimal Planning for Air Traffic Control</a></h2>
<p><strong>Authors:</strong> Ekaterina Tolstaya, Alejandro Ribeiro, Vijay Kumar, Ashish Kapoor</p>
<p><strong>Summary:</strong> We envision a system that concisely describes the rules of air traffic
control, assists human operators and supports dense autonomous air traffic
around commercial airports. We develop a method to learn the rules of air
traffic control from real data as a cost function via maximum entropy inverse
reinforcement learning. This cost function is used as a penalty for a
search-based motion planning method that discretizes both the control and the
state space. We illustrate the methodology by showing ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.13855v1' target='_blank'>Average-Reward Learning and Planning with Options</a></h2>
<p><strong>Authors:</strong> Yi Wan, Abhishek Naik, Richard S. Sutton</p>
<p><strong>Summary:</strong> We extend the options framework for temporal abstraction in reinforcement
learning from discounted Markov decision processes (MDPs) to average-reward
MDPs. Our contributions include general convergent off-policy inter-option
learning algorithms, intra-option algorithms for learning values and models, as
well as sample-based planning variants of our learning algorithms. Our
algorithms and convergence proofs extend those recently developed by Wan, Naik,
and Sutton. We also extend the notion of opt...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.01845v2' target='_blank'>Planning with RL and episodic-memory behavioral priors</a></h2>
<p><strong>Authors:</strong> Shivansh Beohar, Andrew Melnik</p>
<p><strong>Summary:</strong> The practical application of learning agents requires sample efficient and
interpretable algorithms. Learning from behavioral priors is a promising way to
bootstrap agents with a better-than-random exploration policy or a safe-guard
against the pitfalls of early learning. Existing solutions for imitation
learning require a large number of expert demonstrations and rely on
hard-to-interpret learning methods like Deep Q-learning. In this work we
present a planning-based approach that can use these...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.00650v1' target='_blank'>Hybrid Deep Reinforcement Learning and Planning for Safe and Comfortable
  Automated Driving</a></h2>
<p><strong>Authors:</strong> Dikshant Gupta, Mathias Klusch</p>
<p><strong>Summary:</strong> We present a novel hybrid learning method, HyLEAR, for solving the
collision-free navigation problem for self-driving cars in POMDPs. HyLEAR
leverages interposed learning to embed knowledge of a hybrid planner into a
deep reinforcement learner to faster determine safe and comfortable driving
policies. In particular, the hybrid planner combines pedestrian path prediction
and risk-aware path planning with driving-behavior rule-based reasoning such
that the driving policies also take into account, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.16652v1' target='_blank'>Trajectory Planning of Robotic Manipulator in Dynamic Environment
  Exploiting DRL</a></h2>
<p><strong>Authors:</strong> Osama Ahmad, Zawar Hussain, Hammad Naeem</p>
<p><strong>Summary:</strong> This study is about the implementation of a reinforcement learning algorithm
in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick
and place the randomly placed block at a random target point in an unknown
environment. The obstacle is randomly moving which creates a hurdle in picking
the object. The objective of the robot is to avoid the obstacle and pick the
block with constraints to a fixed timestamp. In this literature, we have
applied a deep deterministic policy gra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.11231v1' target='_blank'>Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
  Vehicles</a></h2>
<p><strong>Authors:</strong> Szil√°rd Aradi</p>
<p><strong>Summary:</strong> Academic research in the field of autonomous vehicles has reached high
popularity in recent years related to several topics as sensor technologies,
V2X communications, safety, security, decision making, control, and even legal
and standardization rules. Besides classic control design approaches,
Artificial Intelligence and Machine Learning methods are present in almost all
of these fields. Another part of research focuses on different layers of Motion
Planning, such as strategic decisions, traje...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.11792v2' target='_blank'>Are We On The Same Page? Hierarchical Explanation Generation for
  Planning Tasks in Human-Robot Teaming using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mehrdad Zakershahrak, Samira Ghodratnama</p>
<p><strong>Summary:</strong> Providing explanations is considered an imperative ability for an AI agent in
a human-robot teaming framework. The right explanation provides the rationale
behind an AI agent's decision-making. However, to maintain the human teammate's
cognitive demand to comprehend the provided explanations, prior works have
focused on providing explanations in a specific order or intertwining the
explanation generation with plan execution. Moreover, these approaches do not
consider the degree of details requir...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.00229v2' target='_blank'>Driving with Style: Inverse Reinforcement Learning in General-Purpose
  Planning for Automated Driving</a></h2>
<p><strong>Authors:</strong> Sascha Rosbach, Vinit James, Simon Gro√üjohann, Silviu Homoceanu, Stefan Roth</p>
<p><strong>Summary:</strong> Behavior and motion planning play an important role in automated driving.
Traditionally, behavior planners instruct local motion planners with predefined
behaviors. Due to the high scene complexity in urban environments,
unpredictable situations may occur in which behavior planners fail to match
predefined behavior templates. Recently, general-purpose planners have been
introduced, combining behavior and local motion planning. These general-purpose
planners allow behavior-aware motion planning g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.08345v2' target='_blank'>Distilling a Hierarchical Policy for Planning and Control via
  Representation and Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, Han-Lim Choi</p>
<p><strong>Summary:</strong> We present a hierarchical planning and control framework that enables an
agent to perform various tasks and adapt to a new task flexibly. Rather than
learning an individual policy for each particular task, the proposed framework,
DISH, distills a hierarchical policy from a set of tasks by representation and
reinforcement learning. The framework is based on the idea of latent variable
models that represent high-dimensional observations using low-dimensional
latent variables. The resulting policy ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00640v1' target='_blank'>Motion Planning for Autonomous Vehicles in the Presence of Uncertainty
  Using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kasra Rezaee, Peyman Yadmellat, Simon Chamorro</p>
<p><strong>Summary:</strong> Motion planning under uncertainty is one of the main challenges in developing
autonomous driving vehicles. In this work, we focus on the uncertainty in
sensing and perception, resulted from a limited field of view, occlusions, and
sensing range. This problem is often tackled by considering hypothetical hidden
objects in occluded areas or beyond the sensing range to guarantee passive
safety. However, this may result in conservative planning and expensive
computation, particularly when numerous hy...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.11201v1' target='_blank'>Deep Reinforcement Learning for Trajectory Path Planning and Distributed
  Inference in Resource-Constrained UAV Swarms</a></h2>
<p><strong>Authors:</strong> Marwan Dhuheir, Emna Baccour, Aiman Erbad, Sinan Sabeeh Al-Obaidi, Mounir Hamdi</p>
<p><strong>Summary:</strong> The deployment flexibility and maneuverability of Unmanned Aerial Vehicles
(UAVs) increased their adoption in various applications, such as wildfire
tracking, border monitoring, etc. In many critical applications, UAVs capture
images and other sensory data and then send the captured data to remote servers
for inference and data processing tasks. However, this approach is not always
practical in real-time applications due to the connection instability, limited
bandwidth, and end-to-end latency. O...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.16563v2' target='_blank'>Skill Reinforcement Learning and Planning for Open-World Long-Horizon
  Tasks</a></h2>
<p><strong>Authors:</strong> Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, Zongqing Lu</p>
<p><strong>Summary:</strong> We study building multi-task agents in open-world environments. Without human
demonstrations, learning to accomplish long-horizon tasks in a large open-world
environment with reinforcement learning (RL) is extremely inefficient. To
tackle this challenge, we convert the multi-task learning problem into learning
basic skills and planning over the skills. Using the popular open-world game
Minecraft as the testbed, we propose three types of fine-grained basic skills,
and use RL with intrinsic reward...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.06236v3' target='_blank'>iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed
  Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xiyang Wu, Rohan Chandra, Tianrui Guan, Amrit Singh Bedi, Dinesh Manocha</p>
<p><strong>Summary:</strong> Navigating safely and efficiently in dense and heterogeneous traffic
scenarios is challenging for autonomous vehicles (AVs) due to their inability
to infer the behaviors or intentions of nearby drivers. In this work, we
introduce a distributed multi-agent reinforcement learning (MARL) algorithm
that can predict trajectories and intents in dense and heterogeneous traffic
scenarios. Our approach for intent-aware planning, iPLAN, allows agents to
infer nearby drivers' intents solely from their loca...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.14079v2' target='_blank'>Fighting Uncertainty with Gradients: Offline Reinforcement Learning via
  Diffusion Score Matching</a></h2>
<p><strong>Authors:</strong> H. J. Terry Suh, Glen Chou, Hongkai Dai, Lujie Yang, Abhishek Gupta, Russ Tedrake</p>
<p><strong>Summary:</strong> Gradient-based methods enable efficient search capabilities in high
dimensions. However, in order to apply them effectively in offline optimization
paradigms such as offline Reinforcement Learning (RL) or Imitation Learning
(IL), we require a more careful consideration of how uncertainty estimation
interplays with first-order methods that attempt to minimize them. We study
smoothed distance to data as an uncertainty metric, and claim that it has two
beneficial properties: (i) it allows gradient-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.03758v1' target='_blank'>Hybrid of representation learning and reinforcement learning for dynamic
  and complex robotic motion planning</a></h2>
<p><strong>Authors:</strong> Chengmin Zhou, Xin Lu, Jiapeng Dai, Bingding Huang, Xiaoxu Liu, Pasi Fr√§nti</p>
<p><strong>Summary:</strong> Motion planning is the soul of robot decision making. Classical planning
algorithms like graph search and reaction-based algorithms face challenges in
cases of dense and dynamic obstacles. Deep learning algorithms generate
suboptimal one-step predictions that cause many collisions. Reinforcement
learning algorithms generate optimal or near-optimal time-sequential
predictions. However, they suffer from slow convergence, suboptimal converged
results, and overfittings. This paper introduces a hybri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.09454v1' target='_blank'>LgTS: Dynamic Task Sampling using LLM-generated sub-goals for
  Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, Jivko Sinapov</p>
<p><strong>Summary:</strong> Recent advancements in reasoning abilities of Large Language Models (LLM) has
promoted their usage in problems that require high-level planning for robots
and artificial agents. However, current techniques that utilize LLMs for such
planning tasks make certain key assumptions such as, access to datasets that
permit finetuning, meticulously engineered prompts that only provide relevant
and essential information to the LLM, and most importantly, a deterministic
approach to allow execution of the L...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.20025v3' target='_blank'>GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with
  Learned Models</a></h2>
<p><strong>Authors:</strong> Mianchu Wang, Rui Yang, Xi Chen, Hao Sun, Meng Fang, Giovanni Montana</p>
<p><strong>Summary:</strong> Offline Goal-Conditioned RL (GCRL) offers a feasible paradigm for learning
general-purpose policies from diverse and multi-task offline datasets. Despite
notable recent progress, the predominant offline GCRL methods, mainly
model-free, face constraints in handling limited data and generalizing to
unseen goals. In this work, we propose Goal-conditioned Offline Planning
(GOPlan), a novel model-based framework that contains two key phases: (1)
pretraining a prior policy capable of capturing multi-m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.04894v2' target='_blank'>Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative
  Path Planning</a></h2>
<p><strong>Authors:</strong> Apoorva Vashisth, Julius R√ºckin, Federico Magistri, Cyrill Stachniss, Marija Popoviƒá</p>
<p><strong>Summary:</strong> Autonomous robots are often employed for data collection due to their
efficiency and low labour costs. A key task in robotic data acquisition is
planning paths through an initially unknown environment to collect observations
given platform-specific resource constraints, such as limited battery life.
Adaptive online path planning in 3D environments is challenging due to the
large set of valid actions and the presence of unknown occlusions. To address
these issues, we propose a novel deep reinforc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.05748v1' target='_blank'>Image-Guided Autonomous Guidewire Navigation in Robot-Assisted
  Endovascular Interventions using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Wentao Liu, Tong Tian, Weijin Xu, Bowen Liang, Qingsheng Lu, Xipeng Pan, Wenyi Zhao, Huihua Yang, Ruisheng Su</p>
<p><strong>Summary:</strong> Autonomous robots in endovascular interventions possess the potential to
navigate guidewires with safety and reliability, while reducing human error and
shortening surgical time. However, current methods of guidewire navigation
based on Reinforcement Learning (RL) depend on manual demonstration data or
magnetic guidance. In this work, we propose an Image-guided Autonomous
Guidewire Navigation (IAGN) method. Specifically, we introduce BDA-star, a path
planning algorithm with boundary distance con...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.08497v1' target='_blank'>Prescribing Optimal Health-Aware Operation for Urban Air Mobility with
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mina Montazeri, Chetan Kulkarni, Olga Fink</p>
<p><strong>Summary:</strong> Urban Air Mobility (UAM) aims to expand existing transportation networks in
metropolitan areas by offering short flights either to transport passengers or
cargo. Electric vertical takeoff and landing aircraft powered by lithium-ion
battery packs are considered promising for such applications. Efficient mission
planning is cru-cial, maximizing the number of flights per battery charge while
ensuring completion even under unforeseen events. As batteries degrade, precise
mission planning becomes cha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.09927v1' target='_blank'>Autonomous Path Planning for Intercostal Robotic Ultrasound Imaging
  Using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yuan Bi, Cheng Qian, Zhicheng Zhang, Nassir Navab, Zhongliang Jiang</p>
<p><strong>Summary:</strong> Ultrasound (US) has been widely used in daily clinical practice for screening
internal organs and guiding interventions. However, due to the acoustic shadow
cast by the subcutaneous rib cage, the US examination for thoracic application
is still challenging. To fully cover and reconstruct the region of interest in
US for diagnosis, an intercostal scanning path is necessary. To tackle this
challenge, we present a reinforcement learning (RL) approach for planning
scanning paths between ribs to moni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.12594v1' target='_blank'>Random Network Distillation Based Deep Reinforcement Learning for AGV
  Path Planning</a></h2>
<p><strong>Authors:</strong> Huilin Yin, Shengkai Su, Yinjia Lin, Pengju Zhen, Karin Festl, Daniel Watzenig</p>
<p><strong>Summary:</strong> With the flourishing development of intelligent warehousing systems, the
technology of Automated Guided Vehicle (AGV) has experienced rapid growth.
Within intelligent warehousing environments, AGV is required to safely and
rapidly plan an optimal path in complex and dynamic environments. Most research
has studied deep reinforcement learning to address this challenge. However, in
the environments with sparse extrinsic rewards, these algorithms often converge
slowly, learn inefficiently or fail to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.00754v1' target='_blank'>Cooperative Path Planning with Asynchronous Multiagent Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Jiaming Yin, Weixiong Rao, Yu Xiao, Keshuang Tang</p>
<p><strong>Summary:</strong> In this paper, we study the shortest path problem (SPP) with multiple
source-destination pairs (MSD), namely MSD-SPP, to minimize average travel time
of all shortest paths. The inherent traffic capacity limits within a road
network contributes to the competition among vehicles. Multi-agent
reinforcement learning (MARL) model cannot offer effective and efficient path
planning cooperation due to the asynchronous decision making setting in
MSD-SPP, where vehicles (a.k.a agents) cannot simultaneousl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.02057v1' target='_blank'>Comparative Analysis of Multi-Agent Reinforcement Learning Policies for
  Crop Planning Decision Support</a></h2>
<p><strong>Authors:</strong> Anubha Mahajan, Shreya Hegde, Ethan Shay, Daniel Wu, Aviva Prins</p>
<p><strong>Summary:</strong> In India, the majority of farmers are classified as small or marginal, making
their livelihoods particularly vulnerable to economic losses due to market
saturation and climate risks. Effective crop planning can significantly impact
their expected income, yet existing decision support systems (DSS) often
provide generic recommendations that fail to account for real-time market
dynamics and the interactions among multiple farmers. In this paper, we
evaluate the viability of three multi-agent reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.00034v1' target='_blank'>Towards Efficient Multi-Objective Optimisation for Real-World Power Grid
  Topology Control</a></h2>
<p><strong>Authors:</strong> Yassine El Manyari, Anton R. Fuxjager, Stefan Zahlner, Joost Van Dijk, Alberto Castagna, Davide Barbieri, Jan Viebahn, Marcel Wasserer</p>
<p><strong>Summary:</strong> Power grid operators face increasing difficulties in the control room as the
increase in energy demand and the shift to renewable energy introduce new
complexities in managing congestion and maintaining a stable supply. Effective
grid topology control requires advanced tools capable of handling
multi-objective trade-offs. While Reinforcement Learning (RL) offers a
promising framework for tackling such challenges, existing Multi-Objective
Reinforcement Learning (MORL) approaches fail to scale to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.09393v1' target='_blank'>Generalizable Reinforcement Learning with Biologically Inspired
  Hyperdimensional Occupancy Grid Maps for Exploration and Goal-Directed Path
  Planning</a></h2>
<p><strong>Authors:</strong> Shay Snyder, Ryan Shea, Andrew Capodieci, David Gorsich, Maryam Parsa</p>
<p><strong>Summary:</strong> Real-time autonomous systems utilize multi-layer computational frameworks to
perform critical tasks such as perception, goal finding, and path planning.
Traditional methods implement perception using occupancy grid mapping (OGM),
segmenting the environment into discretized cells with probabilistic
information. This classical approach is well-established and provides a
structured input for downstream processes like goal finding and path planning
algorithms. Recent approaches leverage a biological...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.08024v1' target='_blank'>Bayesian inference for data-efficient, explainable, and safe robotic
  motion planning: A review</a></h2>
<p><strong>Authors:</strong> Chengmin Zhou, Chao Wang, Haseeb Hassan, Himat Shah, Bingding Huang, Pasi Fr√§nti</p>
<p><strong>Summary:</strong> Bayesian inference has many advantages in robotic motion planning over four
perspectives: The uncertainty quantification of the policy, safety (risk-aware)
and optimum guarantees of robot motions, data-efficiency in training of
reinforcement learning, and reducing the sim2real gap when the robot is applied
to real-world tasks. However, the application of Bayesian inference in robotic
motion planning is lagging behind the comprehensive theory of Bayesian
inference. Further, there are no comprehen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.09629v1' target='_blank'>Adaptive Online Replanning with Diffusion Models</a></h2>
<p><strong>Authors:</strong> Siyuan Zhou, Yilun Du, Shun Zhang, Mengdi Xu, Yikang Shen, Wei Xiao, Dit-Yan Yeung, Chuang Gan</p>
<p><strong>Summary:</strong> Diffusion models have risen as a promising approach to data-driven planning,
and have demonstrated impressive robotic control, reinforcement learning, and
video planning performance. Given an effective planner, an important question
to consider is replanning -- when given plans should be regenerated due to both
action execution error and external environment changes. Direct plan execution,
without replanning, is problematic as errors from individual actions rapidly
accumulate and environments ar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.05406v1' target='_blank'>RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio
  Applications</a></h2>
<p><strong>Authors:</strong> Daniel Rosen, Illa Rochez, Caleb McIrvin, Joshua Lee, Kevin D'Alessandro, Max Wiecek, Nhan Hoang, Ramzy Saffarini, Sam Philips, Vanessa Jones, Will Ivey, Zavier Harris-Smart, Zavion Harris-Smart, Zayden Chin, Amos Johnson, Alyse M. Jones, William C. Headley</p>
<p><strong>Summary:</strong> Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely
applicable technology in the next generation of wireless communication systems,
particularly 6G and next-gen military communications. Given this, our research
is focused on developing a tool to promote the development of RFRL techniques
that leverage spectrum sensing. In particular, the tool was designed to address
two cognitive radio applications, specifically dynamic spectrum access and
jamming. In order to train and t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1302.4966v1' target='_blank'>Probabilistic Exploration in Planning while Learning</a></h2>
<p><strong>Authors:</strong> Grigoris I. Karakoulas</p>
<p><strong>Summary:</strong> Sequential decision tasks with incomplete information are characterized by
the exploration problem; namely the trade-off between further exploration for
learning more about the environment and immediate exploitation of the accrued
information for decision-making. Within artificial intelligence, there has been
an increasing interest in studying planning-while-learning algorithms for these
decision tasks. In this paper we focus on the exploration problem in
reinforcement learning and Q-learning in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1602.02867v4' target='_blank'>Value Iteration Networks</a></h2>
<p><strong>Authors:</strong> Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel</p>
<p><strong>Summary:</strong> We introduce the value iteration network (VIN): a fully differentiable neural
network with a `planning module' embedded within. VINs can learn to plan, and
are suitable for predicting outcomes that involve planning-based reasoning,
such as policies for reinforcement learning. Key to our approach is a novel
differentiable approximation of the value-iteration algorithm, which can be
represented as a convolutional neural network, and trained end-to-end using
standard backpropagation. We evaluate VI...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1711.10462v1' target='_blank'>Plan, Attend, Generate: Planning for Sequence-to-Sequence Models</a></h2>
<p><strong>Authors:</strong> Francis Dutil, Caglar Gulcehre, Adam Trischler, Yoshua Bengio</p>
<p><strong>Summary:</strong> We investigate the integration of a planning mechanism into
sequence-to-sequence models using attention. We develop a model which can plan
ahead in the future when it computes its alignments between input and output
sequences, constructing a matrix of proposed future alignments and a commitment
vector that governs whether to follow or recompute the plan. This mechanism is
inspired by the recently proposed strategic attentive reader and writer (STRAW)
model for Reinforcement Learning. Our propose...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.03557v1' target='_blank'>Learning Manipulation States and Actions for Efficient Non-prehensile
  Rearrangement Planning</a></h2>
<p><strong>Authors:</strong> Joshua A. Haustein, Isac Arnekvist, Johannes Stork, Kaiyu Hang, Danica Kragic</p>
<p><strong>Summary:</strong> This paper addresses non-prehensile rearrangement planning problems where a
robot is tasked to rearrange objects among obstacles on a planar surface. We
present an efficient planning algorithm that is designed to impose few
assumptions on the robot's non-prehensile manipulation abilities and is simple
to adapt to different robot embodiments. For this, we combine sampling-based
motion planning with reinforcement learning and generative modeling. Our
algorithm explores the composite configuration ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.06569v1' target='_blank'>Learning retrosynthetic planning through self-play</a></h2>
<p><strong>Authors:</strong> John S. Schreck, Connor W. Coley, Kyle J. M. Bishop</p>
<p><strong>Summary:</strong> The problem of retrosynthetic planning can be framed as one player game, in
which the chemist (or a computer program) works backwards from a molecular
target to simpler starting materials though a series of choices regarding which
reactions to perform. This game is challenging as the combinatorial space of
possible choices is astronomical, and the value of each choice remains
uncertain until the synthesis plan is completed and its cost evaluated. Here,
we address this problem using deep reinforc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.08093v4' target='_blank'>Unsupervised Grounding of Plannable First-Order Logic Representation
  from Images</a></h2>
<p><strong>Authors:</strong> Masataro Asai</p>
<p><strong>Summary:</strong> Recently, there is an increasing interest in obtaining the relational
structures of the environment in the Reinforcement Learning community. However,
the resulting "relations" are not the discrete, logical predicates compatible
to the symbolic reasoning such as classical planning or goal recognition.
Meanwhile, Latplan (Asai and Fukunaga 2018) bridged the gap between
deep-learning perceptual systems and symbolic classical planners. One key
component of the system is a Neural Network called State...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.03548v3' target='_blank'>Reset-Free Lifelong Learning with Skill-Space Planning</a></h2>
<p><strong>Authors:</strong> Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch</p>
<p><strong>Summary:</strong> The objective of lifelong reinforcement learning (RL) is to optimize agents
which can continuously adapt and interact in changing environments. However,
current RL approaches fail drastically when environments are non-stationary and
interactions are non-episodic. We propose Lifelong Skill Planning (LiSP), an
algorithmic framework for non-episodic lifelong RL based on planning in an
abstract space of higher-order skills. We learn the skills in an unsupervised
manner using intrinsic rewards and pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.01191v4' target='_blank'>Planning with Expectation Models</a></h2>
<p><strong>Authors:</strong> Yi Wan, Zaheer Abbas, Adam White, Martha White, Richard S. Sutton</p>
<p><strong>Summary:</strong> Distribution and sample models are two popular model choices in model-based
reinforcement learning (MBRL). However, learning these models can be
intractable, particularly when the state and action spaces are large.
Expectation models, on the other hand, are relatively easier to learn due to
their compactness and have also been widely used for deterministic
environments. For stochastic environments, it is not obvious how expectation
models can be used for planning as they only partially character...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.03138v1' target='_blank'>An advantage actor-critic algorithm for robotic motion planning in dense
  and dynamic scenarios</a></h2>
<p><strong>Authors:</strong> Chengmin Zhou, Bingding Huang, Pasi Fr√§nti</p>
<p><strong>Summary:</strong> Intelligent robots provide a new insight into efficiency improvement in
industrial and service scenarios to replace human labor. However, these
scenarios include dense and dynamic obstacles that make motion planning of
robots challenging. Traditional algorithms like A* can plan collision-free
trajectories in static environment, but their performance degrades and
computational cost increases steeply in dense and dynamic scenarios.
Optimal-value reinforcement learning algorithms (RL) can address t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.09991v2' target='_blank'>Planning with Diffusion for Flexible Behavior Synthesis</a></h2>
<p><strong>Authors:</strong> Michael Janner, Yilun Du, Joshua B. Tenenbaum, Sergey Levine</p>
<p><strong>Summary:</strong> Model-based reinforcement learning methods often use learning only for the
purpose of estimating an approximate dynamics model, offloading the rest of the
decision-making work to classical trajectory optimizers. While conceptually
simple, this combination has a number of empirical shortcomings, suggesting
that learned models may not be well-suited to standard trajectory optimization.
In this paper, we consider what it would look like to fold as much of the
trajectory optimization pipeline as pos...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.02993v2' target='_blank'>Learning to Coordinate for a Worker-Station Multi-robot System in Planar
  Coverage Tasks</a></h2>
<p><strong>Authors:</strong> Jingtao Tang, Yuan Gao, Tin Lun Lam</p>
<p><strong>Summary:</strong> For massive large-scale tasks, a multi-robot system (MRS) can effectively
improve efficiency by utilizing each robot's different capabilities, mobility,
and functionality. In this paper, we focus on the multi-robot coverage path
planning (mCPP) problem in large-scale planar areas with random dynamic
interferers in the environment, where the robots have limited resources. We
introduce a worker-station MRS consisting of multiple workers with limited
resources for actual work, and one station with ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.09628v1' target='_blank'>Efficient Learning of High Level Plans from Play</a></h2>
<p><strong>Authors:</strong> N√∫ria Armengol Urp√≠, Marco Bagatella, Otmar Hilliges, Georg Martius, Stelian Coros</p>
<p><strong>Summary:</strong> Real-world robotic manipulation tasks remain an elusive challenge, since they
involve both fine-grained environment interaction, as well as the ability to
plan for long-horizon goals. Although deep reinforcement learning (RL) methods
have shown encouraging results when planning end-to-end in high-dimensional
environments, they remain fundamentally limited by poor sample efficiency due
to inefficient exploration, and by the complexity of credit assignment over
long horizons. In this work, we pres...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.12410v2' target='_blank'>EDGI: Equivariant Diffusion for Planning with Embodied Agents</a></h2>
<p><strong>Authors:</strong> Johann Brehmer, Joey Bose, Pim de Haan, Taco Cohen</p>
<p><strong>Summary:</strong> Embodied agents operate in a structured world, often solving tasks with
spatial, temporal, and permutation symmetries. Most algorithms for planning and
model-based reinforcement learning (MBRL) do not take this rich geometric
structure into account, leading to sample inefficiency and poor generalization.
We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an
algorithm for MBRL and planning that is equivariant with respect to the product
of the spatial symmetry group SE(3), ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.09997v1' target='_blank'>Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels</a></h2>
<p><strong>Authors:</strong> Thomas Jiralerspong, Flemming Kondrup, Doina Precup, Khimya Khetarpal</p>
<p><strong>Summary:</strong> The ability to plan at many different levels of abstraction enables agents to
envision the long-term repercussions of their decisions and thus enables
sample-efficient learning. This becomes particularly beneficial in complex
environments from high-dimensional state space such as pixels, where the goal
is distant and the reward sparse. We introduce Forecaster, a deep hierarchical
reinforcement learning approach which plans over high-level goals leveraging a
temporally abstract world model. Forec...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.08219v1' target='_blank'>SpaceOctopus: An Octopus-inspired Motion Planning Framework for
  Multi-arm Space Robot</a></h2>
<p><strong>Authors:</strong> Wenbo Zhao, Shengjie Wang, Yixuan Fan, Yang Gao, Tao Zhang</p>
<p><strong>Summary:</strong> Space robots have played a critical role in autonomous maintenance and space
junk removal. Multi-arm space robots can efficiently complete the target
capture and base reorientation tasks due to their flexibility and the
collaborative capabilities between the arms. However, the complex coupling
properties arising from both the multiple arms and the free-floating base
present challenges to the motion planning problems of multi-arm space robots.
We observe that the octopus elegantly achieves simila...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.10794v1' target='_blank'>Diffusion-Reinforcement Learning Hierarchical Motion Planning in
  Adversarial Multi-agent Games</a></h2>
<p><strong>Authors:</strong> Zixuan Wu, Sean Ye, Manisha Natarajan, Matthew C. Gombolay</p>
<p><strong>Summary:</strong> Reinforcement Learning- (RL-)based motion planning has recently shown the
potential to outperform traditional approaches from autonomous navigation to
robot manipulation. In this work, we focus on a motion planning task for an
evasive target in a partially observable multi-agent adversarial
pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to
various applications, such as search and rescue operations and surveillance
robots, where robots must effectively plan their actions...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.18248v1' target='_blank'>Extreme Value Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Masataro Asai, Stephen Wissow</p>
<p><strong>Summary:</strong> Despite being successful in board games and reinforcement learning (RL), UCT,
a Monte-Carlo Tree Search (MCTS) combined with UCB1 Multi-Armed Bandit (MAB),
has had limited success in domain-independent planning until recently. Previous
work showed that UCB1, designed for $[0,1]$-bounded rewards, is not appropriate
for estimating the distance-to-go which are potentially unbounded in
$\mathbb{R}$, such as heuristic functions used in classical planning, then
proposed combining MCTS with MABs design...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.05374v1' target='_blank'>Planning Like Human: A Dual-process Framework for Dialogue Planning</a></h2>
<p><strong>Authors:</strong> Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, Bing Qin</p>
<p><strong>Summary:</strong> In proactive dialogue, the challenge lies not just in generating responses
but in steering conversations toward predetermined goals, a task where Large
Language Models (LLMs) typically struggle due to their reactive nature.
Traditional approaches to enhance dialogue planning in LLMs, ranging from
elaborate prompt engineering to the integration of policy networks, either face
efficiency issues or deliver suboptimal performance. Inspired by the
dualprocess theory in psychology, which identifies tw...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.01510v1' target='_blank'>Adaptive Planning with Generative Models under Uncertainty</a></h2>
<p><strong>Authors:</strong> Pascal Jutras-Dub√©, Ruqi Zhang, Aniket Bera</p>
<p><strong>Summary:</strong> Planning with generative models has emerged as an effective decision-making
paradigm across a wide range of domains, including reinforcement learning and
autonomous navigation. While continuous replanning at each timestep might seem
intuitive because it allows decisions to be made based on the most recent
environmental observations, it results in substantial computational challenges,
primarily due to the complexity of the generative model's underlying deep
learning architecture. Our work address...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.01326v1' target='_blank'>Grounding Language Models in Autonomous Loco-manipulation Tasks</a></h2>
<p><strong>Authors:</strong> Jin Wang, Nikos Tsagarakis</p>
<p><strong>Summary:</strong> Humanoid robots with behavioral autonomy have consistently been regarded as
ideal collaborators in our daily lives and promising representations of
embodied intelligence. Compared to fixed-based robotic arms, humanoid robots
offer a larger operational space while significantly increasing the difficulty
of control and planning. Despite the rapid progress towards general-purpose
humanoid robots, most studies remain focused on locomotion ability with few
investigations into whole-body coordination ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.05289v3' target='_blank'>Developing Path Planning with Behavioral Cloning and Proximal Policy
  Optimization for Path-Tracking and Static Obstacle Nudging</a></h2>
<p><strong>Authors:</strong> Mingyan Zhou, Biao Wang, Tian Tan, Xiatao Sun</p>
<p><strong>Summary:</strong> In autonomous driving, end-to-end methods utilizing Imitation Learning (IL)
and Reinforcement Learning (RL) are becoming more and more common. However,
they do not involve explicit reasoning like classic robotics workflow and
planning with horizons, resulting in strategies implicit and myopic. In this
paper, we introduce a path planning method that uses Behavioral Cloning (BC)
for path-tracking and Proximal Policy Optimization (PPO) for static obstacle
nudging. It outputs lateral offset values t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.09580v1' target='_blank'>SAPIENT: Mastering Multi-turn Conversational Recommendation with
  Strategic Planning and Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Hanwen Du, Bo Peng, Xia Ning</p>
<p><strong>Summary:</strong> Conversational Recommender Systems (CRS) proactively engage users in
interactive dialogues to elicit user preferences and provide personalized
recommendations. Existing methods train Reinforcement Learning (RL)-based agent
with greedy action selection or sampling strategy, and may suffer from
suboptimal conversational planning. To address this, we present a novel Monte
Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a
conversational agent (S-agent) and a conversational ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.17186v1' target='_blank'>DyPNIPP: Predicting Environment Dynamics for RL-based Robust Informative
  Path Planning</a></h2>
<p><strong>Authors:</strong> Srujan Deolasee, Siva Kailas, Wenhao Luo, Katia Sycara, Woojun Kim</p>
<p><strong>Summary:</strong> Informative path planning (IPP) is an important planning paradigm for various
real-world robotic applications such as environment monitoring. IPP involves
planning a path that can learn an accurate belief of the quantity of interest,
while adhering to planning constraints. Traditional IPP methods typically
require high computation time during execution, giving rise to reinforcement
learning (RL) based IPP methods. However, the existing RL-based methods do not
consider spatio-temporal environment...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.01348v2' target='_blank'>Hierarchical Object-Oriented POMDP Planning for Object Rearrangement</a></h2>
<p><strong>Authors:</strong> Rajesh Mangannavar, Alan Fern, Prasad Tadepalli</p>
<p><strong>Summary:</strong> We present an online planning framework for solving multi-object
rearrangement problems in partially observable, multi-room environments.
Current object rearrangement solutions, primarily based on Reinforcement
Learning or hand-coded planning methods, often lack adaptability to diverse
challenges. To address this limitation, we introduce a novel Hierarchical
Object-Oriented Partially Observed Markov Decision Process (HOO-POMDP) planning
approach. This approach comprises of (a) an object-oriented...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.12461v3' target='_blank'>Multi-UAV Path Planning for Wireless Data Harvesting with Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Harald Bayerlein, Mirco Theile, Marco Caccamo, David Gesbert</p>
<p><strong>Summary:</strong> Harvesting data from distributed Internet of Things (IoT) devices with
multiple autonomous unmanned aerial vehicles (UAVs) is a challenging problem
requiring flexible path planning methods. We propose a multi-agent
reinforcement learning (MARL) approach that, in contrast to previous work, can
adapt to profound changes in the scenario parameters defining the data
harvesting mission, such as the number of deployed UAVs, number, position and
data amount of IoT devices, or the maximum flying time, w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.03559v2' target='_blank'>An investigation of model-free planning</a></h2>
<p><strong>Authors:</strong> Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, S√©bastien Racani√®re, Th√©ophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, Timothy Lillicrap</p>
<p><strong>Summary:</strong> The field of reinforcement learning (RL) is facing increasingly challenging
domains with combinatorial complexity. For an RL agent to address these
challenges, it is essential that it can plan effectively. Prior work has
typically utilized an explicit model of the environment, combined with a
specific planning algorithm (such as tree search). More recently, a new family
of methods have been proposed that learn how to plan, by providing the
structure for planning via an inductive bias in the func...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.09608v1' target='_blank'>Graph Value Iteration</a></h2>
<p><strong>Authors:</strong> Dieqiao Feng, Carla P. Gomes, Bart Selman</p>
<p><strong>Summary:</strong> In recent years, deep Reinforcement Learning (RL) has been successful in
various combinatorial search domains, such as two-player games and scientific
discovery. However, directly applying deep RL in planning domains is still
challenging. One major difficulty is that without a human-crafted heuristic
function, reward signals remain zero unless the learning framework discovers
any solution plan. Search space becomes \emph{exponentially larger} as the
minimum length of plans grows, which is a seri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.09854v1' target='_blank'>Effective Baselines for Multiple Object Rearrangement Planning in
  Partially Observable Mapped Environments</a></h2>
<p><strong>Authors:</strong> Engin Tekin, Elaheh Barati, Nitin Kamra, Ruta Desai</p>
<p><strong>Summary:</strong> Many real-world tasks, from house-cleaning to cooking, can be formulated as
multi-object rearrangement problems -- where an agent needs to get specific
objects into appropriate goal states. For such problems, we focus on the
setting that assumes a pre-specified goal state, availability of perfect
manipulation and object recognition capabilities, and a static map of the
environment but unknown initial location of objects to be rearranged. Our goal
is to enable home-assistive intelligent agents to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.03264v2' target='_blank'>Learning-based Near-optimal Motion Planning for Intelligent Vehicles
  with Uncertain Dynamics</a></h2>
<p><strong>Authors:</strong> Yang Lu, Xinglong Zhang, Xin Xu, Weijia Yao</p>
<p><strong>Summary:</strong> Motion planning has been an important research topic in achieving safe and
flexible maneuvers for intelligent vehicles. However, it remains challenging to
realize efficient and optimal planning in the presence of uncertain model
dynamics. In this paper, a sparse kernel-based reinforcement learning (RL)
algorithm with Gaussian Process (GP) Regression (called GP-SKRL) is proposed to
achieve online adaption and near-optimal motion planning performance. In this
algorithm, we design an efficient spar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.08693v2' target='_blank'>AI planning in the imagination: High-level planning on learned abstract
  search spaces</a></h2>
<p><strong>Authors:</strong> Carlos Martin, Tuomas Sandholm</p>
<p><strong>Summary:</strong> Search and planning algorithms have been a cornerstone of artificial
intelligence since the field's inception. Giving reinforcement learning agents
the ability to plan during execution time has resulted in significant
performance improvements in various domains. However, in real-world
environments, the model with respect to which the agent plans has been
constrained to be grounded in the real environment itself, as opposed to a more
abstract model which allows for planning over compound actions ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.18687v1' target='_blank'>Socially Adaptive Path Planning Based on Generative Adversarial Network</a></h2>
<p><strong>Authors:</strong> Yao Wang, Yuqi Kong, Wenzheng Chi, Lining Sun</p>
<p><strong>Summary:</strong> The natural interaction between robots and pedestrians in the process of
autonomous navigation is crucial for the intelligent development of mobile
robots, which requires robots to fully consider social rules and guarantee the
psychological comfort of pedestrians. Among the research results in the field
of robotic path planning, the learning-based socially adaptive algorithms have
performed well in some specific human-robot interaction environments. However,
human-robot interaction scenarios are...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1401.3871v1' target='_blank'>Non-Deterministic Policies in Markovian Decision Processes</a></h2>
<p><strong>Authors:</strong> Mahdi Milani Fard, Joelle Pineau</p>
<p><strong>Summary:</strong> Markovian processes have long been used to model stochastic environments.
Reinforcement learning has emerged as a framework to solve sequential planning
and decision-making problems in such environments. In recent years, attempts
were made to apply methods from reinforcement learning to construct decision
support systems for action selection in Markovian environments. Although
conventional methods in reinforcement learning have proved to be useful in
problems concerning sequential decision-makin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1604.06508v1' target='_blank'>HIRL: Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks
  with Delayed Rewards</a></h2>
<p><strong>Authors:</strong> Sanjay Krishnan, Animesh Garg, Richard Liaw, Lauren Miller, Florian T. Pokorny, Ken Goldberg</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) struggles in problems with delayed rewards, and
one approach is to segment the task into sub-tasks with incremental rewards. We
propose a framework called Hierarchical Inverse Reinforcement Learning (HIRL),
which is a model for learning sub-task structure from demonstrations. HIRL
decomposes the task into sub-tasks based on transitions that are consistent
across demonstrations. These transitions are defined as changes in local
linearity w.r.t to a kernel function. The...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.09207v2' target='_blank'>Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, Wei Pan</p>
<p><strong>Summary:</strong> Humans are capable of attributing latent mental contents such as beliefs or
intentions to others. The social skill is critical in daily life for reasoning
about the potential consequences of others' behaviors so as to plan ahead. It
is known that humans use such reasoning ability recursively by considering what
others believe about their own beliefs. In this paper, we start from level-$1$
recursion and introduce a probabilistic recursive reasoning (PR2) framework for
multi-agent reinforcement le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1908.09453v6' target='_blank'>OpenSpiel: A Framework for Reinforcement Learning in Games</a></h2>
<p><strong>Authors:</strong> Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi, Satyaki Upadhyay, Julien P√©rolat, Sriram Srinivasan, Finbarr Timbers, Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill, Paul Muller, Timo Ewalds, Ryan Faulkner, J√°nos Kram√°r, Bart De Vylder, Brennan Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, Jonah Ryan-Davis</p>
<p><strong>Summary:</strong> OpenSpiel is a collection of environments and algorithms for research in
general reinforcement learning and search/planning in games. OpenSpiel supports
n-player (single- and multi- agent) zero-sum, cooperative and general-sum,
one-shot and sequential, strictly turn-taking and simultaneous-move, perfect
and imperfect information games, as well as traditional multiagent environments
such as (partially- and fully- observable) grid worlds and social dilemmas.
OpenSpiel also includes tools to analyz...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.09470v1' target='_blank'>Self-Supervised Sim-to-Real Adaptation for Visual Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Rae Jeong, Yusuf Aytar, David Khosid, Yuxiang Zhou, Jackie Kay, Thomas Lampe, Konstantinos Bousmalis, Francesco Nori</p>
<p><strong>Summary:</strong> Collecting and automatically obtaining reward signals from real robotic
visual data for the purposes of training reinforcement learning algorithms can
be quite challenging and time-consuming. Methods for utilizing unlabeled data
can have a huge potential to further accelerate robotic learning. We consider
here the problem of performing manipulation tasks from pixels. In such tasks,
choosing an appropriate state representation is crucial for planning and
control. This is even more relevant with r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.06860v1' target='_blank'>Resolving Congestions in the Air Traffic Management Domain via
  Multiagent Reinforcement Learning Methods</a></h2>
<p><strong>Authors:</strong> Theocharis Kravaris, Christos Spatharis, Alevizos Bastas, George A. Vouros, Konstantinos Blekas, Gennady Andrienko, Natalia Andrienko, Jose Manuel Cordero Garcia</p>
<p><strong>Summary:</strong> In this article, we report on the efficiency and effectiveness of multiagent
reinforcement learning methods (MARL) for the computation of flight delays to
resolve congestion problems in the Air Traffic Management (ATM) domain.
Specifically, we aim to resolve cases where demand of airspace use exceeds
capacity (demand-capacity problems), via imposing ground delays to flights at
the pre-tactical stage of operations (i.e. few days to few hours before
operation). Casting this into the multiagent dom...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.00119v2' target='_blank'>Long-Term Visitation Value for Deep Exploration in Sparse Reward
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Simone Parisi, Davide Tateo, Maximilian Hensel, Carlo D'Eramo, Jan Peters, Joni Pajarinen</p>
<p><strong>Summary:</strong> Reinforcement learning with sparse rewards is still an open challenge.
Classic methods rely on getting feedback via extrinsic rewards to train the
agent, and in situations where this occurs very rarely the agent learns slowly
or cannot learn at all. Similarly, if the agent receives also rewards that
create suboptimal modes of the objective function, it will likely prematurely
stop exploring. More recent methods add auxiliary intrinsic rewards to
encourage exploration. However, auxiliary rewards ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.02667v2' target='_blank'>Automated Lane Change Strategy using Proximal Policy Optimization-based
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Fei Ye, Xuxin Cheng, Pin Wang, Ching-Yao Chan, Jiucai Zhang</p>
<p><strong>Summary:</strong> Lane-change maneuvers are commonly executed by drivers to follow a certain
routing plan, overtake a slower vehicle, adapt to a merging lane ahead, etc.
However, improper lane change behaviors can be a major cause of traffic flow
disruptions and even crashes. While many rule-based methods have been proposed
to solve lane change problems for autonomous driving, they tend to exhibit
limited performance due to the uncertainty and complexity of the driving
environment. Machine learning-based methods ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.01584v3' target='_blank'>MARS: Malleable Actor-Critic Reinforcement Learning Scheduler</a></h2>
<p><strong>Authors:</strong> Betis Baheri, Jacob Tronge, Bo Fang, Ang Li, Vipin Chaudhary, Qiang Guan</p>
<p><strong>Summary:</strong> In this paper, we introduce MARS, a new scheduling system for HPC-cloud
infrastructures based on a cost-aware, flexible reinforcement learning
approach, which serves as an intermediate layer for next generation HPC-cloud
resource manager. MARS ensembles the pre-trained models from heuristic
workloads and decides on the most cost-effective strategy for optimization. A
whole workflow application would be split into several optimizable dependent
sub-tasks, then based on the pre-defined resource man...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.04974v1' target='_blank'>Deep Reinforcement Learning for Organ Localization in CT</a></h2>
<p><strong>Authors:</strong> Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C. Peeken, Stephanie E. Combs, Bjoern H. Menze</p>
<p><strong>Summary:</strong> Robust localization of organs in computed tomography scans is a constant
pre-processing requirement for organ-specific image retrieval, radiotherapy
planning, and interventional image analysis. In contrast to current solutions
based on exhaustive search or region proposals, which require large amounts of
annotated data, we propose a deep reinforcement learning approach for organ
localization in CT. In this work, an artificial agent is actively self-taught
to localize organs in CT by learning fro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.06923v1' target='_blank'>Potential Field Guided Actor-Critic Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Weiya Ren</p>
<p><strong>Summary:</strong> In this paper, we consider the problem of actor-critic reinforcement
learning. Firstly, we extend the actor-critic architecture to actor-critic-N
architecture by introducing more critics beyond rewards. Secondly, we combine
the reward-based critic with a potential-field-based critic to formulate the
proposed potential field guided actor-critic reinforcement learning approach
(actor-critic-2). This can be seen as a combination of the model-based
gradients and the model-free gradients in policy im...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.04406v1' target='_blank'>NavRep: Unsupervised Representations for Reinforcement Learning of Robot
  Navigation in Dynamic Human Environments</a></h2>
<p><strong>Authors:</strong> Daniel Dugas, Juan Nieto, Roland Siegwart, Jen Jen Chung</p>
<p><strong>Summary:</strong> Robot navigation is a task where reinforcement learning approaches are still
unable to compete with traditional path planning. State-of-the-art methods
differ in small ways, and do not all provide reproducible, openly available
implementations. This makes comparing methods a challenge. Recent research has
shown that unsupervised learning methods can scale impressively, and be
leveraged to solve difficult problems. In this work, we design ways in which
unsupervised learning can be used to assist ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.10369v2' target='_blank'>Effective Communications: A Joint Learning and Communication Framework
  for Multi-Agent Reinforcement Learning over Noisy Channels</a></h2>
<p><strong>Authors:</strong> Tze-Yang Tung, Szymon Kobus, Joan Roig Pujol, Deniz Gunduz</p>
<p><strong>Summary:</strong> We propose a novel formulation of the "effectiveness problem" in
communications, put forth by Shannon and Weaver in their seminal work [2], by
considering multiple agents communicating over a noisy channel in order to
achieve better coordination and cooperation in a multi-agent reinforcement
learning (MARL) framework. Specifically, we consider a multi-agent partially
observable Markov decision process (MA-POMDP), in which the agents, in addition
to interacting with the environment can also commu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.00936v1' target='_blank'>Least-Restrictive Multi-Agent Collision Avoidance via Deep Meta
  Reinforcement Learning and Optimal Control</a></h2>
<p><strong>Authors:</strong> Salar Asayesh, Mo Chen, Mehran Mehrandezh, Kamal Gupta</p>
<p><strong>Summary:</strong> Multi-agent collision-free trajectory planning and control subject to
different goal requirements and system dynamics has been extensively studied,
and is gaining recent attention in the realm of machine and reinforcement
learning. However, in particular when using a large number of agents,
constructing a least-restrictive collision avoidance policy is of utmost
importance for both classical and learning-based methods. In this paper, we
propose a Least-Restrictive Collision Avoidance Module (LR-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.09146v2' target='_blank'>Contrastive Reinforcement Learning of Symbolic Reasoning Domains</a></h2>
<p><strong>Authors:</strong> Gabriel Poesia, WenXin Dong, Noah Goodman</p>
<p><strong>Summary:</strong> Abstract symbolic reasoning, as required in domains such as mathematics and
logic, is a key component of human intelligence. Solvers for these domains have
important applications, especially to computer-assisted education. But learning
to solve symbolic problems is challenging for machine learning algorithms.
Existing models either learn from human solutions or use hand-engineered
features, making them expensive to apply in new domains. In this paper, we
instead consider symbolic domains as simp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.09521v1' target='_blank'>Escape Room: A Configurable Testbed for Hierarchical Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Jacob Menashe, Peter Stone</p>
<p><strong>Summary:</strong> Recent successes in Reinforcement Learning have encouraged a fast-growing
network of RL researchers and a number of breakthroughs in RL research. As the
RL community and the body of RL work grows, so does the need for widely
applicable benchmarks that can fairly and effectively evaluate a variety of RL
algorithms.
  This need is particularly apparent in the realm of Hierarchical Reinforcement
Learning (HRL). While many existing test domains may exhibit hierarchical
action or state structures, mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.02655v2' target='_blank'>PPMC RL Training Algorithm: Rough Terrain Intelligent Robots through
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tamir Blum, Kazuya Yoshida</p>
<p><strong>Summary:</strong> Robots can now learn how to make decisions and control themselves,
generalizing learned behaviors to unseen scenarios. In particular, AI powered
robots show promise in rough environments like the lunar surface, due to the
environmental uncertainties. We address this critical generalization aspect for
robot locomotion in rough terrain through a training algorithm we have created
called the Path Planning and Motion Control (PPMC) Training Algorithm. This
algorithm is coupled with any generic reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.12909v2' target='_blank'>Policy Teaching via Environment Poisoning: Training-time Adversarial
  Attacks against Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, Adish Singla</p>
<p><strong>Summary:</strong> We study a security threat to reinforcement learning where an attacker
poisons the learning environment to force the agent into executing a target
policy chosen by the attacker. As a victim, we consider RL agents whose
objective is to find a policy that maximizes average reward in undiscounted
infinite-horizon problem settings. The attacker can manipulate the rewards or
the transition dynamics in the learning environment at training-time and is
interested in doing so in a stealthy manner. We pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.01359v1' target='_blank'>Adaptive Reinforcement Learning Model for Simulation of Urban Mobility
  during Crises</a></h2>
<p><strong>Authors:</strong> Chao Fan, Xiangqi Jiang, Ali Mostafavi</p>
<p><strong>Summary:</strong> The objective of this study is to propose and test an adaptive reinforcement
learning model that can learn the patterns of human mobility in a normal
context and simulate the mobility during perturbations caused by crises, such
as flooding, wildfire, and hurricanes. Understanding and predicting human
mobility patterns, such as destination and trajectory selection, can inform
emerging congestion and road closures raised by disruptions in emergencies.
Data related to human movement trajectories ar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.03934v1' target='_blank'>Metis: Multi-Agent Based Crisis Simulation System</a></h2>
<p><strong>Authors:</strong> George Sidiropoulos, Chairi Kiourt, Lefteris Moussiades</p>
<p><strong>Summary:</strong> With the advent of the computational technologies (Graphics Processing Units
- GPUs) and Machine Learning, the research domain of crowd simulation for
crisis management has flourished. Along with the new techniques and
methodologies that have been proposed all those years, aiming to increase the
realism of crowd simulation, several crisis simulation systems/tools have been
developed, but most of them focus on special cases without providing users the
ability to adapt them based on their needs. T...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.06387v2' target='_blank'>Dynamic allocation of limited memory resources in reinforcement learning</a></h2>
<p><strong>Authors:</strong> Nisheet Patel, Luigi Acerbi, Alexandre Pouget</p>
<p><strong>Summary:</strong> Biological brains are inherently limited in their capacity to process and
store information, but are nevertheless capable of solving complex tasks with
apparent ease. Intelligent behavior is related to these limitations, since
resource constraints drive the need to generalize and assign importance
differentially to features in the environment or memories of past experiences.
Recently, there have been parallel efforts in reinforcement learning and
neuroscience to understand strategies adopted by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.12962v3' target='_blank'>Bias-reduced Multi-step Hindsight Experience Replay for Efficient
  Multi-goal Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Rui Yang, Jiafei Lyu, Yu Yang, Jiangpeng Yan, Feng Luo, Dijun Luo, Lanqing Li, Xiu Li</p>
<p><strong>Summary:</strong> Multi-goal reinforcement learning is widely applied in planning and robot
manipulation. Two main challenges in multi-goal reinforcement learning are
sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims
to tackle the two challenges via goal relabeling. However, HER-related works
still need millions of samples and a huge computation. In this paper, we
propose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step
relabeled returns based on $n$-step relabe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.08325v1' target='_blank'>Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Peide Cai, Hengli Wang, Huaiyang Huang, Yuxuan Liu, Ming Liu</p>
<p><strong>Summary:</strong> Autonomous car racing is a challenging task in the robotic control area.
Traditional modular methods require accurate mapping, localization and
planning, which makes them computationally inefficient and sensitive to
environmental changes. Recently, deep-learning-based end-to-end systems have
shown promising results for autonomous driving/racing. However, they are
commonly implemented by supervised imitation learning (IL), which suffers from
the distribution mismatch problem, or by reinforcement ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.06383v1' target='_blank'>Distilling Motion Planner Augmented Policies into Visual Control
  Policies for Robot Manipulation</a></h2>
<p><strong>Authors:</strong> I-Chun Arthur Liu, Shagun Uppal, Gaurav S. Sukhatme, Joseph J. Lim, Peter Englert, Youngwoon Lee</p>
<p><strong>Summary:</strong> Learning complex manipulation tasks in realistic, obstructed environments is
a challenging problem due to hard exploration in the presence of obstacles and
high-dimensional visual observations. Prior work tackles the exploration
problem by integrating motion planning and reinforcement learning. However, the
motion planner augmented policy requires access to state information, which is
often not available in the real-world settings. To this end, we propose to
distill a state-based motion planner ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.10810v1' target='_blank'>Vulcan: Solving the Steiner Tree Problem with Graph Neural Networks and
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haizhou Du, Zong Yan, Qiao Xiang, Qinqing Zhan</p>
<p><strong>Summary:</strong> Steiner Tree Problem (STP) in graphs aims to find a tree of minimum weight in
the graph that connects a given set of vertices. It is a classic NP-hard
combinatorial optimization problem and has many real-world applications (e.g.,
VLSI chip design, transportation network planning and wireless sensor
networks). Many exact and approximate algorithms have been developed for STP,
but they suffer from high computational complexity and weak worst-case solution
guarantees, respectively. Heuristic algori...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.00039v1' target='_blank'>Stochastic convex optimization for provably efficient apprenticeship
  learning</a></h2>
<p><strong>Authors:</strong> Angeliki Kamoutsi, Goran Banjac, John Lygeros</p>
<p><strong>Summary:</strong> We consider large-scale Markov decision processes (MDPs) with an unknown cost
function and employ stochastic convex optimization tools to address the problem
of imitation learning, which consists of learning a policy from a finite set of
expert demonstrations.
  We adopt the apprenticeship learning formalism, which carries the assumption
that the true cost function can be represented as a linear combination of some
known features. Existing inverse reinforcement learning algorithms come with
stro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.02447v3' target='_blank'>DL-DRL: A double-level deep reinforcement learning approach for
  large-scale task scheduling of multi-UAV</a></h2>
<p><strong>Authors:</strong> Xiao Mao, Zhiguang Cao, Mingfeng Fan, Guohua Wu, Witold Pedrycz</p>
<p><strong>Summary:</strong> Exploiting unmanned aerial vehicles (UAVs) to execute tasks is gaining
growing popularity recently. To solve the underlying task scheduling problem,
the deep reinforcement learning (DRL) based methods demonstrate notable
advantage over the conventional heuristics as they rely less on hand-engineered
rules. However, their decision space will become prohibitively huge as the
problem scales up, thus deteriorating the computation efficiency. To alleviate
this issue, we propose a double-level deep re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.13220v2' target='_blank'>Exploiting Transformer in Sparse Reward Reinforcement Learning for
  Interpretable Temporal Logic Motion Planning</a></h2>
<p><strong>Authors:</strong> Hao Zhang, Hao Wang, Zhen Kan</p>
<p><strong>Summary:</strong> Automaton based approaches have enabled robots to perform various complex
tasks. However, most existing automaton based algorithms highly rely on the
manually customized representation of states for the considered task, limiting
its applicability in deep reinforcement learning algorithms. To address this
issue, by incorporating Transformer into reinforcement learning, we develop a
Double-Transformer-guided Temporal Logic framework (T2TL) that exploits the
structural feature of Transformer twice,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.08238v1' target='_blank'>Near-Optimal Regret Bounds for Multi-batch Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Yuhang Jiang, Yuan Zhou, Xiangyang Ji</p>
<p><strong>Summary:</strong> In this paper,
  we study the episodic reinforcement learning (RL) problem modeled by
finite-horizon Markov Decision Processes (MDPs) with constraint on the number
of batches. The multi-batch reinforcement learning framework, where the agent
is required to provide a time schedule to update policy before everything,
which is particularly suitable for the scenarios where the agent suffers
extensively from changing the policy adaptively. Given a finite-horizon MDP
with $S$ states, $A$ actions and p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.12927v1' target='_blank'>The Design and Realization of Multi-agent Obstacle Avoidance based on
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Enyu Zhao, Chanjuan Liu, Houfu Su, Yang Liu</p>
<p><strong>Summary:</strong> Intelligence agents and multi-agent systems play important roles in scenes
like the control system of grouped drones, and multi-agent navigation and
obstacle avoidance which is the foundational function of advanced application
has great importance. In multi-agent navigation and obstacle avoidance tasks,
the decision-making interactions and dynamic changes of agents are difficult
for traditional route planning algorithms or reinforcement learning algorithms
with the increased complexity of the en...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.02674v1' target='_blank'>Wind Power Forecasting Considering Data Privacy Protection: A Federated
  Deep Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Yang Li, Ruinong Wang, Yuanzheng Li, Meng Zhang, Chao Long</p>
<p><strong>Summary:</strong> In a modern power system with an increasing proportion of renewable energy,
wind power prediction is crucial to the arrangement of power grid dispatching
plans due to the volatility of wind power. However, traditional centralized
forecasting methods raise concerns regarding data privacy-preserving and data
islands problem. To handle the data privacy and openness, we propose a
forecasting scheme that combines federated learning and deep reinforcement
learning (DRL) for ultra-short-term wind power...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.05243v1' target='_blank'>Vision-based navigation and obstacle avoidance via deep reinforcement
  learning</a></h2>
<p><strong>Authors:</strong> Paul Blum, Peter Crowley, George Lykotrafitis</p>
<p><strong>Summary:</strong> Development of navigation algorithms is essential for the successful
deployment of robots in rapidly changing hazardous environments for which prior
knowledge of configuration is often limited or unavailable. Use of traditional
path-planning algorithms, which are based on localization and require detailed
obstacle maps with goal locations, is not possible. In this regard,
vision-based algorithms hold great promise, as visual information can be
readily acquired by a robot's onboard sensors and pr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.05855v2' target='_blank'>Robust N-1 secure HV Grid Flexibility Estimation for TSO-DSO coordinated
  Congestion Management with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhenqi Wang, Sebastian Wende-von Berg, Martin Braun</p>
<p><strong>Summary:</strong> Nowadays, the PQ flexibility from the distributed energy resources (DERs) in
the high voltage (HV) grids plays a more critical and significant role in grid
congestion management in TSO grids. This work proposed a multi-stage deep
reinforcement learning approach to estimate the PQ flexibility (PQ area) at the
TSO-DSO interfaces and identifies the DER PQ setpoints for each operating point
in a way, that DERs in the meshed HV grid can be coordinated to offer
flexibility for the transmission grid. I...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.02306v1' target='_blank'>Look-Ahead AC Optimal Power Flow: A Model-Informed Reinforcement
  Learning Approach</a></h2>
<p><strong>Authors:</strong> Xinyue Wang, Haiwang Zhong, Guanglun Zhang, Guangchun Ruan, Yiliu He, Zekuan Yu</p>
<p><strong>Summary:</strong> With the increasing proportion of renewable energy in the generation side, it
becomes more difficult to accurately predict the power generation and adapt to
the large deviations between the optimal dispatch scheme and the day-ahead
scheduling in the process of real-time dispatch. Therefore, it is necessary to
conduct look-ahead dispatches to revise the operation plan according to the
real-time status of the power grid and reliable ultra-short-term prediction.
Application of traditional model-dri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.04022v1' target='_blank'>A Reinforcement Learning-assisted Genetic Programming Algorithm for Team
  Formation Problem Considering Person-Job Matching</a></h2>
<p><strong>Authors:</strong> Yangyang Guo, Hao Wang, Lei He, Witold Pedrycz, P. N. Suganthan, Yanjie Song</p>
<p><strong>Summary:</strong> An efficient team is essential for the company to successfully complete new
projects. To solve the team formation problem considering person-job matching
(TFP-PJM), a 0-1 integer programming model is constructed, which considers both
person-job matching and team members' willingness to communicate on team
efficiency, with the person-job matching score calculated using intuitionistic
fuzzy numbers. Then, a reinforcement learning-assisted genetic programming
algorithm (RL-GP) is proposed to enhanc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.05099v6' target='_blank'>Feudal Graph Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tommaso Marzi, Arshjot Khehra, Andrea Cini, Cesare Alippi</p>
<p><strong>Summary:</strong> Graph-based representations and message-passing modular policies constitute
prominent approaches to tackling composable control problems in reinforcement
learning (RL). However, as shown by recent graph deep learning literature, such
local message-passing operators can create information bottlenecks and hinder
global coordination. The issue becomes more serious in tasks requiring
high-level planning. In this work, we propose a novel methodology, named Feudal
Graph Reinforcement Learning (FGRL), ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.03526v1' target='_blank'>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Micha√´l Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad ≈ªo≈Çna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio G√≥mez Colmenarejo, A√§ron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</p>
<p><strong>Summary:</strong> StarCraft II is one of the most challenging simulated reinforcement learning
environments; it is partially observable, stochastic, multi-agent, and
mastering StarCraft II requires strategic planning over long time horizons with
real-time low-level execution. It also has an active professional competitive
scene. StarCraft II is uniquely suited for advancing offline RL algorithms,
both because of its challenging nature and because Blizzard has released a
massive dataset of millions of StarCraft II...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.01038v1' target='_blank'>Neurosymbolic Reinforcement Learning and Planning: A Survey</a></h2>
<p><strong>Authors:</strong> K. Acharya, W. Raza, C. M. J. M. Dourado Jr, A. Velasquez, H. Song</p>
<p><strong>Summary:</strong> The area of Neurosymbolic Artificial Intelligence (Neurosymbolic AI) is
rapidly developing and has become a popular research topic, encompassing
sub-fields such as Neurosymbolic Deep Learning (Neurosymbolic DL) and
Neurosymbolic Reinforcement Learning (Neurosymbolic RL). Compared to
traditional learning methods, Neurosymbolic AI offers significant advantages by
simplifying complexity and providing transparency and explainability.
Reinforcement Learning(RL), a long-standing Artificial Intelligenc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.13508v2' target='_blank'>Guided Cooperation in Hierarchical Reinforcement Learning via
  Model-based Rollout</a></h2>
<p><strong>Authors:</strong> Haoran Wang, Zeshen Tang, Leya Yang, Yaoru Sun, Fang Wang, Siyu Zhang, Yeming Chen</p>
<p><strong>Summary:</strong> Goal-conditioned hierarchical reinforcement learning (HRL) presents a
promising approach for enabling effective exploration in complex, long-horizon
reinforcement learning (RL) tasks through temporal abstraction. Empirically,
heightened inter-level communication and coordination can induce more stable
and robust policy improvement in hierarchical systems. Yet, most existing
goal-conditioned HRL algorithms have primarily focused on the subgoal
discovery, neglecting inter-level cooperation. Here, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.17433v1' target='_blank'>DREAM: Decentralized Reinforcement Learning for Exploration and
  Efficient Energy Management in Multi-Robot Systems</a></h2>
<p><strong>Authors:</strong> Dipam Patel, Phu Pham, Kshitij Tiwari, Aniket Bera</p>
<p><strong>Summary:</strong> Resource-constrained robots often suffer from energy inefficiencies,
underutilized computational abilities due to inadequate task allocation, and a
lack of robustness in dynamic environments, all of which strongly affect their
performance. This paper introduces DREAM - Decentralized Reinforcement Learning
for Exploration and Efficient Energy Management in Multi-Robot Systems, a
comprehensive framework that optimizes the allocation of resources for
efficient exploration. It advances beyond conven...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.01468v1' target='_blank'>Remember what you did so you know what to do next</a></h2>
<p><strong>Authors:</strong> Manuel R. Ciosici, Alex Hedges, Yash Kankanampati, Justin Martin, Marjorie Freedman, Ralph Weischedel</p>
<p><strong>Summary:</strong> We explore using a moderately sized large language model (GPT-J 6B
parameters) to create a plan for a simulated robot to achieve 30 classes of
goals in ScienceWorld, a text game simulator for elementary science
experiments. Previously published empirical work claimed that large language
models (LLMs) are a poor fit (Wang et al., 2022) compared to reinforcement
learning. Using the Markov assumption (a single previous step), the LLM
outperforms the reinforcement learning-based approach by a factor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.11287v1' target='_blank'>Tactile Active Inference Reinforcement Learning for Efficient Robotic
  Manipulation Skill Acquisition</a></h2>
<p><strong>Authors:</strong> Zihao Liu, Xing Liu, Yizhai Zhang, Zhengxiong Liu, Panfeng Huang</p>
<p><strong>Summary:</strong> Robotic manipulation holds the potential to replace humans in the execution
of tedious or dangerous tasks. However, control-based approaches are not
suitable due to the difficulty of formally describing open-world manipulation
in reality, and the inefficiency of existing learning methods. Thus, applying
manipulation in a wide range of scenarios presents significant challenges. In
this study, we propose a novel method for skill learning in robotic
manipulation called Tactile Active Inference Rein...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.01058v1' target='_blank'>A Survey of Progress on Cooperative Multi-agent Reinforcement Learning
  in Open Environment</a></h2>
<p><strong>Authors:</strong> Lei Yuan, Ziqian Zhang, Lihe Li, Cong Guan, Yang Yu</p>
<p><strong>Summary:</strong> Multi-agent Reinforcement Learning (MARL) has gained wide attention in recent
years and has made progress in various fields. Specifically, cooperative MARL
focuses on training a team of agents to cooperatively achieve tasks that are
difficult for a single agent to handle. It has shown great potential in
applications such as path planning, autonomous driving, active voltage control,
and dynamic algorithm configuration. One of the research focuses in the field
of cooperative MARL is how to improve...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.15484v1' target='_blank'>R$\times$R: Rapid eXploration for Reinforcement Learning via
  Sampling-based Reset Distributions and Imitation Pre-training</a></h2>
<p><strong>Authors:</strong> Gagan Khandate, Tristan L. Saidi, Siqi Shang, Eric T. Chang, Yang Liu, Seth Dennis, Johnson Adams, Matei Ciocarlie</p>
<p><strong>Summary:</strong> We present a method for enabling Reinforcement Learning of motor control
policies for complex skills such as dexterous manipulation. We posit that a key
difficulty for training such policies is the difficulty of exploring the
problem state space, as the accessible and useful regions of this space form a
complex structure along manifolds of the original high-dimensional state space.
This work presents a method to enable and support exploration with
Sampling-based Planning. We use a generally appl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.19128v1' target='_blank'>ARMCHAIR: integrated inverse reinforcement learning and model predictive
  control for human-robot collaboration</a></h2>
<p><strong>Authors:</strong> Angelo Caregnato-Neto, Luciano Cavalcante Siebert, Arkady Zgonnikov, Marcos Ricardo Omena de Albuquerque Maximo, Rubens Junqueira Magalh√£es Afonso</p>
<p><strong>Summary:</strong> One of the key issues in human-robot collaboration is the development of
computational models that allow robots to predict and adapt to human behavior.
Much progress has been achieved in developing such models, as well as control
techniques that address the autonomy problems of motion planning and
decision-making in robotics. However, the integration of computational models
of human behavior with such control techniques still poses a major challenge,
resulting in a bottleneck for efficient colla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.15417v2' target='_blank'>The Power of Resets in Online Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zakaria Mhammedi, Dylan J. Foster, Alexander Rakhlin</p>
<p><strong>Summary:</strong> Simulators are a pervasive tool in reinforcement learning, but most existing
algorithms cannot efficiently exploit simulator access -- particularly in
high-dimensional domains that require general function approximation. We
explore the power of simulators through online reinforcement learning with
{local simulator access} (or, local planning), an RL protocol where the agent
is allowed to reset to previously observed states and follow their dynamics
during training. We use local simulator access ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.14655v2' target='_blank'>Multi-turn Reinforcement Learning from Preference Human Feedback</a></h2>
<p><strong>Authors:</strong> Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan Hassidim, Yossi Matias, R√©mi Munos</p>
<p><strong>Summary:</strong> Reinforcement Learning from Human Feedback (RLHF) has become the standard
approach for aligning Large Language Models (LLMs) with human preferences,
allowing LLMs to demonstrate remarkable abilities in various tasks. Existing
methods work by emulating the preferences at the single decision (turn) level,
limiting their capabilities in settings that require planning or multi-turn
interactions to achieve a long-term goal. In this paper, we address this issue
by developing novel methods for Reinforc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.00364v1' target='_blank'>Cognitive Manipulation: Semi-supervised Visual Representation and
  Classroom-to-real Reinforcement Learning for Assembly in Semi-structured
  Environments</a></h2>
<p><strong>Authors:</strong> Chuang Wang, Lie Yang, Ze Lin, Yizhi Liao, Gang Chen, Longhan Xie</p>
<p><strong>Summary:</strong> Assembling a slave object into a fixture-free master object represents a
critical challenge in flexible manufacturing. Existing deep reinforcement
learning-based methods, while benefiting from visual or operational priors,
often struggle with small-batch precise assembly tasks due to their reliance on
insufficient priors and high-costed model development. To address these
limitations, this paper introduces a cognitive manipulation and learning
approach that utilizes skill graphs to integrate lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01047v1' target='_blank'>An Advanced Reinforcement Learning Framework for Online Scheduling of
  Deferrable Workloads in Cloud Computing</a></h2>
<p><strong>Authors:</strong> Hang Dong, Liwen Zhu, Zhao Shan, Bo Qiao, Fangkai Yang, Si Qin, Chuan Luo, Qingwei Lin, Yuwen Yang, Gurpreet Virdi, Saravan Rajmohan, Dongmei Zhang, Thomas Moscibroda</p>
<p><strong>Summary:</strong> Efficient resource utilization and perfect user experience usually conflict
with each other in cloud computing platforms. Great efforts have been invested
in increasing resource utilization but trying not to affect users' experience
for cloud computing platforms. In order to better utilize the remaining pieces
of computing resources spread over the whole platform, deferrable jobs are
provided with a discounted price to users. For this type of deferrable jobs,
users are allowed to submit jobs tha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.14054v1' target='_blank'>Urban-Focused Multi-Task Offline Reinforcement Learning with Contrastive
  Data Sharing</a></h2>
<p><strong>Authors:</strong> Xinbo Zhao, Yingxue Zhang, Xin Zhang, Yu Yang, Yiqun Xie, Yanhua Li, Jun Luo</p>
<p><strong>Summary:</strong> Enhancing diverse human decision-making processes in an urban environment is
a critical issue across various applications, including ride-sharing vehicle
dispatching, public transportation management, and autonomous driving. Offline
reinforcement learning (RL) is a promising approach to learn and optimize human
urban strategies (or policies) from pre-collected human-generated
spatial-temporal urban data. However, standard offline RL faces two significant
challenges: (1) data scarcity and data he...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.08164v2' target='_blank'>Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for
  Multi-Robot Cooperation Tasks</a></h2>
<p><strong>Authors:</strong> Pu Feng, Junkang Liang, Size Wang, Xin Yu, Xin Ji, Yiting Chen, Kui Zhang, Rongye Shi, Wenjun Wu</p>
<p><strong>Summary:</strong> In multi-agent reinforcement learning (MARL), the Centralized Training with
Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap:
global state guidance in training versus reliance on local observations in
execution, lacking global signals. Inspired by human societal consensus
mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent
Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL
employs contrastive learning to foster a global ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.02316v1' target='_blank'>Optimizing Plastic Waste Collection in Water Bodies Using Heterogeneous
  Autonomous Surface Vehicles with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alejandro Mendoza Barrionuevo, Samuel Yanes Luis, Daniel Guti√©rrez Reina, Sergio L. Toral Mar√≠n</p>
<p><strong>Summary:</strong> This paper presents a model-free deep reinforcement learning framework for
informative path planning with heterogeneous fleets of autonomous surface
vehicles to locate and collect plastic waste. The system employs two teams of
vehicles: scouts and cleaners. Coordination between these teams is achieved
through a deep reinforcement approach, allowing agents to learn strategies to
maximize cleaning efficiency. The primary objective is for the scout team to
provide an up-to-date contamination model,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.14451v1' target='_blank'>MARL-OT: Multi-Agent Reinforcement Learning Guided Online Fuzzing to
  Detect Safety Violation in Autonomous Driving Systems</a></h2>
<p><strong>Authors:</strong> Linfeng Liang, Xi Zheng</p>
<p><strong>Summary:</strong> Autonomous Driving Systems (ADSs) are safety-critical, as real-world safety
violations can result in significant losses. Rigorous testing is essential
before deployment, with simulation testing playing a key role. However, ADSs
are typically complex, consisting of multiple modules such as perception and
planning, or well-trained end-to-end autonomous driving systems. Offline
methods, such as the Genetic Algorithm (GA), can only generate predefined
trajectories for dynamics, which struggle to cau...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.11134v1' target='_blank'>Solving Online Resource-Constrained Scheduling for Follow-Up Observation
  in Astronomy: a Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Yajie Zhang, Ce Yu, Chao Sun, Jizeng Wei, Junhan Ju, Shanjiang Tang</p>
<p><strong>Summary:</strong> In the astronomical observation field, determining the allocation of
observation resources of the telescope array and planning follow-up
observations for targets of opportunity (ToOs) are indispensable components of
astronomical scientific discovery. This problem is computationally challenging,
given the online observation setting and the abundance of time-varying factors
that can affect whether an observation can be conducted. This paper presents
ROARS, a reinforcement learning approach for onl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.13006v1' target='_blank'>Integrating Reinforcement Learning, Action Model Learning, and Numeric
  Planning for Tackling Complex Tasks</a></h2>
<p><strong>Authors:</strong> Yarin Benyamin, Argaman Mordoch, Shahaf S. Shperberg, Roni Stern</p>
<p><strong>Summary:</strong> Automated Planning algorithms require a model of the domain that specifies
the preconditions and effects of each action. Obtaining such a domain model is
notoriously hard. Algorithms for learning domain models exist, yet it remains
unclear whether learning a domain model and planning is an effective approach
for numeric planning environments, i.e., where states include discrete and
numeric state variables. In this work, we explore the benefits of learning a
numeric domain model and compare it wi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1710.03937v2' target='_blank'>PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement
  Learning and Sampling-based Planning</a></h2>
<p><strong>Authors:</strong> Aleksandra Faust, Oscar Ramirez, Marek Fiser, Kenneth Oslund, Anthony Francis, James Davidson, Lydia Tapia</p>
<p><strong>Summary:</strong> We present PRM-RL, a hierarchical method for long-range navigation task
completion that combines sampling based path planning with reinforcement
learning (RL). The RL agents learn short-range, point-to-point navigation
policies that capture robot dynamics and task constraints without knowledge of
the large-scale topology. Next, the sampling-based planners provide roadmaps
which connect robot configurations that can be successfully navigated by the RL
agent. The same RL agents are used to control...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1710.11417v2' target='_blank'>TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Gregory Farquhar, Tim Rockt√§schel, Maximilian Igl, Shimon Whiteson</p>
<p><strong>Summary:</strong> Combining deep model-free reinforcement learning with on-line planning is a
promising approach to building on the successes of deep RL. On-line planning
with look-ahead trees has proven successful in environments where transition
models are known a priori. However, in complex environments where transition
models need to be learned from data, the deficiencies of learned models have
limited their utility for planning. To address these challenges, we propose
TreeQN, a differentiable, recursive, tre...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.06627v1' target='_blank'>Multi-agent Motion Planning for Dense and Dynamic Environments via Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Samaneh Hosseini Semnani, Hugh Liu, Michael Everett, Anton de Ruiter, Jonathan P. How</p>
<p><strong>Summary:</strong> This paper introduces a hybrid algorithm of deep reinforcement learning (RL)
and Force-based motion planning (FMP) to solve distributed motion planning
problem in dense and dynamic environments. Individually, RL and FMP algorithms
each have their own limitations. FMP is not able to produce time-optimal paths
and existing RL solutions are not able to produce collision-free paths in dense
environments. Therefore, we first tried improving the performance of recent RL
approaches by introducing a new...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.06061v2' target='_blank'>TOMA: Topological Map Abstraction for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhao-Heng Yin, Wu-Jun Li</p>
<p><strong>Summary:</strong> Animals are able to discover the topological map (graph) of surrounding
environment, which will be used for navigation. Inspired by this biological
phenomenon, researchers have recently proposed to generate graph representation
for Markov decision process (MDP) and use such graphs for planning in
reinforcement learning (RL). However, existing graph generation methods suffer
from many drawbacks. One drawback is that existing methods do not learn an
abstraction for graphs, which results in high me...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.08501v1' target='_blank'>DOP: Deep Optimistic Planning with Approximate Value Function Evaluation</a></h2>
<p><strong>Authors:</strong> Francesco Riccio, Roberto Capobianco, Daniele Nardi</p>
<p><strong>Summary:</strong> Research on reinforcement learning has demonstrated promising results in
manifold applications and domains. Still, efficiently learning effective robot
behaviors is very difficult, due to unstructured scenarios, high uncertainties,
and large state dimensionality (e.g. multi-agent systems or hyper-redundant
robots). To alleviate this problem, we present DOP, a deep model-based
reinforcement learning algorithm, which exploits action values to both (1)
guide the exploration of the state space and (...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.00482v1' target='_blank'>Few-Shot Goal Inference for Visuomotor Learning and Planning</a></h2>
<p><strong>Authors:</strong> Annie Xie, Avi Singh, Sergey Levine, Chelsea Finn</p>
<p><strong>Summary:</strong> Reinforcement learning and planning methods require an objective or reward
function that encodes the desired behavior. Yet, in practice, there is a wide
range of scenarios where an objective is difficult to provide programmatically,
such as tasks with visual observations involving unknown object positions or
deformable objects. In these cases, prior methods use engineered
problem-specific solutions, e.g., by instrumenting the environment with
additional sensors to measure a proxy for the objecti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.12068v2' target='_blank'>Deep Reinforcement Learning with a Stage Incentive Mechanism of Dense
  Reward for Robotic Trajectory Planning</a></h2>
<p><strong>Authors:</strong> Gang Peng, Jin Yang, Xinde Lia, Mohammad Omar Khyam</p>
<p><strong>Summary:</strong> (This work has been submitted to the IEEE for possible publication. Copyright
may be transferred without notice, after which this version may no longer be
accessible.)
  To improve the efficiency of deep reinforcement learning (DRL)-based methods
for robot manipulator trajectory planning in random working environments, we
present three dense reward functions. These rewards differ from the traditional
sparse reward. First, a posture reward function is proposed to speed up the
learning process wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.02608v1' target='_blank'>Learning a Decentralized Multi-arm Motion Planner</a></h2>
<p><strong>Authors:</strong> Huy Ha, Jingxi Xu, Shuran Song</p>
<p><strong>Summary:</strong> We present a closed-loop multi-arm motion planner that is scalable and
flexible with team size. Traditional multi-arm robot systems have relied on
centralized motion planners, whose runtimes often scale exponentially with team
size, and thus, fail to handle dynamic environments with open-loop control. In
this paper, we tackle this problem with multi-agent reinforcement learning,
where a decentralized policy is trained to control one robot arm in the
multi-arm system to reach its target end-effec...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.04752v1' target='_blank'>Trajectory Planning for Autonomous Vehicles Using Hierarchical
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kaleb Ben Naveed, Zhiqian Qiao, John M. Dolan</p>
<p><strong>Summary:</strong> Planning safe trajectories under uncertain and dynamic conditions makes the
autonomous driving problem significantly complex. Current sampling-based
methods such as Rapidly Exploring Random Trees (RRTs) are not ideal for this
problem because of the high computational cost. Supervised learning methods
such as Imitation Learning lack generalization and safety guarantees. To
address these problems and in order to ensure a robust framework, we propose a
Hierarchical Reinforcement Learning (HRL) stru...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.05605v2' target='_blank'>Decentralized Motion Planning for Multi-Robot Navigation using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sivanathan Kandhasamy, Vinayagam Babu Kuppusamy, Tanmay Vilas Samak, Chinmay Vilas Samak</p>
<p><strong>Summary:</strong> This work presents a decentralized motion planning framework for addressing
the task of multi-robot navigation using deep reinforcement learning. A custom
simulator was developed in order to experimentally investigate the navigation
problem of 4 cooperative non-holonomic robots sharing limited state information
with each other in 3 different settings. The notion of decentralized motion
planning with common and shared policy learning was adopted, which allowed
robust training and testing of this ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.13365v1' target='_blank'>Optimization of the Model Predictive Control Update Interval Using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Eivind B√∏hn, Sebastien Gros, Signe Moe, Tor Arne Johansen</p>
<p><strong>Summary:</strong> In control applications there is often a compromise that needs to be made
with regards to the complexity and performance of the controller and the
computational resources that are available. For instance, the typical hardware
platform in embedded control applications is a microcontroller with limited
memory and processing power, and for battery powered applications the control
system can account for a significant portion of the energy consumption. We
propose a controller architecture in which th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.11042v3' target='_blank'>Learning the Subsystem of Local Planning for Autonomous Racing</a></h2>
<p><strong>Authors:</strong> Benjamin Evans, Hendrik W. Jordaan, Herman A. Engelbrecht</p>
<p><strong>Summary:</strong> The problem of autonomous racing is to navigate through a race course as
quickly as possible while not colliding with any obstacles. We approach the
autonomous racing problem with the added constraint of not maintaining an
updated obstacle map of the environment. Several current approaches to this
problem use end-to-end learning systems where an agent replaces the entire
navigation pipeline. This paper presents a hierarchical planning architecture
that combines a high level planner and path foll...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.09858v1' target='_blank'>Successor Feature Landmarks for Long-Horizon Goal-Conditioned
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Christopher Hoang, Sungryull Sohn, Jongwook Choi, Wilka Carvalho, Honglak Lee</p>
<p><strong>Summary:</strong> Operating in the real-world often requires agents to learn about a complex
environment and apply this understanding to achieve a breadth of goals. This
problem, known as goal-conditioned reinforcement learning (GCRL), becomes
especially challenging for long-horizon goals. Current methods have tackled
this problem by augmenting goal-conditioned policies with graph-based planning
algorithms. However, they struggle to scale to large, high-dimensional state
spaces and assume access to exploration me...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.08129v2' target='_blank'>Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in
  Latent Space</a></h2>
<p><strong>Authors:</strong> Kuan Fang, Patrick Yin, Ashvin Nair, Sergey Levine</p>
<p><strong>Summary:</strong> General-purpose robots require diverse repertoires of behaviors to complete
challenging tasks in real-world unstructured environments. To address this
issue, goal-conditioned reinforcement learning aims to acquire policies that
can reach configurable goals for a wide range of tasks on command. However,
such goal-conditioned policies are notoriously difficult and time-consuming to
train from scratch. In this paper, we propose Planning to Practice (PTP), a
method that makes it practical to train g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.01364v1' target='_blank'>Robotic Planning under Uncertainty in Spatiotemporal Environments in
  Expeditionary Science</a></h2>
<p><strong>Authors:</strong> Victoria Preston, Genevieve Flaspohler, Anna P. M. Michel, John W. Fisher III, Nicholas Roy</p>
<p><strong>Summary:</strong> In the expeditionary sciences, spatiotemporally varying environments --
hydrothermal plumes, algal blooms, lava flows, or animal migrations -- are
ubiquitous. Mobile robots are uniquely well-suited to study these dynamic,
mesoscale natural environments. We formalize expeditionary science as a
sequential decision-making problem, modeled using the language of
partially-observable Markov decision processes (POMDPs). Solving the
expeditionary science POMDP under real-world constraints requires effic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.00808v2' target='_blank'>A Maintenance Planning Framework using Online and Offline Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zaharah A. Bukhsh, Nils Jansen, Hajo Molegraaf</p>
<p><strong>Summary:</strong> Cost-effective asset management is an area of interest across several
industries. Specifically, this paper develops a deep reinforcement learning
(DRL) solution to automatically determine an optimal rehabilitation policy for
continuously deteriorating water pipes. We approach the problem of
rehabilitation planning in an online and offline DRL setting. In online DRL,
the agent interacts with a simulated environment of multiple pipes with
distinct lengths, materials, and failure rate characteristi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.08211v1' target='_blank'>Path Planning of Cleaning Robot with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Woohyeon Moon, Bumgeun Park, Sarvar Hussain Nengroo, Taeyoung Kim, Dongsoo Har</p>
<p><strong>Summary:</strong> Recently, as the demand for cleaning robots has steadily increased, therefore
household electricity consumption is also increasing. To solve this electricity
consumption issue, the problem of efficient path planning for cleaning robot
has become important and many studies have been conducted. However, most of
them are about moving along a simple path segment, not about the whole path to
clean all places. As the emerging deep learning technique, reinforcement
learning (RL) has been adopted for cl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.05662v1' target='_blank'>Optimal Planning of Hybrid Energy Storage Systems using Curtailed
  Renewable Energy through Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dongju Kang, Doeun Kang, Sumin Hwangbo, Haider Niaz, Won Bo Lee, J. Jay Liu, Jonggeol Na</p>
<p><strong>Summary:</strong> Energy management systems (EMS) are becoming increasingly important in order
to utilize the continuously growing curtailed renewable energy. Promising
energy storage systems (ESS), such as batteries and green hydrogen should be
employed to maximize the efficiency of energy stakeholders. However, optimal
decision-making, i.e., planning the leveraging between different strategies, is
confronted with the complexity and uncertainties of large-scale problems. Here,
we propose a sophisticated deep rei...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.13396v1' target='_blank'>Bayesian Optimization Enhanced Deep Reinforcement Learning for
  Trajectory Planning and Network Formation in Multi-UAV Networks</a></h2>
<p><strong>Authors:</strong> Shimin Gong, Meng Wang, Bo Gu, Wenjie Zhang, Dinh Thai Hoang, Dusit Niyato</p>
<p><strong>Summary:</strong> In this paper, we employ multiple UAVs coordinated by a base station (BS) to
help the ground users (GUs) to offload their sensing data. Different UAVs can
adapt their trajectories and network formation to expedite data transmissions
via multi-hop relaying. The trajectory planning aims to collect all GUs' data,
while the UAVs' network formation optimizes the multi-hop UAV network topology
to minimize the energy consumption and transmission delay. The joint network
formation and trajectory optimiz...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.09758v1' target='_blank'>A deep reinforcement learning approach to assess the low-altitude
  airspace capacity for urban air mobility</a></h2>
<p><strong>Authors:</strong> Asal Mehditabrizi, Mahdi Samadzad, Sina Sabzekar</p>
<p><strong>Summary:</strong> Urban air mobility is the new mode of transportation aiming to provide a fast
and secure way of travel by utilizing the low-altitude airspace. This goal
cannot be achieved without the implementation of new flight regulations which
can assure safe and efficient allocation of flight paths to a large number of
vertical takeoff/landing aerial vehicles. Such rules should also allow
estimating the effective capacity of the low-altitude airspace for planning
purposes. Path planning is a vital subject i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.03812v1' target='_blank'>Deep Reinforcement Learning for Beam Angle Optimization of
  Intensity-Modulated Radiation Therapy</a></h2>
<p><strong>Authors:</strong> Peng Bao, Gong Wang, Ruijie Yang, Bin Dong</p>
<p><strong>Summary:</strong> Objective: Intensity-modulated radiation therapy (IMRT) beam angle
optimization (BAO) is a challenging combinatorial optimization problem that is
NP-hard. In this study, we aim to develop a personalized BAO algorithm for IMRT
that improves the quality of the final treatment. Methods: To improve the
quality of IMRT treatment planning, we propose a deep reinforcement learning
(DRL)-based approach for IMRT BAO. We consider the task as a sequential
decision-making problem and formulate it as a Marko...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.05351v2' target='_blank'>Intent-based Deep Reinforcement Learning for Multi-agent Informative
  Path Planning</a></h2>
<p><strong>Authors:</strong> Tianze Yang, Yuhong Cao, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> In multi-agent informative path planning (MAIPP), agents must collectively
construct a global belief map of an underlying distribution of interest (e.g.,
gas concentration, light intensity, or pollution levels) over a given domain,
based on measurements taken along their trajectory. They must frequently replan
their path to balance the exploration of new areas with the exploitation of
known high-interest areas, to maximize information gain within a predefined
budget. Traditional approaches rely ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.00225v1' target='_blank'>Adaptive formation motion planning and control of autonomous underwater
  vehicles using deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Behnaz Hadi, Alireza Khosravi, Pouria Sarhadi</p>
<p><strong>Summary:</strong> Creating safe paths in unknown and uncertain environments is a challenging
aspect of leader-follower formation control. In this architecture, the leader
moves toward the target by taking optimal actions, and followers should also
avoid obstacles while maintaining their desired formation shape. Most of the
studies in this field have inspected formation control and obstacle avoidance
separately. The present study proposes a new approach based on deep
reinforcement learning (DRL) for end-to-end mot...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.08359v1' target='_blank'>Hierarchical Task Network Planning for Facilitating Cooperative
  Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xuechen Mu, Hankz Hankui Zhuo, Chen Chen, Kai Zhang, Chao Yu, Jianye Hao</p>
<p><strong>Summary:</strong> Exploring sparse reward multi-agent reinforcement learning (MARL)
environments with traps in a collaborative manner is a complex task. Agents
typically fail to reach the goal state and fall into traps, which affects the
overall performance of the system. To overcome this issue, we present SOMARL, a
framework that uses prior knowledge to reduce the exploration space and assist
learning. In SOMARL, agents are treated as part of the MARL environment, and
symbolic knowledge is embedded using a tree ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.16769v3' target='_blank'>2-Level Reinforcement Learning for Ships on Inland Waterways: Path
  Planning and Following</a></h2>
<p><strong>Authors:</strong> Martin Waltz, Niklas Paulig, Ostap Okhrin</p>
<p><strong>Summary:</strong> This paper proposes a realistic modularized framework for controlling
autonomous surface vehicles (ASVs) on inland waterways (IWs) based on deep
reinforcement learning (DRL). The framework improves operational safety and
comprises two levels: a high-level local path planning (LPP) unit and a
low-level path following (PF) unit, each consisting of a DRL agent. The LPP
agent is responsible for planning a path under consideration of dynamic
vessels, closing a gap in the current research landscape. I...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.12052v1' target='_blank'>Optimizing V2V Unicast Communication Transmission with Reinforcement
  Learning and Vehicle Clustering</a></h2>
<p><strong>Authors:</strong> Yu Wang</p>
<p><strong>Summary:</strong> Efficient routing algorithms based on vehicular ad hoc networks (VANETs) play
an important role in emerging intelligent transportation systems. This highly
dynamic topology faces a number of wireless communication service challenges.
In this paper, we propose a protocol based on reinforcement learning and
vehicle node clustering, the protocol is called Qucts, solve
vehicle-to-fixed-destination or V2V messaging problems. Improve message
delivery rates with minimal hops and latency, link stability...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.11095v1' target='_blank'>PyroTrack: Belief-Based Deep Reinforcement Learning Path Planning for
  Aerial Wildfire Monitoring in Partially Observable Environments</a></h2>
<p><strong>Authors:</strong> Sahand Khoshdel, Qi Luo, Fatemeh Afghah</p>
<p><strong>Summary:</strong> Motivated by agility, 3D mobility, and low-risk operation compared to
human-operated management systems of autonomous unmanned aerial vehicles
(UAVs), this work studies UAV-based active wildfire monitoring where a UAV
detects fire incidents in remote areas and tracks the fire frontline. A UAV
path planning solution is proposed considering realistic wildfire management
missions, where a single low-altitude drone with limited power and flight time
is available. Noting the limited field of view of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.04920v2' target='_blank'>Sim-to-Real Transfer of Deep Reinforcement Learning Agents for Online
  Coverage Path Planning</a></h2>
<p><strong>Authors:</strong> Arvi Jonnarth, Ola Johansson, Michael Felsberg</p>
<p><strong>Summary:</strong> Sim-to-real transfer presents a difficult challenge, where models trained in
simulation are to be deployed in the real world. The distribution shift between
the two settings leads to biased representations of the dynamics, and thus to
suboptimal predictions in the real-world environment. In this work, we tackle
the challenge of sim-to-real transfer of reinforcement learning (RL) agents for
coverage path planning (CPP). In CPP, the task is for a robot to find a path
that covers every point of a c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.16142v1' target='_blank'>Diffusion Models as Optimizers for Efficient Planning in Offline RL</a></h2>
<p><strong>Authors:</strong> Renming Huang, Yunqiang Pei, Guoqing Wang, Yangming Zhang, Yang Yang, Peng Wang, Hengtao Shen</p>
<p><strong>Summary:</strong> Diffusion models have shown strong competitiveness in offline reinforcement
learning tasks by formulating decision-making as sequential generation.
However, the practicality of these methods is limited due to the lengthy
inference processes they require. In this paper, we address this problem by
decomposing the sampling process of diffusion models into two decoupled
subprocesses: 1) generating a feasible trajectory, which is a time-consuming
process, and 2) optimizing the trajectory. With this d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.11816v2' target='_blank'>Efficient Exploration and Discriminative World Model Learning with an
  Object-Centric Abstraction</a></h2>
<p><strong>Authors:</strong> Anthony GX-Chen, Kenneth Marino, Rob Fergus</p>
<p><strong>Summary:</strong> In the face of difficult exploration problems in reinforcement learning, we
study whether giving an agent an object-centric mapping (describing a set of
items and their attributes) allow for more efficient learning. We found this
problem is best solved hierarchically by modelling items at a higher level of
state abstraction to pixels, and attribute change at a higher level of temporal
abstraction to primitive actions. This abstraction simplifies the transition
dynamic by making specific future s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.19226v1' target='_blank'>Learning to Bridge the Gap: Efficient Novelty Recovery with Planning and
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alicia Li, Nishanth Kumar, Tom√°s Lozano-P√©rez, Leslie Kaelbling</p>
<p><strong>Summary:</strong> The real world is unpredictable. Therefore, to solve long-horizon
decision-making problems with autonomous robots, we must construct agents that
are capable of adapting to changes in the environment during deployment.
Model-based planning approaches can enable robots to solve complex,
long-horizon tasks in a variety of environments. However, such approaches tend
to be brittle when deployed into an environment featuring a novel situation
that their underlying model does not account for. In this w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.19373v1' target='_blank'>An Enhanced Hierarchical Planning Framework for Multi-Robot Autonomous
  Exploration</a></h2>
<p><strong>Authors:</strong> Gengyuan Cai, Luosong Guo, Xiangmao Chang</p>
<p><strong>Summary:</strong> The autonomous exploration of environments by multi-robot systems is a
critical task with broad applications in rescue missions, exploration
endeavors, and beyond. Current approaches often rely on either greedy frontier
selection or end-to-end deep reinforcement learning (DRL) methods, yet these
methods are frequently hampered by limitations such as short-sightedness,
overlooking long-term implications, and convergence difficulties stemming from
the intricate high-dimensional learning space. To ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.04407v1' target='_blank'>Illuminating Spaces: Deep Reinforcement Learning and Laser-Wall
  Partitioning for Architectural Layout Generation</a></h2>
<p><strong>Authors:</strong> Reza Kakooee, Benjamin Dillenburger</p>
<p><strong>Summary:</strong> Space layout design (SLD), occurring in the early stages of the design
process, nonetheless influences both the functionality and aesthetics of the
ultimate architectural outcome. The complexity of SLD necessitates innovative
approaches to efficiently explore vast solution spaces. While image-based
generative AI has emerged as a potential solution, they often rely on
pixel-based space composition methods that lack intuitive representation of
architectural processes. This paper leverages deep Rei...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.06244v1' target='_blank'>A reinforcement learning application of guided Monte Carlo Tree Search
  algorithm for beam orientation selection in radiation therapy</a></h2>
<p><strong>Authors:</strong> Azar Sadeghnejad-Barkousaraie, Gyanendra Bohara, Steve Jiang, Dan Nguyen</p>
<p><strong>Summary:</strong> Due to the large combinatorial problem, current beam orientation optimization
algorithms for radiotherapy, such as column generation (CG), are typically
heuristic or greedy in nature, leading to suboptimal solutions. We propose a
reinforcement learning strategy using Monte Carlo Tree Search capable of
finding a superior beam orientation set and in less time than CG.We utilized a
reinforcement learning structure involving a supervised learning network to
guide Monte Carlo tree search (GTS) to exp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/cs/0604010v2' target='_blank'>Nearly optimal exploration-exploitation decision thresholds</a></h2>
<p><strong>Authors:</strong> Christos Dimitrakakis</p>
<p><strong>Summary:</strong> While in general trading off exploration and exploitation in reinforcement
learning is hard, under some formulations relatively simple solutions exist. In
this paper, we first derive upper bounds for the utility of selecting different
actions in the multi-armed bandit setting. Unlike the common statistical upper
confidence bounds, these explicitly link the planning horizon, uncertainty and
the need for exploration explicit. The resulting algorithm can be seen as a
generalisation of the classical...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1205.3109v4' target='_blank'>Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based
  Search</a></h2>
<p><strong>Authors:</strong> Arthur Guez, David Silver, Peter Dayan</p>
<p><strong>Summary:</strong> Bayesian model-based reinforcement learning is a formally elegant approach to
learning optimal behaviour under model uncertainty, trading off exploration and
exploitation in an ideal way. Unfortunately, finding the resulting
Bayes-optimal policies is notoriously taxing, since the search space becomes
enormous. In this paper we introduce a tractable, sample-based method for
approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our
approach outperformed prior Bayesian model-ba...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1303.3183v2' target='_blank'>Toggling a Genetic Switch Using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Aivar Sootla, Natalja Strelkowa, Damien Ernst, Mauricio Barahona, Guy-Bart Stan</p>
<p><strong>Summary:</strong> In this paper, we consider the problem of optimal exogenous control of gene
regulatory networks. Our approach consists in adapting an established
reinforcement learning algorithm called the fitted Q iteration. This algorithm
infers the control law directly from the measurements of the system's response
to external control inputs without the use of a mathematical model of the
system. The measurement data set can either be collected from wet-lab
experiments or artificially created by computer simu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1306.6189v1' target='_blank'>Scaling Up Robust MDPs by Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Aviv Tamar, Huan Xu, Shie Mannor</p>
<p><strong>Summary:</strong> We consider large-scale Markov decision processes (MDPs) with parameter
uncertainty, under the robust MDP paradigm. Previous studies showed that robust
MDPs, based on a minimax approach to handle uncertainty, can be solved using
dynamic programming for small to medium sized problems. However, due to the
"curse of dimensionality", MDPs that model real-life problems are typically
prohibitively large for such approaches. In this work we employ a reinforcement
learning approach to tackle this planni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1602.07764v2' target='_blank'>Reinforcement Learning of POMDPs using Spectral Methods</a></h2>
<p><strong>Authors:</strong> Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar</p>
<p><strong>Summary:</strong> We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through episodes, in each epis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1603.03267v1' target='_blank'>Hierarchical Linearly-Solvable Markov Decision Problems</a></h2>
<p><strong>Authors:</strong> Anders Jonsson, Vicen√ß G√≥mez</p>
<p><strong>Summary:</strong> We present a hierarchical reinforcement learning framework that formulates
each task in the hierarchy as a special type of Markov decision process for
which the Bellman equation is linear and has analytical solution. Problems of
this type, called linearly-solvable MDPs (LMDPs) have interesting properties
that can be exploited in a hierarchical setting, such as efficient learning of
the optimal value function or task compositionality. The proposed hierarchical
approach can also be seen as a novel...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.08862v2' target='_blank'>Socially Aware Motion Planning with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yu Fan Chen, Michael Everett, Miao Liu, Jonathan P. How</p>
<p><strong>Summary:</strong> For robotic vehicles to navigate safely and efficiently in pedestrian-rich
environments, it is important to model subtle human behaviors and navigation
rules (e.g., passing on the right). However, while instinctive to humans,
socially compliant navigation is still difficult to quantify due to the
stochasticity in people's behaviors. Existing works are mostly focused on using
feature-matching techniques to describe and imitate human paths, but often do
not generalize well since the feature values...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1705.02553v1' target='_blank'>Experimental results : Reinforcement Learning of POMDPs using Spectral
  Methods</a></h2>
<p><strong>Authors:</strong> Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar</p>
<p><strong>Summary:</strong> We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through epochs, in each epoch ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1706.06491v2' target='_blank'>Data-Efficient Reinforcement Learning with Probabilistic Model
  Predictive Control</a></h2>
<p><strong>Authors:</strong> Sanket Kamthe, Marc Peter Deisenroth</p>
<p><strong>Summary:</strong> Trial-and-error based reinforcement learning (RL) has seen rapid advancements
in recent times, especially with the advent of deep neural networks. However,
the majority of autonomous RL algorithms require a large number of interactions
with the environment. A large number of interactions may be impractical in many
real-world applications, such as robotics, and many practical systems have to
obey limitations in the form of state space or control constraints. To reduce
the number of system interac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1708.00133v2' target='_blank'>Grounding Language for Transfer in Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola</p>
<p><strong>Summary:</strong> In this paper, we explore the utilization of natural language to drive
transfer for reinforcement learning (RL). Despite the wide-spread application
of deep RL techniques, learning generalized policy representations that work
across domains remains a challenging problem. We demonstrate that textual
descriptions of environments provide a compact intermediate channel to
facilitate effective policy transfer. Specifically, by learning to ground the
meaning of text to the dynamics of the environment ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1708.00463v1' target='_blank'>Hierarchical Subtask Discovery With Non-Negative Matrix Factorization</a></h2>
<p><strong>Authors:</strong> Adam C. Earle, Andrew M. Saxe, Benjamin Rosman</p>
<p><strong>Summary:</strong> Hierarchical reinforcement learning methods offer a powerful means of
planning flexible behavior in complicated domains. However, learning an
appropriate hierarchical decomposition of a domain into subtasks remains a
substantial challenge. We present a novel algorithm for subtask discovery,
based on the recently introduced multitask linearly-solvable Markov decision
process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by
representing them as a linear combination of a previous...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1710.03592v2' target='_blank'>Meta Inverse Reinforcement Learning via Maximum Reward Sharing for Human
  Motion Analysis</a></h2>
<p><strong>Authors:</strong> Kun Li, Joel W. Burdick</p>
<p><strong>Summary:</strong> This work handles the inverse reinforcement learning (IRL) problem where only
a small number of demonstrations are available from a demonstrator for each
high-dimensional task, insufficient to estimate an accurate reward function.
Observing that each demonstrator has an inherent reward for each state and the
task-specific behaviors mainly depend on a small number of key states, we
propose a meta IRL algorithm that first models the reward function for each
task as a distribution conditioned on a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1710.11089v3' target='_blank'>Eigenoption Discovery through the Deep Successor Representation</a></h2>
<p><strong>Authors:</strong> Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro, Murray Campbell</p>
<p><strong>Summary:</strong> Options in reinforcement learning allow agents to hierarchically decompose a
task into subtasks, having the potential to speed up learning and planning.
However, autonomously learning effective sets of options is still a major
challenge in the field. In this paper we focus on the recently introduced idea
of using representation learning methods to guide the option discovery process.
Specifically, we look at eigenoptions, options obtained from representations
that encode diffusive information flo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1711.06006v3' target='_blank'>Hindsight policy gradients</a></h2>
<p><strong>Authors:</strong> Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, Juergen Schmidhuber</p>
<p><strong>Summary:</strong> A reinforcement learning agent that needs to pursue different goals across
episodes requires a goal-conditional policy. In addition to their potential to
generalize desirable behavior to unseen goals, such policies may also enable
higher-level planning based on subgoals. In sparse-reward environments, the
capacity to exploit information about the degree to which an arbitrary goal has
been achieved while another goal was intended appears crucial to enable sample
efficient learning. However, reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1802.03006v1' target='_blank'>Learning and Querying Fast Generative Models for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Lars Buesing, Theophane Weber, Sebastien Racaniere, S. M. Ali Eslami, Danilo Rezende, David P. Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, Daan Wierstra</p>
<p><strong>Summary:</strong> A key challenge in model-based reinforcement learning (RL) is to synthesize
computationally efficient and accurate environment models. We show that
carefully designed generative models that learn and operate on compact state
representations, so-called state-space models, substantially reduce the
computational costs for predicting outcomes of sequences of actions. Extensive
experiments establish that state-space models accurately capture the dynamics
of Atari games from the Arcade Learning Enviro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.02698v1' target='_blank'>Hierarchical Modular Reinforcement Learning Method and Knowledge
  Acquisition of State-Action Rule for Multi-target Problem</a></h2>
<p><strong>Authors:</strong> Takumi Ichimura, Daisuke Igaue</p>
<p><strong>Summary:</strong> Hierarchical Modular Reinforcement Learning (HMRL), consists of 2 layered
learning where Profit Sharing works to plan a prey position in the higher layer
and Q-learning method trains the state-actions to the target in the lower
layer. In this paper, we expanded HMRL to multi-target problem to take the
distance between targets to the consideration. The function, called `AT field',
can estimate the interests for an agent according to the distance between 2
agents and the advantage/disadvantage of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.01956v1' target='_blank'>Motion Planning Among Dynamic, Decision-Making Agents with Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Michael Everett, Yu Fan Chen, Jonathan P. How</p>
<p><strong>Summary:</strong> Robots that navigate among pedestrians use collision avoidance algorithms to
enable safe and efficient operation. Recent works present deep reinforcement
learning as a framework to model the complex interactions and cooperation.
However, they are implemented using key assumptions about other agents'
behavior that deviate from reality as the number of agents in the environment
increases. This work extends our previous approach to develop an algorithm that
learns collision avoidance among a variet...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.04752v1' target='_blank'>Generating Rescheduling Knowledge using Reinforcement Learning in a
  Cognitive Architecture</a></h2>
<p><strong>Authors:</strong> Jorge A. Palombarini, Juan Cruz Barsce, Ernesto C. Mart√≠nez</p>
<p><strong>Summary:</strong> In order to reach higher degrees of flexibility, adaptability and autonomy in
manufacturing systems, it is essential to develop new rescheduling
methodologies which resort to cognitive capabilities, similar to those found in
human beings. Artificial cognition is important for designing planning and
control systems that generate and represent knowledge about heuristics for
repair-based scheduling. Rescheduling knowledge in the form of decision rules
is used to deal with unforeseen events and dist...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.06150v1' target='_blank'>FollowNet: Robot Navigation by Following Natural Language Directions
  with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Pararth Shah, Marek Fiser, Aleksandra Faust, J. Chase Kew, Dilek Hakkani-Tur</p>
<p><strong>Summary:</strong> Understanding and following directions provided by humans can enable robots
to navigate effectively in unknown situations. We present FollowNet, an
end-to-end differentiable neural architecture for learning multi-modal
navigation policies. FollowNet maps natural language instructions as well as
visual and depth inputs to locomotion primitives. FollowNet processes
instructions using an attention mechanism conditioned on its visual and depth
input to focus on the relevant parts of the command whil...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.07708v1' target='_blank'>A Lyapunov-based Approach to Safe Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, Mohammad Ghavamzadeh</p>
<p><strong>Summary:</strong> In many real-world reinforcement learning (RL) problems, besides optimizing
the main objective function, an agent must concurrently avoid violating a
number of constraints. In particular, besides optimizing performance it is
crucial to guarantee the safety of an agent during training as well as
deployment (e.g. a robot should avoid taking actions - exploratory or not -
which irrevocably harm its hardware). To incorporate safety in RL, we derive
algorithms under the framework of constrained Marko...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.01368v1' target='_blank'>Adversarial Reinforcement Learning Framework for Benchmarking Collision
  Avoidance Mechanisms in Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Vahid Behzadan, Arslan Munir</p>
<p><strong>Summary:</strong> With the rapidly growing interest in autonomous navigation, the body of
research on motion planning and collision avoidance techniques has enjoyed an
accelerating rate of novel proposals and developments. However, the complexity
of new techniques and their safety requirements render the bulk of current
benchmarking frameworks inadequate, thus leaving the need for efficient
comparison techniques unanswered. This work proposes a novel framework based on
deep reinforcement learning for benchmarking...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.01830v2' target='_blank'>Relational Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, Peter Battaglia</p>
<p><strong>Summary:</strong> We introduce an approach for deep reinforcement learning (RL) that improves
upon the efficiency, generalization capacity, and interpretability of
conventional approaches through structured perception and relational reasoning.
It uses self-attention to iteratively reason about the relations between
entities in a scene and to guide a model-free policy. Our results show that in
a novel navigation and planning task called Box-World, our agent finds
interpretable solutions that improve upon baselines...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.08479v1' target='_blank'>Human-Interactive Subgoal Supervision for Efficient Inverse
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xinlei Pan, Eshed Ohn-Bar, Nicholas Rhinehart, Yan Xu, Yilin Shen, Kris M. Kitani</p>
<p><strong>Summary:</strong> Humans are able to understand and perform complex tasks by strategically
structuring the tasks into incremental steps or subgoals. For a robot
attempting to learn to perform a sequential task with critical subgoal states,
such states can provide a natural opportunity for interaction with a human
expert. This paper analyzes the benefit of incorporating a notion of subgoals
into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL)
framework. The learning process is interactive, wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1807.06096v2' target='_blank'>Safe Reinforcement Learning via Probabilistic Shields</a></h2>
<p><strong>Authors:</strong> Nils Jansen, Bettina K√∂nighofer, Sebastian Junges, Alexandru C. Serban, Roderick Bloem</p>
<p><strong>Summary:</strong> This paper targets the efficient construction of a safety shield for decision
making in scenarios that incorporate uncertainty. Markov decision processes
(MDPs) are prominent models to capture such planning problems. Reinforcement
learning (RL) is a machine learning technique to determine near-optimal
policies in MDPs that may be unknown prior to exploring the model. However,
during exploration, RL is prone to induce behavior that is undesirable or not
allowed in safety- or mission-critical cont...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1808.03196v2' target='_blank'>Learning to Optimize Join Queries With Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, Ion Stoica</p>
<p><strong>Summary:</strong> Exhaustive enumeration of all possible join orders is often avoided, and most
optimizers leverage heuristics to prune the search space. The design and
implementation of heuristics are well-understood when the cost model is roughly
linear, and we find that these heuristics can be significantly suboptimal when
there are non-linearities in cost. Ideally, instead of a fixed heuristic, we
would want a strategy to guide the search space in a more data-driven
way---tailoring the search to a specific da...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.04506v2' target='_blank'>Combined Reinforcement Learning via Abstract Representations</a></h2>
<p><strong>Authors:</strong> Vincent Fran√ßois-Lavet, Yoshua Bengio, Doina Precup, Joelle Pineau</p>
<p><strong>Summary:</strong> In the quest for efficient and robust reinforcement learning methods, both
model-free and model-based approaches offer advantages. In this paper we
propose a new way of explicitly bridging both approaches via a shared
low-dimensional learned encoding of the environment, meant to capture
summarizing abstractions. We show that the modularity brought by this approach
leads to good generalization while being computationally efficient, with
planning happening in a smaller latent state space. In addit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.11074v3' target='_blank'>Robot Representation and Reasoning with Knowledge from Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Keting Lu, Shiqi Zhang, Peter Stone, Xiaoping Chen</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) agents aim at learning by interacting with an
environment, and are not designed for representing or reasoning with
declarative knowledge. Knowledge representation and reasoning (KRR) paradigms
are strong in declarative KRR tasks, but are ill-equipped to learn from such
experiences. In this work, we integrate logical-probabilistic KRR with
model-based RL, enabling agents to simultaneously reason with declarative
knowledge and learn from interaction experiences. The kno...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.01855v2' target='_blank'>A* Tree Search for Portfolio Management</a></h2>
<p><strong>Authors:</strong> Xiaojie Gao, Shikui Tu, Lei Xu</p>
<p><strong>Summary:</strong> We propose a planning-based method to teach an agent to manage portfolio from
scratch. Our approach combines deep reinforcement learning techniques with
search techniques like AlphaGo. By uniting the advantages in A* search
algorithm with Monte Carlo tree search, we come up with a new algorithm named
A* tree search in which best information is returned to guide next search.
Also, the expansion mode of Monte Carlo tree is improved for a higher
utilization of the neural network. The suggested algo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.05195v1' target='_blank'>GridSim: A Vehicle Kinematics Engine for Deep Neuroevolutionary Control
  in Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Bogdan Trasnea, Andrei Vasilcoi, Claudiu Pozna, Sorin Grigorescu</p>
<p><strong>Summary:</strong> Current state of the art solutions in the control of an autonomous vehicle
mainly use supervised end-to-end learning, or decoupled perception, planning
and action pipelines. Another possible solution is deep reinforcement learning,
but such a method requires that the agent interacts with its surroundings in a
simulated environment. In this paper we introduce GridSim, which is an
autonomous driving simulator engine running a car-like robot architecture to
generate occupancy grids from simulated s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.04118v1' target='_blank'>WiseMove: A Framework for Safe Deep Reinforcement Learning for
  Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Jaeyoung Lee, Aravind Balakrishnan, Ashish Gaurav, Krzysztof Czarnecki, Sean Sedwards</p>
<p><strong>Summary:</strong> Machine learning can provide efficient solutions to the complex problems
encountered in autonomous driving, but ensuring their safety remains a
challenge. A number of authors have attempted to address this issue, but there
are few publicly-available tools to adequately explore the trade-offs between
functionality, scalability, and safety.
  We thus present WiseMove, a software framework to investigate safe deep
reinforcement learning in the context of motion planning for autonomous
driving. Wise...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.05644v2' target='_blank'>Active Perception in Adversarial Scenarios using Maximum Entropy Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Macheng Shen, Jonathan P How</p>
<p><strong>Summary:</strong> We pose an active perception problem where an autonomous agent actively
interacts with a second agent with potentially adversarial behaviors. Given the
uncertainty in the intent of the other agent, the objective is to collect
further evidence to help discriminate potential threats. The main technical
challenges are the partial observability of the agent intent, the adversary
modeling, and the corresponding uncertainty modeling. Note that an adversary
agent may act to mislead the autonomous agent...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.10140v2' target='_blank'>Planning in Hierarchical Reinforcement Learning: Guarantees for Using
  Local Policies</a></h2>
<p><strong>Authors:</strong> Tom Zahavy, Avinatan Hasidim, Haim Kaplan, Yishay Mansour</p>
<p><strong>Summary:</strong> We consider a settings of hierarchical reinforcement learning, in which the
reward is a sum of components. For each component we are given a policy that
maximizes it and our goal is to assemble a policy from the individual policies
that maximizes the sum of the components. We provide theoretical guarantees for
assembling such policies in deterministic MDPs with collectible rewards. Our
approach builds on formulating this problem as a traveling salesman problem
with discounted reward. We focus on...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.05926v4' target='_blank'>Reinforcement Learning with Dynamic Boltzmann Softmax Updates</a></h2>
<p><strong>Authors:</strong> Ling Pan, Qingpeng Cai, Qi Meng, Wei Chen, Longbo Huang, Tie-Yan Liu</p>
<p><strong>Summary:</strong> Value function estimation is an important task in reinforcement learning,
i.e., prediction. The Boltzmann softmax operator is a natural value estimator
and can provide several benefits. However, it does not satisfy the
non-expansion property, and its direct use may fail to converge even in value
iteration. In this paper, we propose to update the value function with dynamic
Boltzmann softmax (DBS) operator, which has good convergence property in the
setting of planning and learning. Experimental ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.05243v1' target='_blank'>When to use parametric models in reinforcement learning?</a></h2>
<p><strong>Authors:</strong> Hado van Hasselt, Matteo Hessel, John Aslanides</p>
<p><strong>Summary:</strong> We examine the question of when and how parametric models are most useful in
reinforcement learning. In particular, we look at commonalities and differences
between parametric models and experience replay. Replay-based learning
algorithms share important traits with model-based approaches, including the
ability to plan: to use more computation without additional data to improve
predictions and behaviour. We discuss when to expect benefits from either
approach, and interpret prior work in this co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.06588v1' target='_blank'>Reinforcement Learning with Non-uniform State Representations for
  Adaptive Search</a></h2>
<p><strong>Authors:</strong> Sandeep Manjanna, Herke van Hoof, Gregory Dudek</p>
<p><strong>Summary:</strong> Efficient spatial exploration is a key aspect of search and rescue. In this
paper, we present a search algorithm that generates efficient trajectories that
optimize the rate at which probability mass is covered by a searcher. This
should allow an autonomous vehicle find one or more lost targets as rapidly as
possible. We do this by performing non-uniform sampling of the search region.
The path generated minimizes the expected time to locate the missing target by
visiting high probability regions...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.11392v2' target='_blank'>From self-tuning regulators to reinforcement learning and back again</a></h2>
<p><strong>Authors:</strong> Nikolai Matni, Alexandre Proutiere, Anders Rantzer, Stephen Tu</p>
<p><strong>Summary:</strong> Machine and reinforcement learning (RL) are increasingly being applied to
plan and control the behavior of autonomous systems interacting with the
physical world. Examples include self-driving vehicles, distributed sensor
networks, and agile robots. However, when machine learning is to be applied in
these new settings, the algorithms had better come with the same type of
reliability, robustness, and safety bounds that are hallmarks of control
theory, or failures could be catastrophic. Thus, as l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1908.00177v1' target='_blank'>Learning When to Drive in Intersections by Combining Reinforcement
  Learning and Model Predictive Control</a></h2>
<p><strong>Authors:</strong> Tommy Tram, Ivo Batkovic, Mohammad Ali, Jonas Sj√∂berg</p>
<p><strong>Summary:</strong> In this paper, we propose a decision making algorithm intended for automated
vehicles that negotiate with other possibly non-automated vehicles in
intersections. The decision algorithm is separated into two parts: a high-level
decision module based on reinforcement learning, and a low-level planning
module based on model predictive control. Traffic is simulated with numerous
predefined driver behaviors and intentions, and the performance of the proposed
decision algorithm was evaluated against a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1908.05546v1' target='_blank'>Sample-efficient Deep Reinforcement Learning with Imaginary Rollouts for
  Human-Robot Interaction</a></h2>
<p><strong>Authors:</strong> Mohammad Thabet, Massimiliano Patacchiola, Angelo Cangelosi</p>
<p><strong>Summary:</strong> Deep reinforcement learning has proven to be a great success in allowing
agents to learn complex tasks. However, its application to actual robots can be
prohibitively expensive. Furthermore, the unpredictability of human behavior in
human-robot interaction tasks can hinder convergence to a good policy. In this
paper, we present an architecture that allows agents to learn models of
stochastic environments and use them to accelerate learning. We descirbe how an
environment model can be learned onl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.07299v2' target='_blank'>Control Synthesis from Linear Temporal Logic Specifications using
  Model-Free Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alper Kamil Bozkurt, Yu Wang, Michael M. Zavlanos, Miroslav Pajic</p>
<p><strong>Summary:</strong> We present a reinforcement learning (RL) framework to synthesize a control
policy from a given linear temporal logic (LTL) specification in an unknown
stochastic environment that can be modeled as a Markov Decision Process (MDP).
Specifically, we learn a policy that maximizes the probability of satisfying
the LTL formula without learning the transition probabilities. We introduce a
novel rewarding and path-dependent discounting mechanism based on the LTL
formula such that (i) an optimal policy m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.09705v1' target='_blank'>A Layered Architecture for Active Perception: Image Classification using
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hossein K. Mousavi, Guangyi Liu, Weihang Yuan, Martin Tak√°ƒç, H√©ctor Mu√±oz-Avila, Nader Motee</p>
<p><strong>Summary:</strong> We propose a planning and perception mechanism for a robot (agent), that can
only observe the underlying environment partially, in order to solve an image
classification problem. A three-layer architecture is suggested that consists
of a meta-layer that decides the intermediate goals, an action-layer that
selects local actions as the agent navigates towards a goal, and a
classification-layer that evaluates the reward and makes a prediction. We
design and implement these layers using deep reinfor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.12324v1' target='_blank'>Learning Generalizable Locomotion Skills with Hierarchical Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Tianyu Li, Nathan Lambert, Roberto Calandra, Franziska Meier, Akshara Rai</p>
<p><strong>Summary:</strong> Learning to locomote to arbitrary goals on hardware remains a challenging
problem for reinforcement learning. In this paper, we present a hierarchical
learning framework that improves sample-efficiency and generalizability of
locomotion skills on real-world robots. Our approach divides the problem of
goal-oriented locomotion into two sub-problems: learning diverse primitives
skills, and using model-based planning to sequence these skills. We parametrize
our primitives as cyclic movements, improv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.12465v1' target='_blank'>Playing Atari Ball Games with Hierarchical Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hua Huang, Adrian Barbu</p>
<p><strong>Summary:</strong> Human beings are particularly good at reasoning and inference from just a few
examples. When facing new tasks, humans will leverage knowledge and skills
learned before, and quickly integrate them with the new task. In addition to
learning by experimentation, human also learn socio-culturally through
instructions and learning by example. In this way humans can learn much faster
compared with most current artificial intelligence algorithms in many tasks. In
this paper, we test the idea of speeding...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.14002v1' target='_blank'>A Distributed Model-Free Algorithm for Multi-hop Ride-sharing using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ashutosh Singh, Abubakr Alabbasi, Vaneet Aggarwal</p>
<p><strong>Summary:</strong> The growth of autonomous vehicles, ridesharing systems, and self driving
technology will bring a shift in the way ride hailing platforms plan out their
services. However, these advances in technology coupled with road congestion,
environmental concerns, fuel usage, vehicles emissions, and the high cost of
the vehicle usage have brought more attention to better utilize the use of
vehicles and their capacities. In this paper, we propose a novel multi-hop
ride-sharing (MHRS) algorithm that uses dee...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.03074v2' target='_blank'>Mapless Navigation among Dynamics with Social-safety-awareness: a
  reinforcement learning approach from 2D laser scans</a></h2>
<p><strong>Authors:</strong> Jun Jin, Nhat M. Nguyen, Nazmus Sakib, Daniel Graves, Hengshuai Yao, Martin Jagersand</p>
<p><strong>Summary:</strong> We propose a method to tackle the problem of mapless collision-avoidance
navigation where humans are present using 2D laser scans. Our proposed method
uses ego-safety to measure collision from the robot's perspective while
social-safety to measure the impact of our robot's actions on surrounding
pedestrians. Specifically, the social-safety part predicts the intrusion impact
of our robot's action into the interaction area with surrounding humans. We
train the policy using reinforcement learning o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.11689v1' target='_blank'>Join Query Optimization with Deep Reinforcement Learning Algorithms</a></h2>
<p><strong>Authors:</strong> Jonas Heitz, Kurt Stockinger</p>
<p><strong>Summary:</strong> Join query optimization is a complex task and is central to the performance
of query processing. In fact it belongs to the class of NP-hard problems.
Traditional query optimizers use dynamic programming (DP) methods combined with
a set of rules and restrictions to avoid exhaustive enumeration of all possible
join orders. However, DP methods are very resource intensive. Moreover, given
simplifying assumptions of attribute independence, traditional query optimizers
rely on erroneous cost estimatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.00127v2' target='_blank'>Reinforcement Learning with Goal-Distance Gradient</a></h2>
<p><strong>Authors:</strong> Kai Jiang, XiaoLong Qin</p>
<p><strong>Summary:</strong> Reinforcement learning usually uses the feedback rewards of environmental to
train agents. But the rewards in the actual environment are sparse, and even
some environments will not rewards. Most of the current methods are difficult
to get good performance in sparse reward or non-reward environments. Although
using shaped rewards is effective when solving sparse reward tasks, it is
limited to specific problems and learning is also susceptible to local optima.
We propose a model-free method that d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.02286v2' target='_blank'>EgoMap: Projective mapping and structured egocentric memory for Deep RL</a></h2>
<p><strong>Authors:</strong> Edward Beeching, Christian Wolf, Jilles Dibangoye, Olivier Simonin</p>
<p><strong>Summary:</strong> Tasks involving localization, memorization and planning in partially
observable 3D environments are an ongoing challenge in Deep Reinforcement
Learning. We present EgoMap, a spatially structured neural memory architecture.
EgoMap augments a deep reinforcement learning agent's performance in 3D
environments on challenging tasks with multi-step objectives. The EgoMap
architecture incorporates several inductive biases including a differentiable
inverse projection of CNN feature vectors onto a top-d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.02794v1' target='_blank'>Reward-Free Exploration for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Chi Jin, Akshay Krishnamurthy, Max Simchowitz, Tiancheng Yu</p>
<p><strong>Summary:</strong> Exploration is widely regarded as one of the most challenging aspects of
reinforcement learning (RL), with many naive approaches succumbing to
exponential sample complexity. To isolate the challenges of exploration, we
propose a new "reward-free RL" framework. In the exploration phase, the agent
first collects trajectories from an MDP $\mathcal{M}$ without a pre-specified
reward function. After exploration, it is tasked with computing near-optimal
policies under for $\mathcal{M}$ for a collectio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.04109v1' target='_blank'>On Reward Shaping for Mobile Robot Navigation: A Reinforcement Learning
  and SLAM Based Approach</a></h2>
<p><strong>Authors:</strong> Nicol√≤ Botteghi, Beril Sirmacek, Khaled A. A. Mustafa, Mannes Poel, Stefano Stramigioli</p>
<p><strong>Summary:</strong> We present a map-less path planning algorithm based on Deep Reinforcement
Learning (DRL) for mobile robots navigating in unknown environment that only
relies on 40-dimensional raw laser data and odometry information. The planner
is trained using a reward function shaped based on the online knowledge of the
map of the training environment, obtained using grid-based Rao-Blackwellized
particle filter, in an attempt to enhance the obstacle awareness of the agent.
The agent is trained in a complex si...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.04349v1' target='_blank'>Robot Navigation with Map-Based Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Guangda Chen, Lifan Pan, Yu'an Chen, Pei Xu, Zhiqiang Wang, Peichen Wu, Jianmin Ji, Xiaoping Chen</p>
<p><strong>Summary:</strong> This paper proposes an end-to-end deep reinforcement learning approach for
mobile robot navigation with dynamic obstacles avoidance. Using experience
collected in a simulation environment, a convolutional neural network (CNN) is
trained to predict proper steering actions of a robot from its egocentric local
occupancy maps, which accommodate various sensors and fusion algorithms. The
trained neural network is then transferred and executed on a real-world mobile
robot to guide its local path plann...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.12207v1' target='_blank'>Assembly robots with optimized control stiffness through reinforcement
  learning</a></h2>
<p><strong>Authors:</strong> Masahide Oikawa, Kyo Kutsuzawa, Sho Sakaino, Toshiaki Tsuji</p>
<p><strong>Summary:</strong> There is an increased demand for task automation in robots. Contact-rich
tasks, wherein multiple contact transitions occur in a series of operations,
are extensively being studied to realize high accuracy. In this study, we
propose a methodology that uses reinforcement learning (RL) to achieve high
performance in robots for the execution of assembly tasks that require precise
contact with objects without causing damage. The proposed method ensures the
online generation of stiffness matrices that...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.06800v3' target='_blank'>Context-aware Dynamics Model for Generalization in Model-Based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, Jinwoo Shin</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL) enjoys several benefits, such as
data-efficiency and planning, by learning a model of the environment's
dynamics. However, learning a global model that can generalize across different
dynamics is a challenging task. To tackle this problem, we decompose the task
of learning a global dynamics model into two stages: (a) learning a context
latent vector that captures the local dynamics, then (b) predicting the next
state conditioned on it. In order to encode d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.07023v4' target='_blank'>From Simulation to Real World Maneuver Execution using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alessandro Paolo Capasso, Giulio Bacchiani, Alberto Broggi</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning has proved to be able to solve many control tasks
in different fields, but the behavior of these systems is not always as
expected when deployed in real-world scenarios. This is mainly due to the lack
of domain adaptation between simulated and real-world data together with the
absence of distinction between train and test datasets. In this work, we
investigate these problems in the autonomous driving field, especially for a
maneuver planning module for roundabout inse...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.08006v1' target='_blank'>Lifelong Control of Off-grid Microgrid with Model Based Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Simone Totaro, Ioannis Boukas, Anders Jonsson, Bertrand Corn√©lusse</p>
<p><strong>Summary:</strong> The lifelong control problem of an off-grid microgrid is composed of two
tasks, namely estimation of the condition of the microgrid devices and
operational planning accounting for the uncertainties by forecasting the future
consumption and the renewable production. The main challenge for the effective
control arises from the various changes that take place over time. In this
paper, we present an open-source reinforcement framework for the modeling of an
off-grid microgrid for rural electrificati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.10804v3' target='_blank'>Reinforcement Learning with General Value Function Approximation:
  Provably Efficient Approach via Bounded Eluder Dimension</a></h2>
<p><strong>Authors:</strong> Ruosong Wang, Ruslan Salakhutdinov, Lin F. Yang</p>
<p><strong>Summary:</strong> Value function approximation has demonstrated phenomenal empirical success in
reinforcement learning (RL). Nevertheless, despite a handful of recent progress
on developing theory for RL with linear function approximation, the
understanding of general function approximation schemes largely remains
missing. In this paper, we establish a provably efficient RL algorithm with
general value function approximation. We show that if the value functions admit
an approximation with a function class $\mathc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.10524v3' target='_blank'>Reinforcement Learning as Iterative and Amortised Inference</a></h2>
<p><strong>Authors:</strong> Beren Millidge, Alexander Tschantz, Anil K Seth, Christopher L Buckley</p>
<p><strong>Summary:</strong> There are several ways to categorise reinforcement learning (RL) algorithms,
such as either model-based or model-free, policy-based or planning-based,
on-policy or off-policy, and online or offline. Broad classification schemes
such as these help provide a unified perspective on disparate techniques and
can contextualise and guide the development of new algorithms. In this paper,
we utilise the control as inference framework to outline a novel classification
scheme based on amortised and iterati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.02040v1' target='_blank'>Discount Factor as a Regularizer in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ron Amit, Ron Meir, Kamil Ciosek</p>
<p><strong>Summary:</strong> Specifying a Reinforcement Learning (RL) task involves choosing a suitable
planning horizon, which is typically modeled by a discount factor. It is known
that applying RL algorithms with a lower discount factor can act as a
regularizer, improving performance in the limited data regime. Yet the exact
nature of this regularizer has not been investigated. In this work, we fill in
this gap. For several Temporal-Difference (TD) learning methods, we show an
explicit equivalence between using a reduced...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.06669v1' target='_blank'>Reinforcement Learning of Musculoskeletal Control from Functional
  Simulations</a></h2>
<p><strong>Authors:</strong> Emanuel Joos, Fabien P√©an, Orcun Goksel</p>
<p><strong>Summary:</strong> To diagnose, plan, and treat musculoskeletal pathologies, understanding and
reproducing muscle recruitment for complex movements is essential. With muscle
activations for movements often being highly redundant, nonlinear, and time
dependent, machine learning can provide a solution for their modeling and
control for anatomy-specific musculoskeletal simulations. Sophisticated
biomechanical simulations often require specialized computational environments,
being numerically complex and slow, hinderi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.10934v1' target='_blank'>UAV Target Tracking in Urban Environments Using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Sarthak Bhagat, Sujit PB</p>
<p><strong>Summary:</strong> Persistent target tracking in urban environments using UAV is a difficult
task due to the limited field of view, visibility obstruction from obstacles
and uncertain target motion. The vehicle needs to plan intelligently in 3D such
that the target visibility is maximized. In this paper, we introduce Target
Following DQN (TF-DQN), a deep reinforcement learning technique based on Deep
Q-Networks with a curriculum training framework for the UAV to persistently
track the target in the presence of obs...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.11808v1' target='_blank'>Deep Reinforcement Learning based Automatic Exploration for Navigation
  in Unknown Environment</a></h2>
<p><strong>Authors:</strong> Haoran Li, Qichao Zhang, Dongbin Zhao</p>
<p><strong>Summary:</strong> This paper investigates the automatic exploration problem under the unknown
environment, which is the key point of applying the robotic system to some
social tasks. The solution to this problem via stacking decision rules is
impossible to cover various environments and sensor properties. Learning based
control methods are adaptive for these scenarios. However, these methods are
damaged by low learning efficiency and awkward transferability from simulation
to reality. In this paper, we construct ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.12640v1' target='_blank'>Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning
  on Graphs</a></h2>
<p><strong>Authors:</strong> Fanfei Chen, John D. Martin, Yewei Huang, Jinkun Wang, Brendan Englot</p>
<p><strong>Summary:</strong> We consider an autonomous exploration problem in which a range-sensing mobile
robot is tasked with accurately mapping the landmarks in an a priori unknown
environment efficiently in real-time; it must choose sensing actions that both
curb localization uncertainty and achieve information gain. For this problem,
belief space planning methods that forward-simulate robot sensing and
estimation may often fail in real-time implementation, scaling poorly with
increasing size of the state, belief and ac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.01050v1' target='_blank'>Model-Free Reinforcement Learning for Stochastic Games with Linear
  Temporal Logic Objectives</a></h2>
<p><strong>Authors:</strong> Alper Kamil Bozkurt, Yu Wang, Michael Zavlanos, Miroslav Pajic</p>
<p><strong>Summary:</strong> We study the problem of synthesizing control strategies for Linear Temporal
Logic (LTL) objectives in unknown environments. We model this problem as a
turn-based zero-sum stochastic game between the controller and the environment,
where the transition probabilities and the model topology are fully unknown.
The winning condition for the controller in this game is the satisfaction of
the given LTL specification, which can be captured by the acceptance condition
of a deterministic Rabin automaton (...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.01298v1' target='_blank'>Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for
  Physically Embedded 3D Sokoban</a></h2>
<p><strong>Authors:</strong> Peter Karkus, Mehdi Mirza, Arthur Guez, Andrew Jaegle, Timothy Lillicrap, Lars Buesing, Nicolas Heess, Theophane Weber</p>
<p><strong>Summary:</strong> Intelligent robots need to achieve abstract objectives using concrete,
spatiotemporally complex sensory information and motor control. Tabula rasa
deep reinforcement learning (RL) has tackled demanding tasks in terms of either
visual, abstract, or physical reasoning, but solving these jointly remains a
formidable challenge. One recent, unsolved benchmark task that integrates these
challenges is Mujoban, where a robot needs to arrange 3D warehouses generated
from 2D Sokoban puzzles. We explore wh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.02663v1' target='_blank'>Heterogeneous Multi-Agent Reinforcement Learning for Unknown Environment
  Mapping</a></h2>
<p><strong>Authors:</strong> Ceyer Wakilpoor, Patrick J. Martin, Carrie Rebhuhn, Amanda Vu</p>
<p><strong>Summary:</strong> Reinforcement learning in heterogeneous multi-agent scenarios is important
for real-world applications but presents challenges beyond those seen in
homogeneous settings and simple benchmarks. In this work, we present an
actor-critic algorithm that allows a team of heterogeneous agents to learn
decentralized control policies for covering an unknown environment. This task
is of interest to national security and emergency response organizations that
would like to enhance situational awareness in ha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.04689v1' target='_blank'>LaND: Learning to Navigate from Disengagements</a></h2>
<p><strong>Authors:</strong> Gregory Kahn, Pieter Abbeel, Sergey Levine</p>
<p><strong>Summary:</strong> Consistently testing autonomous mobile robots in real world scenarios is a
necessary aspect of developing autonomous navigation systems. Each time the
human safety monitor disengages the robot's autonomy system due to the robot
performing an undesirable maneuver, the autonomy developers gain insight into
how to improve the autonomy system. However, we believe that these
disengagements not only show where the system fails, which is useful for
troubleshooting, but also provide a direct learning si...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.05673v2' target='_blank'>Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement
  Learning?</a></h2>
<p><strong>Authors:</strong> Qiwen Cui, Lin F. Yang</p>
<p><strong>Summary:</strong> It is believed that a model-based approach for reinforcement learning (RL) is
the key to reduce sample complexity. However, the understanding of the sample
optimality of model-based RL is still largely missing, even for the linear
case. This work considers sample complexity of finding an $\epsilon$-optimal
policy in a Markov decision process (MDP) that admits a linear additive feature
representation, given only access to a generative model. We solve this problem
via a plug-in solver approach, wh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.06460v1' target='_blank'>Deep Reinforcement Learning for Real-Time Optimization of Pumps in Water
  Distribution Systems</a></h2>
<p><strong>Authors:</strong> Gergely Hajgat√≥, Gy√∂rgy Pa√°l, B√°lint Gyires-T√≥th</p>
<p><strong>Summary:</strong> Real-time control of pumps can be an infeasible task in water distribution
systems (WDSs) because the calculation to find the optimal pump speeds is
resource-intensive. The computational need cannot be lowered even with the
capabilities of smart water networks when conventional optimization techniques
are used. Deep reinforcement learning (DRL) is presented here as a controller
of pumps in two WDSs. An agent based on a dueling deep q-network is trained to
maintain the pump speeds based on instan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.08169v3' target='_blank'>Uncertainty-aware Contact-safe Model-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Cheng-Yu Kuo, Andreas Schaarschmidt, Yunduan Cui, Tamim Asfour, Takamitsu Matsubara</p>
<p><strong>Summary:</strong> This letter presents contact-safe Model-based Reinforcement Learning (MBRL)
for robot applications that achieves contact-safe behaviors in the learning
process. In typical MBRL, we cannot expect the data-driven model to generate
accurate and reliable policies to the intended robotic tasks during the
learning process due to sample scarcity. Operating these unreliable policies in
a contact-rich environment could cause damage to the robot and its
surroundings. To alleviate the risk of causing damag...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.10380v1' target='_blank'>Negotiating Team Formation Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z. Leibo, Marc Lanctot, Michael Johanson, Wojciech M. Czarnecki, Thore Graepel</p>
<p><strong>Summary:</strong> When autonomous agents interact in the same environment, they must often
cooperate to achieve their goals. One way for agents to cooperate effectively
is to form a team, make a binding agreement on a joint plan, and execute it.
However, when agents are self-interested, the gains from team formation must be
allocated appropriately to incentivize agreement. Various approaches for
multi-agent negotiation have been proposed, but typically only work for
particular negotiation protocols. More general ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.00583v1' target='_blank'>Obtain Employee Turnover Rate and Optimal Reduction Strategy Based On
  Neural Network and Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xiaohan Cheng</p>
<p><strong>Summary:</strong> Nowadays, human resource is an important part of various resources of
enterprises. For enterprises, high-loyalty and high-quality talented persons
are often the core competitiveness of enterprises. Therefore, it is of great
practical significance to predict whether employees leave and reduce the
turnover rate of employees. First, this paper established a multi-layer
perceptron predictive model of employee turnover rate. A model based on Sarsa
which is a kind of reinforcement learning algorithm i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.04422v2' target='_blank'>Automated Synthesis of Steady-State Continuous Processes using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Quirin G√∂ttl, Dominik G. Grimm, Jakob Burger</p>
<p><strong>Summary:</strong> Automated flowsheet synthesis is an important field in computer-aided process
engineering. The present work demonstrates how reinforcement learning can be
used for automated flowsheet synthesis without any heuristics of prior
knowledge of conceptual design. The environment consists of a steady-state
flowsheet simulator that contains all physical knowledge. An agent is trained
to take discrete actions and sequentially built up flowsheets that solve a
given process problem. A novel method named Sy...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.04080v1' target='_blank'>Design and implementation of an environment for Learning to Run a Power
  Network (L2RPN)</a></h2>
<p><strong>Authors:</strong> Marvin Lerousseau</p>
<p><strong>Summary:</strong> This report summarizes work performed as part of an internship at INRIA, in
partial requirement for the completion of a master degree in math and
informatics. The goal of the internship was to develop a software environment
to simulate electricity transmission in a power grid and actions performed by
operators to maintain this grid in security. Our environment lends itself to
automate the control of the power grid with reinforcement learning agents,
assisting human operators. It is amenable to o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.04625v1' target='_blank'>Don't Get Yourself into Trouble! Risk-aware Decision-Making for
  Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Kasra Mokhtari, Alan R. Wagner</p>
<p><strong>Summary:</strong> Risk is traditionally described as the expected likelihood of an undesirable
outcome, such as collisions for autonomous vehicles. Accurately predicting risk
or potentially risky situations is critical for the safe operation of
autonomous vehicles. In our previous work, we showed that risk could be
characterized by two components: 1) the probability of an undesirable outcome
and 2) an estimate of how undesirable the outcome is (loss). This paper is an
extension to our previous work. In this paper...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.07814v1' target='_blank'>Sample Efficient Reinforcement Learning In Continuous State Spaces: A
  Perspective Beyond Linearity</a></h2>
<p><strong>Authors:</strong> Dhruv Malik, Aldo Pacchiano, Vishwak Srinivasan, Yuanzhi Li</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is empirically successful in complex nonlinear
Markov decision processes (MDPs) with continuous state spaces. By contrast, the
majority of theoretical RL literature requires the MDP to satisfy some form of
linear structure, in order to guarantee sample efficient RL. Such efforts
typically assume the transition dynamics or value function of the MDP are
described by linear functions of the state features. To resolve this
discrepancy between theory and practice, we intro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.07841v2' target='_blank'>Randomized Exploration for Reinforcement Learning with General Value
  Function Approximation</a></h2>
<p><strong>Authors:</strong> Haque Ishfaq, Qiwen Cui, Viet Nguyen, Alex Ayoub, Zhuoran Yang, Zhaoran Wang, Doina Precup, Lin F. Yang</p>
<p><strong>Summary:</strong> We propose a model-free reinforcement learning algorithm inspired by the
popular randomized least squares value iteration (RLSVI) algorithm as well as
the optimism principle. Unlike existing upper-confidence-bound (UCB) based
approaches, which are often computationally intractable, our algorithm drives
exploration by simply perturbing the training data with judiciously chosen
i.i.d. scalar noises. To attain optimistic value function estimation without
resorting to a UCB-style bonus, we introduce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.15316v1' target='_blank'>Scalable Online Planning via Reinforcement Learning Fine-Tuning</a></h2>
<p><strong>Authors:</strong> Arnaud Fickinger, Hengyuan Hu, Brandon Amos, Stuart Russell, Noam Brown</p>
<p><strong>Summary:</strong> Lookahead search has been a critical component of recent AI successes, such
as in the games of chess, go, and poker. However, the search methods used in
these games, and in many other settings, are tabular. Tabular search methods do
not scale well with the size of the search space, and this problem is
exacerbated by stochasticity and partial observability. In this work we replace
tabular search with online model-based fine-tuning of a policy neural network
via reinforcement learning, and show th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1509.02413v1' target='_blank'>Learning Efficient Representations for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yanping Huang</p>
<p><strong>Summary:</strong> Markov decision processes (MDPs) are a well studied framework for solving
sequential decision making problems under uncertainty. Exact methods for
solving MDPs based on dynamic programming such as policy iteration and value
iteration are effective on small problems. In problems with a large discrete
state space or with continuous state spaces, a compact representation is
essential for providing an efficient approximation solutions to MDPs. Commonly
used approximation algorithms involving constru...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.02632v2' target='_blank'>Extracting Action Sequences from Texts Based on Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Wenfeng Feng, Hankz Hankui Zhuo, Subbarao Kambhampati</p>
<p><strong>Summary:</strong> Extracting action sequences from natural language texts is challenging, as it
requires commonsense inferences based on world knowledge. Although there has
been work on extracting action scripts, instructions, navigation actions, etc.,
they require that either the set of candidate actions be provided in advance,
or that action descriptions are restricted to a specific form, e.g.,
description templates. In this paper, we aim to extract action sequences from
texts in free natural language, i.e., wi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.05752v1' target='_blank'>Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Weihao Yuan, Johannes A. Stork, Danica Kragic, Michael Y. Wang, Kaiyu Hang</p>
<p><strong>Summary:</strong> Rearranging objects on a tabletop surface by means of nonprehensile
manipulation is a task which requires skillful interaction with the physical
world. Usually, this is achieved by precisely modeling physical properties of
the objects, robot, and the environment for explicit planning. In contrast, as
explicitly modeling the physical environment is not always feasible and
involves various uncertainties, we learn a nonprehensile rearrangement strategy
with deep reinforcement learning based on only...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.08700v2' target='_blank'>Safe Reinforcement Learning with Model Uncertainty Estimates</a></h2>
<p><strong>Authors:</strong> Bj√∂rn L√ºtjens, Michael Everett, Jonathan P. How</p>
<p><strong>Summary:</strong> Many current autonomous systems are being designed with a strong reliance on
black box predictions from deep neural networks (DNNs). However, DNNs tend to
be overconfident in predictions on unseen data and can give unpredictable
results for far-from-distribution test data. The importance of predictions that
are robust to this distributional shift is evident for safety-critical
applications, such as collision avoidance around pedestrians. Measures of model
uncertainty can be used to identify unse...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.11181v2' target='_blank'>Neural Modular Control for Embodied Question Answering</a></h2>
<p><strong>Authors:</strong> Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</p>
<p><strong>Summary:</strong> We present a modular approach for learning policies for navigation over long
planning horizons from language input. Our hierarchical policy operates at
multiple timescales, where the higher-level master policy proposes subgoals to
be executed by specialized sub-policies. Our choice of subgoals is
compositional and semantic, i.e. they can be sequentially combined in arbitrary
orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find
kitchen', 'find refrigerator', etc.).
  We...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.00045v1' target='_blank'>Using Monte Carlo Tree Search as a Demonstrator within Asynchronous Deep
  RL</a></h2>
<p><strong>Authors:</strong> Bilal Kartal, Pablo Hernandez-Leal, Matthew E. Taylor</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) has achieved great successes in recent
years with the help of novel methods and higher compute power. However, there
are still several challenges to be addressed such as convergence to locally
optimal policies and long training times. In this paper, firstly, we augment
Asynchronous Advantage Actor-Critic (A3C) method with a novel self-supervised
auxiliary task, i.e. \emph{Terminal Prediction}, measuring temporal closeness
to terminal states, namely A3C-TP. Secon...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.01129v2' target='_blank'>Mitigating Planner Overfitting in Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dilip Arumugam, David Abel, Kavosh Asadi, Nakul Gopalan, Christopher Grimm, Jun Ki Lee, Lucas Lehnert, Michael L. Littman</p>
<p><strong>Summary:</strong> An agent with an inaccurate model of its environment faces a difficult
choice: it can ignore the errors in its model and act in the real world in
whatever way it determines is optimal with respect to its model. Alternatively,
it can take a more conservative stance and eschew its model in favor of
optimizing its behavior solely via real-world interaction. This latter approach
can be exceedingly slow to learn from experience, while the former can lead to
"planner overfitting" - aspects of the agen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.09647v2' target='_blank'>Learning to Prevent Monocular SLAM Failure using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vignesh Prasad, Karmesh Yadav, Rohitashva Singh Saurabh, Swapnil Daga, Nahas Pareekutty, K. Madhava Krishna, Balaraman Ravindran, Brojeshwar Bhowmick</p>
<p><strong>Summary:</strong> Monocular SLAM refers to using a single camera to estimate robot ego motion
while building a map of the environment. While Monocular SLAM is a well studied
problem, automating Monocular SLAM by integrating it with trajectory planning
frameworks is particularly challenging. This paper presents a novel formulation
based on Reinforcement Learning (RL) that generates fail safe trajectories
wherein the SLAM generated outputs do not deviate largely from their true
values. Quintessentially, the RL fram...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.06933v3' target='_blank'>Learning to Navigate in Indoor Environments: from Memorizing to
  Reasoning</a></h2>
<p><strong>Authors:</strong> Liulong Ma, Yanjie Liu, Jiao Chen, Dong Jin</p>
<p><strong>Summary:</strong> Autonomous navigation is an essential capability of smart mobility for mobile
robots. Traditional methods must have the environment map to plan a
collision-free path in workspace. Deep reinforcement learning (DRL) is a
promising technique to realize the autonomous navigation task without a map,
with which deep neural network can fit the mapping from observation to
reasonable action through explorations. It should not only memorize the trained
target, but more importantly, the planner can reason ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.00475v1' target='_blank'>Efficient Model-free Reinforcement Learning in Metric Spaces</a></h2>
<p><strong>Authors:</strong> Zhao Song, Wen Sun</p>
<p><strong>Summary:</strong> Model-free Reinforcement Learning (RL) algorithms such as Q-learning
[Watkins, Dayan 92] have been widely used in practice and can achieve human
level performance in applications such as video games [Mnih et al. 15].
Recently, equipped with the idea of optimism in the face of uncertainty,
Q-learning algorithms [Jin, Allen-Zhu, Bubeck, Jordan 18] can be proven to be
sample efficient for discrete tabular Markov Decision Processes (MDPs) which
have finite number of states and actions. In this work,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.02057v1' target='_blank'>Benchmarking Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, Jimmy Ba</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) is widely seen as having the
potential to be significantly more sample efficient than model-free RL.
However, research in model-based RL has not been very standardized. It is
fairly common for authors to experiment with self-designed environments, and
there are several separate lines of research, which are sometimes
closed-sourced or not reproducible. Accordingly, it is an open question how
these various existing MBRL algorithms perform relative to each ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.01384v3' target='_blank'>Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> William Agnew, Pedro Domingos</p>
<p><strong>Summary:</strong> Current deep reinforcement learning (RL) approaches incorporate minimal prior
knowledge about the environment, limiting computational and sample efficiency.
\textit{Objects} provide a succinct and causal description of the world, and
many recent works have proposed unsupervised object representation learning
using priors and losses over static object properties like visual consistency.
However, object dynamics and interactions are also critical cues for
objectness. In this paper we propose a fra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.10923v1' target='_blank'>Autonomous UAV Navigation: A DDPG-based Deep Reinforcement Learning
  Approach</a></h2>
<p><strong>Authors:</strong> Omar Bouhamed, Hakim Ghazzai, Hichem Besbes, Yehia Massoud</p>
<p><strong>Summary:</strong> In this paper, we propose an autonomous UAV path planning framework using
deep reinforcement learning approach. The objective is to employ a self-trained
UAV as a flying mobile unit to reach spatially distributed moving or static
targets in a given three dimensional urban area. In this approach, a Deep
Deterministic Policy Gradient (DDPG) with continuous action space is designed
to train the UAV to navigate through or over the obstacles to reach its
assigned target. A customized reward function ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.11642v1' target='_blank'>Giving Up Control: Neurons as Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Jordan Ott</p>
<p><strong>Summary:</strong> Artificial Intelligence has historically relied on planning, heuristics, and
handcrafted approaches designed by experts. All the while claiming to pursue
the creation of Intelligence. This approach fails to acknowledge that
intelligence emerges from the dynamics within a complex system. Neurons in the
brain are governed by local rules, where no single neuron, or group of neurons,
coordinates or controls the others. This local structure gives rise to the
appropriate dynamics in which intelligence...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.03499v1' target='_blank'>Online Constrained Model-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Benjamin van Niekerk, Andreas Damianou, Benjamin Rosman</p>
<p><strong>Summary:</strong> Applying reinforcement learning to robotic systems poses a number of
challenging problems. A key requirement is the ability to handle continuous
state and action spaces while remaining within a limited time and resource
budget. Additionally, for safe operation, the system must make robust decisions
under hard constraints. To address these challenges, we propose a model based
approach that combines Gaussian Process regression and Receding Horizon
Control. Using sparse spectrum Gaussian Processes,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.07333v1' target='_blank'>Reinforcement Learning in a Physics-Inspired Semi-Markov Environment</a></h2>
<p><strong>Authors:</strong> Colin Bellinger, Rory Coles, Mark Crowley, Isaac Tamblyn</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has been demonstrated to have great potential in
many applications of scientific discovery and design. Recent work includes, for
example, the design of new structures and compositions of molecules for
therapeutic drugs. Much of the existing work related to the application of RL
to scientific domains, however, assumes that the available state representation
obeys the Markov property. For reasons associated with time, cost, sensor
accuracy, and gaps in scientific knowle...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.08410v1' target='_blank'>Deep Reinforcement Learning for Adaptive Learning Systems</a></h2>
<p><strong>Authors:</strong> Xiao Li, Hanchen Xu, Jinming Zhang, Hua-hua Chang</p>
<p><strong>Summary:</strong> In this paper, we formulate the adaptive learning problem---the problem of
how to find an individualized learning plan (called policy) that chooses the
most appropriate learning materials based on learner's latent traits---faced in
adaptive learning systems as a Markov decision process (MDP). We assume latent
traits to be continuous with an unknown transition model. We apply a model-free
deep reinforcement learning algorithm---the deep Q-learning algorithm---that
can effectively find the optimal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.13319v3' target='_blank'>Efficient Reinforcement Learning in Factored MDPs with Application to
  Constrained RL</a></h2>
<p><strong>Authors:</strong> Xiaoyu Chen, Jiachen Hu, Lihong Li, Liwei Wang</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) in episodic, factored Markov decision processes
(FMDPs) is studied. We propose an algorithm called FMDP-BF, which leverages the
factorization structure of FMDP. The regret of FMDP-BF is shown to be
exponentially smaller than that of optimal algorithms designed for non-factored
MDPs, and improves on the best previous result for FMDPs~\citep{osband2014near}
by a factored of $\sqrt{H|\mathcal{S}_i|}$, where $|\mathcal{S}_i|$ is the
cardinality of the factored state subsp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.00563v2' target='_blank'>Flightmare: A Flexible Quadrotor Simulator</a></h2>
<p><strong>Authors:</strong> Yunlong Song, Selim Naji, Elia Kaufmann, Antonio Loquercio, Davide Scaramuzza</p>
<p><strong>Summary:</strong> State-of-the-art quadrotor simulators have a rigid and highly-specialized
structure: either are they really fast, physically accurate, or
photo-realistic. In this work, we propose a novel quadrotor simulator:
Flightmare. Flightmare is composed of two main components: a configurable
rendering engine built on Unity and a flexible physics engine for dynamics
simulation. Those two components are totally decoupled and can run
independently of each other. This makes our simulator extremely fast: rende...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.04607v3' target='_blank'>Multi-Objective Model-based Reinforcement Learning for Infectious
  Disease Control</a></h2>
<p><strong>Authors:</strong> Runzhe Wan, Xinyu Zhang, Rui Song</p>
<p><strong>Summary:</strong> Severe infectious diseases such as the novel coronavirus (COVID-19) pose a
huge threat to public health. Stringent control measures, such as school
closures and stay-at-home orders, while having significant effects, also bring
huge economic losses. In the face of an emerging infectious disease, a crucial
question for policymakers is how to make the trade-off and implement the
appropriate interventions timely given the huge uncertainty. In this work, we
propose a Multi-Objective Model-based Reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.11277v1' target='_blank'>Multi-Agent Deep Reinforcement Learning Based Trajectory Planning for
  Multi-UAV Assisted Mobile Edge Computing</a></h2>
<p><strong>Authors:</strong> Liang Wang, Kezhi Wang, Cunhua Pan, Wei Xu, Nauman Aslam, Lajos Hanzo</p>
<p><strong>Summary:</strong> An unmanned aerial vehicle (UAV)-aided mobile edge computing (MEC) framework
is proposed, where several UAVs having different trajectories fly over the
target area and support the user equipments (UEs) on the ground. We aim to
jointly optimize the geographical fairness among all the UEs, the fairness of
each UAV' UE-load and the overall energy consumption of UEs. The above
optimization problem includes both integer and continues variables and it is
challenging to solve. To address the above prob...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.14136v2' target='_blank'>Time your hedge with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Eric Benhamou, David Saltiel, Sandrine Ungari, Abhishek Mukhopadhyay</p>
<p><strong>Summary:</strong> Can an asset manager plan the optimal timing for her/his hedging strategies
given market conditions? The standard approach based on Markowitz or other more
or less sophisticated financial rules aims to find the best portfolio
allocation thanks to forecasted expected returns and risk but fails to fully
relate market conditions to hedging strategies decision. In contrast, Deep
Reinforcement Learning (DRL) can tackle this challenge by creating a dynamic
dependency between market information and hed...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.08421v3' target='_blank'>Reachability-based Trajectory Safeguard (RTS): A Safe and Fast
  Reinforcement Learning Safety Layer for Continuous Control</a></h2>
<p><strong>Authors:</strong> Yifei Simon Shao, Chao Chen, Shreyas Kousik, Ram Vasudevan</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) algorithms have achieved remarkable performance
in decision making and control tasks due to their ability to reason about
long-term, cumulative reward using trial and error. However, during RL
training, applying this trial-and-error approach to real-world robots operating
in safety critical environment may lead to collisions. To address this
challenge, this paper proposes a Reachability-based Trajectory Safeguard (RTS),
which leverages reachability analysis to ensure ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.08484v3' target='_blank'>Combining Reinforcement Learning with Model Predictive Control for
  On-Ramp Merging</a></h2>
<p><strong>Authors:</strong> Joseph Lubars, Harsh Gupta, Sandeep Chinchali, Liyun Li, Adnan Raja, R. Srikant, Xinzhou Wu</p>
<p><strong>Summary:</strong> We consider the problem of designing an algorithm to allow a car to
autonomously merge on to a highway from an on-ramp. Two broad classes of
techniques have been proposed to solve motion planning problems in autonomous
driving: Model Predictive Control (MPC) and Reinforcement Learning (RL). In
this paper, we first establish the strengths and weaknesses of state-of-the-art
MPC and RL-based techniques through simulations. We show that the performance
of the RL agent is worse than that of the MPC s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.10167v1' target='_blank'>When to stop value iteration: stability and near-optimality versus
  computation</a></h2>
<p><strong>Authors:</strong> Mathieu Granzotto, Romain Postoyan, Dragan Ne≈°iƒá, Lucian Bu≈üoniu, Jamal Daafouz</p>
<p><strong>Summary:</strong> Value iteration (VI) is a ubiquitous algorithm for optimal control, planning,
and reinforcement learning schemes. Under the right assumptions, VI is a vital
tool to generate inputs with desirable properties for the controlled system,
like optimality and Lyapunov stability. As VI usually requires an infinite
number of iterations to solve general nonlinear optimal control problems, a key
question is when to terminate the algorithm to produce a "good" solution, with
a measurable impact on optimalit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.14267v1' target='_blank'>Minimax Sample Complexity for Turn-based Stochastic Game</a></h2>
<p><strong>Authors:</strong> Qiwen Cui, Lin F. Yang</p>
<p><strong>Summary:</strong> The empirical success of Multi-agent reinforcement learning is encouraging,
while few theoretical guarantees have been revealed. In this work, we prove
that the plug-in solver approach, probably the most natural reinforcement
learning algorithm, achieves minimax sample complexity for turn-based
stochastic game (TBSG). Specifically, we plan in an empirical TBSG by utilizing
a `simulator' that allows sampling from arbitrary state-action pair. We show
that the empirical Nash equilibrium strategy is...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.01297v1' target='_blank'>Reinforcement Learning with Probabilistic Boolean Network Models of
  Smart Grid Devices</a></h2>
<p><strong>Authors:</strong> Pedro J. Rivera Torres, Carlos Gershenson Garc√≠a, Samir Kanaan Izquierdo</p>
<p><strong>Summary:</strong> The area of Smart Power Grids needs to constantly improve its efficiency and
resilience, to pro-vide high quality electrical power, in a resistant grid,
managing faults and avoiding failures. Achieving this requires high component
reliability, adequate maintenance, and a studied failure occurrence. Correct
system operation involves those activities, and novel methodologies to detect,
classify, and isolate faults and failures, model and simulate processes with
predictive algorithms and analytics ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.05599v1' target='_blank'>Improving Model-Based Reinforcement Learning with Internal State
  Representations through Self-Supervision</a></h2>
<p><strong>Authors:</strong> Julien Scholz, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter</p>
<p><strong>Summary:</strong> Using a model of the environment, reinforcement learning agents can plan
their future moves and achieve superhuman performance in board games like
Chess, Shogi, and Go, while remaining relatively sample-efficient. As
demonstrated by the MuZero Algorithm, the environment model can even be learned
dynamically, generalizing the agent to many more tasks while at the same time
achieving state-of-the-art performance. Notably, MuZero uses internal state
representations derived from real environment sta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.10536v1' target='_blank'>Learning Efficient Navigation in Vortical Flow Fields</a></h2>
<p><strong>Authors:</strong> Peter Gunnarson, Ioannis Mandralis, Guido Novati, Petros Koumoutsakos, John O. Dabiri</p>
<p><strong>Summary:</strong> Efficient point-to-point navigation in the presence of a background flow
field is important for robotic applications such as ocean surveying. In such
applications, robots may only have knowledge of their immediate surroundings or
be faced with time-varying currents, which limits the use of optimal control
techniques for planning trajectories. Here, we apply a novel Reinforcement
Learning algorithm to discover time-efficient navigation policies to steer a
fixed-speed swimmer through an unsteady t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.11176v1' target='_blank'>Deep Reinforcement Learning for Dynamic Spectrum Sharing of LTE and NR</a></h2>
<p><strong>Authors:</strong> Ursula Challita, David Sandberg</p>
<p><strong>Summary:</strong> In this paper, a proactive dynamic spectrum sharing scheme between 4G and 5G
systems is proposed. In particular, a controller decides on the resource split
between NR and LTE every subframe while accounting for future network states
such as high interference subframes and multimedia broadcast single frequency
network (MBSFN) subframes. To solve this problem, a deep reinforcement learning
(RL) algorithm based on Monte Carlo Tree Search (MCTS) is proposed. The
introduced deep RL architecture is tr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.04727v1' target='_blank'>Vision-Based Mobile Robotics Obstacle Avoidance With Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Patrick Wenzel, Torsten Sch√∂n, Laura Leal-Taix√©, Daniel Cremers</p>
<p><strong>Summary:</strong> Obstacle avoidance is a fundamental and challenging problem for autonomous
navigation of mobile robots. In this paper, we consider the problem of obstacle
avoidance in simple 3D environments where the robot has to solely rely on a
single monocular camera. In particular, we are interested in solving this
problem without relying on localization, mapping, or planning techniques. Most
of the existing work consider obstacle avoidance as two separate problems,
namely obstacle detection, and control. I...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.05895v3' target='_blank'>WFA-IRL: Inverse Reinforcement Learning of Autonomous Behaviors Encoded
  as Weighted Finite Automata</a></h2>
<p><strong>Authors:</strong> Tianyu Wang, Nikolay Atanasov</p>
<p><strong>Summary:</strong> This paper presents a method for learning logical task specifications and
cost functions from demonstrations. Constructing specifications by hand is
challenging for complex objectives and constraints in autonomous systems.
Instead, we consider demonstrated task executions, whose logic structure and
transition costs need to be inferred by an autonomous agent. We employ a
spectral learning approach to extract a weighted finite automaton (WFA),
approximating the unknown task logic. Thereafter, we d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.06807v1' target='_blank'>Adapting User Interfaces with Model-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kashyap Todi, Gilles Bailly, Luis A. Leiva, Antti Oulasvirta</p>
<p><strong>Summary:</strong> Adapting an interface requires taking into account both the positive and
negative effects that changes may have on the user. A carelessly picked
adaptation may impose high costs to the user -- for example, due to surprise or
relearning effort -- or "trap" the process to a suboptimal design immaturely.
However, effects on users are hard to predict as they depend on factors that
are latent and evolve over the course of interaction. We propose a novel
approach for adaptive user interfaces that yiel...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.07119v2' target='_blank'>Goal-Driven Autonomous Exploration Through Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Reinis Cimurs, Il Hong Suh, Jin Han Lee</p>
<p><strong>Summary:</strong> In this paper, we present an autonomous navigation system for goal-driven
exploration of unknown environments through deep reinforcement learning (DRL).
Points of interest (POI) for possible navigation directions are obtained from
the environment and an optimal waypoint is selected, based on the available
data. Following the waypoints, the robot is guided towards the global goal and
the local optimum problem of reactive navigation is mitigated. Then, a motion
policy for local navigation is learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.08624v2' target='_blank'>Autonomous Drone Racing with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yunlong Song, Mats Steinweg, Elia Kaufmann, Davide Scaramuzza</p>
<p><strong>Summary:</strong> In many robotic tasks, such as autonomous drone racing, the goal is to travel
through a set of waypoints as fast as possible. A key challenge for this task
is planning the time-optimal trajectory, which is typically solved by assuming
perfect knowledge of the waypoints to pass in advance. The resulting solution
is either highly specialized for a single-track layout, or suboptimal due to
simplifying assumptions about the platform dynamics. In this work, a new
approach to near-time-optimal traject...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.01620v2' target='_blank'>Data-Efficient Reinforcement Learning for Malaria Control</a></h2>
<p><strong>Authors:</strong> Lixin Zou, Long Xia, Linfang Hou, Xiangyu Zhao, Dawei Yin</p>
<p><strong>Summary:</strong> Sequential decision-making under cost-sensitive tasks is prohibitively
daunting, especially for the problem that has a significant impact on people's
daily lives, such as malaria control, treatment recommendation. The main
challenge faced by policymakers is to learn a policy from scratch by
interacting with a complex environment in a few trials. This work introduces a
practical, data-efficient policy learning method, named Variance-Bonus Monte
Carlo Tree Search~(VB-MCTS), which can copy with ver...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.01904v2' target='_blank'>Solving Sokoban with forward-backward reinforcement learning</a></h2>
<p><strong>Authors:</strong> Yaron Shoham, Gal Elidan</p>
<p><strong>Summary:</strong> Despite seminal advances in reinforcement learning in recent years, many
domains where the rewards are sparse, e.g. given only at task completion,
remain quite challenging. In such cases, it can be beneficial to tackle the
task both from its beginning and end, and make the two ends meet. Existing
approaches that do so, however, are not effective in the common scenario where
the strategy needed near the end goal is very different from the one that is
effective earlier on.
  In this work we propos...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.01825v1' target='_blank'>Sample Efficient Reinforcement Learning via Model-Ensemble Exploration
  and Exploitation</a></h2>
<p><strong>Authors:</strong> Yao Yao, Li Xiao, Zhicheng An, Wanpeng Zhang, Dijun Luo</p>
<p><strong>Summary:</strong> Model-based deep reinforcement learning has achieved success in various
domains that require high sample efficiencies, such as Go and robotics.
However, there are some remaining issues, such as planning efficient
explorations to learn more accurate dynamic models, evaluating the uncertainty
of the learned models, and more rational utilization of models. To mitigate
these issues, we present MEEE, a model-ensemble method that consists of
optimistic exploration and weighted exploitation. During exp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.01867v1' target='_blank'>Control of rough terrain vehicles using deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Viktor Wiberg, Erik Wallin, Martin Servin, Tomas Nordfjell</p>
<p><strong>Summary:</strong> We explore the potential to control terrain vehicles using deep reinforcement
in scenarios where human operators and traditional control methods are
inadequate. This letter presents a controller that perceives, plans, and
successfully controls a 16-tonne forestry vehicle with two frame articulation
joints, six wheels, and their actively articulated suspensions to traverse
rough terrain. The carefully shaped reward signal promotes safe, environmental,
and efficient driving, which leads to the eme...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.02603v1' target='_blank'>Meta-Reinforcement Learning for Heuristic Planning</a></h2>
<p><strong>Authors:</strong> Ricardo Luna Gutierrez, Matteo Leonetti</p>
<p><strong>Summary:</strong> In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of
tasks to prepare for and learn faster in new, unseen, but related tasks. The
training tasks are usually hand-crafted to be representative of the expected
distribution of test tasks and hence all used in training. We show that given a
set of training tasks, learning can be both faster and more effective (leading
to better performance in the test tasks), if the training tasks are
appropriately selected. We propose a task sele...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.04897v1' target='_blank'>Learning-to-Dispatch: Reinforcement Learning Based Flight Planning under
  Emergency</a></h2>
<p><strong>Authors:</strong> Kai Zhang, Yupeng Yang, Chengtao Xu, Dahai Liu, Houbing Song</p>
<p><strong>Summary:</strong> The effectiveness of resource allocation under emergencies especially
hurricane disasters is crucial. However, most researchers focus on emergency
resource allocation in a ground transportation system. In this paper, we
propose Learning-to-Dispatch (L2D), a reinforcement learning (RL) based air
route dispatching system, that aims to add additional flights for hurricane
evacuation while minimizing the airspace's complexity and air traffic
controller's workload. Given a bipartite graph with weight...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.06106v2' target='_blank'>Conservative Offline Distributional Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yecheng Jason Ma, Dinesh Jayaraman, Osbert Bastani</p>
<p><strong>Summary:</strong> Many reinforcement learning (RL) problems in practice are offline, learning
purely from observational data. A key challenge is how to ensure the learned
policy is safe, which requires quantifying the risk associated with different
actions. In the online setting, distributional RL algorithms do so by learning
the distribution over returns (i.e., cumulative rewards) instead of the
expected return; beyond quantifying risk, they have also been shown to learn
better representations for planning. We p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.07410v1' target='_blank'>PC-MLP: Model-based Reinforcement Learning with Policy Cover Guided
  Exploration</a></h2>
<p><strong>Authors:</strong> Yuda Song, Wen Sun</p>
<p><strong>Summary:</strong> Model-based Reinforcement Learning (RL) is a popular learning paradigm due to
its potential sample efficiency compared to model-free RL. However, existing
empirical model-based RL approaches lack the ability to explore. This work
studies a computationally and statistically efficient model-based algorithm for
both Kernelized Nonlinear Regulators (KNR) and linear Markov Decision Processes
(MDPs). For both models, our algorithm guarantees polynomial sample complexity
and only uses access to a plann...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.09796v1' target='_blank'>Offline Reinforcement Learning with Value-based Episodic Memory</a></h2>
<p><strong>Authors:</strong> Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, Bin Liang</p>
<p><strong>Summary:</strong> Offline reinforcement learning (RL) shows promise of applying RL to
real-world problems by effectively utilizing previously collected data. Most
existing offline RL algorithms use regularization or constraints to suppress
extrapolation error for actions outside the dataset. In this paper, we adopt a
different framework, which learns the V-function instead of the Q-function to
naturally keep the learning procedure within the support of an offline dataset.
To enable effective generalization while ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.10133v1' target='_blank'>Locally Differentially Private Reinforcement Learning for Linear Mixture
  Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Chonghua Liao, Jiafan He, Quanquan Gu</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) algorithms can be used to provide personalized
services, which rely on users' private and sensitive data. To protect the
users' privacy, privacy-preserving RL algorithms are in demand. In this paper,
we study RL with linear function approximation and local differential privacy
(LDP) guarantees. We propose a novel $(\varepsilon, \delta)$-LDP algorithm for
learning a class of Markov decision processes (MDPs) dubbed linear mixture
MDPs, and obtains an $\tilde{\mathcal{O}...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.05218v1' target='_blank'>Learning Generalizable Behavior via Visual Rewrite Rules</a></h2>
<p><strong>Authors:</strong> Yiheng Xie, Mingxuan Li, Shangqun Yu, Michael Littman</p>
<p><strong>Summary:</strong> Though deep reinforcement learning agents have achieved unprecedented success
in recent years, their learned policies can be brittle, failing to generalize
to even slight modifications of their environments or unfamiliar situations.
The black-box nature of the neural network learning dynamics makes it
impossible to audit trained deep agents and recover from such failures. In this
paper, we propose a novel representation and learning approach to capture
environment dynamics without using neural n...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.11731v1' target='_blank'>Graph augmented Deep Reinforcement Learning in the GameRLand3D
  environment</a></h2>
<p><strong>Authors:</strong> Edward Beeching, Maxim Peter, Philippe Marcotte, Jilles Debangoye, Olivier Simonin, Joshua Romoff, Christian Wolf</p>
<p><strong>Summary:</strong> We address planning and navigation in challenging 3D video games featuring
maps with disconnected regions reachable by agents using special actions. In
this setting, classical symbolic planners are not applicable or difficult to
adapt. We introduce a hybrid technique combining a low level policy trained
with reinforcement learning and a graph based high level classical planner. In
addition to providing human-interpretable paths, the approach improves the
generalization performance of an end-to-e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.00360v1' target='_blank'>Accelerating Deep Reinforcement Learning for Digital Twin Network
  Optimization with Evolutionary Strategies</a></h2>
<p><strong>Authors:</strong> Carlos G√ºemes-Palau, Paul Almasan, Shihan Xiao, Xiangle Cheng, Xiang Shi, Pere Barlet-Ros, Albert Cabellos-Aparicio</p>
<p><strong>Summary:</strong> The recent growth of emergent network applications (e.g., satellite networks,
vehicular networks) is increasing the complexity of managing modern
communication networks. As a result, the community proposed the Digital Twin
Networks (DTN) as a key enabler of efficient network management. Network
operators can leverage the DTN to perform different optimization tasks (e.g.,
Traffic Engineering, Network Planning). Deep Reinforcement Learning (DRL)
showed a high performance when applied to solve netw...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.03983v1' target='_blank'>Provable Reinforcement Learning with a Short-Term Memory</a></h2>
<p><strong>Authors:</strong> Yonathan Efroni, Chi Jin, Akshay Krishnamurthy, Sobhan Miryoosefi</p>
<p><strong>Summary:</strong> Real-world sequential decision making problems commonly involve partial
observability, which requires the agent to maintain a memory of history in
order to infer the latent states, plan and make good decisions. Coping with
partial observability in general is extremely challenging, as a number of
worst-case statistical and computational barriers are known in learning
Partially Observable Markov Decision Processes (POMDPs). Motivated by the
problem structure in several physical applications, as we...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.09481v2' target='_blank'>TransDreamer: Reinforcement Learning with Transformer World Models</a></h2>
<p><strong>Authors:</strong> Chang Chen, Yi-Fu Wu, Jaesik Yoon, Sungjin Ahn</p>
<p><strong>Summary:</strong> The Dreamer agent provides various benefits of Model-Based Reinforcement
Learning (MBRL) such as sample efficiency, reusable knowledge, and safe
planning. However, its world model and policy networks inherit the limitations
of recurrent neural networks and thus an important question is how an MBRL
framework can benefit from the recent advances of transformers and what the
challenges are in doing so. In this paper, we propose a transformer-based MBRL
agent, called TransDreamer. We first introduce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.03516v1' target='_blank'>Distributed Reinforcement Learning for Robot Teams: A Review</a></h2>
<p><strong>Authors:</strong> Yutong Wang, Mehul Damani, Pamela Wang, Yuhong Cao, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> Purpose of review: Recent advances in sensing, actuation, and computation
have opened the door to multi-robot systems consisting of hundreds/thousands of
robots, with promising applications to automated manufacturing, disaster
relief, harvesting, last-mile delivery, port/airport operations, or search and
rescue. The community has leveraged model-free multi-agent reinforcement
learning (MARL) to devise efficient, scalable controllers for multi-robot
systems (MRS). This review aims to provide an a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.07417v2' target='_blank'>Safe Reinforcement Learning Using Black-Box Reachability Analysis</a></h2>
<p><strong>Authors:</strong> Mahmoud Selim, Amr Alanwar, Shreyas Kousik, Grace Gao, Marco Pavone, Karl H. Johansson</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is capable of sophisticated motion planning and
control for robots in uncertain environments. However, state-of-the-art deep RL
approaches typically lack safety guarantees, especially when the robot and
environment models are unknown. To justify widespread deployment, robots must
respect safety constraints without sacrificing performance. Thus, we propose a
Black-box Reachability-based Safety Layer (BRSL) with three main components:
(1) data-driven reachability analys...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.07543v1' target='_blank'>CryoRL: Reinforcement Learning Enables Efficient Cryo-EM Data Collection</a></h2>
<p><strong>Authors:</strong> Quanfu Fan, Yilai Li, Yuguang Yao, John Cohn, Sijia Liu, Seychelle M. Vos, Michael A. Cianfrocco</p>
<p><strong>Summary:</strong> Single-particle cryo-electron microscopy (cryo-EM) has become one of the
mainstream structural biology techniques because of its ability to determine
high-resolution structures of dynamic bio-molecules. However, cryo-EM data
acquisition remains expensive and labor-intensive, requiring substantial
expertise. Structural biologists need a more efficient and objective method to
collect the best data in a limited time frame. We formulate the cryo-EM data
collection task as an optimization problem in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.02003v3' target='_blank'>Multi-subgoal Robot Navigation in Crowds with History Information and
  Interactions</a></h2>
<p><strong>Authors:</strong> Xinyi Yu, Jianan Hu, Yuehai Fan, Wancai Zheng, Linlin Ou</p>
<p><strong>Summary:</strong> Robot navigation in dynamic environments shared with humans is an important
but challenging task, which suffers from performance deterioration as the crowd
grows. In this paper, multi-subgoal robot navigation approach based on deep
reinforcement learning is proposed, which can reason about more comprehensive
relationships among all agents (robot and humans). Specifically, the next
position point is planned for the robot by introducing history information and
interactions in our work. Firstly, ba...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.06311v1' target='_blank'>Provably Safe Deep Reinforcement Learning for Robotic Manipulation in
  Human Environments</a></h2>
<p><strong>Authors:</strong> Jakob Thumm, Matthias Althoff</p>
<p><strong>Summary:</strong> Deep reinforcement learning (RL) has shown promising results in the motion
planning of manipulators. However, no method guarantees the safety of highly
dynamic obstacles, such as humans, in RL-based manipulator control. This lack
of formal safety assurances prevents the application of RL for manipulators in
real-world human environments. Therefore, we propose a shielding mechanism that
ensures ISO-verified human safety while training and deploying RL algorithms on
manipulators. We utilize a fast...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.09056v3' target='_blank'>Slowly Changing Adversarial Bandit Algorithms are Efficient for
  Discounted MDPs</a></h2>
<p><strong>Authors:</strong> Ian A. Kash, Lev Reyzin, Zishun Yu</p>
<p><strong>Summary:</strong> Reinforcement learning generalizes multi-armed bandit problems with
additional difficulties of a longer planning horizon and unknown transition
kernel. We explore a black-box reduction from discounted infinite-horizon
tabular reinforcement learning to multi-armed bandits, where, specifically, an
independent bandit learner is placed in each state. We show that, under
ergodicity and fast mixing assumptions, any slowly changing adversarial bandit
algorithm achieving optimal regret in the adversaria...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.11507v1' target='_blank'>Computationally Efficient Horizon-Free Reinforcement Learning for Linear
  Mixture MDPs</a></h2>
<p><strong>Authors:</strong> Dongruo Zhou, Quanquan Gu</p>
<p><strong>Summary:</strong> Recent studies have shown that episodic reinforcement learning (RL) is not
more difficult than contextual bandits, even with a long planning horizon and
unknown state transitions. However, these results are limited to either tabular
Markov decision processes (MDPs) or computationally inefficient algorithms for
linear mixture MDPs. In this paper, we propose the first computationally
efficient horizon-free algorithm for linear mixture MDPs, which achieves the
optimal $\tilde O(d\sqrt{K} +d^2)$ reg...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.13561v3' target='_blank'>Physics-Guided Hierarchical Reward Mechanism for Learning-Based Robotic
  Grasping</a></h2>
<p><strong>Authors:</strong> Yunsik Jung, Lingfeng Tao, Michael Bowman, Jiucai Zhang, Xiaoli Zhang</p>
<p><strong>Summary:</strong> Learning-based grasping can afford real-time grasp motion planning of
multi-fingered robotics hands thanks to its high computational efficiency.
However, learning-based methods are required to explore large search spaces
during the learning process. The search space causes low learning efficiency,
which has been the main barrier to its practical adoption. In addition, the
trained policy lacks a generalizable outcome unless objects are identical to
the trained objects. In this work, we develop a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.01011v2' target='_blank'>Policy Gradient Algorithms with Monte Carlo Tree Learning for Non-Markov
  Decision Processes</a></h2>
<p><strong>Authors:</strong> Tetsuro Morimura, Kazuhiro Ota, Kenshi Abe, Peinan Zhang</p>
<p><strong>Summary:</strong> Policy gradient (PG) is a reinforcement learning (RL) approach that optimizes
a parameterized policy model for an expected return using gradient ascent.
While PG can work well even in non-Markovian environments, it may encounter
plateaus or peakiness issues. As another successful RL approach, algorithms
based on Monte Carlo Tree Search (MCTS), which include AlphaZero, have obtained
groundbreaking results, especially in the game-playing domain. They are also
effective when applied to non-Markov d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02544v1' target='_blank'>RLSS: A Deep Reinforcement Learning Algorithm for Sequential Scene
  Generation</a></h2>
<p><strong>Authors:</strong> Azimkhon Ostonov, Peter Wonka, Dominik L. Michels</p>
<p><strong>Summary:</strong> We present RLSS: a reinforcement learning algorithm for sequential scene
generation. This is based on employing the proximal policy optimization (PPO)
algorithm for generative problems. In particular, we consider how to
effectively reduce the action space by including a greedy search algorithm in
the learning process. Our experiments demonstrate that our method converges for
a relatively large number of actions and learns to generate scenes with
predefined design objectives. This approach is pla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02678v2' target='_blank'>Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR
  and Worst Path</a></h2>
<p><strong>Authors:</strong> Yihan Du, Siwei Wang, Longbo Huang</p>
<p><strong>Summary:</strong> In this paper, we study a novel episodic risk-sensitive Reinforcement
Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail
of the reward-to-go at each step, and focuses on tightly controlling the risk
of getting into catastrophic situations at each stage. This formulation is
applicable to real-world tasks that demand strong risk avoidance throughout the
decision process, such as autonomous driving, clinical treatment planning and
robotics. We investigate two performance...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.05835v1' target='_blank'>Deep Reinforcement Learning for Optimal Investment and Saving Strategy
  Selection in Heterogeneous Profiles: Intelligent Agents working towards
  retirement</a></h2>
<p><strong>Authors:</strong> Fatih Ozhamaratli, Paolo Barucca</p>
<p><strong>Summary:</strong> The transition from defined benefit to defined contribution pension plans
shifts the responsibility for saving toward retirement from governments and
institutions to the individuals. Determining optimal saving and investment
strategy for individuals is paramount for stable financial stance and for
avoiding poverty during work-life and retirement, and it is a particularly
challenging task in a world where form of employment and income trajectory
experienced by different occupation groups are high...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.08569v2' target='_blank'>Bootstrapped Transformer for Offline Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kerong Wang, Hanye Zhao, Xufang Luo, Kan Ren, Weinan Zhang, Dongsheng Li</p>
<p><strong>Summary:</strong> Offline reinforcement learning (RL) aims at learning policies from previously
collected static trajectory data without interacting with the real environment.
Recent works provide a novel perspective by viewing offline RL as a generic
sequence generation problem, adopting sequence models such as Transformer
architecture to model distributions over trajectories, and repurposing beam
search as a planning algorithm. However, the training datasets utilized in
general offline RL tasks are quite limite...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.10464v1' target='_blank'>Hybridization of evolutionary algorithm and deep reinforcement learning
  for multi-objective orienteering optimization</a></h2>
<p><strong>Authors:</strong> Wei Liu, Rui Wang, Tao Zhang, Kaiwen Li, Wenhua Li, Hisao Ishibuchi</p>
<p><strong>Summary:</strong> Multi-objective orienteering problems (MO-OPs) are classical multi-objective
routing problems and have received a lot of attention in the past decades. This
study seeks to solve MO-OPs through a problem-decomposition framework, that is,
a MO-OP is decomposed into a multi-objective knapsack problem (MOKP) and a
travelling salesman problem (TSP). The MOKP and TSP are then solved by a
multi-objective evolutionary algorithm (MOEA) and a deep reinforcement learning
(DRL) method, respectively. While t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.02249v2' target='_blank'>Learning Task Embeddings for Teamwork Adaptation in Multi-Agent
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Lukas Sch√§fer, Filippos Christianos, Amos Storkey, Stefano V. Albrecht</p>
<p><strong>Summary:</strong> Successful deployment of multi-agent reinforcement learning often requires
agents to adapt their behaviour. In this work, we discuss the problem of
teamwork adaptation in which a team of agents needs to adapt their policies to
solve novel tasks with limited fine-tuning. Motivated by the intuition that
agents need to be able to identify and distinguish tasks in order to adapt
their behaviour to the current task, we propose to learn multi-agent task
embeddings (MATE). These task embeddings are tra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.02294v1' target='_blank'>Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Deborah Cohen, Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael Fink, Yossi Matias, Idan Szpektor, Craig Boutilier, Gal Elidan</p>
<p><strong>Summary:</strong> Despite recent advances in natural language understanding and generation, and
decades of research on the development of conversational bots, building
automated agents that can carry on rich open-ended conversations with humans
"in the wild" remains a formidable challenge. In this work we develop a
real-time, open-ended dialogue system that uses reinforcement learning (RL) to
power a bot's conversational skill at scale. Our work pairs the succinct
embedding of the conversation state generated usi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.11535v1' target='_blank'>A model-based approach to meta-Reinforcement Learning: Transformers and
  tree search</a></h2>
<p><strong>Authors:</strong> Brieuc Pinon, Jean-Charles Delvenne, Rapha√´l Jungers</p>
<p><strong>Summary:</strong> Meta-learning is a line of research that develops the ability to leverage
past experiences to efficiently solve new learning problems. Meta-Reinforcement
Learning (meta-RL) methods demonstrate a capability to learn behaviors that
efficiently acquire and exploit information in several meta-RL problems.
  In this context, the Alchemy benchmark has been proposed by Wang et al.
[2021]. Alchemy features a rich structured latent space that is challenging for
state-of-the-art model-free RL methods. The...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.08169v2' target='_blank'>Value Summation: A Novel Scoring Function for MPC-based Model-based
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mehran Raisi, Amirhossein Noohian, Luc Mccutcheon, Saber Fallah</p>
<p><strong>Summary:</strong> This paper proposes a novel scoring function for the planning module of
MPC-based reinforcement learning methods to address the inherent bias of using
the reward function to score trajectories. The proposed method enhances the
learning efficiency of existing MPC-based MBRL methods using the discounted sum
of values. The method utilizes optimal trajectories to guide policy learning
and updates its state-action value function based on real-world and augmented
onboard data. The learning efficiency ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.11789v2' target='_blank'>SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory
  Search with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mario Srouji, Hugues Thomas, Hubert Tsai, Ali Farhadi, Jian Zhang</p>
<p><strong>Summary:</strong> Collision avoidance is key for mobile robots and agents to operate safely in
the real world. In this work we present SAFER, an efficient and effective
collision avoidance system that is able to improve safety by correcting the
control commands sent by an operator. It combines real-world reinforcement
learning (RL), search-based online trajectory planning, and automatic emergency
intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to
learn an effective corrective control a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.00701v2' target='_blank'>Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning
  with Linear Function Approximation</a></h2>
<p><strong>Authors:</strong> Dan Qiao, Yu-Xiang Wang</p>
<p><strong>Summary:</strong> We study the problem of deployment efficient reinforcement learning (RL) with
linear function approximation under the \emph{reward-free} exploration setting.
This is a well-motivated problem because deploying new policies is costly in
real-life RL applications. Under the linear MDP setting with feature dimension
$d$ and planning horizon $H$, we propose a new algorithm that collects at most
$\widetilde{O}(\frac{d^2H^5}{\epsilon^2})$ trajectories within $H$ deployments
to identify $\epsilon$-optim...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.08412v1' target='_blank'>Towards an Interpretable Hierarchical Agent Framework using Semantic
  Goals</a></h2>
<p><strong>Authors:</strong> Bharat Prakash, Nicholas Waytowich, Tim Oates, Tinoosh Mohsenin</p>
<p><strong>Summary:</strong> Learning to solve long horizon temporally extended tasks with reinforcement
learning has been a challenge for several years now. We believe that it is
important to leverage both the hierarchical structure of complex tasks and to
use expert supervision whenever possible to solve such tasks. This work
introduces an interpretable hierarchical agent framework by combining planning
and semantic goal directed reinforcement learning. We assume access to certain
spatial and haptic predicates and constru...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.10639v1' target='_blank'>Robot Navigation with Reinforcement Learned Path Generation and
  Fine-Tuned Motion Control</a></h2>
<p><strong>Authors:</strong> Longyuan Zhang, Ziyue Hou, Ji Wang, Ziang Liu, Wei Li</p>
<p><strong>Summary:</strong> In this paper, we propose a novel reinforcement learning (RL) based path
generation (RL-PG) approach for mobile robot navigation without a prior
exploration of an unknown environment. Multiple predictive path points are
dynamically generated by a deep Markov model optimized using RL approach for
robot to track. To ensure the safety when tracking the predictive points, the
robot's motion is fine-tuned by a motion fine-tuning module. Such an approach,
using the deep Markov model with RL algorithm ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.14524v1' target='_blank'>A Bibliometric Analysis and Review on Reinforcement Learning for
  Transportation Applications</a></h2>
<p><strong>Authors:</strong> Can Li, Lei Bai, Lina Yao, S. Travis Waller, Wei Liu</p>
<p><strong>Summary:</strong> Transportation is the backbone of the economy and urban development.
Improving the efficiency, sustainability, resilience, and intelligence of
transportation systems is critical and also challenging. The constantly
changing traffic conditions, the uncertain influence of external factors (e.g.,
weather, accidents), and the interactions among multiple travel modes and
multi-type flows result in the dynamic and stochastic natures of transportation
systems. The planning, operation, and control of tr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.01004v2' target='_blank'>Spatial-temporal recurrent reinforcement learning for autonomous ships</a></h2>
<p><strong>Authors:</strong> Martin Waltz, Ostap Okhrin</p>
<p><strong>Summary:</strong> This paper proposes a spatial-temporal recurrent neural network architecture
for deep $Q$-networks that can be used to steer an autonomous ship. The network
design makes it possible to handle an arbitrary number of surrounding target
ships while offering robustness to partial observability. Furthermore, a
state-of-the-art collision risk metric is proposed to enable an easier
assessment of different situations by the agent. The COLREG rules of maritime
traffic are explicitly considered in the des...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.02147v3' target='_blank'>A Survey on Reinforcement Learning in Aviation Applications</a></h2>
<p><strong>Authors:</strong> Pouria Razzaghi, Amin Tabrizian, Wei Guo, Shulu Chen, Abenezer Taye, Ellis Thompson, Alexis Bregeon, Ali Baheri, Peng Wei</p>
<p><strong>Summary:</strong> Compared with model-based control and optimization methods, reinforcement
learning (RL) provides a data-driven, learning-based framework to formulate and
solve sequential decision-making problems. The RL framework has become
promising due to largely improved data availability and computing power in the
aviation industry. Many aviation-based applications can be formulated or
treated as sequential decision-making problems. Some of them are offline
planning problems, while others need to be solved ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.11603v1' target='_blank'>Model-based Trajectory Stitching for Improved Offline Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Charles A. Hepburn, Giovanni Montana</p>
<p><strong>Summary:</strong> In many real-world applications, collecting large and high-quality datasets
may be too costly or impractical. Offline reinforcement learning (RL) aims to
infer an optimal decision-making policy from a fixed set of data. Getting the
most information from historical data is then vital for good performance once
the policy is deployed. We propose a model-based data augmentation strategy,
Trajectory Stitching (TS), to improve the quality of sub-optimal historical
trajectories. TS introduces unseen ac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.15920v2' target='_blank'>Discrete Control in Real-World Driving Environments using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Avinash Amballa, Advaith P., Pradip Sasmal, Sumohana Channappayya</p>
<p><strong>Summary:</strong> Training self-driving cars is often challenging since they require a vast
amount of labeled data in multiple real-world contexts, which is
computationally and memory intensive. Researchers often resort to driving
simulators to train the agent and transfer the knowledge to a real-world
setting. Since simulators lack realistic behavior, these methods are quite
inefficient. To address this issue, we introduce a framework (perception,
planning, and control) in a real-world driving environment that t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.01441v2' target='_blank'>Multi-Agent Reinforcement Learning with Reward Delays</a></h2>
<p><strong>Authors:</strong> Yuyang Zhang, Runyu Zhang, Yuantao Gu, Na Li</p>
<p><strong>Summary:</strong> This paper considers multi-agent reinforcement learning (MARL) where the
rewards are received after delays and the delay time varies across agents and
across time steps. Based on the V-learning framework, this paper proposes MARL
algorithms that efficiently deal with reward delays. When the delays are
finite, our algorithm reaches a coarse correlated equilibrium (CCE) with rate
$\tilde{\mathcal{O}}(\frac{H^3\sqrt{S\mathcal{T}_K}}{K}+\frac{H^3\sqrt{SA}}{\sqrt{K}})$
where $K$ is the number of epis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.04680v2' target='_blank'>Near-Optimal Differentially Private Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dan Qiao, Yu-Xiang Wang</p>
<p><strong>Summary:</strong> Motivated by personalized healthcare and other applications involving
sensitive data, we study online exploration in reinforcement learning with
differential privacy (DP) constraints. Existing work on this problem
established that no-regret learning is possible under joint differential
privacy (JDP) and local differential privacy (LDP) but did not provide an
algorithm with optimal regret. We close this gap for the JDP case by designing
an $\epsilon$-JDP algorithm with a regret of
$\widetilde{O}(...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.05698v1' target='_blank'>MoDem: Accelerating Visual Model-Based Reinforcement Learning with
  Demonstrations</a></h2>
<p><strong>Authors:</strong> Nicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, Aravind Rajeswaran</p>
<p><strong>Summary:</strong> Poor sample efficiency continues to be the primary challenge for deployment
of deep Reinforcement Learning (RL) algorithms for real-world applications, and
in particular for visuo-motor control. Model-based RL has the potential to be
highly sample efficient by concurrently learning a world model and using
synthetic rollouts for planning and policy improvement. However, in practice,
sample-efficient learning with model-based RL is bottlenecked by the
exploration challenge. In this work, we find t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.06132v3' target='_blank'>Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision
  Processes</a></h2>
<p><strong>Authors:</strong> Jiafan He, Heyang Zhao, Dongruo Zhou, Quanquan Gu</p>
<p><strong>Summary:</strong> We study reinforcement learning (RL) with linear function approximation. For
episodic time-inhomogeneous linear Markov decision processes (linear MDPs)
whose transition probability can be parameterized as a linear function of a
given feature mapping, we propose the first computationally efficient algorithm
that achieves the nearly minimax optimal regret $\tilde O(d\sqrt{H^3K})$, where
$d$ is the dimension of the feature mapping, $H$ is the planning horizon, and
$K$ is the number of episodes. Our...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.08801v1' target='_blank'>Comparison of Model-Free and Model-Based Learning-Informed Planning for
  PointGoal Navigation</a></h2>
<p><strong>Authors:</strong> Yimeng Li, Arnab Debnath, Gregory J. Stein, Jana Kosecka</p>
<p><strong>Summary:</strong> In recent years several learning approaches to point goal navigation in
previously unseen environments have been proposed. They vary in the
representations of the environments, problem decomposition, and experimental
evaluation. In this work, we compare the state-of-the-art Deep Reinforcement
Learning based approaches with Partially Observable Markov Decision Process
(POMDP) formulation of the point goal navigation problem. We adapt the (POMDP)
sub-goal framework proposed by [1] and modify the c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.09510v1' target='_blank'>Near-optimal Policy Identification in Active Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xiang Li, Viraj Mehta, Johannes Kirschner, Ian Char, Willie Neiswanger, Jeff Schneider, Andreas Krause, Ilija Bogunovic</p>
<p><strong>Summary:</strong> Many real-world reinforcement learning tasks require control of complex
dynamical systems that involve both costly data acquisition processes and large
state spaces. In cases where the transition dynamics can be readily evaluated
at specified states (e.g., via a simulator), agents can operate in what is
often referred to as planning with a \emph{generative model}. We propose the
AE-LSVI algorithm for best-policy identification, a novel variant of the
kernelized least-squares value iteration (LSV...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.03142v1' target='_blank'>Exploration in Model-based Reinforcement Learning with Randomized Reward</a></h2>
<p><strong>Authors:</strong> Lingxiao Wang, Ping Li</p>
<p><strong>Summary:</strong> Model-based Reinforcement Learning (MBRL) has been widely adapted due to its
sample efficiency. However, existing worst-case regret analysis typically
requires optimistic planning, which is not realistic in general. In contrast,
motivated by the theory, empirical study utilizes ensemble of models, which
achieve state-of-the-art performance on various testing environments. Such
deviation between theory and empirical study leads us to question whether
randomized model ensemble guarantee optimism, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.02061v2' target='_blank'>Reinforcement Learning with History-Dependent Dynamic Contexts</a></h2>
<p><strong>Authors:</strong> Guy Tennenholtz, Nadav Merlis, Lior Shani, Martin Mladenov, Craig Boutilier</p>
<p><strong>Summary:</strong> We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel
reinforcement learning framework for history-dependent environments that
generalizes the contextual MDP framework to handle non-Markov environments,
where contexts change over time. We consider special cases of the model, with a
focus on logistic DCMDPs, which break the exponential dependence on history
length by leveraging aggregation functions to determine context transitions.
This special structure allows us to derive...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.05063v1' target='_blank'>Improving Zero-Shot Coordination Performance Based on Policy Similarity</a></h2>
<p><strong>Authors:</strong> Lebin Yu, Yunbo Qiu, Quanming Yao, Xudong Zhang, Jian Wang</p>
<p><strong>Summary:</strong> Over these years, multi-agent reinforcement learning has achieved remarkable
performance in multi-agent planning and scheduling tasks. It typically follows
the self-play setting, where agents are trained by playing with a fixed group
of agents. However, in the face of zero-shot coordination, where an agent must
coordinate with unseen partners, self-play agents may fail. Several methods
have been proposed to handle this problem, but they either take a lot of time
or lack generalizability. In this...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.09522v1' target='_blank'>Interactive Video Corpus Moment Retrieval using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhixin Ma, Chong-Wah Ngo</p>
<p><strong>Summary:</strong> Known-item video search is effective with human-in-the-loop to interactively
investigate the search result and refine the initial query. Nevertheless, when
the first few pages of results are swamped with visually similar items, or the
search target is hidden deep in the ranked list, finding the know-item target
usually requires a long duration of browsing and result inspection. This paper
tackles the problem by reinforcement learning, aiming to reach a search target
within a few rounds of intera...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.10850v2' target='_blank'>Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management</a></h2>
<p><strong>Authors:</strong> Dhawal Gupta, Yinlam Chow, Aza Tulepbergenov, Mohammad Ghavamzadeh, Craig Boutilier</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has shown great promise for developing dialogue
management (DM) agents that are non-myopic, conduct rich conversations, and
maximize overall user satisfaction. Despite recent developments in RL and
language models (LMs), using RL to power conversational chatbots remains
challenging, in part because RL requires online exploration to learn
effectively, whereas collecting novel human-bot interactions can be expensive
and unsafe. This issue is exacerbated by the combinato...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.11634v1' target='_blank'>Provably Efficient Reinforcement Learning via Surprise Bound</a></h2>
<p><strong>Authors:</strong> Hanlin Zhu, Ruosong Wang, Jason D. Lee</p>
<p><strong>Summary:</strong> Value function approximation is important in modern reinforcement learning
(RL) problems especially when the state space is (infinitely) large. Despite
the importance and wide applicability of value function approximation, its
theoretical understanding is still not as sophisticated as its empirical
success, especially in the context of general function approximation. In this
paper, we propose a provably efficient RL algorithm (both computationally and
statistically) with general value function a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.01305v1' target='_blank'>PyFlyt -- UAV Simulation Environments for Reinforcement Learning
  Research</a></h2>
<p><strong>Authors:</strong> Jun Jet Tai, Jim Wong, Mauro Innocente, Nadjim Horri, James Brusey, Swee King Phang</p>
<p><strong>Summary:</strong> Unmanned aerial vehicles (UAVs) have numerous applications, but their
efficient and optimal flight can be a challenge. Reinforcement Learning (RL)
has emerged as a promising approach to address this challenge, yet there is no
standardized library for testing and benchmarking RL algorithms on UAVs. In
this paper, we introduce PyFlyt, a platform built on the Bullet physics engine
with native Gymnasium API support. PyFlyt provides modular implementations of
simple components, such as motors and lif...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.04150v3' target='_blank'>RoboPianist: Dexterous Piano Playing with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kevin Zakka, Philipp Wu, Laura Smith, Nimrod Gileadi, Taylor Howell, Xue Bin Peng, Sumeet Singh, Yuval Tassa, Pete Florence, Andy Zeng, Pieter Abbeel</p>
<p><strong>Summary:</strong> Replicating human-like dexterity in robot hands represents one of the largest
open problems in robotics. Reinforcement learning is a promising approach that
has achieved impressive progress in the last few years; however, the class of
problems it has typically addressed corresponds to a rather narrow definition
of dexterity as compared to human capabilities. To address this gap, we
investigate piano-playing, a skill that challenges even the human limits of
dexterity, as a means to test high-dime...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.05839v1' target='_blank'>Optimal Interpretability-Performance Trade-off of Classification Trees
  with Black-Box Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hector Kohler, Riad Akrour, Philippe Preux</p>
<p><strong>Summary:</strong> Interpretability of AI models allows for user safety checks to build trust in
these models. In particular, decision trees (DTs) provide a global view on the
learned model and clearly outlines the role of the features that are critical
to classify a given data. However, interpretability is hindered if the DT is
too large. To learn compact trees, a Reinforcement Learning (RL) framework has
been recently proposed to explore the space of DTs. A given supervised
classification task is modeled as a Ma...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.09434v1' target='_blank'>Torque-based Deep Reinforcement Learning for Task-and-Robot Agnostic
  Learning on Bipedal Robots Using Sim-to-Real Transfer</a></h2>
<p><strong>Authors:</strong> Donghyeon Kim, Glen Berseth, Mathew Schwartz, Jaeheung Park</p>
<p><strong>Summary:</strong> In this paper, we review the question of which action space is best suited
for controlling a real biped robot in combination with Sim2Real training.
Position control has been popular as it has been shown to be more sample
efficient and intuitive to combine with other planning algorithms. However, for
position control gain tuning is required to achieve the best possible policy
performance. We show that instead, using a torque-based action space enables
task-and-robot agnostic learning with less p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.04750v1' target='_blank'>Sense, Imagine, Act: Multimodal Perception Improves Model-Based
  Reinforcement Learning for Head-to-Head Autonomous Racing</a></h2>
<p><strong>Authors:</strong> Elena Shrestha, Chetan Reddy, Hanxi Wan, Yulun Zhuang, Ram Vasudevan</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) techniques have recently yielded
promising results for real-world autonomous racing using high-dimensional
observations. MBRL agents, such as Dreamer, solve long-horizon tasks by
building a world model and planning actions by latent imagination. This
approach involves explicitly learning a model of the system dynamics and using
it to learn the optimal policy for continuous control over multiple timesteps.
As a result, MBRL agents may converge to sub-opti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.12687v2' target='_blank'>Learn to Flap: Foil Non-parametric Path Planning via Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Z. P. Wang, R. J. Lin, Z. Y. Zhao, P. M. Guo, N. Yang, D. X. Fan</p>
<p><strong>Summary:</strong> To optimize flapping foil performance, the application of deep reinforcement
learning (DRL) on controlling foil non-parametric motion is conducted in the
present study. Traditional control techniques and simplified motions cannot
fully model nonlinear, unsteady and high-dimensional foil-vortex interactions.
A DRL-training framework based on Proximal Policy Optimization and Transformer
architecture is proposed. The policy is initialized from the sinusoidal expert
display. We first demonstrate the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.13206v1' target='_blank'>Know your Enemy: Investigating Monte-Carlo Tree Search with Opponent
  Models in Pommerman</a></h2>
<p><strong>Authors:</strong> Jannis Weil, Johannes Czech, Tobias Meuser, Kristian Kersting</p>
<p><strong>Summary:</strong> In combination with Reinforcement Learning, Monte-Carlo Tree Search has shown
to outperform human grandmasters in games such as Chess, Shogi and Go with
little to no prior domain knowledge. However, most classical use cases only
feature up to two players. Scaling the search to an arbitrary number of players
presents a computational challenge, especially if decisions have to be planned
over a longer time horizon. In this work, we investigate techniques that
transform general-sum multiplayer games...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.16505v1' target='_blank'>Reward-Machine-Guided, Self-Paced Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Cevahir Koprulu, Ufuk Topcu</p>
<p><strong>Summary:</strong> Self-paced reinforcement learning (RL) aims to improve the data efficiency of
learning by automatically creating sequences, namely curricula, of probability
distributions over contexts. However, existing techniques for self-paced RL
fail in long-horizon planning tasks that involve temporally extended behaviors.
We hypothesize that taking advantage of prior knowledge about the underlying
task structure can improve the effectiveness of self-paced RL. We develop a
self-paced RL algorithm guided by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.03220v2' target='_blank'>Risk-Aware Reward Shaping of Reinforcement Learning Agents for
  Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Lin-Chi Wu, Zengjie Zhang, Sofie Haesaert, Zhiqiang Ma, Zhiyong Sun</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is an effective approach to motion planning in
autonomous driving, where an optimal driving policy can be automatically
learned using the interaction data with the environment. Nevertheless, the
reward function for an RL agent, which is significant to its performance, is
challenging to be determined. The conventional work mainly focuses on rewarding
safe driving states but does not incorporate the awareness of risky driving
behaviors of the vehicles. In this paper, we...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.06253v2' target='_blank'>Decision Stacks: Flexible Reinforcement Learning via Modular Generative
  Models</a></h2>
<p><strong>Authors:</strong> Siyan Zhao, Aditya Grover</p>
<p><strong>Summary:</strong> Reinforcement learning presents an attractive paradigm to reason about
several distinct aspects of sequential decision making, such as specifying
complex goals, planning future observations and actions, and critiquing their
utilities. However, the combined integration of these capabilities poses
competing algorithmic challenges in retaining maximal expressivity while
allowing for flexibility in modeling choices for efficient learning and
inference. We present Decision Stacks, a generative framew...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.08810v2' target='_blank'>Deep Generative Models for Decision-Making and Control</a></h2>
<p><strong>Authors:</strong> Michael Janner</p>
<p><strong>Summary:</strong> Deep model-based reinforcement learning methods offer a conceptually simple
approach to the decision-making and control problem: use learning for the
purpose of estimating an approximate dynamics model, and offload the rest of
the work to classical trajectory optimization. However, this combination has a
number of empirical shortcomings, limiting the usefulness of model-based
methods in practice. The dual purpose of this thesis is to study the reasons
for these shortcomings and to propose soluti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.05209v4' target='_blank'>Contextual Pre-planning on Reward Machine Abstractions for Enhanced
  Transfer in Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Guy Azran, Mohamad H. Danesh, Stefano V. Albrecht, Sarah Keren</p>
<p><strong>Summary:</strong> Recent studies show that deep reinforcement learning (DRL) agents tend to
overfit to the task on which they were trained and fail to adapt to minor
environment changes. To expedite learning when transferring to unseen tasks, we
propose a novel approach to representing the current task using reward machines
(RMs), state machine abstractions that induce subtasks based on the current
task's rewards and dynamics. Our method provides agents with symbolic
representations of optimal transitions from th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.11704v2' target='_blank'>JoinGym: An Efficient Query Optimization Environment for Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Kaiwen Wang, Junxiong Wang, Yueying Li, Nathan Kallus, Immanuel Trummer, Wen Sun</p>
<p><strong>Summary:</strong> Join order selection (JOS) is the problem of ordering join operations to
minimize total query execution cost and it is the core NP-hard combinatorial
optimization problem of query optimization. In this paper, we present JoinGym,
a lightweight and easy-to-use query optimization environment for reinforcement
learning (RL) that captures both the left-deep and bushy variants of the JOS
problem. Compared to existing query optimization environments, the key
advantages of JoinGym are usability and sign...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.12143v1' target='_blank'>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</p>
<p><strong>Summary:</strong> Adapting to regularities of the environment is critical for biological
organisms to anticipate events and plan. A prominent example is the circadian
rhythm corresponding to the internalization by organisms of the $24$-hour
period of the Earth's rotation. In this work, we study the emergence of
circadian-like rhythms in deep reinforcement learning agents. In particular, we
deployed agents in an environment with a reliable periodic variation while
solving a foraging task. We systematically charact...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.06973v1' target='_blank'>Routing Recovery for UAV Networks with Deliberate Attacks: A
  Reinforcement Learning based Approach</a></h2>
<p><strong>Authors:</strong> Sijie He, Ziye Jia, Chao Dong, Wei Wang, Yilu Cao, Yang Yang, Qihui Wu</p>
<p><strong>Summary:</strong> The unmanned aerial vehicle (UAV) network is popular these years due to its
various applications. In the UAV network, routing is significantly affected by
the distributed network topology, leading to the issue that UAVs are vulnerable
to deliberate damage. Hence, this paper focuses on the routing plan and
recovery for UAV networks with attacks. In detail, a deliberate attack model
based on the importance of nodes is designed to represent enemy attacks. Then,
a node importance ranking mechanism i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.02671v3' target='_blank'>RLSynC: Offline-Online Reinforcement Learning for Synthon Completion</a></h2>
<p><strong>Authors:</strong> Frazier N. Baker, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning</p>
<p><strong>Summary:</strong> Retrosynthesis is the process of determining the set of reactant molecules
that can react to form a desired product. Semi-template-based retrosynthesis
methods, which imitate the reverse logic of synthesis reactions, first predict
the reaction centers in the products, and then complete the resulting synthons
back into reactants. We develop a new offline-online reinforcement learning
method RLSynC for synthon completion in semi-template-based methods. RLSynC
assigns one agent to each synthon, all...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.02722v1' target='_blank'>Reinforcement Learning of Action and Query Policies with LTL
  Instructions under Uncertain Event Detector</a></h2>
<p><strong>Authors:</strong> Wataru Hatanaka, Ryota Yamashina, Takamitsu Matsubara</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) with linear temporal logic (LTL) objectives can
allow robots to carry out symbolic event plans in unknown environments. Most
existing methods assume that the event detector can accurately map
environmental states to symbolic events; however, uncertainty is inevitable for
real-world event detectors. Such uncertainty in an event detector generates
multiple branching possibilities on LTL instructions, confusing action
decisions. Moreover, the queries to the uncertain eve...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.02997v2' target='_blank'>Multi-log grasping using reinforcement learning and virtual visual
  servoing</a></h2>
<p><strong>Authors:</strong> Erik Wallin, Viktor Wiberg, Martin Servin</p>
<p><strong>Summary:</strong> We explore multi-log grasping using reinforcement learning and virtual visual
servoing for automated forwarding in a simulated environment. Automation of
forest processes is a major challenge, and many techniques regarding robot
control pose different challenges due to the unstructured and harsh outdoor
environment. Grasping multiple logs involves various problems of dynamics and
path planning, where understanding the interaction between the grapple, logs,
terrain, and obstacles requires visual ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.06021v1' target='_blank'>Emergent Communication in Multi-Agent Reinforcement Learning for Future
  Wireless Networks</a></h2>
<p><strong>Authors:</strong> Marwa Chafii, Salmane Naoumi, Reda Alami, Ebtesam Almazrouei, Mehdi Bennis, Merouane Debbah</p>
<p><strong>Summary:</strong> In different wireless network scenarios, multiple network entities need to
cooperate in order to achieve a common task with minimum delay and energy
consumption. Future wireless networks mandate exchanging high dimensional data
in dynamic and uncertain environments, therefore implementing communication
control tasks becomes challenging and highly complex. Multi-agent reinforcement
learning with emergent communication (EC-MARL) is a promising solution to
address high dimensional continuous contro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.06687v2' target='_blank'>Self-Refined Large Language Model as Automated Reward Function Designer
  for Deep Reinforcement Learning in Robotics</a></h2>
<p><strong>Authors:</strong> Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, Lei Ma</p>
<p><strong>Summary:</strong> Although Deep Reinforcement Learning (DRL) has achieved notable success in
numerous robotic applications, designing a high-performing reward function
remains a challenging task that often requires substantial manual input.
Recently, Large Language Models (LLMs) have been extensively adopted to address
tasks demanding in-depth common-sense knowledge, such as reasoning and
planning. Recognizing that reward function design is also inherently linked to
such knowledge, LLM offers a promising potentia...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.09942v1' target='_blank'>OptiRoute: A Heuristic-assisted Deep Reinforcement Learning Framework
  for UAV-UGV Collaborative Route Planning</a></h2>
<p><strong>Authors:</strong> Md Safwan Mondal, Subramanian Ramasamy, Pranav Bhounsule</p>
<p><strong>Summary:</strong> Unmanned aerial vehicles (UAVs) are capable of surveying expansive areas, but
their operational range is constrained by limited battery capacity. The
deployment of mobile recharging stations using unmanned ground vehicles (UGVs)
significantly extends the endurance and effectiveness of UAVs. However,
optimizing the routes of both UAVs and UGVs, known as the UAV-UGV cooperative
routing problem, poses substantial challenges, particularly with respect to the
selection of recharging locations. Here i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.06208v2' target='_blank'>Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot
  Collaboration</a></h2>
<p><strong>Authors:</strong> Jakob Thumm, Felix Trost, Matthias Althoff</p>
<p><strong>Summary:</strong> Deep reinforcement learning (RL) has shown promising results in robot motion
planning with first attempts in human-robot collaboration (HRC). However, a
fair comparison of RL approaches in HRC under the constraint of guaranteed
safety is yet to be made. We, therefore, present human-robot gym, a benchmark
suite for safe RL in HRC. Our benchmark suite provides eight challenging,
realistic HRC tasks in a modular simulation framework. Most importantly,
human-robot gym includes a safety shield that p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.09971v4' target='_blank'>AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents</a></h2>
<p><strong>Authors:</strong> Jake Grigsby, Linxi Fan, Yuke Zhu</p>
<p><strong>Summary:</strong> We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses
sequence models to tackle the challenges of generalization, long-term memory,
and meta-learning. Recent works have shown that off-policy learning can make
in-context RL with recurrent policies viable. Nonetheless, these approaches
require extensive tuning and limit scalability by creating key bottlenecks in
agents' memory capacity, planning horizon, and model size. AMAGO revisits and
redesigns the off-policy in-context...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.18354v1' target='_blank'>A Review of Reinforcement Learning for Natural Language Processing, and
  Applications in Healthcare</a></h2>
<p><strong>Authors:</strong> Ying Liu, Haozhu Wang, Huixue Zhou, Mingchen Li, Yu Hou, Sicheng Zhou, Fang Wang, Rama Hoetzlein, Rui Zhang</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has emerged as a powerful approach for tackling
complex medical decision-making problems such as treatment planning,
personalized medicine, and optimizing the scheduling of surgeries and
appointments. It has gained significant attention in the field of Natural
Language Processing (NLP) due to its ability to learn optimal strategies for
tasks such as dialogue systems, machine translation, and question-answering.
This paper presents a review of the RL techniques in NLP,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.01450v2' target='_blank'>DreamSmooth: Improving Model-based Reinforcement Learning via Reward
  Smoothing</a></h2>
<p><strong>Authors:</strong> Vint Lee, Pieter Abbeel, Youngwoon Lee</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) has gained much attention for its
ability to learn complex behaviors in a sample-efficient way: planning actions
by generating imaginary trajectories with predicted rewards. Despite its
success, we found that surprisingly, reward prediction is often a bottleneck of
MBRL, especially for sparse rewards that are challenging (or even ambiguous) to
predict. Motivated by the intuition that humans can learn from rough reward
estimates, we propose a simple yet e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.15238v1' target='_blank'>A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning
  with General Function Approximation</a></h2>
<p><strong>Authors:</strong> Heyang Zhao, Jiafan He, Quanquan Gu</p>
<p><strong>Summary:</strong> The exploration-exploitation dilemma has been a central challenge in
reinforcement learning (RL) with complex model classes. In this paper, we
propose a new algorithm, Monotonic Q-Learning with Upper Confidence Bound
(MQL-UCB) for RL with general function approximation. Our key algorithmic
design includes (1) a general deterministic policy-switching strategy that
achieves low switching cost, (2) a monotonic value function structure with
carefully controlled function class complexity, and (3) a v...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.17855v1' target='_blank'>Maximum Entropy Model Correction in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Amin Rakhsha, Mete Kemertas, Mohammad Ghavamzadeh, Amir-massoud Farahmand</p>
<p><strong>Summary:</strong> We propose and theoretically analyze an approach for planning with an
approximate model in reinforcement learning that can reduce the adverse impact
of model error. If the model is accurate enough, it accelerates the convergence
to the true value function too. One of its key components is the MaxEnt Model
Correction (MoCo) procedure that corrects the model's next-state distributions
based on a Maximum Entropy density estimation formulation. Based on MoCo, we
introduce the Model Correcting Value ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.06406v2' target='_blank'>Partial End-to-end Reinforcement Learning for Robustness Against
  Modelling Error in Autonomous Racing</a></h2>
<p><strong>Authors:</strong> Andrew Murdoch, Johannes Cornelius Schoeman, Hendrik Willem Jordaan</p>
<p><strong>Summary:</strong> In this paper, we address the issue of increasing the performance of
reinforcement learning (RL) solutions for autonomous racing cars when
navigating under conditions where practical vehicle modelling errors (commonly
known as \emph{model mismatches}) are present. To address this challenge, we
propose a partial end-to-end algorithm that decouples the planning and control
tasks. Within this framework, an RL agent generates a trajectory comprising a
path and velocity, which is subsequently tracked...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.08041v1' target='_blank'>Leveraging User Simulation to Develop and Evaluate Conversational
  Information Access Agents</a></h2>
<p><strong>Authors:</strong> Nolwenn Bernard</p>
<p><strong>Summary:</strong> We observe a change in the way users access information, that is, the rise of
conversational information access (CIA) agents. However, the automatic
evaluation of these agents remains an open challenge. Moreover, the training of
CIA agents is cumbersome as it mostly relies on conversational corpora, expert
knowledge, and reinforcement learning. User simulation has been identified as a
promising solution to tackle automatic evaluation and has been previously used
in reinforcement learning. In thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.06603v1' target='_blank'>Mutual Enhancement of Large Language and Reinforcement Learning Models
  through Bi-Directional Feedback Mechanisms: A Case Study</a></h2>
<p><strong>Authors:</strong> Shangding Gu</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities for
reinforcement learning (RL) models, such as planning and reasoning
capabilities. However, the problems of LLMs and RL model collaboration still
need to be solved. In this study, we employ a teacher-student learning
framework to tackle these problems, specifically by offering feedback for LLMs
using RL models and providing high-level information for RL models with LLMs in
a cooperative multi-agent setting. Within this fram...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.12537v2' target='_blank'>Motion Hologram: Jointly optimized hologram generation and motion
  planning for photorealistic and speckle-free 3D displays via reinforcement
  learning</a></h2>
<p><strong>Authors:</strong> Zhenxing Dong, Yuye Ling, Yan Li, Yikai Su</p>
<p><strong>Summary:</strong> Holography is capable of rendering three-dimensional scenes with full-depth
control, and delivering transformative experiences across numerous domains,
including virtual and augmented reality, education, and communication. However,
traditional holography presents 3D scenes with unnatural defocus and severe
speckles due to the limited space bandwidth product of the spatial light
modulator (SLM). Here, we introduce Motion Hologram, a novel holographic
technique to accurately portray photorealistic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.11650v2' target='_blank'>Programmatic Reinforcement Learning: Navigating Gridworlds</a></h2>
<p><strong>Authors:</strong> Guruprerana Shabadi, Nathana√´l Fijalkow, Th√©o Matricon</p>
<p><strong>Summary:</strong> The field of reinforcement learning (RL) is concerned with algorithms for
learning optimal policies in unknown stochastic environments. Programmatic RL
studies representations of policies as programs, meaning involving higher order
constructs such as control loops. Despite attracting a lot of attention at the
intersection of the machine learning and formal methods communities, very
little is known on the theoretical front about programmatic RL: what are good
classes of programmatic policies? How...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.12914v1' target='_blank'>Large Language Model-based Human-Agent Collaboration for Complex Task
  Solving</a></h2>
<p><strong>Authors:</strong> Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen</p>
<p><strong>Summary:</strong> In recent developments within the research community, the integration of
Large Language Models (LLMs) in creating fully autonomous agents has garnered
significant interest. Despite this, LLM-based agents frequently demonstrate
notable shortcomings in adjusting to dynamic environments and fully grasping
human needs. In this work, we introduce the problem of LLM-based human-agent
collaboration for complex task-solving, exploring their synergistic potential.
In addition, we propose a Reinforcement ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.17003v2' target='_blank'>Monitoring Fidelity of Online Reinforcement Learning Algorithms in
  Clinical Trials</a></h2>
<p><strong>Authors:</strong> Anna L. Trella, Kelly W. Zhang, Inbal Nahum-Shani, Vivek Shetty, Iris Yan, Finale Doshi-Velez, Susan A. Murphy</p>
<p><strong>Summary:</strong> Online reinforcement learning (RL) algorithms offer great potential for
personalizing treatment for participants in clinical trials. However, deploying
an online, autonomous algorithm in the high-stakes healthcare setting makes
quality control and data quality especially difficult to achieve. This paper
proposes algorithm fidelity as a critical requirement for deploying online RL
algorithms in clinical trials. It emphasizes the responsibility of the
algorithm to (1) safeguard participants and (2...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.19299v1' target='_blank'>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</a></h2>
<p><strong>Authors:</strong> Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated proficiency in utilizing
various tools by coding, yet they face limitations in handling intricate logic
and precise control. In embodied tasks, high-level planning is amenable to
direct coding, while low-level actions often necessitate task-specific
refinement, such as Reinforcement Learning (RL). To seamlessly integrate both
modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising
a slow agent and a fast agent. The slow agent...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.06571v2' target='_blank'>Scalable Online Exploration via Coverability</a></h2>
<p><strong>Authors:</strong> Philip Amortila, Dylan J. Foster, Akshay Krishnamurthy</p>
<p><strong>Summary:</strong> Exploration is a major challenge in reinforcement learning, especially for
high-dimensional domains that require function approximation. We propose
exploration objectives -- policy optimization objectives that enable downstream
maximization of any reward function -- as a conceptual framework to systematize
the study of exploration. Within this framework, we introduce a new objective,
$L_1$-Coverage, which generalizes previous exploration schemes and supports
three fundamental desiderata:
  1. In...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.08687v1' target='_blank'>Digital Twin-assisted Reinforcement Learning for Resource-aware
  Microservice Offloading in Edge Computing</a></h2>
<p><strong>Authors:</strong> Xiangchun Chen, Jiannong Cao, Zhixuan Liang, Yuvraj Sahni, Mingjin Zhang</p>
<p><strong>Summary:</strong> Collaborative edge computing (CEC) has emerged as a promising paradigm,
enabling edge nodes to collaborate and execute microservices from end devices.
Microservice offloading, a fundamentally important problem, decides when and
where microservices are executed upon the arrival of services. However, the
dynamic nature of the real-world CEC environment often leads to inefficient
microservice offloading strategies, resulting in underutilized resources and
network congestion. To address this challen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.15648v2' target='_blank'>SRLM: Human-in-Loop Interactive Social Robot Navigation with Large
  Language Model and Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Weizheng Wang, Ike Obi, Byung-Cheol Min</p>
<p><strong>Summary:</strong> An interactive social robotic assistant must provide services in complex and
crowded spaces while adapting its behavior based on real-time human language
commands or feedback. In this paper, we propose a novel hybrid approach called
Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and
Deep Reinforcement Learning (DRL) to navigate through human-filled public
spaces and provide multiple social services. SRLM infers global planning from
human-in-loop commands in real-time, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.18524v1' target='_blank'>Bridging the Gap: Regularized Reinforcement Learning for Improved
  Classical Motion Planning with Safety Modules</a></h2>
<p><strong>Authors:</strong> Elias Goldsztejn, Ronen I. Brafman</p>
<p><strong>Summary:</strong> Classical navigation planners can provide safe navigation, albeit often
suboptimally and with hindered human norm compliance. ML-based, contemporary
autonomous navigation algorithms can imitate more natural and humancompliant
navigation, but usually require large and realistic datasets and do not always
provide safety guarantees. We present an approach that leverages a classical
algorithm to guide reinforcement learning. This greatly improves the results
and convergence rate of the underlying RL...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.05203v1' target='_blank'>MeSA-DRL: Memory-Enhanced Deep Reinforcement Learning for Advanced
  Socially Aware Robot Navigation in Crowded Environments</a></h2>
<p><strong>Authors:</strong> Mannan Saeed Muhammad, Estrella Montero</p>
<p><strong>Summary:</strong> Autonomous navigation capabilities play a critical role in service robots
operating in environments where human interactions are pivotal, due to the
dynamic and unpredictable nature of these environments. However, the
variability in human behavior presents a substantial challenge for robots in
predicting and anticipating movements, particularly in crowded scenarios. To
address this issue, a memory-enabled deep reinforcement learning framework is
proposed for autonomous robot navigation in divers...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.05894v3' target='_blank'>Learning Heuristics for Transit Network Design and Improvement with Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Andrew Holliday, Ahmed El-Geneidy, Gregory Dudek</p>
<p><strong>Summary:</strong> Transit agencies world-wide face tightening budgets. To maintain quality of
service while cutting costs, efficient transit network design is essential. But
planning a network of public transit routes is a challenging optimization
problem. The most successful approaches to date use metaheuristic algorithms to
search through the space of possible transit networks by applying low-level
heuristics that randomly alter routes in a network. The design of these
low-level heuristics has a major impact on...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.13678v1' target='_blank'>Adaptive Social Force Window Planner with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mauro Martini, No√© P√©rez-Higueras, Andrea Ostuni, Marcello Chiaberge, Fernando Caballero, Luis Merino</p>
<p><strong>Summary:</strong> Human-aware navigation is a complex task for mobile robots, requiring an
autonomous navigation system capable of achieving efficient path planning
together with socially compliant behaviors. Social planners usually add costs
or constraints to the objective function, leading to intricate tuning processes
or tailoring the solution to the specific social scenario. Machine Learning can
enhance planners' versatility and help them learn complex social behaviors from
data. This work proposes an adaptiv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.19370v1' target='_blank'>Numeric Reward Machines</a></h2>
<p><strong>Authors:</strong> Kristina Levina, Nikolaos Pappas, Athanasios Karapantelakis, Aneta Vulgarakis Feljan, Jendrik Seipp</p>
<p><strong>Summary:</strong> Reward machines inform reinforcement learning agents about the reward
structure of the environment and often drastically speed up the learning
process. However, reward machines only accept Boolean features such as
robot-reached-gold. Consequently, many inherently numeric tasks cannot profit
from the guidance offered by reward machines. To address this gap, we aim to
extend reward machines with numeric features such as distance-to-gold. For
this, we present two types of reward machines: numeric-B...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.06771v1' target='_blank'>Space Processor Computation Time Analysis for Reinforcement Learning and
  Run Time Assurance Control Policies</a></h2>
<p><strong>Authors:</strong> Kyle Dunlap, Nathaniel Hamilton, Francisco Viramontes, Derrek Landauer, Evan Kain, Kerianne L. Hobbs</p>
<p><strong>Summary:</strong> As the number of spacecraft on orbit continues to grow, it is challenging for
human operators to constantly monitor and plan for all missions. Autonomous
control methods such as reinforcement learning (RL) have the power to solve
complex tasks while reducing the need for constant operator intervention. By
combining RL solutions with run time assurance (RTA), safety of these systems
can be assured in real time. However, in order to use these algorithms on board
a spacecraft, they must be able to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.08691v3' target='_blank'>Enhancing Reinforcement Learning in Sensor Fusion: A Comparative
  Analysis of Cubature and Sampling-based Integration Methods for Rover Search
  Planning</a></h2>
<p><strong>Authors:</strong> Jan-Hendrik Ewers, Sarah Swinton, David Anderson, Euan McGookin, Douglas Thomson</p>
<p><strong>Summary:</strong> This study investigates the computational speed and accuracy of two numerical
integration methods, cubature and sampling-based, for integrating an integrand
over a 2D polygon. Using a group of rovers searching the Martian surface with a
limited sensor footprint as a test bed, the relative error and computational
time are compared as the area was subdivided to improve accuracy in the
sampling-based approach. The results show that the sampling-based approach
exhibits a $14.75\%$ deviation in relat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.17031v1' target='_blank'>Any-step Dynamics Model Improves Future Predictions for Online and
  Offline Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haoxin Lin, Yu-Yan Xu, Yihao Sun, Zhilong Zhang, Yi-Chen Li, Chengxing Jia, Junyin Ye, Jiaji Zhang, Yang Yu</p>
<p><strong>Summary:</strong> Model-based methods in reinforcement learning offer a promising approach to
enhance data efficiency by facilitating policy exploration within a dynamics
model. However, accurately predicting sequential steps in the dynamics model
remains a challenge due to the bootstrapping prediction, which attributes the
next state to the prediction of the current state. This leads to accumulated
errors during model roll-out. In this paper, we propose the Any-step Dynamics
Model (ADM) to mitigate the compoundi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01853v1' target='_blank'>Multi-Agent Reinforcement Learning Meets Leaf Sequencing in Radiotherapy</a></h2>
<p><strong>Authors:</strong> Riqiang Gao, Florin C. Ghesu, Simon Arberet, Shahab Basiri, Esa Kuusela, Martin Kraus, Dorin Comaniciu, Ali Kamen</p>
<p><strong>Summary:</strong> In contemporary radiotherapy planning (RTP), a key module leaf sequencing is
predominantly addressed by optimization-based approaches. In this paper, we
propose a novel deep reinforcement learning (DRL) model termed as Reinforced
Leaf Sequencer (RLS) in a multi-agent framework for leaf sequencing. The RLS
model offers improvements to time-consuming iterative optimization steps via
large-scale training and can control movement patterns through the design of
reward mechanisms. We have conducted ex...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.02258v2' target='_blank'>Reinforcement Learning with Lookahead Information</a></h2>
<p><strong>Authors:</strong> Nadav Merlis</p>
<p><strong>Summary:</strong> We study reinforcement learning (RL) problems in which agents observe the
reward or transition realizations at their current state before deciding which
action to take. Such observations are available in many applications, including
transactions, navigation and more. When the environment is known, previous work
shows that this lookahead information can drastically increase the collected
reward. However, outside of specific applications, existing approaches for
interacting with unknown environmen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.16707v1' target='_blank'>Probabilistic Subgoal Representations for Hierarchical Reinforcement
  learning</a></h2>
<p><strong>Authors:</strong> Vivienne Huiling Wang, Tinghuai Wang, Wenyan Yang, Joni-Kristian K√§m√§r√§inen, Joni Pajarinen</p>
<p><strong>Summary:</strong> In goal-conditioned hierarchical reinforcement learning (HRL), a high-level
policy specifies a subgoal for the low-level policy to reach. Effective HRL
hinges on a suitable subgoal represen tation function, abstracting state space
into latent subgoal space and inducing varied low-level behaviors. Existing
methods adopt a subgoal representation that provides a deterministic mapping
from state space to latent subgoal space. Instead, this paper utilizes Gaussian
Processes (GPs) for the first probab...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.00741v6' target='_blank'>Diffusion Models for Offline Multi-agent Reinforcement Learning with
  Safety Constraints</a></h2>
<p><strong>Authors:</strong> Jianuo Huang</p>
<p><strong>Summary:</strong> In recent advancements in Multi-agent Reinforcement Learning (MARL), its
application has extended to various safety-critical scenarios. However, most
methods focus on online learning, which presents substantial risks when
deployed in real-world settings. Addressing this challenge, we introduce an
innovative framework integrating diffusion models within the MARL paradigm.
This approach notably enhances the safety of actions taken by multiple agents
through risk mitigation while modeling coordinat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.16220v1' target='_blank'>ODGR: Online Dynamic Goal Recognition</a></h2>
<p><strong>Authors:</strong> Matan Shamir, Osher Elhadad, Matthew E. Taylor, Reuth Mirsky</p>
<p><strong>Summary:</strong> Traditionally, Reinforcement Learning (RL) problems are aimed at optimization
of the behavior of an agent. This paper proposes a novel take on RL, which is
used to learn the policy of another agent, to allow real-time recognition of
that agent's goals. Goal Recognition (GR) has traditionally been framed as a
planning problem where one must recognize an agent's objectives based on its
observed actions. Recent approaches have shown how reinforcement learning can
be used as part of the GR pipeline,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.02069v2' target='_blank'>A Deployed Online Reinforcement Learning Algorithm In An Oral Health
  Clinical Trial</a></h2>
<p><strong>Authors:</strong> Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy</p>
<p><strong>Summary:</strong> Dental disease is a prevalent chronic condition associated with substantial
financial burden, personal suffering, and increased risk of systemic diseases.
Despite widespread recommendations for twice-daily tooth brushing, adherence to
recommended oral self-care behaviors remains sub-optimal due to factors such as
forgetfulness and disengagement. To address this, we developed Oralytics, a
mHealth intervention system designed to complement clinician-delivered
preventative care for marginalized ind...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.10832v1' target='_blank'>DIGIMON: Diagnosis and Mitigation of Sampling Skew for Reinforcement
  Learning based Meta-Planner in Robot Navigation</a></h2>
<p><strong>Authors:</strong> Shiwei Feng, Xuan Chen, Zhiyuan Cheng, Zikang Xiong, Yifei Gao, Siyuan Cheng, Sayali Kate, Xiangyu Zhang</p>
<p><strong>Summary:</strong> Robot navigation is increasingly crucial across applications like delivery
services and warehouse management. The integration of Reinforcement Learning
(RL) with classical planning has given rise to meta-planners that combine the
adaptability of RL with the explainable decision-making of classical planners.
However, the exploration capabilities of RL-based meta-planners during training
are often constrained by the capabilities of the underlying classical planners.
This constraint can result in l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.17659v1' target='_blank'>Hierarchical End-to-End Autonomous Driving: Integrating BEV Perception
  with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Siyi Lu, Lei He, Shengbo Eben Li, Yugong Luo, Jianqiang Wang, Keqiang Li</p>
<p><strong>Summary:</strong> End-to-end autonomous driving offers a streamlined alternative to the
traditional modular pipeline, integrating perception, prediction, and planning
within a single framework. While Deep Reinforcement Learning (DRL) has recently
gained traction in this domain, existing approaches often overlook the critical
connection between feature extraction of DRL and perception. In this paper, we
bridge this gap by mapping the DRL feature extraction network directly to the
perception phase, enabling clearer...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.03472v2' target='_blank'>Deep Reinforcement Learning for Delay-Optimized Task Offloading in
  Vehicular Fog Computing</a></h2>
<p><strong>Authors:</strong> Mohammad Parsa Toopchinezhad, Mahmood Ahmadi</p>
<p><strong>Summary:</strong> The imminent rise of autonomous vehicles (AVs) is revolutionizing the future
of transport. The Vehicular Fog Computing (VFC) paradigm has emerged to
alleviate the load of compute-intensive and delay-sensitive AV programs via
task offloading to nearby vehicles. Effective VFC requires an intelligent and
dynamic offloading algorithm. As a result, this paper adapts Deep Reinforcement
Learning (DRL) for VFC offloading. First, a simulation environment utilizing
realistic hardware and task specificatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.07933v1' target='_blank'>Offline Hierarchical Reinforcement Learning via Inverse Optimization</a></h2>
<p><strong>Authors:</strong> Carolin Schmidt, Daniele Gammelli, James Harrison, Marco Pavone, Filipe Rodrigues</p>
<p><strong>Summary:</strong> Hierarchical policies enable strong performance in many sequential
decision-making problems, such as those with high-dimensional action spaces,
those requiring long-horizon planning, and settings with sparse rewards.
However, learning hierarchical policies from static offline datasets presents a
significant challenge. Crucially, actions taken by higher-level policies may
not be directly observable within hierarchical controllers, and the offline
dataset might have been generated using a differen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.09486v2' target='_blank'>ActSafe: Active Exploration with Safety Constraints for Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Yarden As, Bhavya Sukhija, Lenart Treven, Carmelo Sferrazza, Stelian Coros, Andreas Krause</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is ubiquitous in the development of modern AI
systems. However, state-of-the-art RL agents require extensive, and potentially
unsafe, interactions with their environments to learn effectively. These
limitations confine RL agents to simulated environments, hindering their
ability to learn directly in real-world settings. In this work, we present
ActSafe, a novel model-based RL algorithm for safe and efficient exploration.
ActSafe learns a well-calibrated probabilistic ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.18495v1' target='_blank'>Multi-UAV Behavior-based Formation with Static and Dynamic Obstacles
  Avoidance via Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yuqing Xie, Chao Yu, Hongzhi Zang, Feng Gao, Wenhao Tang, Jingyi Huang, Jiayu Chen, Botian Xu, Yi Wu, Yu Wang</p>
<p><strong>Summary:</strong> Formation control of multiple Unmanned Aerial Vehicles (UAVs) is vital for
practical applications. This paper tackles the task of behavior-based UAV
formation while avoiding static and dynamic obstacles during directed flight.
We present a two-stage reinforcement learning (RL) training pipeline to tackle
the challenge of multi-objective optimization, large exploration spaces, and
the sim-to-real gap. The first stage searches in a simplified scenario for a
linear utility function that balances al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.22578v1' target='_blank'>Energy-Aware Multi-Agent Reinforcement Learning for Collaborative
  Execution in Mission-Oriented Drone Networks</a></h2>
<p><strong>Authors:</strong> Ying Li, Changling Li, Jiyao Chen, Christine Roinou</p>
<p><strong>Summary:</strong> Mission-oriented drone networks have been widely used for structural
inspection, disaster monitoring, border surveillance, etc. Due to the limited
battery capacity of drones, mission execution strategy impacts network
performance and mission completion. However, collaborative execution is a
challenging problem for drones in such a dynamic environment as it also
involves efficient trajectory design. We leverage multi-agent reinforcement
learning (MARL) to manage the challenge in this study, letti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.22752v1' target='_blank'>SoftCTRL: Soft conservative KL-control of Transformer Reinforcement
  Learning for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Minh Tri Huynh, Duc Dung Nguyen</p>
<p><strong>Summary:</strong> In recent years, motion planning for urban self-driving cars (SDV) has become
a popular problem due to its complex interaction of road components. To tackle
this, many methods have relied on large-scale, human-sampled data processed
through Imitation learning (IL). Although effective, IL alone cannot adequately
handle safety and reliability concerns. Combining IL with Reinforcement
learning (RL) by adding KL divergence between RL and IL policy to the RL loss
can alleviate IL's weakness but suffe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.01000v1' target='_blank'>Enhancing Model-Based Step Adaptation for Push Recovery through
  Reinforcement Learning of Step Timing and Region</a></h2>
<p><strong>Authors:</strong> Tobias Egle, Yashuai Yan, Dongheui Lee, Christian Ott</p>
<p><strong>Summary:</strong> This paper introduces a new approach to enhance the robustness of humanoid
walking under strong perturbations, such as substantial pushes. Effective
recovery from external disturbances requires bipedal robots to dynamically
adjust their stepping strategies, including footstep positions and timing.
Unlike most advanced walking controllers that restrict footstep locations to a
predefined convex region, substantially limiting recoverable disturbances, our
method leverages reinforcement learning to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.04915v1' target='_blank'>Evaluating Robustness of Reinforcement Learning Algorithms for
  Autonomous Shipping</a></h2>
<p><strong>Authors:</strong> Bavo Lesy, Ali Anwar, Siegfried Mercelis</p>
<p><strong>Summary:</strong> Recently, there has been growing interest in autonomous shipping due to its
potential to improve maritime efficiency and safety. The use of advanced
technologies, such as artificial intelligence, can address the current
navigational and operational challenges in autonomous shipping. In particular,
inland waterway transport (IWT) presents a unique set of challenges, such as
crowded waterways and variable environmental conditions. In such dynamic
settings, the reliability and robustness of autonom...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.15212v1' target='_blank'>Effective Analog ICs Floorplanning with Relational Graph Neural Networks
  and Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Davide Basso, Luca Bortolussi, Mirjana Videnovic-Misic, Husni Habal</p>
<p><strong>Summary:</strong> Analog integrated circuit (IC) floorplanning is typically a manual process
with the placement of components (devices and modules) planned by a layout
engineer. This process is further complicated by the interdependence of
floorplanning and routing steps, numerous electric and layout-dependent
constraints, as well as the high level of customization expected in analog
design. This paper presents a novel automatic floorplanning algorithm based on
reinforcement learning. It is augmented by a relatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.18195v1' target='_blank'>Scalable Multi-Objective Reinforcement Learning with Fairness Guarantees
  using Lorenz Dominance</a></h2>
<p><strong>Authors:</strong> Dimitris Michailidis, Willem R√∂pke, Diederik M. Roijers, Sennay Ghebreab, Fernando P. Santos</p>
<p><strong>Summary:</strong> Multi-Objective Reinforcement Learning (MORL) aims to learn a set of policies
that optimize trade-offs between multiple, often conflicting objectives. MORL
is computationally more complex than single-objective RL, particularly as the
number of objectives increases. Additionally, when objectives involve the
preferences of agents or groups, ensuring fairness is socially desirable. This
paper introduces a principled algorithm that incorporates fairness into MORL
while improving scalability to many-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.06486v1' target='_blank'>SimuDICE: Offline Policy Optimization Through World Model Updates and
  DICE Estimation</a></h2>
<p><strong>Authors:</strong> Catalin E. Brita, Stephan Bongers, Frans A. Oliehoek</p>
<p><strong>Summary:</strong> In offline reinforcement learning, deriving an effective policy from a
pre-collected set of experiences is challenging due to the distribution
mismatch between the target policy and the behavioral policy used to collect
the data, as well as the limited sample size. Model-based reinforcement
learning improves sample efficiency by generating simulated experiences using a
learned dynamic model of the environment. However, these synthetic experiences
often suffer from the same distribution mismatch....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.20338v1' target='_blank'>Exploiting Hybrid Policy in Reinforcement Learning for Interpretable
  Temporal Logic Manipulation</a></h2>
<p><strong>Authors:</strong> Hao Zhang, Hao Wang, Xiucai Huang, Wenrui Chen, Zhen Kan</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) based methods have been increasingly explored for
robot learning. However, RL based methods often suffer from low sampling
efficiency in the exploration phase, especially for long-horizon manipulation
tasks, and generally neglect the semantic information from the task level,
resulted in a delayed convergence or even tasks failure. To tackle these
challenges, we propose a Temporal-Logic-guided Hybrid policy framework (HyTL)
which leverages three-level decision layers t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.09081v1' target='_blank'>Inferring Transition Dynamics from Value Functions</a></h2>
<p><strong>Authors:</strong> Jacob Adamczyk</p>
<p><strong>Summary:</strong> In reinforcement learning, the value function is typically trained to solve
the Bellman equation, which connects the current value to future values. This
temporal dependency hints that the value function may contain implicit
information about the environment's transition dynamics. By rearranging the
Bellman equation, we show that a converged value function encodes a model of
the underlying dynamics of the environment. We build on this insight to propose
a simple method for inferring dynamics mod...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.13200v1' target='_blank'>SRMT: Shared Memory for Multi-agent Lifelong Pathfinding</a></h2>
<p><strong>Authors:</strong> Alsu Sagirova, Yuri Kuratov, Mikhail Burtsev</p>
<p><strong>Summary:</strong> Multi-agent reinforcement learning (MARL) demonstrates significant progress
in solving cooperative and competitive multi-agent problems in various
environments. One of the principal challenges in MARL is the need for explicit
prediction of the agents' behavior to achieve cooperation. To resolve this
issue, we propose the Shared Recurrent Memory Transformer (SRMT) which extends
memory transformers to multi-agent settings by pooling and globally
broadcasting individual working memories, enabling a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.16098v1' target='_blank'>Multi-Agent Meta-Offline Reinforcement Learning for Timely UAV Path
  Planning and Data Collection</a></h2>
<p><strong>Authors:</strong> Eslam Eldeeb, Hirley Alves</p>
<p><strong>Summary:</strong> Multi-agent reinforcement learning (MARL) has been widely adopted in
high-performance computing and complex data-driven decision-making in the
wireless domain. However, conventional MARL schemes face many obstacles in
real-world scenarios. First, most MARL algorithms are online, which might be
unsafe and impractical. Second, MARL algorithms are environment-specific,
meaning network configuration changes require model retraining. This letter
proposes a novel meta-offline MARL algorithm that combi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.16142v1' target='_blank'>Towards General-Purpose Model-Free Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Scott Fujimoto, Pierluca D'Oro, Amy Zhang, Yuandong Tian, Michael Rabbat</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) promises a framework for near-universal
problem-solving. In practice however, RL algorithms are often tailored to
specific benchmarks, relying on carefully tuned hyperparameters and algorithmic
choices. Recently, powerful model-based RL methods have shown impressive
general results across benchmarks but come at the cost of increased complexity
and slow run times, limiting their broader applicability. In this paper, we
attempt to find a unifying model-free deep RL algo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.16918v1' target='_blank'>On Rollouts in Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Bernd Frauenknecht, Devdutt Subhasish, Friedrich Solowjow, Sebastian Trimpe</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by
learning a model of the environment and generating synthetic rollouts from it.
However, accumulated model errors during these rollouts can distort the data
distribution, negatively impacting policy learning and hindering long-term
planning. Thus, the accumulation of model errors is a key bottleneck in current
MBRL methods. We propose Infoprop, a model-based rollout mechanism that
separates aleatoric from epistemic mode...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.02071v1' target='_blank'>Sequential Multi-objective Multi-agent Reinforcement Learning Approach
  for Predictive Maintenance</a></h2>
<p><strong>Authors:</strong> Yan Chen, Cheng Liu</p>
<p><strong>Summary:</strong> Existing predictive maintenance (PdM) methods typically focus solely on
whether to replace system components without considering the costs incurred by
inspection. However, a well-considered approach should be able to minimize
Remaining Useful Life (RUL) at engine replacement while maximizing inspection
interval. To achieve this, multi-agent reinforcement learning (MARL) can be
introduced. However, due to the sequential and mutually constraining nature of
these 2 objectives, conventional MARL is ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.03960v1' target='_blank'>Bilevel Multi-Armed Bandit-Based Hierarchical Reinforcement Learning for
  Interaction-Aware Self-Driving at Unsignalized Intersections</a></h2>
<p><strong>Authors:</strong> Zengqi Peng, Yubin Wang, Lei Zheng, Jun Ma</p>
<p><strong>Summary:</strong> In this work, we present BiM-ACPPO, a bilevel multi-armed bandit-based
hierarchical reinforcement learning framework for interaction-aware
decision-making and planning at unsignalized intersections. Essentially, it
proactively takes the uncertainties associated with surrounding vehicles (SVs)
into consideration, which encompass those stemming from the driver's intention,
interactive behaviors, and the varying number of SVs. Intermediate decision
variables are introduced to enable the high-level ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.05014v1' target='_blank'>Seasonal Station-Keeping of Short Duration High Altitude Balloons using
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tristan K. Schuler, Chinthan Prasad, Georgiy Kiselev, Donald Sofge</p>
<p><strong>Summary:</strong> Station-Keeping short-duration high-altitude balloons (HABs) in a region of
interest is a challenging path-planning problem due to partially observable,
complex, and dynamic wind flows. Deep reinforcement learning is a popular
strategy for solving the station-keeping problem. A custom simulation
environment was developed to train and evaluate Deep Q-Learning (DQN) for
short-duration HAB agents in the simulation. To train the agents on realistic
winds, synthetic wind forecasts were generated from...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.12631v2' target='_blank'>Score-Based Diffusion Policy Compatible with Reinforcement Learning via
  Optimal Transport</a></h2>
<p><strong>Authors:</strong> Mingyang Sun, Pengxiang Ding, Weinan Zhang, Donglin Wang</p>
<p><strong>Summary:</strong> Diffusion policies have shown promise in learning complex behaviors from
demonstrations, particularly for tasks requiring precise control and long-term
planning. However, they face challenges in robustness when encountering
distribution shifts. This paper explores improving diffusion-based imitation
learning models through online interactions with the environment. We propose
OTPR (Optimal Transport-guided score-based diffusion Policy for Reinforcement
learning fine-tuning), a novel method that i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.15214v1' target='_blank'>The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, Matthew E. Taylor</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has shown impressive results in sequential
decision-making tasks. Meanwhile, Large Language Models (LLMs) and
Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities
in multimodal understanding and reasoning. These advances have led to a surge
of research integrating LLMs and VLMs into RL. In this survey, we review
representative works in which LLMs and VLMs are used to overcome key challenges
in RL, such as lack of prior knowledge, long-horizon...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.01825v3' target='_blank'>The Effect of Planning Shape on Dyna-style Planning in High-dimensional
  State Spaces</a></h2>
<p><strong>Authors:</strong> G. Zacharias Holland, Erin J. Talvitie, Michael Bowling</p>
<p><strong>Summary:</strong> Dyna is a fundamental approach to model-based reinforcement learning (MBRL)
that interleaves planning, acting, and learning in an online setting. In the
most typical application of Dyna, the dynamics model is used to generate
one-step transitions from selected start states from the agent's history, which
are used to update the agent's value function or policy as if they were real
experiences. In this work, one-step Dyna was applied to several games from the
Arcade Learning Environment (ALE). We ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.07351v3' target='_blank'>Model-Based Offline Planning with Trajectory Pruning</a></h2>
<p><strong>Authors:</strong> Xianyuan Zhan, Xiangyu Zhu, Haoran Xu</p>
<p><strong>Summary:</strong> The recent offline reinforcement learning (RL) studies have achieved much
progress to make RL usable in real-world systems by learning policies from
pre-collected datasets without environment interaction. Unfortunately, existing
offline RL methods still face many practical challenges in real-world system
control tasks, such as computational restriction during agent training and the
requirement of extra control flexibility. The model-based planning framework
provides an attractive alternative. Ho...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.04735v2' target='_blank'>Planning in Observable POMDPs in Quasipolynomial Time</a></h2>
<p><strong>Authors:</strong> Noah Golowich, Ankur Moitra, Dhruv Rohatgi</p>
<p><strong>Summary:</strong> Partially Observable Markov Decision Processes (POMDPs) are a natural and
general model in reinforcement learning that take into account the agent's
uncertainty about its current state. In the literature on POMDPs, it is
customary to assume access to a planning oracle that computes an optimal policy
when the parameters are known, even though the problem is known to be
computationally hard. Almost all existing planning algorithms either run in
exponential time, lack provable performance guarantee...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.05354v2' target='_blank'>Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement</a></h2>
<p><strong>Authors:</strong> Yunke Ao, Hooman Esfandiari, Fabio Carrillo, Yarden As, Mazda Farshad, Benjamin F. Grewe, Andreas Krause, Philipp Fuernstahl</p>
<p><strong>Summary:</strong> Spinal fusion surgery requires highly accurate implantation of pedicle screw
implants, which must be conducted in critical proximity to vital structures
with a limited view of anatomy. Robotic surgery systems have been proposed to
improve placement accuracy, however, state-of-the-art systems suffer from the
limitations of open-loop approaches, as they follow traditional concepts of
preoperative planning and intraoperative registration, without real-time
recalculation of the surgical plan. In thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.05832v3' target='_blank'>Bag of Views: An Appearance-based Approach to Next-Best-View Planning
  for 3D Reconstruction</a></h2>
<p><strong>Authors:</strong> Sara Hatami Gazani, Matthew Tucsok, Iraj Mantegh, Homayoun Najjaran</p>
<p><strong>Summary:</strong> UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has experienced an increasing surge of interest due to recent
advancements in image processing and deep learning-based techniques. View
planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish vie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.00194v4' target='_blank'>Improving Planning with Large Language Models: A Modular Agentic
  Architecture</a></h2>
<p><strong>Authors:</strong> Taylor Webb, Shanka Subhra Mondal, Ida Momennejad</p>
<p><strong>Summary:</strong> Large language models (LLMs) demonstrate impressive performance on a wide
variety of tasks, but they often struggle with tasks that require multi-step
reasoning or goal-directed planning. Both cognitive neuroscience and
reinforcement learning (RL) have proposed a number of interacting functional
components that together implement search and evaluation in multi-step decision
making. These components include conflict monitoring, state prediction, state
evaluation, task decomposition, and orchestra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.07368v1' target='_blank'>Sequential Planning in Large Partially Observable Environments guided by
  LLMs</a></h2>
<p><strong>Authors:</strong> Swarna Kamal Paul</p>
<p><strong>Summary:</strong> Sequential planning in large state space and action space quickly becomes
intractable due to combinatorial explosion of the search space. Heuristic
methods, like monte-carlo tree search, though effective for large state space,
but struggle if action space is large. Pure reinforcement learning methods,
relying only on reward signals, needs prohibitively large interactions with the
environment to device a viable plan. If the state space, observations and
actions can be represented in natural langu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.08642v2' target='_blank'>CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning
  Tasks</a></h2>
<p><strong>Authors:</strong> Tianlong Wang, Junzhe Chen, Xueting Han, Jing Bai</p>
<p><strong>Summary:</strong> Post-training, particularly reinforcement learning (RL) using
self-play-generated data, has become a new learning paradigm for large language
models (LLMs). However, scaling RL to develop a general reasoner remains a
research challenge, as existing methods focus on task-specific reasoning
without adequately addressing generalization across a broader range of tasks.
Moreover, unlike traditional RL with limited action space, LLMs operate in an
infinite space, making it crucial to search for valuab...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.05582v1' target='_blank'>Gen-Drive: Enhancing Diffusion Generative Driving Policies with Reward
  Modeling and Reinforcement Learning Fine-tuning</a></h2>
<p><strong>Authors:</strong> Zhiyu Huang, Xinshuo Weng, Maximilian Igl, Yuxiao Chen, Yulong Cao, Boris Ivanovic, Marco Pavone, Chen Lv</p>
<p><strong>Summary:</strong> Autonomous driving necessitates the ability to reason about future
interactions between traffic agents and to make informed evaluations for
planning. This paper introduces the \textit{Gen-Drive} framework, which shifts
from the traditional prediction and deterministic planning framework to a
generation-then-evaluation planning paradigm. The framework employs a behavior
diffusion model as a scene generator to produce diverse possible future
scenarios, thereby enhancing the capability for joint in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.14091v2' target='_blank'>Towards Effective Planning Strategies for Dynamic Opinion Networks</a></h2>
<p><strong>Authors:</strong> Bharath Muppasani, Protik Nag, Vignesh Narayanan, Biplav Srivastava, Michael N. Huhns</p>
<p><strong>Summary:</strong> In this study, we investigate the under-explored intervention planning aimed
at disseminating accurate information within dynamic opinion networks by
leveraging learning strategies. Intervention planning involves identifying key
nodes (search) and exerting control (e.g., disseminating accurate or official
information through the nodes) to mitigate the influence of misinformation.
However, as the network size increases, the problem becomes computationally
intractable. To address this, we first in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1606.04695v1' target='_blank'>Strategic Attentive Writer for Learning Macro-Actions</a></h2>
<p><strong>Authors:</strong> Alexander, Vezhnevets, Volodymyr Mnih, John Agapiou, Simon Osindero, Alex Graves, Oriol Vinyals, Koray Kavukcuoglu</p>
<p><strong>Summary:</strong> We present a novel deep recurrent neural network architecture that learns to
build implicit plans in an end-to-end manner by purely interacting with an
environment in reinforcement learning setting. The network builds an internal
plan, which is continuously updated upon observation of the next input from the
environment. It can also partition this internal representation into contiguous
sub- sequences by learning for how long the plan can be committed to - i.e.
followed without re-planing. Combi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.07887v1' target='_blank'>Combining Neural Networks and Tree Search for Task and Motion Planning
  in Challenging Environments</a></h2>
<p><strong>Authors:</strong> Chris Paxton, Vasumathi Raman, Gregory D. Hager, Marin Kobilarov</p>
<p><strong>Summary:</strong> We consider task and motion planning in complex dynamic environments for
problems expressed in terms of a set of Linear Temporal Logic (LTL)
constraints, and a reward function. We propose a methodology based on
reinforcement learning that employs deep neural networks to learn low-level
control policies as well as task-level option policies. A major challenge in
this setting, both for neural network approaches and classical planning, is the
need to explore future worlds of a complex and interacti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.08259v2' target='_blank'>Fast Motion Planning for High-DOF Robot Systems Using Hierarchical
  System Identification</a></h2>
<p><strong>Authors:</strong> Biao Jia, Zherong Pan, Dinesh Manocha</p>
<p><strong>Summary:</strong> We present an efficient algorithm for motion planning and control of a robot
system with a high number of degrees-of-freedom. These include high-DOF soft
robots or an articulated robot interacting with a deformable environment. Our
approach takes into account dynamics constraints and present a novel technique
to accelerate the forward dynamic computation using a data-driven method. We
precompute the forward dynamic function of the robot system on a hierarchical
adaptive grid. Furthermore, we exp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.10842v1' target='_blank'>Learning and Planning with a Semantic Model</a></h2>
<p><strong>Authors:</strong> Yi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian</p>
<p><strong>Summary:</strong> Building deep reinforcement learning agents that can generalize and adapt to
unseen environments remains a fundamental challenge for AI. This paper
describes progresses on this challenge in the context of man-made environments,
which are visually diverse but contain intrinsic semantic regularities. We
propose a hybrid model-based and model-free approach, LEArning and Planning
with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on
visual inputs, and a Bayesian model over sem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.07268v3' target='_blank'>A Joint Planning and Learning Framework for Human-Aided Decision-Making</a></h2>
<p><strong>Authors:</strong> Daoming Lyu, Fangkai Yang, Bo Liu, Steven Gustafson</p>
<p><strong>Summary:</strong> Conventional reinforcement learning (RL) allows an agent to learn policies
via environmental rewards only, with a long and slow learning curve, especially
at the beginning stage. On the contrary, human learning is usually much faster
because prior and general knowledge and multiple information resources are
utilized. In this paper, we propose a
\textbf{P}lanner-\textbf{A}ctor-\textbf{C}ritic architecture for
hu\textbf{MAN}-centered planning and learning (\textbf{PACMAN}), where an agent
uses pri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.05527v1' target='_blank'>Regularizing Model-Based Planning with Energy-Based Models</a></h2>
<p><strong>Authors:</strong> Rinu Boney, Juho Kannala, Alexander Ilin</p>
<p><strong>Summary:</strong> Model-based reinforcement learning could enable sample-efficient learning by
quickly acquiring rich knowledge about the world and using it to improve
behaviour without additional data. Learned dynamics models can be directly used
for planning actions but this has been challenging because of inaccuracies in
the learned models. In this paper, we focus on planning with learned dynamics
models and propose to regularize it using energy estimates of state transitions
in the environment. We visually de...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.05010v2' target='_blank'>Efficient Planning under Partial Observability with Unnormalized Q
  Functions and Spectral Learning</a></h2>
<p><strong>Authors:</strong> Tianyu Li, Bogdan Mazoure, Doina Precup, Guillaume Rabusseau</p>
<p><strong>Summary:</strong> Learning and planning in partially-observable domains is one of the most
difficult problems in reinforcement learning. Traditional methods consider
these two problems as independent, resulting in a classical two-stage paradigm:
first learn the environment dynamics and then plan accordingly. This approach,
however, disconnects the two problems and can consequently lead to algorithms
that are sample inefficient and time consuming. In this paper, we propose a
novel algorithm that combines learning ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.06721v1' target='_blank'>Long-Term Planning and Situational Awareness in OpenAI Five</a></h2>
<p><strong>Authors:</strong> Jonathan Raiman, Susan Zhang, Filip Wolski</p>
<p><strong>Summary:</strong> Understanding how knowledge about the world is represented within model-free
deep reinforcement learning methods is a major challenge given the black box
nature of its learning process within high-dimensional observation and action
spaces. AlphaStar and OpenAI Five have shown that agents can be trained without
any explicit hierarchical macro-actions to reach superhuman skill in games that
require taking thousands of actions before reaching the final goal. Assessing
the agent's plans and game und...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.07544v2' target='_blank'>Planning with Abstract Learned Models While Learning Transferable
  Subtasks</a></h2>
<p><strong>Authors:</strong> John Winder, Stephanie Milani, Matthew Landen, Erebus Oh, Shane Parr, Shawn Squire, Marie desJardins, Cynthia Matuszek</p>
<p><strong>Summary:</strong> We introduce an algorithm for model-based hierarchical reinforcement learning
to acquire self-contained transition and reward models suitable for
probabilistic planning at multiple levels of abstraction. We call this
framework Planning with Abstract Learned Models (PALM). By representing
subtasks symbolically using a new formal structure, the lifted abstract Markov
decision process (L-AMDP), PALM learns models that are independent and modular.
Through our experiments, we show how PALM integrates...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.03648v1' target='_blank'>Plan2Vec: Unsupervised Representation Learning by Latent Plans</a></h2>
<p><strong>Authors:</strong> Ge Yang, Amy Zhang, Ari S. Morcos, Joelle Pineau, Pieter Abbeel, Roberto Calandra</p>
<p><strong>Summary:</strong> In this paper we introduce plan2vec, an unsupervised representation learning
approach that is inspired by reinforcement learning. Plan2vec constructs a
weighted graph on an image dataset using near-neighbor distances, and then
extrapolates this local metric to a global embedding by distilling
path-integral over planned path. When applied to control, plan2vec offers a way
to learn goal-conditioned value estimates that are accurate over long horizons
that is both compute and sample efficient. We d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.09832v1' target='_blank'>Dream and Search to Control: Latent Space Planning for Continuous
  Control</a></h2>
<p><strong>Authors:</strong> Anurag Koul, Varun V. Kumar, Alan Fern, Somdeb Majumdar</p>
<p><strong>Summary:</strong> Learning and planning with latent space dynamics has been shown to be useful
for sample efficiency in model-based reinforcement learning (MBRL) for discrete
and continuous control tasks. In particular, recent work, for discrete action
spaces, demonstrated the effectiveness of latent-space planning via Monte-Carlo
Tree Search (MCTS) for bootstrapping MBRL during learning and at test time.
However, the potential gains from latent-space tree search have not yet been
demonstrated for environments wi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.13685v1' target='_blank'>Forethought and Hindsight in Credit Assignment</a></h2>
<p><strong>Authors:</strong> Veronica Chelu, Doina Precup, Hado van Hasselt</p>
<p><strong>Summary:</strong> We address the problem of credit assignment in reinforcement learning and
explore fundamental questions regarding the way in which an agent can best use
additional computation to propagate new information, by planning with internal
models of the world to improve its predictions. Particularly, we work to
understand the gains and peculiarities of planning employed as forethought via
forward models or as hindsight operating with backward models. We establish the
relative merits, limitations and com...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.07156v1' target='_blank'>Temporal Predictive Coding For Model-Based Planning In Latent Space</a></h2>
<p><strong>Authors:</strong> Tung Nguyen, Rui Shu, Tuan Pham, Hung Bui, Stefano Ermon</p>
<p><strong>Summary:</strong> High-dimensional observations are a major challenge in the application of
model-based reinforcement learning (MBRL) to real-world environments. To handle
high-dimensional sensory inputs, existing approaches use representation
learning to map high-dimensional observations into a lower-dimensional latent
space that is more amenable to dynamics estimation and planning. In this work,
we present an information-theoretic approach that employs temporal predictive
coding to encode elements in the enviro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.02018v1' target='_blank'>Intent-aware Multi-agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Siyuan Qi, Song-Chun Zhu</p>
<p><strong>Summary:</strong> This paper proposes an intent-aware multi-agent planning framework as well as
a learning algorithm. Under this framework, an agent plans in the goal space to
maximize the expected utility. The planning process takes the belief of other
agents' intents into consideration. Instead of formulating the learning problem
as a partially observable Markov decision process (POMDP), we propose a simple
but effective linear function approximation of the utility function. It is
based on the observation that ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.00377v1' target='_blank'>A New Challenge: Approaching Tetris Link with AI</a></h2>
<p><strong>Authors:</strong> Matthias Muller-Brockhausen, Mike Preuss, Aske Plaat</p>
<p><strong>Summary:</strong> Decades of research have been invested in making computer programs for
playing games such as Chess and Go. This paper focuses on a new game, Tetris
Link, a board game that is still lacking any scientific analysis. Tetris Link
has a large branching factor, hampering a traditional heuristic planning
approach. We explore heuristic planning and two other approaches: Reinforcement
Learning, Monte Carlo tree search. We document our approach and report on their
relative performance in a tournament. Cur...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.01518v3' target='_blank'>Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans</a></h2>
<p><strong>Authors:</strong> Lirui Wang, Xiangyun Meng, Yu Xiang, Dieter Fox</p>
<p><strong>Summary:</strong> 6D grasping in cluttered scenes is a longstanding problem in robotic
manipulation. Open-loop manipulation pipelines may fail due to inaccurate state
estimation, while most end-to-end grasping methods have not yet scaled to
complex scenes with obstacles. In this work, we propose a new method for
end-to-end learning of 6D grasping in cluttered scenes. Our hierarchical
framework learns collision-free target-driven grasping based on partial point
cloud observations. We learn an embedding space to en...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.06661v1' target='_blank'>Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks</a></h2>
<p><strong>Authors:</strong> Ingmar Schubert, Ozgur S. Oguz, Marc Toussaint</p>
<p><strong>Summary:</strong> In high-dimensional state spaces, the usefulness of Reinforcement Learning
(RL) is limited by the problem of exploration. This issue has been addressed
using potential-based reward shaping (PB-RS) previously. In the present work,
we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the
strict optimality guarantees of PB-RS to a guarantee of preserved long-term
behavior. Being less restrictive, FV-RS allows for reward shaping functions
that are even better suited for improvi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.12403v2' target='_blank'>Planning and Learning with Adaptive Lookahead</a></h2>
<p><strong>Authors:</strong> Aviv Rosenberg, Assaf Hallak, Shie Mannor, Gal Chechik, Gal Dalal</p>
<p><strong>Summary:</strong> Some of the most powerful reinforcement learning frameworks use planning for
action selection. Interestingly, their planning horizon is either fixed or
determined arbitrarily by the state visitation history. Here, we expand beyond
the naive fixed horizon and propose a theoretically justified strategy for
adaptive selection of the planning horizon as a function of the state-dependent
value estimate. We propose two variants for lookahead selection and analyze the
trade-off between iteration count ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.07081v1' target='_blank'>GoalNet: Inferring Conjunctive Goal Predicates from Human Plan
  Demonstrations for Robot Instruction Following</a></h2>
<p><strong>Authors:</strong> Shreya Sharma, Jigyasa Gupta, Shreshth Tuli, Rohan Paul, Mausam</p>
<p><strong>Summary:</strong> Our goal is to enable a robot to learn how to sequence its actions to perform
tasks specified as natural language instructions, given successful
demonstrations from a human partner. The ability to plan high-level tasks can
be factored as (i) inferring specific goal predicates that characterize the
task implied by a language instruction for a given world state and (ii)
synthesizing a feasible goal-reaching action-sequence with such predicates. For
the former, we leverage a neural network predicti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.10289v2' target='_blank'>PushWorld: A benchmark for manipulation planning with tools and movable
  obstacles</a></h2>
<p><strong>Authors:</strong> Ken Kansky, Skanda Vaidyanath, Scott Swingle, Xinghua Lou, Miguel Lazaro-Gredilla, Dileep George</p>
<p><strong>Summary:</strong> While recent advances in artificial intelligence have achieved human-level
performance in environments like Starcraft and Go, many physical reasoning
tasks remain challenging for modern algorithms. To date, few algorithms have
been evaluated on physical tasks that involve manipulating objects when movable
obstacles are present and when tools must be used to perform the manipulation.
To promote research on such tasks, we introduce PushWorld, an environment with
simplistic physics that requires ma...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.12962v2' target='_blank'>Hierarchical Imitation Learning with Vector Quantized Models</a></h2>
<p><strong>Authors:</strong> Kalle Kujanp√§√§, Joni Pajarinen, Alexander Ilin</p>
<p><strong>Summary:</strong> The ability to plan actions on multiple levels of abstraction enables
intelligent agents to solve complex tasks effectively. However, learning the
models for both low and high-level planning from demonstrations has proven
challenging, especially with higher-dimensional inputs. To address this issue,
we propose to use reinforcement learning to identify subgoals in expert
trajectories by associating the magnitude of the rewards with the
predictability of low-level actions given the state and the c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.11166v1' target='_blank'>Imitating Graph-Based Planning with Goal-Conditioned Policies</a></h2>
<p><strong>Authors:</strong> Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, Jinwoo Shin</p>
<p><strong>Summary:</strong> Recently, graph-based planning algorithms have gained much attention to solve
goal-conditioned reinforcement learning (RL) tasks: they provide a sequence of
subgoals to reach the target-goal, and the agents learn to execute
subgoal-conditioned policies. However, the sample-efficiency of such RL schemes
still remains a challenge, particularly for long-horizon tasks. To address this
issue, we present a simple yet effective self-imitation scheme which distills a
subgoal-conditioned policy into the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.13002v1' target='_blank'>Planning Goals for Exploration</a></h2>
<p><strong>Authors:</strong> Edward S. Hu, Richard Chang, Oleh Rybkin, Dinesh Jayaraman</p>
<p><strong>Summary:</strong> Dropped into an unknown environment, what should an agent do to quickly learn
about the environment and how to accomplish diverse tasks within it? We address
this question within the goal-conditioned reinforcement learning paradigm, by
identifying how the agent should set its goals at training time to maximize
exploration. We propose "Planning Exploratory Goals" (PEG), a method that sets
goals for each training episode to directly optimize an intrinsic exploration
reward. PEG first chooses goal ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.04440v2' target='_blank'>Dual policy as self-model for planning</a></h2>
<p><strong>Authors:</strong> Jaesung Yoo, Fernanda de la Torre, Guangyu Robert Yang</p>
<p><strong>Summary:</strong> Planning is a data efficient decision-making strategy where an agent selects
candidate actions by exploring possible future states. To simulate future
states when there is a high-dimensional action space, the knowledge of one's
decision making strategy must be used to limit the number of actions to be
explored. We refer to the model used to simulate one's decisions as the agent's
self-model. While self-models are implicitly used widely in conjunction with
world models to plan actions, it remains...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.11208v1' target='_blank'>The Unintended Consequences of Discount Regularization: Improving
  Regularization in Certainty Equivalence Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sarah Rathnam, Sonali Parbhoo, Weiwei Pan, Susan A. Murphy, Finale Doshi-Velez</p>
<p><strong>Summary:</strong> Discount regularization, using a shorter planning horizon when calculating
the optimal policy, is a popular choice to restrict planning to a less complex
set of policies when estimating an MDP from sparse or noisy data (Jiang et al.,
2015). It is commonly understood that discount regularization functions by
de-emphasizing or ignoring delayed effects. In this paper, we reveal an
alternate view of discount regularization that exposes unintended consequences.
We demonstrate that planning under a lo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.14110v1' target='_blank'>Reinforced Potential Field for Multi-Robot Motion Planning in Cluttered
  Environments</a></h2>
<p><strong>Authors:</strong> Dengyu Zhang, Xinyu Zhang, Zheng Zhang, Bo Zhu, Qingrui Zhang</p>
<p><strong>Summary:</strong> Motion planning is challenging for multiple robots in cluttered environments
without communication, especially in view of real-time efficiency, motion
safety, distributed computation, and trajectory optimality, etc. In this paper,
a reinforced potential field method is developed for distributed multi-robot
motion planning, which is a synthesized design of reinforcement learning and
artificial potential fields. An observation embedding with a self-attention
mechanism is presented to model the rob...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.03395v1' target='_blank'>Diffused Task-Agnostic Milestone Planner</a></h2>
<p><strong>Authors:</strong> Mineui Hong, Minjae Kang, Songhwai Oh</p>
<p><strong>Summary:</strong> Addressing decision-making problems using sequence modeling to predict future
trajectories shows promising results in recent years. In this paper, we take a
step further to leverage the sequence predictive method in wider areas such as
long-term planning, vision-based control, and multi-task decision-making. To
this end, we propose a method to utilize a diffusion-based generative sequence
model to plan a series of milestones in a latent space and to have an agent to
follow the milestones to acco...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.10053v1' target='_blank'>Towards Goal-oriented Intelligent Tutoring Systems in Online Education</a></h2>
<p><strong>Authors:</strong> Yang Deng, Zifeng Ren, An Zhang, Wenqiang Lei, Tat-Seng Chua</p>
<p><strong>Summary:</strong> Interactive Intelligent Tutoring Systems (ITSs) enhance traditional ITSs by
promoting effective learning through interactions and problem resolution in
online education. Yet, proactive engagement, prioritizing resource optimization
with planning and assessment capabilities, is often overlooked in current ITS
designs. In this work, we investigate a new task, named Goal-oriented
Intelligent Tutoring Systems (GITS), which aims to enable the student's mastery
of a designated concept by strategically...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.08191v1' target='_blank'>Synchronized Dual-arm Rearrangement via Cooperative mTSP</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Shishun Zhang, Sisi Dai, Hui Huang, Ruizhen Hu, Xiaohong Chen, Kai Xu</p>
<p><strong>Summary:</strong> Synchronized dual-arm rearrangement is widely studied as a common scenario in
industrial applications. It often faces scalability challenges due to the
computational complexity of robotic arm rearrangement and the high-dimensional
nature of dual-arm planning. To address these challenges, we formulated the
problem as cooperative mTSP, a variant of mTSP where agents share cooperative
costs, and utilized reinforcement learning for its solution. Our approach
involved representing rearrangement tasks...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.08053v1' target='_blank'>Radio Resource Management and Path Planning in Intelligent
  Transportation Systems via Reinforcement Learning for Environmental
  Sustainability</a></h2>
<p><strong>Authors:</strong> S. Norouzi, N. Azarasa, M. R. Abedi, N. Mokari, S. E. Seyedabrishami, H. Saeedi, E. A. Jorswieck</p>
<p><strong>Summary:</strong> Efficient and dynamic path planning has become an important topic for urban
areas with larger density of connected vehicles (CV) which results in reduction
of travel time and directly contributes to environmental sustainability through
reducing energy consumption. CVs exploit the cellular wireless
vehicle-to-everything (C-V2X) communication technology to disseminate the
vehicle-to-infrastructure (V2I) messages to the Base-station (BS) to improve
situation awareness on urban roads. In this paper,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.16335v1' target='_blank'>RoboArm-NMP: a Learning Environment for Neural Motion Planning</a></h2>
<p><strong>Authors:</strong> Tom Jurgenson, Matan Sudry, Gal Avineri, Aviv Tamar</p>
<p><strong>Summary:</strong> We present RoboArm-NMP, a learning and evaluation environment that allows
simple and thorough evaluations of Neural Motion Planning (NMP) algorithms,
focused on robotic manipulators. Our Python-based environment provides baseline
implementations for learning control policies (either supervised or
reinforcement learning based), a simulator based on PyBullet, data of solved
instances using a classical motion planning solver, various representation
learning methods for encoding the obstacles, and a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.15850v1' target='_blank'>Learning Abstract World Model for Value-preserving Planning with Options</a></h2>
<p><strong>Authors:</strong> Rafael Rodriguez-Sanchez, George Konidaris</p>
<p><strong>Summary:</strong> General-purpose agents require fine-grained controls and rich sensory inputs
to perform a wide range of tasks. However, this complexity often leads to
intractable decision-making. Traditionally, agents are provided with
task-specific action and observation spaces to mitigate this challenge, but
this reduces autonomy. Instead, agents must be capable of building state-action
spaces at the correct abstraction level from their sensorimotor experiences. We
leverage the structure of a given set of tem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.19561v1' target='_blank'>Meta-Gradient Search Control: A Method for Improving the Efficiency of
  Dyna-style Planning</a></h2>
<p><strong>Authors:</strong> Bradley Burega, John D. Martin, Luke Kapeluck, Michael Bowling</p>
<p><strong>Summary:</strong> We study how a Reinforcement Learning (RL) system can remain sample-efficient
when learning from an imperfect model of the environment. This is particularly
challenging when the learning system is resource-constrained and in continual
settings, where the environment dynamics change. To address these challenges,
our paper introduces an online, meta-gradient algorithm that tunes a
probability with which states are queried during Dyna-style planning. Our study
compares the aggregate, empirical perf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.18812v1' target='_blank'>Online Planning in POMDPs with State-Requests</a></h2>
<p><strong>Authors:</strong> Raphael Avalos, Eugenio Bargiacchi, Ann Now√©, Diederik M. Roijers, Frans A. Oliehoek</p>
<p><strong>Summary:</strong> In key real-world problems, full state information is sometimes available but
only at a high cost, like activating precise yet energy-intensive sensors or
consulting humans, thereby compelling the agent to operate under partial
observability. For this scenario, we propose AEMS-SR (Anytime Error
Minimization Search with State Requests), a principled online planning
algorithm tailored for POMDPs with state requests. By representing the search
space as a graph instead of a tree, AEMS-SR avoids the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.10162v2' target='_blank'>Physics-Aware Combinatorial Assembly Sequence Planning using Data-free
  Action Masking</a></h2>
<p><strong>Authors:</strong> Ruixuan Liu, Alan Chen, Weiye Zhao, Changliu Liu</p>
<p><strong>Summary:</strong> Combinatorial assembly uses standardized unit primitives to build objects
that satisfy user specifications. This paper studies assembly sequence planning
(ASP) for physical combinatorial assembly. Given the shape of the desired
object, the goal is to find a sequence of actions for placing unit primitives
to build the target object. In particular, we aim to ensure the planned
assembly sequence is physically executable. However, ASP for combinatorial
assembly is particularly challenging due to its...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.16950v1' target='_blank'>Dynamic Obstacle Avoidance through Uncertainty-Based Adaptive Planning
  with Diffusion</a></h2>
<p><strong>Authors:</strong> Vineet Punyamoorty, Pascal Jutras-Dub√©, Ruqi Zhang, Vaneet Aggarwal, Damon Conover, Aniket Bera</p>
<p><strong>Summary:</strong> By framing reinforcement learning as a sequence modeling problem, recent work
has enabled the use of generative models, such as diffusion models, for
planning. While these models are effective in predicting long-horizon state
trajectories in deterministic environments, they face challenges in dynamic
settings with moving obstacles. Effective collision avoidance demands
continuous monitoring and adaptive decision-making. While replanning at every
timestep could ensure safety, it introduces substa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.04664v1' target='_blank'>A Universal Formulation for Path-Parametric Planning and Control</a></h2>
<p><strong>Authors:</strong> Jon Arrizabalaga, Markus Ryll</p>
<p><strong>Summary:</strong> This work presents a unified framework for path-parametric planning and
control. This formulation is universal as it standardizes the entire spectrum
of path-parametric techniques -- from traditional path following to more recent
contouring or progress-maximizing Model Predictive Control and Reinforcement
Learning -- under a single framework. The ingredients underlying this
universality are twofold: First, we present a compact and efficient technique
capable of computing singularity-free, smooth...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.06372v2' target='_blank'>Cooperative and Asynchronous Transformer-based Mission Planning for
  Heterogeneous Teams of Mobile Robots</a></h2>
<p><strong>Authors:</strong> Milad Farjadnasab, Shahin Sirouspour</p>
<p><strong>Summary:</strong> Cooperative mission planning for heterogeneous teams of mobile robots
presents a unique set of challenges, particularly when operating under
communication constraints and limited computational resources. To address these
challenges, we propose the Cooperative and Asynchronous Transformer-based
Mission Planning (CATMiP) framework, which leverages multi-agent reinforcement
learning (MARL) to coordinate distributed decision making among agents with
diverse sensing, motion, and actuation capabilitie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.04408v1' target='_blank'>Transforming Multimodal Models into Action Models for Radiotherapy</a></h2>
<p><strong>Authors:</strong> Matteo Ferrante, Alessandra Carosi, Rolando Maria D Angelillo, Nicola Toschi</p>
<p><strong>Summary:</strong> Radiotherapy is a crucial cancer treatment that demands precise planning to
balance tumor eradication and preservation of healthy tissue. Traditional
treatment planning (TP) is iterative, time-consuming, and reliant on human
expertise, which can potentially introduce variability and inefficiency. We
propose a novel framework to transform a large multimodal foundation model
(MLM) into an action model for TP using a few-shot reinforcement learning (RL)
approach. Our method leverages the MLM's exte...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.12834v1' target='_blank'>NTP-INT: Network Traffic Prediction-Driven In-band Network Telemetry for
  High-load Switches</a></h2>
<p><strong>Authors:</strong> Penghui Zhang, Hua Zhang, Yuqi Dai, Cheng Zeng, Jingyu Wang, Jianxin Liao</p>
<p><strong>Summary:</strong> In-band network telemetry (INT) is essential to network management due to its
real-time visibility. However, because of the rapid increase in network devices
and services, it has become crucial to have targeted access to detailed network
information in a dynamic network environment. This paper proposes an
intelligent network telemetry system called NTP-INT to obtain more fine-grained
network information on high-load switches. Specifically, NTP-INT consists of
three modules: network traffic predi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.09054v2' target='_blank'>Model-free conventions in multi-agent reinforcement learning with
  heterogeneous preferences</a></h2>
<p><strong>Authors:</strong> Raphael K√∂ster, Kevin R. McKee, Richard Everett, Laura Weidinger, William S. Isaac, Edward Hughes, Edgar A. Du√©√±ez-Guzm√°n, Thore Graepel, Matthew Botvinick, Joel Z. Leibo</p>
<p><strong>Summary:</strong> Game theoretic views of convention generally rest on notions of common
knowledge and hyper-rational models of individual behavior. However, decades of
work in behavioral economics have questioned the validity of both foundations.
Meanwhile, computational neuroscience has contributed a modernized 'dual
process' account of decision-making where model-free (MF) reinforcement
learning trades off with model-based (MB) reinforcement learning. The former
captures habitual and procedural learning while ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.10079v3' target='_blank'>The MineRL 2019 Competition on Sample Efficient Reinforcement Learning
  using Human Priors</a></h2>
<p><strong>Authors:</strong> William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, Manuela Veloso, Phillip Wang</p>
<p><strong>Summary:</strong> Though deep reinforcement learning has led to breakthroughs in many difficult
domains, these successes have required an ever-increasing number of samples. As
state-of-the-art reinforcement learning (RL) systems require an exponentially
increasing number of samples, their development is restricted to a continually
shrinking segment of the AI community. Likewise, many of these systems cannot
be applied to real-world problems, where environment samples are expensive.
Resolution of these limitations...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1302.4971v1' target='_blank'>On the Complexity of Solving Markov Decision Problems</a></h2>
<p><strong>Authors:</strong> Michael L. Littman, Thomas L. Dean, Leslie Pack Kaelbling</p>
<p><strong>Summary:</strong> Markov decision problems (MDPs) provide the foundations for a number of
problems of interest to AI researchers studying automated planning and
reinforcement learning. In this paper, we summarize results regarding the
complexity of solving MDPs and the running time of MDP solution algorithms. We
argue that, although MDPs can be solved efficiently in theory, more study is
needed to reveal practical algorithms for solving large problems quickly. To
encourage future research, we sketch some alternat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1609.05524v3' target='_blank'>Principled Option Learning in Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Roy Fox, Michal Moshkovitz, Naftali Tishby</p>
<p><strong>Summary:</strong> It is well known that options can make planning more efficient, among their
many benefits. Thus far, algorithms for autonomously discovering a set of
useful options were heuristic. Naturally, a principled way of finding a set of
useful options may be more promising and insightful. In this paper we suggest a
mathematical characterization of good sets of options using tools from
information theory. This characterization enables us to find conditions for a
set of options to be optimal and an algori...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.10689v2' target='_blank'>Decoupling Dynamics and Reward for Transfer Learning</a></h2>
<p><strong>Authors:</strong> Amy Zhang, Harsh Satija, Joelle Pineau</p>
<p><strong>Summary:</strong> Current reinforcement learning (RL) methods can successfully learn single
tasks but often generalize poorly to modest perturbations in task domain or
training procedure. In this work, we present a decoupled learning strategy for
RL that creates a shared representation space where knowledge can be robustly
transferred. We separate learning the task representation, the forward
dynamics, the inverse dynamics and the reward function of the domain, and show
that this decoupling improves performance w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.02870v3' target='_blank'>Worst-Case Regret Bounds for Exploration via Randomized Value Functions</a></h2>
<p><strong>Authors:</strong> Daniel Russo</p>
<p><strong>Summary:</strong> This paper studies a recent proposal to use randomized value functions to
drive exploration in reinforcement learning. These randomized value functions
are generated by injecting random noise into the training data, making the
approach compatible with many popular methods for estimating parameterized
value functions. By providing a worst-case regret bound for tabular
finite-horizon Markov decision processes, we show that planning with respect to
these randomized value functions can induce provab...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.07246v1' target='_blank'>IKEA Furniture Assembly Environment for Long-Horizon Complex
  Manipulation Tasks</a></h2>
<p><strong>Authors:</strong> Youngwoon Lee, Edward S. Hu, Zhengyu Yang, Alex Yin, Joseph J. Lim</p>
<p><strong>Summary:</strong> The IKEA Furniture Assembly Environment is one of the first benchmarks for
testing and accelerating the automation of complex manipulation tasks. The
environment is designed to advance reinforcement learning from simple toy tasks
to complex tasks requiring both long-term planning and sophisticated low-level
control. Our environment supports over 80 different furniture models, Sawyer
and Baxter robot simulation, and domain randomization. The IKEA Furniture
Assembly Environment is a testbed for me...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.02352v1' target='_blank'>Mission schedule of agile satellites based on Proximal Policy
  Optimization Algorithm</a></h2>
<p><strong>Authors:</strong> Xinrui Liu</p>
<p><strong>Summary:</strong> Mission schedule of satellites is an important part of space operation
nowadays, since the number and types of satellites in orbit are increasing
tremendously and their corresponding tasks are also becoming more and more
complicated. In this paper, a mission schedule model combined with Proximal
Policy Optimization Algorithm(PPO) is proposed. Different from the traditional
heuristic planning method, this paper incorporate reinforcement learning
algorithms into it and find a new way to describe t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.04428v1' target='_blank'>Discourse Coherence, Reference Grounding and Goal Oriented Dialogue</a></h2>
<p><strong>Authors:</strong> Baber Khalid, Malihe Alikhani, Michael Fellner, Brian McMahan, Matthew Stone</p>
<p><strong>Summary:</strong> Prior approaches to realizing mixed-initiative human--computer referential
communication have adopted information-state or collaborative problem-solving
approaches. In this paper, we argue for a new approach, inspired by
coherence-based models of discourse such as SDRT \cite{asher-lascarides:2003a},
in which utterances attach to an evolving discourse structure and the
associated knowledge graph of speaker commitments serves as an interface to
real-world reasoning and conversational strategy. As ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.03309v1' target='_blank'>Identifying Decision Points for Safe and Interpretable Reinforcement
  Learning in Hypotension Treatment</a></h2>
<p><strong>Authors:</strong> Kristine Zhang, Yuanheng Wang, Jianzhun Du, Brian Chu, Leo Anthony Celi, Ryan Kindle, Finale Doshi-Velez</p>
<p><strong>Summary:</strong> Many batch RL health applications first discretize time into fixed intervals.
However, this discretization both loses resolution and forces a policy
computation at each (potentially fine) interval. In this work, we develop a
novel framework to compress continuous trajectories into a few, interpretable
decision points --places where the batch data support multiple alternatives. We
apply our approach to create recommendations from a cohort of hypotensive
patients dataset. Our reduced state space r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.01593v1' target='_blank'>Learning Transition Models with Time-delayed Causal Relations</a></h2>
<p><strong>Authors:</strong> Junchi Liang, Abdeslam Boularias</p>
<p><strong>Summary:</strong> This paper introduces an algorithm for discovering implicit and delayed
causal relations between events observed by a robot at arbitrary times, with
the objective of improving data-efficiency and interpretability of model-based
reinforcement learning (RL) techniques. The proposed algorithm initially
predicts observations with the Markov assumption, and incrementally introduces
new hidden variables to explain and reduce the stochasticity of the
observations. The hidden variables are memory units ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.09699v1' target='_blank'>Selective Credit Assignment</a></h2>
<p><strong>Authors:</strong> Veronica Chelu, Diana Borsa, Doina Precup, Hado van Hasselt</p>
<p><strong>Summary:</strong> Efficient credit assignment is essential for reinforcement learning
algorithms in both prediction and control settings. We describe a unified view
on temporal-difference algorithms for selective credit assignment. These
selective algorithms apply weightings to quantify the contribution of learning
updates. We present insights into applying weightings to value-based learning
and planning algorithms, and describe their role in mediating the backward
credit distribution in prediction and control. W...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.04840v1' target='_blank'>What are the mechanisms underlying metacognitive learning?</a></h2>
<p><strong>Authors:</strong> Ruiqi He, Falk Lieder</p>
<p><strong>Summary:</strong> How is it that humans can solve complex planning tasks so efficiently despite
limited cognitive resources? One reason is its ability to know how to use its
limited computational resources to make clever choices. We postulate that
people learn this ability from trial and error (metacognitive reinforcement
learning). Here, we systematize models of the underlying learning mechanisms
and enhance them with more sophisticated additional mechanisms. We fit the
resulting 86 models to human data collecte...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.04218v1' target='_blank'>Deep Occupancy-Predictive Representations for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Eivind Meyer, Lars Frederik Peiss, Matthias Althoff</p>
<p><strong>Summary:</strong> Manually specifying features that capture the diversity in traffic
environments is impractical. Consequently, learning-based agents cannot realize
their full potential as neural motion planners for autonomous vehicles.
Instead, this work proposes to learn which features are task-relevant. Given
its immediate relevance to motion planning, our proposed architecture encodes
the probabilistic occupancy map as a proxy for obtaining pre-trained state
representations. By leveraging a map-aware graph fo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.10833v1' target='_blank'>PTDRL: Parameter Tuning using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Elias Goldsztejn, Tal Feiner, Ronen Brafman</p>
<p><strong>Summary:</strong> A variety of autonomous navigation algorithms exist that allow robots to move
around in a safe and fast manner. However, many of these algorithms require
parameter re-tuning when facing new environments. In this paper, we propose
PTDRL, a parameter-tuning strategy that adaptively selects from a fixed set of
parameters those that maximize the expected reward for a given navigation
system. Our learning strategy can be used for different environments, different
platforms, and different user prefere...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.15512v1' target='_blank'>Trajectory Generation, Control, and Safety with Denoising Diffusion
  Probabilistic Models</a></h2>
<p><strong>Authors:</strong> Nicol√≤ Botteghi, Federico Califano, Mannes Poel, Christoph Brune</p>
<p><strong>Summary:</strong> We present a framework for safety-critical optimal control of physical
systems based on denoising diffusion probabilistic models (DDPMs). The
technology of control barrier functions (CBFs), encoding desired safety
constraints, is used in combination with DDPMs to plan actions by iteratively
denoising trajectories through a CBF-based guided sampling procedure. At the
same time, the generated trajectories are also guided to maximize a future
cumulative reward representing a specific task to be opt...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.17668v1' target='_blank'>Precise Object Placement Using Force-Torque Feedback</a></h2>
<p><strong>Authors:</strong> Osher Lerner, Zachary Tam, Michael Equi</p>
<p><strong>Summary:</strong> Precise object manipulation and placement is a common problem for household
robots, surgery robots, and robots working on in-situ construction. Prior work
using computer vision, depth sensors, and reinforcement learning lacks the
ability to reactively recover from planning errors, execution errors, or sensor
noise. This work introduces a method that uses force-torque sensing to robustly
place objects in stable poses, even in adversarial environments. On 46 trials,
our method finds success rates ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.13655v1' target='_blank'>Improving GFlowNets with Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Nikita Morozov, Daniil Tiapkin, Sergey Samsonov, Alexey Naumov, Dmitry Vetrov</p>
<p><strong>Summary:</strong> Generative Flow Networks (GFlowNets) treat sampling from distributions over
compositional discrete spaces as a sequential decision-making problem, training
a stochastic policy to construct objects step by step. Recent studies have
revealed strong connections between GFlowNets and entropy-regularized
reinforcement learning. Building on these insights, we propose to enhance
planning capabilities of GFlowNets by applying Monte Carlo Tree Search (MCTS).
Specifically, we show how the MENTS algorithm ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.01991v2' target='_blank'>Generation of Geodesics with Actor-Critic Reinforcement Learning to
  Predict Midpoints</a></h2>
<p><strong>Authors:</strong> Kazumi Kasaura</p>
<p><strong>Summary:</strong> To find the shortest paths for all pairs on manifolds with infinitesimally
defined metrics, we propose to generate them by predicting midpoints
recursively and an actor-critic method to learn midpoint prediction. We prove
the soundness of our approach and show experimentally that the proposed method
outperforms existing methods on both local and global path planning tasks....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.02862v1' target='_blank'>The Unreasonable Effectiveness of LLMs for Query Optimization</a></h2>
<p><strong>Authors:</strong> Peter Akioyamen, Zixuan Yi, Ryan Marcus</p>
<p><strong>Summary:</strong> Recent work in database query optimization has used complex machine learning
strategies, such as customized reinforcement learning schemes. Surprisingly, we
show that LLM embeddings of query text contain useful semantic information for
query optimization. Specifically, we show that a simple binary classifier
deciding between alternative query plans, trained only on a small number of
labeled embedded query vectors, can outperform existing heuristic systems.
Although we only present some prelimina...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.09208v1' target='_blank'>Autonomous Task Completion Based on Goal-directed Answer Set Programming</a></h2>
<p><strong>Authors:</strong> Alexis R. Tudor</p>
<p><strong>Summary:</strong> Task planning for autonomous agents has typically been done using deep
learning models and simulation-based reinforcement learning. This research
proposes combining inductive learning techniques with goal-directed answer set
programming to increase the explainability and reliability of systems for task
breakdown and completion. Preliminary research has led to the creation of a
Python harness that utilizes s(CASP) to solve task problems in a
computationally efficient way. Although this research i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.08649v1' target='_blank'>Exploring Model-based Planning with Policy Networks</a></h2>
<p><strong>Authors:</strong> Tingwu Wang, Jimmy Ba</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) with model-predictive control or
online planning has shown great potential for locomotion control tasks in terms
of both sample efficiency and asymptotic performance. Despite their initial
successes, the existing planning methods search from candidate sequences
randomly generated in the action space, which is inefficient in complex
high-dimensional environments. In this paper, we propose a novel MBRL
algorithm, model-based policy planning (POPLIN), that ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.03509v2' target='_blank'>Driving Style Encoder: Situational Reward Adaptation for General-Purpose
  Planning in Automated Driving</a></h2>
<p><strong>Authors:</strong> Sascha Rosbach, Vinit James, Simon Gro√üjohann, Silviu Homoceanu, Xing Li, Stefan Roth</p>
<p><strong>Summary:</strong> General-purpose planning algorithms for automated driving combine mission,
behavior, and local motion planning. Such planning algorithms map features of
the environment and driving kinematics into complex reward functions. To
achieve this, planning experts often rely on linear reward functions. The
specification and tuning of these reward functions is a tedious process and
requires significant experience. Moreover, a manually designed linear reward
function does not generalize across different d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.04799v2' target='_blank'>RL-RRT: Kinodynamic Motion Planning via Learning Reachability Estimators
  from RL Policies</a></h2>
<p><strong>Authors:</strong> Hao-Tien Lewis Chiang, Jasmine Hsu, Marek Fiser, Lydia Tapia, Aleksandra Faust</p>
<p><strong>Summary:</strong> This paper addresses two challenges facing sampling-based kinodynamic motion
planning: a way to identify good candidate states for local transitions and the
subsequent computationally intractable steering between these candidate states.
Through the combination of sampling-based planning, a Rapidly Exploring
Randomized Tree (RRT) and an efficient kinodynamic motion planner through
machine learning, we propose an efficient solution to long-range planning for
kinodynamic motion planning. First, we ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.02615v2' target='_blank'>Learning Topological Motion Primitives for Knot Planning</a></h2>
<p><strong>Authors:</strong> Mengyuan Yan, Gen Li, Yilin Zhu, Jeannette Bohg</p>
<p><strong>Summary:</strong> In this paper, we approach the challenging problem of motion planning for
knot tying. We propose a hierarchical approach in which the top layer produces
a topological plan and the bottom layer translates this plan into continuous
robot motion. The top layer decomposes a knotting task into sequences of
abstract topological actions based on knot theory. The bottom layer translates
each of these abstract actions into robot motion trajectories through learned
topological motion primitives. To adapt ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.01851v1' target='_blank'>Risk Conditioned Neural Motion Planning</a></h2>
<p><strong>Authors:</strong> Xin Huang, Meng Feng, Ashkan Jasour, Guy Rosman, Brian Williams</p>
<p><strong>Summary:</strong> Risk-bounded motion planning is an important yet difficult problem for
safety-critical tasks. While existing mathematical programming methods offer
theoretical guarantees in the context of constrained Markov decision processes,
they either lack scalability in solving larger problems or produce conservative
plans. Recent advances in deep reinforcement learning improve scalability by
learning policy networks as function approximators. In this paper, we propose
an extension of soft actor critic mod...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.02149v1' target='_blank'>Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search</a></h2>
<p><strong>Authors:</strong> Man Luo, Bowen Du, Konstantin Klemmer, Hongming Zhu, Hongkai Wen</p>
<p><strong>Summary:</strong> Shared e-mobility services have been widely tested and piloted in cities
across the globe, and already woven into the fabric of modern urban planning.
This paper studies a practical yet important problem in those systems: how to
deploy and manage their infrastructure across space and time, so that the
services are ubiquitous to the users while sustainable in profitability.
However, in real-world systems evaluating the performance of different
deployment strategies and then finding the optimal pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.10787v2' target='_blank'>Global Planning for Contact-Rich Manipulation via Local Smoothing of
  Quasi-dynamic Contact Models</a></h2>
<p><strong>Authors:</strong> Tao Pang, H. J. Terry Suh, Lujie Yang, Russ Tedrake</p>
<p><strong>Summary:</strong> The empirical success of Reinforcement Learning (RL) in the setting of
contact-rich manipulation leaves much to be understood from a model-based
perspective, where the key difficulties are often attributed to (i) the
explosion of contact modes, (ii) stiff, non-smooth contact dynamics and the
resulting exploding / discontinuous gradients, and (iii) the non-convexity of
the planning problem. The stochastic nature of RL addresses (i) and (ii) by
effectively sampling and averaging the contact modes....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.10280v2' target='_blank'>NeSIG: A Neuro-Symbolic Method for Learning to Generate Planning
  Problems</a></h2>
<p><strong>Authors:</strong> Carlos N√∫√±ez-Molina, Pablo Mesejo, Juan Fern√°ndez-Olivares</p>
<p><strong>Summary:</strong> In the field of Automated Planning there is often the need for a set of
planning problems from a particular domain, e.g., to be used as training data
for Machine Learning or as benchmarks in planning competitions. In most cases,
these problems are created either by hand or by a domain-specific generator,
putting a burden on the human designers. In this paper we propose NeSIG, to the
best of our knowledge the first domain-independent method for automatically
generating planning problems that are ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.11491v1' target='_blank'>A Supervisory Learning Control Framework for Autonomous & Real-time Task
  Planning for an Underactuated Cooperative Robotic task</a></h2>
<p><strong>Authors:</strong> Sander De Witte, Tom Lefebvre, Thijs Van Hauwermeiren, Guillaume Crevecoeur</p>
<p><strong>Summary:</strong> We introduce a framework for cooperative manipulation, applied on an
underactuated manipulation problem. Two stationary robotic manipulators are
required to cooperate in order to reposition an object within their shared work
space. Control of multi-agent systems for manipulation tasks cannot rely on
individual control strategies with little to no communication between the
agents that serve the common objective through swarming. Instead a coordination
strategy is required that queries subtasks to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.01569v2' target='_blank'>Iterative Option Discovery for Planning, by Planning</a></h2>
<p><strong>Authors:</strong> Kenny Young, Richard S. Sutton</p>
<p><strong>Summary:</strong> Discovering useful temporal abstractions, in the form of options, is widely
thought to be key to applying reinforcement learning and planning to
increasingly complex domains. Building on the empirical success of the Expert
Iteration approach to policy learning used in AlphaZero, we propose Option
Iteration, an analogous approach to option discovery. Rather than learning a
single strong policy that is trained to match the search results everywhere,
Option Iteration learns a set of option policies...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.17115v1' target='_blank'>Optimal Robotic Assembly Sequence Planning: A Sequential Decision-Making
  Approach</a></h2>
<p><strong>Authors:</strong> Kartik Nagpal, Negar Mehr</p>
<p><strong>Summary:</strong> The optimal robot assembly planning problem is challenging due to the
necessity of finding the optimal solution amongst an exponentially vast number
of possible plans, all while satisfying a selection of constraints.
Traditionally, robotic assembly planning problems have been solved using
heuristics, but these methods are specific to a given objective structure or
set of problem parameters. In this paper, we propose a novel approach to
robotic assembly planning that poses assembly sequencing as ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.06357v2' target='_blank'>FOSS: A Self-Learned Doctor for Query Optimizer</a></h2>
<p><strong>Authors:</strong> Kai Zhong, Luming Sun, Tao Ji, Cuiping Li, Hong Chen</p>
<p><strong>Summary:</strong> Various works have utilized deep learning to address the query optimization
problem in database system. They either learn to construct plans from scratch
in a bottom-up manner or steer the plan generation behavior of traditional
optimizer using hints. While these methods have achieved some success, they
face challenges in either low training efficiency or limited plan search space.
To address these challenges, we introduce FOSS, a novel framework for query
optimization based on deep reinforcemen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.04647v3' target='_blank'>Latent Plan Transformer for Trajectory Abstraction: Planning as Latent
  Space Inference</a></h2>
<p><strong>Authors:</strong> Deqian Kong, Dehong Xu, Minglu Zhao, Bo Pang, Jianwen Xie, Andrew Lizarraga, Yuhao Huang, Sirui Xie, Ying Nian Wu</p>
<p><strong>Summary:</strong> In tasks aiming for long-term returns, planning becomes essential. We study
generative modeling for planning with datasets repurposed from offline
reinforcement learning. Specifically, we identify temporal consistency in the
absence of step-wise rewards as one key technical challenge. We introduce the
Latent Plan Transformer (LPT), a novel model that leverages a latent variable
to connect a Transformer-based trajectory generator and the final return. LPT
can be learned with maximum likelihood es...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.00843v2' target='_blank'>Large Language Models are Learnable Planners for Long-Term
  Recommendation</a></h2>
<p><strong>Authors:</strong> Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng</p>
<p><strong>Summary:</strong> Planning for both immediate and long-term benefits becomes increasingly
important in recommendation. Existing methods apply Reinforcement Learning (RL)
to learn planning capacity by maximizing cumulative reward for long-term
recommendation. However, the scarcity of recommendation data presents
challenges such as instability and susceptibility to overfitting when training
RL models from scratch, resulting in sub-optimal performance. In this light, we
propose to leverage the remarkable planning ca...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.12463v1' target='_blank'>Reinforcement learning based local path planning for mobile robot</a></h2>
<p><strong>Authors:</strong> Mehmet Gok, Mehmet Tekerek, Hamza Aydemir</p>
<p><strong>Summary:</strong> Different methods are used for a mobile robot to go to a specific target
location. These methods work in different ways for online and offline
scenarios. In the offline scenario, an environment map is created once, and
fixed path planning is made on this map to reach the target. Path planning
algorithms such as A* and RRT (Rapidly-Exploring Random Tree) are the examples
of offline methods. The most obvious situation here is the need to re-plan the
path for changing conditions of the loaded map. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.05291v2' target='_blank'>Long-horizon Locomotion and Manipulation on a Quadrupedal Robot with
  Large Language Models</a></h2>
<p><strong>Authors:</strong> Yutao Ouyang, Jinhan Li, Yunfei Li, Zhongyu Li, Chao Yu, Koushil Sreenath, Yi Wu</p>
<p><strong>Summary:</strong> We present a large language model (LLM) based system to empower quadrupedal
robots with problem-solving abilities for long-horizon tasks beyond short-term
motions. Long-horizon tasks for quadrupeds are challenging since they require
both a high-level understanding of the semantics of the problem for task
planning and a broad range of locomotion and manipulation skills to interact
with the environment. Our system builds a high-level reasoning layer with large
language models, which generates hybr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.07511v1' target='_blank'>Generative Probabilistic Planning for Optimizing Supply Chain Networks</a></h2>
<p><strong>Authors:</strong> Hyung-il Ahn, Santiago Olivar, Hershel Mehta, Young Chol Song</p>
<p><strong>Summary:</strong> Supply chain networks in enterprises are typically composed of complex
topological graphs involving various types of nodes and edges, accommodating
numerous products with considerable demand and supply variability. However, as
supply chain networks expand in size and complexity, traditional supply chain
planning methods (e.g., those found in heuristic rule-based and operations
research-based systems) tend to become locally optimal or lack computational
scalability, resulting in substantial imbal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.20475v1' target='_blank'>SatFlow: Scalable Network Planning for LEO Mega-Constellations</a></h2>
<p><strong>Authors:</strong> Sheng Cen, Qiying Pan, Yifei Zhu, Bo Li</p>
<p><strong>Summary:</strong> Low-earth-orbit (LEO) satellite communication networks have evolved into
mega-constellations with hundreds to thousands of satellites inter-connecting
with inter-satellite links (ISLs). Network planning, which plans for network
resources and architecture to improve the network performance and save
operational costs, is crucial for satellite network management. However, due to
the large scale of mega-constellations, high dynamics of satellites, and
complex distribution of real-world traffic, it i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.04839v1' target='_blank'>DRL-Based Medium-Term Planning of Renewable-Integrated Self-Scheduling
  Cascaded Hydropower to Guide Wholesale Market Participation</a></h2>
<p><strong>Authors:</strong> Xianbang Chen, Yikui Liu, Neng Fan, Lei Wu</p>
<p><strong>Summary:</strong> For self-scheduling cascaded hydropower (S-CHP) facilities, medium-term
planning is a critical step that coordinates water availability over the
medium-term horizon, providing water usage guidance for their short-term
operations in wholesale market participation. Typically, medium-term planning
strategies (e.g., reservoir storage targets at the end of each short-term
period) are determined by either optimization methods or rules of thumb.
However, with the integration of variable renewable energ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.03507v3' target='_blank'>A storage expansion planning framework using reinforcement learning and
  simulation-based optimization</a></h2>
<p><strong>Authors:</strong> S. Tsianikas, N. Yousefi, J. Zhou, M. Rodgers, D. W. Coit</p>
<p><strong>Summary:</strong> In the wake of the highly electrified future ahead of us, the role of energy
storage is crucial wherever distributed generation is abundant, such as in
microgrid settings. Given the variety of storage options that are becoming more
and more economical, determining which type of storage technology to invest in,
along with the appropriate timing and capacity becomes a critical research
question. It is inevitable that these problems will continue to become
increasingly relevant in the future and re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.05798v2' target='_blank'>Planning on the fast lane: Learning to interact using attention
  mechanisms in path integral inverse reinforcement learning</a></h2>
<p><strong>Authors:</strong> Sascha Rosbach, Xing Li, Simon Gro√üjohann, Silviu Homoceanu, Stefan Roth</p>
<p><strong>Summary:</strong> General-purpose trajectory planning algorithms for automated driving utilize
complex reward functions to perform a combined optimization of strategic,
behavioral, and kinematic features. The specification and tuning of a single
reward function is a tedious task and does not generalize over a large set of
traffic situations. Deep learning approaches based on path integral inverse
reinforcement learning have been successfully applied to predict local
situation-dependent reward functions using feat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.04136v1' target='_blank'>Design and Planning of Flexible Mobile Micro-Grids Using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Cesare Caputo, Michel-Alexandre Cardin, Pudong Ge, Fei Teng, Anna Korre, Ehecatl Antonio del Rio Chanona</p>
<p><strong>Summary:</strong> Ongoing risks from climate change have impacted the livelihood of global
nomadic communities, and are likely to lead to increased migratory movements in
coming years. As a result, mobility considerations are becoming increasingly
important in energy systems planning, particularly to achieve energy access in
developing countries. Advanced Plug and Play control strategies have been
recently developed with such a decentralized framework in mind, more easily
allowing for the interconnection of nomad...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1101.4003v3' target='_blank'>Dyna-H: a heuristic planning reinforcement learning algorithm applied to
  role-playing-game strategy decision systems</a></h2>
<p><strong>Authors:</strong> Matilde Santos, Jose Antonio Martin H., Victoria Lopez, Guillermo Botella</p>
<p><strong>Summary:</strong> In a Role-Playing Game, finding optimal trajectories is one of the most
important tasks. In fact, the strategy decision system becomes a key component
of a game engine. Determining the way in which decisions are taken (online,
batch or simulated) and the consumed resources in decision making (e.g.
execution time, memory) will influence, in mayor degree, the game performance.
When classical search algorithms such as A* can be used, they are the very
first option. Nevertheless, such methods rely o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1401.3437v1' target='_blank'>Learning Partially Observable Deterministic Action Models</a></h2>
<p><strong>Authors:</strong> Eyal Amir, Allen Chang</p>
<p><strong>Summary:</strong> We present exact algorithms for identifying deterministic-actions effects and
preconditions in dynamic partially observable domains. They apply when one does
not know the action model(the way actions affect the world) of a domain and
must learn it from partial observations over time. Such scenarios are common in
real world applications. They are challenging for AI tasks because traditional
domain structures that underly tractability (e.g., conditional independence)
fail there (e.g., world featur...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1511.09249v1' target='_blank'>On Learning to Think: Algorithmic Information Theory for Novel
  Combinations of Reinforcement Learning Controllers and Recurrent Neural World
  Models</a></h2>
<p><strong>Authors:</strong> Juergen Schmidhuber</p>
<p><strong>Summary:</strong> This paper addresses the general problem of reinforcement learning (RL) in
partially observable environments. In 2013, our large RL recurrent neural
networks (RNNs) learned from scratch to drive simulated cars from
high-dimensional video input. However, real brains are more powerful in many
ways. In particular, they learn a predictive model of their initially unknown
environment, and somehow use it for abstract (e.g., hierarchical) planning and
reasoning. Guided by algorithmic information theory...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.05329v1' target='_blank'>Sub-Goal Trees -- a Framework for Goal-Directed Trajectory Prediction
  and Optimization</a></h2>
<p><strong>Authors:</strong> Tom Jurgenson, Edward Groshev, Aviv Tamar</p>
<p><strong>Summary:</strong> Many AI problems, in robotics and other domains, are goal-directed,
essentially seeking a trajectory leading to some goal state. In such problems,
the way we choose to represent a trajectory underlies algorithms for trajectory
prediction and optimization. Interestingly, most all prior work in imitation
and reinforcement learning builds on a sequential trajectory representation --
calculating the next state in the trajectory given its predecessors. We propose
a different perspective: a goal-condi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06153v2' target='_blank'>HJB Optimal Feedback Control with Deep Differential Value Functions and
  Action Constraints</a></h2>
<p><strong>Authors:</strong> Michael Lutter, Boris Belousov, Kim Listmann, Debora Clever, Jan Peters</p>
<p><strong>Summary:</strong> Learning optimal feedback control laws capable of executing optimal
trajectories is essential for many robotic applications. Such policies can be
learned using reinforcement learning or planned using optimal control. While
reinforcement learning is sample inefficient, optimal control only plans an
optimal trajectory from a specific starting configuration. In this paper we
propose deep optimal feedback control to learn an optimal feedback policy
rather than a single trajectory. By exploiting the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.07294v2' target='_blank'>Selective Network Discovery via Deep Reinforcement Learning on Embedded
  Spaces</a></h2>
<p><strong>Authors:</strong> Peter Morales, Rajmonda Sulo Caceres, Tina Eliassi-Rad</p>
<p><strong>Summary:</strong> Complex networks are often either too large for full exploration, partially
accessible, or partially observed. Downstream learning tasks on these
incomplete networks can produce low quality results. In addition, reducing the
incompleteness of the network can be costly and nontrivial. As a result,
network discovery algorithms optimized for specific downstream learning tasks
given resource collection constraints are of great interest. In this paper, we
formulate the task-specific network discovery...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.12217v4' target='_blank'>Visual Exploration and Energy-aware Path Planning via Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Amir Niaraki, Jeremy Roghair, Ali Jannesari</p>
<p><strong>Summary:</strong> Visual exploration and smart data collection via autonomous vehicles is an
attractive topic in various disciplines. Disturbances like wind significantly
influence both the power consumption of the flying robots and the performance
of the camera. We propose a reinforcement learning approach which combines the
effects of the power consumption and the object detection modules to develop a
policy for object detection in large areas with limited battery life. The
learning model enables dynamic learni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.00434v2' target='_blank'>Integrating Deep Reinforcement Learning with Model-based Path Planners
  for Automated Driving</a></h2>
<p><strong>Authors:</strong> Ekim Yurtsever, Linda Capito, Keith Redmill, Umit Ozguner</p>
<p><strong>Summary:</strong> Automated driving in urban settings is challenging. Human participant
behavior is difficult to model, and conventional, rule-based Automated Driving
Systems (ADSs) tend to fail when they face unmodeled dynamics. On the other
hand, the more recent, end-to-end Deep Reinforcement Learning (DRL) based
model-free ADSs have shown promising results. However, pure learning-based
approaches lack the hard-coded safety measures of model-based controllers. Here
we propose a hybrid approach for integrating a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.11274v1' target='_blank'>On Reward-Free Reinforcement Learning with Linear Function Approximation</a></h2>
<p><strong>Authors:</strong> Ruosong Wang, Simon S. Du, Lin F. Yang, Ruslan Salakhutdinov</p>
<p><strong>Summary:</strong> Reward-free reinforcement learning (RL) is a framework which is suitable for
both the batch RL setting and the setting where there are many reward functions
of interest. During the exploration phase, an agent collects samples without
using a pre-specified reward function. After the exploration phase, a reward
function is given, and the agent uses samples collected during the exploration
phase to compute a near-optimal policy. Jin et al. [2020] showed that in the
tabular setting, the agent only n...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.00544v2' target='_blank'>UAV Path Planning for Wireless Data Harvesting: A Deep Reinforcement
  Learning Approach</a></h2>
<p><strong>Authors:</strong> Harald Bayerlein, Mirco Theile, Marco Caccamo, David Gesbert</p>
<p><strong>Summary:</strong> Autonomous deployment of unmanned aerial vehicles (UAVs) supporting
next-generation communication networks requires efficient trajectory planning
methods. We propose a new end-to-end reinforcement learning (RL) approach to
UAV-enabled data collection from Internet of Things (IoT) devices in an urban
environment. An autonomous drone is tasked with gathering data from distributed
sensor nodes subject to limited flying time and obstacle avoidance. While
previous approaches, learning and non-learnin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.00993v1' target='_blank'>MADRaS : Multi Agent Driving Simulator</a></h2>
<p><strong>Authors:</strong> Anirban Santara, Sohan Rudra, Sree Aditya Buridi, Meha Kaushik, Abhishek Naik, Bharat Kaul, Balaraman Ravindran</p>
<p><strong>Summary:</strong> In this work, we present MADRaS, an open-source multi-agent driving simulator
for use in the design and evaluation of motion planning algorithms for
autonomous driving. MADRaS provides a platform for constructing a wide variety
of highway and track driving scenarios where multiple driving agents can train
for motion planning tasks using reinforcement learning and other machine
learning algorithms. MADRaS is built on TORCS, an open-source car-racing
simulator. TORCS offers a variety of cars with ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.11940v1' target='_blank'>Motion Planner Augmented Reinforcement Learning for Robot Manipulation
  in Obstructed Environments</a></h2>
<p><strong>Authors:</strong> Jun Yamada, Youngwoon Lee, Gautam Salhotra, Karl Pertsch, Max Pflueger, Gaurav S. Sukhatme, Joseph J. Lim, Peter Englert</p>
<p><strong>Summary:</strong> Deep reinforcement learning (RL) agents are able to learn contact-rich
manipulation tasks by maximizing a reward signal, but require large amounts of
experience, especially in environments with many obstacles that complicate
exploration. In contrast, motion planners use explicit models of the agent and
environment to plan collision-free paths to faraway goals, but suffer from
inaccurate models in tasks that require contacts with the environment. To
combine the benefits of both approaches, we pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.10043v2' target='_blank'>Content Masked Loss: Human-Like Brush Stroke Planning in a Reinforcement
  Learning Painting Agent</a></h2>
<p><strong>Authors:</strong> Peter Schaldenbrand, Jean Oh</p>
<p><strong>Summary:</strong> The objective of most Reinforcement Learning painting agents is to minimize
the loss between a target image and the paint canvas. Human painter artistry
emphasizes important features of the target image rather than simply
reproducing it (DiPaola 2007). Using adversarial or L2 losses in the RL
painting models, although its final output is generally a work of finesse,
produces a stroke sequence that is vastly different from that which a human
would produce since the model does not have knowledge a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.15373v1' target='_blank'>Model-Based Visual Planning with Self-Supervised Functional Distances</a></h2>
<p><strong>Authors:</strong> Stephen Tian, Suraj Nair, Frederik Ebert, Sudeep Dasari, Benjamin Eysenbach, Chelsea Finn, Sergey Levine</p>
<p><strong>Summary:</strong> A generalist robot must be able to complete a variety of tasks in its
environment. One appealing way to specify each task is in terms of a goal
observation. However, learning goal-reaching policies with reinforcement
learning remains a challenging problem, particularly when hand-engineered
reward functions are not available. Learned dynamics models are a promising
approach for learning about the environment without rewards or task-directed
data, but planning to reach goals with such a model requ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.01993v1' target='_blank'>A Survey of Deep RL and IL for Autonomous Driving Policy Learning</a></h2>
<p><strong>Authors:</strong> Zeyu Zhu, Huijing Zhao</p>
<p><strong>Summary:</strong> Autonomous driving (AD) agents generate driving policies based on online
perception results, which are obtained at multiple levels of abstraction, e.g.,
behavior planning, motion planning and control. Driving policies are crucial to
the realization of safe, efficient and harmonious driving behaviors, where AD
agents still face substantial challenges in complex scenarios. Due to their
successful application in fields such as robotics and video games, the use of
deep reinforcement learning (DRL) a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.11735v2' target='_blank'>Lifted Model Checking for Relational MDPs</a></h2>
<p><strong>Authors:</strong> Wen-Chi Yang, Jean-Fran√ßois Raskin, Luc De Raedt</p>
<p><strong>Summary:</strong> Probabilistic model checking has been developed for verifying systems that
have stochastic and nondeterministic behavior. Given a probabilistic system, a
probabilistic model checker takes a property and checks whether or not the
property holds in that system. For this reason, probabilistic model checking
provide rigorous guarantees. So far, however, probabilistic model checking has
focused on propositional models where a state is represented by a symbol. On
the other hand, it is commonly require...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1610.06204v2' target='_blank'>A Reinforcement Learning Approach to the View Planning Problem</a></h2>
<p><strong>Authors:</strong> Mustafa Devrim Kaba, Mustafa Gokhan Uzunbas, Ser Nam Lim</p>
<p><strong>Summary:</strong> We present a Reinforcement Learning (RL) solution to the view planning
problem (VPP), which generates a sequence of view points that are capable of
sensing all accessible area of a given object represented as a 3D model. In
doing so, the goal is to minimize the number of view points, making the VPP a
class of set covering optimization problem (SCOP). The SCOP is NP-hard, and the
inapproximability results tell us that the greedy algorithm provides the best
approximation that runs in polynomial ti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.10090v4' target='_blank'>Non-Stationary Markov Decision Processes, a Worst-Case Approach using
  Model-Based Reinforcement Learning, Extended version</a></h2>
<p><strong>Authors:</strong> Erwan Lecarpentier, Emmanuel Rachelson</p>
<p><strong>Summary:</strong> This work tackles the problem of robust zero-shot planning in non-stationary
stochastic environments. We study Markov Decision Processes (MDPs) evolving
over time and consider Model-Based Reinforcement Learning algorithms in this
setting. We make two hypotheses: 1) the environment evolves continuously with a
bounded evolution rate; 2) a current model is known at each decision epoch but
not its evolution. Our contribution can be presented in four points. 1) we
define a specific class of MDPs that...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.11082v1' target='_blank'>How You Act Tells a Lot: Privacy-Leakage Attack on Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, Dawn Song</p>
<p><strong>Summary:</strong> Machine learning has been widely applied to various applications, some of
which involve training with privacy-sensitive data. A modest number of data
breaches have been studied, including credit card information in natural
language data and identities from face dataset. However, most of these studies
focus on supervised learning models. As deep reinforcement learning (DRL) has
been deployed in a number of real-world systems, such as indoor robot
navigation, whether trained DRL policies can leak ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.05965v1' target='_blank'>Autonomous Penetration Testing using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jonathon Schwartz, Hanna Kurniawati</p>
<p><strong>Summary:</strong> Penetration testing (pentesting) involves performing a controlled attack on a
computer system in order to assess it's security. Although an effective method
for testing security, pentesting requires highly skilled practitioners and
currently there is a growing shortage of skilled cyber security professionals.
One avenue for alleviating this problem is automate the pentesting process
using artificial intelligence techniques. Current approaches to automated
pentesting have relied on model-based pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.02609v2' target='_blank'>UAV Coverage Path Planning under Varying Power Constraints using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mirco Theile, Harald Bayerlein, Richard Nai, David Gesbert, Marco Caccamo</p>
<p><strong>Summary:</strong> Coverage path planning (CPP) is the task of designing a trajectory that
enables a mobile agent to travel over every point of an area of interest. We
propose a new method to control an unmanned aerial vehicle (UAV) carrying a
camera on a CPP mission with random start positions and multiple options for
landing positions in an environment containing no-fly zones. While numerous
approaches have been proposed to solve similar CPP problems, we leverage
end-to-end reinforcement learning (RL) to learn a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.10066v5' target='_blank'>Learning Off-Policy with Online Planning</a></h2>
<p><strong>Authors:</strong> Harshit Sikchi, Wenxuan Zhou, David Held</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) in low-data and risk-sensitive domains requires
performant and flexible deployment policies that can readily incorporate
constraints during deployment. One such class of policies are the
semi-parametric H-step lookahead policies, which select actions using
trajectory optimization over a dynamics model for a fixed horizon with a
terminal value function. In this work, we investigate a novel instantiation of
H-step lookahead with a learned model and a terminal value func...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.13744v1' target='_blank'>Reinforcement Learning-based Joint Path and Energy Optimization of
  Cellular-Connected Unmanned Aerial Vehicles</a></h2>
<p><strong>Authors:</strong> Arash Hooshmand</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicles (UAVs) have attracted considerable research interest
recently. Especially when it comes to the realm of Internet of Things, the UAVs
with Internet connectivity are one of the main demands. Furthermore, the energy
constraint i.e. battery limit is a bottle-neck of the UAVs that can limit their
applications. We try to address and solve the energy problem. Therefore, a path
planning method for a cellular-connected UAV is proposed that will enable the
UAV to plan its path in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.12855v7' target='_blank'>Modular Deep Reinforcement Learning for Continuous Motion Planning with
  Temporal Logic</a></h2>
<p><strong>Authors:</strong> Mingyu Cai, Mohammadhosein Hasanbeig, Shaoping Xiao, Alessandro Abate, Zhen Kan</p>
<p><strong>Summary:</strong> This paper investigates the motion planning of autonomous dynamical systems
modeled by Markov decision processes (MDP) with unknown transition
probabilities over continuous state and action spaces. Linear temporal logic
(LTL) is used to specify high-level tasks over infinite horizon, which can be
converted into a limit deterministic generalized B\"uchi automaton (LDGBA) with
several accepting sets. The novelty is to design an embedded product MDP
(EP-MDP) between the LDGBA and the MDP by incorpo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.13431v2' target='_blank'>An Offline Risk-aware Policy Selection Method for Bayesian Markov
  Decision Processes</a></h2>
<p><strong>Authors:</strong> Giorgio Angelotti, Nicolas Drougard, Caroline Ponzoni Carvalho Chanel</p>
<p><strong>Summary:</strong> In Offline Model Learning for Planning and in Offline Reinforcement Learning,
the limited data set hinders the estimate of the Value function of the relative
Markov Decision Process (MDP). Consequently, the performance of the obtained
policy in the real world is bounded and possibly risky, especially when the
deployment of a wrong policy can lead to catastrophic consequences. For this
reason, several pathways are being followed with the scope of reducing the
model error (or the distributional sh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.10949v1' target='_blank'>Optimizing Trajectories for Highway Driving with Offline Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Branka Mirchevska, Moritz Werling, Joschka Boedecker</p>
<p><strong>Summary:</strong> Implementing an autonomous vehicle that is able to output feasible, smooth
and efficient trajectories is a long-standing challenge. Several approaches
have been considered, roughly falling under two categories: rule-based and
learning-based approaches. The rule-based approaches, while guaranteeing safety
and feasibility, fall short when it comes to long-term planning and
generalization. The learning-based approaches are able to account for long-term
planning and generalization to unseen situatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.05196v1' target='_blank'>Automatically Learning Fallback Strategies with Model-Free Reinforcement
  Learning in Safety-Critical Driving Scenarios</a></h2>
<p><strong>Authors:</strong> Ugo Lecerf, Christelle Yemdji-Tchassi, S√©bastien Aubert, Pietro Michiardi</p>
<p><strong>Summary:</strong> When learning to behave in a stochastic environment where safety is critical,
such as driving a vehicle in traffic, it is natural for human drivers to plan
fallback strategies as a backup to use if ever there is an unexpected change in
the environment. Knowing to expect the unexpected, and planning for such
outcomes, increases our capability for being robust to unseen scenarios and may
help prevent catastrophic failures. Control of Autonomous Vehicles (AVs) has a
particular interest in knowing w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.12026v1' target='_blank'>BATS: Best Action Trajectory Stitching</a></h2>
<p><strong>Authors:</strong> Ian Char, Viraj Mehta, Adam Villaflor, John M. Dolan, Jeff Schneider</p>
<p><strong>Summary:</strong> The problem of offline reinforcement learning focuses on learning a good
policy from a log of environment interactions. Past efforts for developing
algorithms in this area have revolved around introducing constraints to online
reinforcement learning algorithms to ensure the actions of the learned policy
are constrained to the logged data. In this work, we explore an alternative
approach by planning on the fixed dataset directly. Specifically, we introduce
an algorithm which forms a tabular Marko...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.11140v2' target='_blank'>Human-in-the-loop: Provably Efficient Preference-based Reinforcement
  Learning with General Function Approximation</a></h2>
<p><strong>Authors:</strong> Xiaoyu Chen, Han Zhong, Zhuoran Yang, Zhaoran Wang, Liwei Wang</p>
<p><strong>Summary:</strong> We study human-in-the-loop reinforcement learning (RL) with trajectory
preferences, where instead of receiving a numeric reward at each step, the
agent only receives preferences over trajectory pairs from a human overseer.
The goal of the agent is to learn the optimal policy which is most preferred by
the human overseer. Despite the empirical successes, the theoretical
understanding of preference-based RL (PbRL) is only limited to the tabular
case. In this paper, we propose the first optimistic ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.14237v1' target='_blank'>Provably Sample-Efficient RL with Side Information about Latent Dynamics</a></h2>
<p><strong>Authors:</strong> Yao Liu, Dipendra Misra, Miro Dud√≠k, Robert E. Schapire</p>
<p><strong>Summary:</strong> We study reinforcement learning (RL) in settings where observations are
high-dimensional, but where an RL agent has access to abstract knowledge about
the structure of the state space, as is the case, for example, when a robot is
tasked to go to a specific room in a building using observations from its own
camera, while having access to the floor plan. We formalize this setting as
transfer reinforcement learning from an abstract simulator, which we assume is
deterministic (such as a simple model...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.02162v1' target='_blank'>Tackling Real-World Autonomous Driving using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Paolo Maramotti, Alessandro Paolo Capasso, Giulio Bacchiani, Alberto Broggi</p>
<p><strong>Summary:</strong> In the typical autonomous driving stack, planning and control systems
represent two of the most crucial components in which data retrieved by sensors
and processed by perception algorithms are used to implement a safe and
comfortable self-driving behavior. In particular, the planning module predicts
the path the autonomous car should follow taking the correct high-level
maneuver, while control systems perform a sequence of low-level actions,
controlling steering angle, throttle and brake. In thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.08087v2' target='_blank'>Autonomous Resource Management in Construction Companies Using Deep
  Reinforcement Learning Based on IoT</a></h2>
<p><strong>Authors:</strong> Maryam Soleymani, Mahdi Bonyani, Meghdad Attarzadeh</p>
<p><strong>Summary:</strong> Resource allocation is one of the most critical issues in planning
construction projects, due to its direct impact on cost, time, and quality.
There are usually specific allocation methods for autonomous resource
management according to the projects objectives. However, integrated planning
and optimization of utilizing resources in an entire construction organization
are scarce. The purpose of this study is to present an automatic resource
allocation structure for construction companies based on...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.05393v2' target='_blank'>Exploration Policies for On-the-Fly Controller Synthesis: A
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Tom√°s Delgado, Marco S√°nchez Sorondo, V√≠ctor Braberman, Sebasti√°n Uchitel</p>
<p><strong>Summary:</strong> Controller synthesis is in essence a case of model-based planning for
non-deterministic environments in which plans (actually ''strategies'') are
meant to preserve system goals indefinitely. In the case of supervisory control
environments are specified as the parallel composition of state machines and
valid strategies are required to be ''non-blocking'' (i.e., always enabling the
environment to reach certain marked states) in addition to safe (i.e., keep the
system within a safe zone). Recently,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.04583v3' target='_blank'>Wall Street Tree Search: Risk-Aware Planning for Offline Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Dan Elbaz, Gal Novik, Oren Salzman</p>
<p><strong>Summary:</strong> Offline reinforcement-learning (RL) algorithms learn to make decisions using
a given, fixed training dataset without online data collection. This problem
setting is captivating because it holds the promise of utilizing previously
collected datasets without any costly or risky interaction with the
environment. However, this promise also bears the drawback of this setting as
the restricted dataset induces uncertainty because the agent can encounter
unfamiliar sequences of states and actions that t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.04036v2' target='_blank'>Deep Reinforcement Learning for Autonomous Ground Vehicle Exploration
  Without A-Priori Maps</a></h2>
<p><strong>Authors:</strong> Shathushan Sivashangaran, Azim Eskandarian</p>
<p><strong>Summary:</strong> Autonomous Ground Vehicles (AGVs) are essential tools for a wide range of
applications stemming from their ability to operate in hazardous environments
with minimal human operator input. Effective motion planning is paramount for
successful operation of AGVs. Conventional motion planning algorithms are
dependent on prior knowledge of environment characteristics and offer limited
utility in information poor, dynamically altering environments such as areas
where emergency hazards like fire and ear...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.11575v1' target='_blank'>ARiADNE: A Reinforcement learning approach using Attention-based Deep
  Networks for Exploration</a></h2>
<p><strong>Authors:</strong> Yuhong Cao, Tianxiang Hou, Yizhuo Wang, Xian Yi, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> In autonomous robot exploration tasks, a mobile robot needs to actively
explore and map an unknown environment as fast as possible. Since the
environment is being revealed during exploration, the robot needs to frequently
re-plan its path online, as new information is acquired by onboard sensors and
used to update its partial map. While state-of-the-art exploration planners are
frontier- and sampling-based, encouraged by the recent development in deep
reinforcement learning (DRL), we propose ARi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.03326v4' target='_blank'>Finite Time Lyapunov Exponent Analysis of Model Predictive Control and
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kartik Krishna, Steven L. Brunton, Zhuoyuan Song</p>
<p><strong>Summary:</strong> Finite-time Lyapunov exponents (FTLEs) provide a powerful approach to compute
time-varying analogs of invariant manifolds in unsteady fluid flow fields.
These manifolds are useful to visualize the transport mechanisms of passive
tracers advecting with the flow. However, many vehicles and mobile sensors are
not passive, but are instead actuated according to some intelligent trajectory
planning or control law; for example, model predictive control and
reinforcement learning are often used to desig...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.11290v4' target='_blank'>Massively Scalable Inverse Reinforcement Learning in Google Maps</a></h2>
<p><strong>Authors:</strong> Matt Barnes, Matthew Abueg, Oliver F. Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus Wulfmeier, Shawn O'Banion</p>
<p><strong>Summary:</strong> Inverse reinforcement learning (IRL) offers a powerful and general framework
for learning humans' latent preferences in route recommendation, yet no
approach has successfully addressed planetary-scale problems with hundreds of
millions of states and demonstration trajectories. In this paper, we introduce
scaling techniques based on graph compression, spatial parallelization, and
improved initialization conditions inspired by a connection to eigenvector
algorithms. We revisit classic IRL methods ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.18459v2' target='_blank'>Diffusion Model is an Effective Planner and Data Synthesizer for
  Multi-Task Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, Xuelong Li</p>
<p><strong>Summary:</strong> Diffusion models have demonstrated highly-expressive generative capabilities
in vision and NLP. Recent studies in reinforcement learning (RL) have shown
that diffusion models are also powerful in modeling complex policies or
trajectories in offline datasets. However, these works have been limited to
single-task settings where a generalist agent capable of addressing multi-task
predicaments is absent. In this paper, we aim to investigate the effectiveness
of a single diffusion model in modeling l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.13372v2' target='_blank'>Submodular Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Manish Prajapat, Mojm√≠r Mutn√Ω, Melanie N. Zeilinger, Andreas Krause</p>
<p><strong>Summary:</strong> In reinforcement learning (RL), rewards of states are typically considered
additive, and following the Markov assumption, they are $\textit{independent}$
of states visited previously. In many important applications, such as coverage
control, experiment design and informative path planning, rewards naturally
have diminishing returns, i.e., their value decreases in light of similar
states visited previously. To tackle this, we propose $\textit{submodular RL}$
(SubRL), a paradigm which seeks to opt...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.10124v1' target='_blank'>Intelligent Communication Planning for Constrained Environmental IoT
  Sensing with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yi Hu, Jinhang Zuo, Bob Iannucci, Carlee Joe-Wong</p>
<p><strong>Summary:</strong> Internet of Things (IoT) technologies have enabled numerous data-driven
mobile applications and have the potential to significantly improve
environmental monitoring and hazard warnings through the deployment of a
network of IoT sensors. However, these IoT devices are often power-constrained
and utilize wireless communication schemes with limited bandwidth. Such power
constraints limit the amount of information each device can share across the
network, while bandwidth limitations hinder sensors' ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.16008v1' target='_blank'>EnsembleFollower: A Hybrid Car-Following Framework Based On
  Reinforcement Learning and Hierarchical Planning</a></h2>
<p><strong>Authors:</strong> Xu Han, Xianda Chen, Meixin Zhu, Pinlong Cai, Jianshan Zhou, Xiaowen Chu</p>
<p><strong>Summary:</strong> Car-following models have made significant contributions to our understanding
of longitudinal driving behavior. However, they often exhibit limited accuracy
and flexibility, as they cannot fully capture the complexity inherent in
car-following processes, or may falter in unseen scenarios due to their
reliance on confined driving skills present in training data. It is worth
noting that each car-following model possesses its own strengths and weaknesses
depending on specific driving scenarios. The...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.02815v1' target='_blank'>Near-continuous time Reinforcement Learning for continuous state-action
  spaces</a></h2>
<p><strong>Authors:</strong> Lorenzo Croissant, Marc Abeille, Bruno Bouchard</p>
<p><strong>Summary:</strong> We consider the Reinforcement Learning problem of controlling an unknown
dynamical system to maximise the long-term average reward along a single
trajectory. Most of the literature considers system interactions that occur in
discrete time and discrete state-action spaces. Although this standpoint is
suitable for games, it is often inadequate for mechanical or digital systems in
which interactions occur at a high frequency, if not in continuous time, and
whose state spaces are large if not inhere...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.12534v1' target='_blank'>Trip Planning for Autonomous Vehicles with Wireless Data Transfer Needs
  Using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yousef AlSaqabi, Bhaskar Krishnamachari</p>
<p><strong>Summary:</strong> With recent advancements in the field of communications and the Internet of
Things, vehicles are becoming more aware of their environment and are evolving
towards full autonomy. Vehicular communication opens up the possibility for
vehicle-to-infrastructure interaction, where vehicles could share information
with components such as cameras, traffic lights, and signage that support a
countrys road system. As a result, vehicles are becoming more than just a means
of transportation; they are collect...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.15578v2' target='_blank'>Explicit-Implicit Subgoal Planning for Long-Horizon Tasks with Sparse
  Reward</a></h2>
<p><strong>Authors:</strong> Fangyuan Wang, Anqing Duan, Peng Zhou, Shengzeng Huo, Guodong Guo, Chenguang Yang, David Navarro-Alarcon</p>
<p><strong>Summary:</strong> The challenges inherent in long-horizon tasks in robotics persist due to the
typical inefficient exploration and sparse rewards in traditional reinforcement
learning approaches. To address these challenges, we have developed a novel
algorithm, termed Explicit-Implicit Subgoal Planning (EISP), designed to tackle
long-horizon tasks through a divide-and-conquer approach. We utilize two
primary criteria, feasibility and optimality, to ensure the quality of the
generated subgoals. EISP consists of th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.06552v1' target='_blank'>Deceptive Path Planning via Reinforcement Learning with Graph Neural
  Networks</a></h2>
<p><strong>Authors:</strong> Michael Y. Fatemi, Wesley A. Suttle, Brian M. Sadler</p>
<p><strong>Summary:</strong> Deceptive path planning (DPP) is the problem of designing a path that hides
its true goal from an outside observer. Existing methods for DPP rely on
unrealistic assumptions, such as global state observability and perfect model
knowledge, and are typically problem-specific, meaning that even minor changes
to a previously solved problem can force expensive computation of an entirely
new solution. Given these drawbacks, such methods do not generalize to unseen
problem instances, lack scalability to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.10833v1' target='_blank'>Deep Reinforcement Learning-based Large-scale Robot Exploration</a></h2>
<p><strong>Authors:</strong> Yuhong Cao, Rui Zhao, Yizhuo Wang, Bairan Xiang, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> In this work, we propose a deep reinforcement learning (DRL) based reactive
planner to solve large-scale Lidar-based autonomous robot exploration problems
in 2D action space. Our DRL-based planner allows the agent to reactively plan
its exploration path by making implicit predictions about unknown areas, based
on a learned estimation of the underlying transition model of the environment.
To this end, our approach relies on learned attention mechanisms for their
powerful ability to capture long-t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.17456v3' target='_blank'>Imitating Cost-Constrained Behaviors in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Qian Shao, Pradeep Varakantham, Shih-Fen Cheng</p>
<p><strong>Summary:</strong> Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcemen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.15194v2' target='_blank'>Extracting Heuristics from Large Language Models for Reward Shaping in
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Siddhant Bhambri, Amrita Bhattacharjee, Durgesh Kalwar, Lin Guan, Huan Liu, Subbarao Kambhampati</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) suffers from sample inefficiency in sparse reward
domains, and the problem is further pronounced in case of stochastic
transitions. To improve the sample efficiency, reward shaping is a well-studied
approach to introduce intrinsic rewards that can help the RL agent converge to
an optimal policy faster. However, designing a useful reward shaping function
for all desirable states in the Markov Decision Process (MDP) is challenging,
even for domain experts. Given that La...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.15223v3' target='_blank'>iVideoGPT: Interactive VideoGPTs are Scalable World Models</a></h2>
<p><strong>Authors:</strong> Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, Mingsheng Long</p>
<p><strong>Summary:</strong> World models empower model-based agents to interactively explore, reason, and
plan within imagined environments for real-world decision-making. However, the
high demand for interactivity poses challenges in harnessing recent
advancements in video generative models for developing world models at scale.
This work introduces Interactive VideoGPT (iVideoGPT), a scalable
autoregressive transformer framework that integrates multimodal signals--visual
observations, actions, and rewards--into a sequence...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.13223v2' target='_blank'>Act Better by Timing: A timing-Aware Reinforcement Learning for
  Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Guanzhou Li, Jianping Wu, Yujing He</p>
<p><strong>Summary:</strong> Autonomous vehicles inevitably encounter a vast array of scenarios in
real-world environments. Addressing long-tail scenarios, particularly those
involving intensive interactions with numerous traffic participants, remains
one of the most significant challenges in achieving high-level autonomous
driving. Reinforcement learning (RL) offers a promising solution for such
scenarios and allows autonomous vehicles to continuously self-evolve during
interactions. However, traditional RL often requires ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.14872v1' target='_blank'>Adapt2Reward: Adapting Video-Language Models to Generalizable Robotic
  Rewards via Failure Prompts</a></h2>
<p><strong>Authors:</strong> Yanting Yang, Minghao Chen, Qibo Qiu, Jiahao Wu, Wenxiao Wang, Binbin Lin, Ziyu Guan, Xiaofei He</p>
<p><strong>Summary:</strong> For a general-purpose robot to operate in reality, executing a broad range of
instructions across various environments is imperative. Central to the
reinforcement learning and planning for such robotic agents is a generalizable
reward function. Recent advances in vision-language models, such as CLIP, have
shown remarkable performance in the domain of deep learning, paving the way for
open-domain visual recognition. However, collecting data on robots executing
various language instructions across...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.07644v1' target='_blank'>SigmaRL: A Sample-Efficient and Generalizable Multi-Agent Reinforcement
  Learning Framework for Motion Planning</a></h2>
<p><strong>Authors:</strong> Jianye Xu, Pan Hu, Bassam Alrifaee</p>
<p><strong>Summary:</strong> This paper introduces an open-source, decentralized framework named SigmaRL,
designed to enhance both sample efficiency and generalization of multi-agent
Reinforcement Learning (RL) for motion planning of connected and automated
vehicles. Most RL agents exhibit a limited capacity to generalize, often
focusing narrowly on specific scenarios, and are usually evaluated in similar
or even the same scenarios seen during training. Various methods have been
proposed to address these challenges, includi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.05717v1' target='_blank'>Learning Soft Driving Constraints from Vectorized Scene Embeddings while
  Imitating Expert Trajectories</a></h2>
<p><strong>Authors:</strong> Niloufar Saeidi Mobarakeh, Behzad Khamidehi, Chunlin Li, Hamidreza Mirkhani, Fazel Arasteh, Mohammed Elmahgiubi, Weize Zhang, Kasra Rezaee, Pascal Poupart</p>
<p><strong>Summary:</strong> The primary goal of motion planning is to generate safe and efficient
trajectories for vehicles. Traditionally, motion planning models are trained
using imitation learning to mimic the behavior of human experts. However, these
models often lack interpretability and fail to provide clear justifications for
their decisions. We propose a method that integrates constraint learning into
imitation learning by extracting driving constraints from expert trajectories.
Our approach utilizes vectorized sce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.02666v1' target='_blank'>Deep Reinforcement Learning Enabled Persistent Surveillance with
  Energy-Aware UAV-UGV Systems for Disaster Management Applications</a></h2>
<p><strong>Authors:</strong> Md Safwan Mondal, Subramanian Ramasamy, Pranav Bhounsule</p>
<p><strong>Summary:</strong> Integrating Unmanned Aerial Vehicles (UAVs) with Unmanned Ground Vehicles
(UGVs) provides an effective solution for persistent surveillance in disaster
management. UAVs excel at covering large areas rapidly, but their range is
limited by battery capacity. UGVs, though slower, can carry larger batteries
for extended missions. By using UGVs as mobile recharging stations, UAVs can
extend mission duration through periodic refueling, leveraging the
complementary strengths of both systems. To optimize...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.06748v4' target='_blank'>Tactics of Adversarial Attack on Deep Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, Min Sun</p>
<p><strong>Summary:</strong> We introduce two tactics to attack agents trained by deep reinforcement
learning algorithms using adversarial examples, namely the strategically-timed
attack and the enchanting attack. In the strategically-timed attack, the
adversary aims at minimizing the agent's reward by only attacking the agent at
a small subset of time steps in an episode. Limiting the attack activity to
this subset helps prevent detection of the attack by the agent. We propose a
novel method to determine when an adversaria...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.11730v3' target='_blank'>Automatic Discovery of Interpretable Planning Strategies</a></h2>
<p><strong>Authors:</strong> Julian Skirzy≈Ñski, Frederic Becker, Falk Lieder</p>
<p><strong>Summary:</strong> When making decisions, people often overlook critical information or are
overly swayed by irrelevant information. A common approach to mitigate these
biases is to provide decision-makers, especially professionals such as medical
doctors, with decision aids, such as decision trees and flowcharts. Designing
effective decision aids is a difficult problem. We propose that recently
developed reinforcement learning methods for discovering clever heuristics for
good decision-making can be partially lev...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.02054v1' target='_blank'>RAPID: Robust and Agile Planner Using Inverse Reinforcement Learning for
  Vision-Based Drone Navigation</a></h2>
<p><strong>Authors:</strong> Minwoo Kim, Geunsik Bae, Jinwoo Lee, Woojae Shin, Changseung Kim, Myong-Yol Choi, Heejung Shin, Hyondong Oh</p>
<p><strong>Summary:</strong> This paper introduces a learning-based visual planner for agile drone flight
in cluttered environments. The proposed planner generates collision-free
waypoints in milliseconds, enabling drones to perform agile maneuvers in
complex environments without building separate perception, mapping, and
planning modules. Learning-based methods, such as behavior cloning (BC) and
reinforcement learning (RL), demonstrate promising performance in visual
navigation but still face inherent limitations. BC is su...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1206.3285v1' target='_blank'>Dyna-Style Planning with Linear Function Approximation and Prioritized
  Sweeping</a></h2>
<p><strong>Authors:</strong> Richard S. Sutton, Csaba Szepesvari, Alborz Geramifard, Michael P. Bowling</p>
<p><strong>Summary:</strong> We consider the problem of efficiently learning optimal control policies and
value functions over large state spaces in an online setting in which estimates
must be available after each interaction with the world. This paper develops an
explicitly model-based approach extending the Dyna architecture to linear
function approximation. Dynastyle planning proceeds by generating imaginary
experience from the world model and then applying model-free reinforcement
learning algorithms to the imagined st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1704.00756v2' target='_blank'>Multi-Advisor Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Romain Laroche, Mehdi Fatemi, Joshua Romoff, Harm van Seijen</p>
<p><strong>Summary:</strong> We consider tackling a single-agent RL problem by distributing it to $n$
learners. These learners, called advisors, endeavour to solve the problem from
a different focus. Their advice, taking the form of action values, is then
communicated to an aggregator, which is in control of the system. We show that
the local planning method for the advisors is critical and that none of the
ones found in the literature is flawless: the egocentric planning overestimates
values of states where the other advis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1705.08080v2' target='_blank'>Visual Semantic Planning using Deep Successor Representations</a></h2>
<p><strong>Authors:</strong> Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi</p>
<p><strong>Summary:</strong> A crucial capability of real-world intelligent agents is their ability to
plan a sequence of actions to achieve their goals in the visual world. In this
work, we address the problem of visual semantic planning: the task of
predicting a sequence of actions from visual observations that transform a
dynamic environment from an initial state to a goal state. Doing so entails
knowledge about objects and their affordances, as well as actions and their
preconditions and effects. We propose learning the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1706.09597v1' target='_blank'>Path Integral Networks: End-to-End Differentiable Optimal Control</a></h2>
<p><strong>Authors:</strong> Masashi Okada, Luca Rigazio, Takenobu Aoshima</p>
<p><strong>Summary:</strong> In this paper, we introduce Path Integral Networks (PI-Net), a recurrent
network representation of the Path Integral optimal control algorithm. The
network includes both system dynamics and cost models, used for optimal control
based planning. PI-Net is fully differentiable, learning both dynamics and cost
models end-to-end by back-propagation and stochastic gradient descent. Because
of this, PI-Net can learn to plan. PI-Net has several advantages: it can
generalize to unseen states thanks to pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1709.00546v1' target='_blank'>Autonomous Waypoint Generation with Safety Guarantees: On-Line Motion
  Planning in Unknown Environments</a></h2>
<p><strong>Authors:</strong> Sanjeev Sharma</p>
<p><strong>Summary:</strong> On-line motion planning in unknown environments is a challenging problem as
it requires (i) ensuring collision avoidance and (ii) minimizing the motion
time, while continuously predicting where to go next. Previous approaches to
on-line motion planning assume that a rough map of the environment is
available, thereby simplifying the problem. This paper presents a reactive
on-line motion planner, Robust Autonomous Waypoint generation (RAW), for mobile
robots navigating in unknown and unstructured ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1802.06839v1' target='_blank'>Human-in-the-Loop Mixed-Initiative Control under Temporal Tasks</a></h2>
<p><strong>Authors:</strong> Meng Guo, Sofie Andersson, Dimos V. Dimarogonas</p>
<p><strong>Summary:</strong> This paper considers the motion control and task planning problem of mobile
robots under complex high-level tasks and human initiatives. The assigned task
is specified as Linear Temporal Logic (LTL) formulas that consist of hard and
soft constraints. The human initiative influences the robot autonomy in two
explicit ways: with additive terms in the continuous controller and with
contingent task assignments. We propose an online coordination scheme that
encapsulates (i) a mixed-initiative continu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.09209v1' target='_blank'>A Human-Centered Data-Driven Planner-Actor-Critic Architecture via Logic
  Programming</a></h2>
<p><strong>Authors:</strong> Daoming Lyu, Fangkai Yang, Bo Liu, Steven Gustafson</p>
<p><strong>Summary:</strong> Recent successes of Reinforcement Learning (RL) allow an agent to learn
policies that surpass human experts but suffers from being time-hungry and
data-hungry. By contrast, human learning is significantly faster because prior
and general knowledge and multiple information resources are utilized. In this
paper, we propose a Planner-Actor-Critic architecture for huMAN-centered
planning and learning (PACMAN), where an agent uses its prior, high-level,
deterministic symbolic knowledge to plan for go...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.09540v1' target='_blank'>Reconnaissance and Planning algorithm for constrained MDP</a></h2>
<p><strong>Authors:</strong> Shin-ichi Maeda, Hayato Watahiki, Shintarou Okada, Masanori Koyama</p>
<p><strong>Summary:</strong> Practical reinforcement learning problems are often formulated as constrained
Markov decision process (CMDP) problems, in which the agent has to maximize the
expected return while satisfying a set of prescribed safety constraints. In
this study, we propose a novel simulator-based method to approximately solve a
CMDP problem without making any compromise on the safety constraints. We
achieve this by decomposing the CMDP into a pair of MDPs; reconnaissance MDP
and planning MDP. The purpose of reco...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.13007v1' target='_blank'>World Programs for Model-Based Learning and Planning in Compositional
  State and Action Spaces</a></h2>
<p><strong>Authors:</strong> Marwin H. S. Segler</p>
<p><strong>Summary:</strong> Some of the most important tasks take place in environments which lack cheap
and perfect simulators, thus hampering the application of model-free
reinforcement learning (RL). While model-based RL aims to learn a dynamics
model, in a more general case the learner does not know a priori what the
action space is. Here we propose a formalism where the learner induces a world
program by learning a dynamics model and the actions in graph-based
compositional environments by observing state-state transi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.05954v4' target='_blank'>Learning Functionally Decomposed Hierarchies for Continuous Control
  Tasks with Path Planning</a></h2>
<p><strong>Authors:</strong> Sammy Christen, Lukas Jendele, Emre Aksan, Otmar Hilliges</p>
<p><strong>Summary:</strong> We present HiDe, a novel hierarchical reinforcement learning architecture
that successfully solves long horizon control tasks and generalizes to unseen
test scenarios. Functional decomposition between planning and low-level control
is achieved by explicitly separating the state-action spaces across the
hierarchy, which allows the integration of task-relevant knowledge per layer.
We propose an RL-based planner to efficiently leverage the information in the
planning layer of the hierarchy, while t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.00589v1' target='_blank'>Deep R-Learning for Continual Area Sweeping</a></h2>
<p><strong>Authors:</strong> Rishi Shah, Yuqian Jiang, Justin Hart, Peter Stone</p>
<p><strong>Summary:</strong> Coverage path planning is a well-studied problem in robotics in which a robot
must plan a path that passes through every point in a given area repeatedly,
usually with a uniform frequency. To address the scenario in which some points
need to be visited more frequently than others, this problem has been extended
to non-uniform coverage planning. This paper considers the variant of
non-uniform coverage in which the robot does not know the distribution of
relevant events beforehand and must neverth...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.07430v2' target='_blank'>Continuous Control for Searching and Planning with a Learned Model</a></h2>
<p><strong>Authors:</strong> Xuxi Yang, Werner Duvaud, Peng Wei</p>
<p><strong>Summary:</strong> Decision-making agents with planning capabilities have achieved huge success
in the challenging domain like Chess, Shogi, and Go. In an effort to generalize
the planning ability to the more general tasks where the environment dynamics
are not available to the agent, researchers proposed the MuZero algorithm that
can learn the dynamical model through the interactions with the environment. In
this paper, we provide a way and the necessary theoretical results to extend
the MuZero algorithm to more ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.14311v1' target='_blank'>Learning Dynamics Models for Model Predictive Agents</a></h2>
<p><strong>Authors:</strong> Michael Lutter, Leonard Hasenclever, Arunkumar Byravan, Gabriel Dulac-Arnold, Piotr Trochim, Nicolas Heess, Josh Merel, Yuval Tassa</p>
<p><strong>Summary:</strong> Model-Based Reinforcement Learning involves learning a \textit{dynamics
model} from data, and then using this model to optimise behaviour, most often
with an online \textit{planner}. Much of the recent research along these lines
presents a particular set of design choices, involving problem definition,
model learning and planning. Given the multiple contributions, it is difficult
to evaluate the effects of each. This paper sets out to disambiguate the role
of different design choices for learnin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.10654v1' target='_blank'>Sample-Efficient Learning of Nonprehensile Manipulation Policies via
  Physics-Based Informed State Distributions</a></h2>
<p><strong>Authors:</strong> Lerrel Pinto, Aditya Mandalika, Brian Hou, Siddhartha Srinivasa</p>
<p><strong>Summary:</strong> This paper proposes a sample-efficient yet simple approach to learning
closed-loop policies for nonprehensile manipulation. Although reinforcement
learning (RL) can learn closed-loop policies without requiring access to
underlying physics models, it suffers from poor sample complexity on
challenging tasks. To overcome this problem, we leverage rearrangement planning
to provide an informative physics-based prior on the environment's optimal
state-visitation distribution. Specifically, we present ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.01657v2' target='_blank'>Dynamics-Aware Unsupervised Discovery of Skills</a></h2>
<p><strong>Authors:</strong> Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, Karol Hausman</p>
<p><strong>Summary:</strong> Conventionally, model-based reinforcement learning (MBRL) aims to learn a
global model for the dynamics of the environment. A good model can potentially
enable planning algorithms to generate a large variety of behaviors and solve
diverse tasks. However, learning an accurate model for complex dynamical
systems is difficult, and even then, the model might not generalize well
outside the distribution of states on which it was trained. In this work, we
combine model-based learning with model-free l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.05300v1' target='_blank'>Learning Safe Unlabeled Multi-Robot Planning with Motion Constraints</a></h2>
<p><strong>Authors:</strong> Arbaaz Khan, Chi Zhang, Shuo Li, Jiayue Wu, Brent Schlotfeldt, Sarah Y. Tang, Alejandro Ribeiro, Osbert Bastani, Vijay Kumar</p>
<p><strong>Summary:</strong> In this paper, we present a learning approach to goal assignment and
trajectory planning for unlabeled robots operating in 2D, obstacle-filled
workspaces. More specifically, we tackle the unlabeled multi-robot motion
planning problem with motion constraints as a multi-agent reinforcement
learning problem with some sparse global reward. In contrast with previous
works, which formulate an entirely new hand-crafted optimization cost or
trajectory generation algorithm for a different robot dynamic m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.08313v1' target='_blank'>Learning High-Level Planning Symbols from Intrinsically Motivated
  Experience</a></h2>
<p><strong>Authors:</strong> Angelo Oddi, Riccardo Rasconi, Emilio Cartoni, Gabriele Sartor, Gianluca Baldassarre, Vieri Giuliano Santucci</p>
<p><strong>Summary:</strong> In symbolic planning systems, the knowledge on the domain is commonly
provided by an expert. Recently, an automatic abstraction procedure has been
proposed in the literature to create a Planning Domain Definition Language
(PDDL) representation, which is the most widely used input format for most
off-the-shelf automated planners, starting from `options', a data structure
used to represent actions within the hierarchical reinforcement learning
framework. We propose an architecture that potentially...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.07155v1' target='_blank'>Bootstrapped model learning and error correction for planning with
  uncertainty in model-based RL</a></h2>
<p><strong>Authors:</strong> Alvaro Ovalle, Simon M. Lucas</p>
<p><strong>Summary:</strong> Having access to a forward model enables the use of planning algorithms such
as Monte Carlo Tree Search and Rolling Horizon Evolution. Where a model is
unavailable, a natural aim is to learn a model that reflects accurately the
dynamics of the environment. In many situations it might not be possible and
minimal glitches in the model may lead to poor performance and failure. This
paper explores the problem of model misspecification through uncertainty-aware
reinforcement learning agents. We propo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.08763v1' target='_blank'>Model-Predictive Control via Cross-Entropy and Gradient-Based
  Optimization</a></h2>
<p><strong>Authors:</strong> Homanga Bharadhwaj, Kevin Xie, Florian Shkurti</p>
<p><strong>Summary:</strong> Recent works in high-dimensional model-predictive control and model-based
reinforcement learning with learned dynamics and reward models have resorted to
population-based optimization methods, such as the Cross-Entropy Method (CEM),
for planning a sequence of actions. To decide on an action to take, CEM
conducts a search for the action sequence with the highest return according to
the dynamics model and reward. Action sequences are typically randomly sampled
from an unconditional Gaussian distri...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.13897v2' target='_blank'>Latent Skill Planning for Exploration and Transfer</a></h2>
<p><strong>Authors:</strong> Kevin Xie, Homanga Bharadhwaj, Danijar Hafner, Animesh Garg, Florian Shkurti</p>
<p><strong>Summary:</strong> To quickly solve new tasks in complex environments, intelligent agents need
to build up reusable knowledge. For example, a learned world model captures
knowledge about the environment that applies to new tasks. Similarly, skills
capture general behaviors that can apply to new tasks. In this paper, we
investigate how these two approaches can be integrated into a single
reinforcement learning agent. Specifically, we leverage the idea of partial
amortization for fast adaptation at test time. For th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.04649v1' target='_blank'>Interactive Hierarchical Guidance using Language</a></h2>
<p><strong>Authors:</strong> Bharat Prakash, Nicholas Waytowich, Tim Oates, Tinoosh Mohsenin</p>
<p><strong>Summary:</strong> Reinforcement learning has been successful in many tasks ranging from robotic
control, games, energy management etc. In complex real world environments with
sparse rewards and long task horizons, sample efficiency is still a major
challenge. Most complex tasks can be easily decomposed into high-level planning
and low level control. Therefore, it is important to enable agents to leverage
the hierarchical structure and decompose bigger tasks into multiple smaller
sub-tasks. We introduce an approac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.15949v2' target='_blank'>Sparsely Changing Latent States for Prediction and Planning in Partially
  Observable Domains</a></h2>
<p><strong>Authors:</strong> Christian Gumbsch, Martin V. Butz, Georg Martius</p>
<p><strong>Summary:</strong> A common approach to prediction and planning in partially observable domains
is to use recurrent neural networks (RNNs), which ideally develop and maintain
a latent memory about hidden, task-relevant factors. We hypothesize that many
of these hidden factors in the physical world are constant over time, changing
only sparsely. To study this hypothesis, we propose Gated $L_0$ Regularized
Dynamics (GateL0RD), a novel recurrent architecture that incorporates the
inductive bias to maintain stable, sp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.07503v1' target='_blank'>Measuring Outcomes in Healthcare Economics using Artificial
  Intelligence: with Application to Resource Management</a></h2>
<p><strong>Authors:</strong> Chih-Hao Huang, Feras A. Batarseh, Adel Boueiz, Ajay Kulkarni, Po-Hsuan Su, Jahan Aman</p>
<p><strong>Summary:</strong> The quality of service in healthcare is constantly challenged by outlier
events such as pandemics (i.e. Covid-19) and natural disasters (such as
hurricanes and earthquakes). In most cases, such events lead to critical
uncertainties in decision making, as well as in multiple medical and economic
aspects at a hospital. External (geographic) or internal factors (medical and
managerial), lead to shifts in planning and budgeting, but most importantly,
reduces confidence in conventional processes. In ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.04502v2' target='_blank'>Dyna-T: Dyna-Q and Upper Confidence Bounds Applied to Trees</a></h2>
<p><strong>Authors:</strong> Tarek Faycal, Claudio Zito</p>
<p><strong>Summary:</strong> In this work we present a preliminary investigation of a novel algorithm
called Dyna-T. In reinforcement learning (RL) a planning agent has its own
representation of the environment as a model. To discover an optimal policy to
interact with the environment, the agent collects experience in a trial and
error fashion. Experience can be used for learning a better model or improve
directly the value function and policy. Typically separated, Dyna-Q is an
hybrid approach which, at each iteration, expl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.11481v1' target='_blank'>"Think Before You Speak": Improving Multi-Action Dialog Policy by
  Planning Single-Action Dialogs</a></h2>
<p><strong>Authors:</strong> Shuo Zhang, Junzhou Zhao, Pinghui Wang, Yu Li, Yi Huang, Junlan Feng</p>
<p><strong>Summary:</strong> Multi-action dialog policy (MADP), which generates multiple atomic dialog
actions per turn, has been widely applied in task-oriented dialog systems to
provide expressive and efficient system responses. Existing MADP models usually
imitate action combinations from the labeled multi-action dialog samples. Due
to data limitations, they generalize poorly toward unseen dialog flows. While
interactive learning and reinforcement learning algorithms can be applied to
incorporate external data sources of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.08235v1' target='_blank'>Technical Report for Trend Prediction Based Intelligent UAV Trajectory
  Planning for Large-scale Dynamic Scenarios</a></h2>
<p><strong>Authors:</strong> Jinjing Wang, Xindi Wang</p>
<p><strong>Summary:</strong> The unmanned aerial vehicle (UAV)-enabled communication technology is
regarded as an efficient and effective solution for some special application
scenarios where existing terrestrial infrastructures are overloaded to provide
reliable services. To maximize the utility of the UAV-enabled system while
meeting the QoS and energy constraints, the UAV needs to plan its trajectory
considering the dynamic characteristics of scenarios, which is formulated as
the Markov Decision Process (MDP). To solve t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.12631v2' target='_blank'>LEAGUE: Guided Skill Learning and Abstraction for Long-Horizon
  Manipulation</a></h2>
<p><strong>Authors:</strong> Shuo Cheng, Danfei Xu</p>
<p><strong>Summary:</strong> To assist with everyday human activities, robots must solve complex
long-horizon tasks and generalize to new settings. Recent deep reinforcement
learning (RL) methods show promise in fully autonomous learning, but they
struggle to reach long-term goals in large environments. On the other hand,
Task and Motion Planning (TAMP) approaches excel at solving and generalizing
across long-horizon tasks, thanks to their powerful state and action
abstractions. But they assume predefined skill sets, which ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.02372v1' target='_blank'>Path Planning Using Wassertein Distributionally Robust Deep Q-learning</a></h2>
<p><strong>Authors:</strong> Cem Alpturk, Venkatraman Renganathan</p>
<p><strong>Summary:</strong> We investigate the problem of risk averse robot path planning using the deep
reinforcement learning and distributionally robust optimization perspectives.
Our problem formulation involves modelling the robot as a stochastic linear
dynamical system, assuming that a collection of process noise samples is
available. We cast the risk averse motion planning problem as a Markov decision
process and propose a continuous reward function design that explicitly takes
into account the risk of collision wit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.09033v1' target='_blank'>Planning Immediate Landmarks of Targets for Model-Free Skill Transfer
  across Agents</a></h2>
<p><strong>Authors:</strong> Minghuan Liu, Zhengbang Zhu, Menghui Zhu, Yuzheng Zhuang, Weinan Zhang, Jianye Hao</p>
<p><strong>Summary:</strong> In reinforcement learning applications like robotics, agents usually need to
deal with various input/output features when specified with different
state/action spaces by their developers or physical restrictions. This
indicates unnecessary re-training from scratch and considerable sample
inefficiency, especially when agents follow similar solution steps to achieve
tasks. In this paper, we aim to transfer similar high-level goal-transition
knowledge to alleviate the challenge. Specifically, we pr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.09544v1' target='_blank'>Learning to View: Decision Transformers for Active Object Detection</a></h2>
<p><strong>Authors:</strong> Wenhao Ding, Nathalie Majcherczyk, Mohit Deshpande, Xuewei Qi, Ding Zhao, Rajasimman Madhivanan, Arnie Sen</p>
<p><strong>Summary:</strong> Active perception describes a broad class of techniques that couple planning
and perception systems to move the robot in a way to give the robot more
information about the environment. In most robotic systems, perception is
typically independent of motion planning. For example, traditional object
detection is passive: it operates only on the images it receives. However, we
have a chance to improve the results if we allow planning to consume detection
signals and move the robot to collect views t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.03939v1' target='_blank'>Learning Interaction-aware Motion Prediction Model for Decision-making
  in Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Zhiyu Huang, Haochen Liu, Jingda Wu, Wenhui Huang, Chen Lv</p>
<p><strong>Summary:</strong> Predicting the behaviors of other road users is crucial to safe and
intelligent decision-making for autonomous vehicles (AVs). However, most motion
prediction models ignore the influence of the AV's actions and the planning
module has to treat other agents as unalterable moving obstacles. To address
this problem, this paper proposes an interaction-aware motion prediction model
that is able to predict other agents' future trajectories according to the ego
agent's future plan, i.e., their reaction...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.02407v3' target='_blank'>Local Path Planning among Pushable Objects based on Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Linghong Yao, Valerio Modugno, Andromachi Maria Delfaki, Yuanchang Liu, Danail Stoyanov, Dimitrios Kanoulas</p>
<p><strong>Summary:</strong> In this paper, we introduce a method to deal with the problem of robot local
path planning among pushable objects -- an open problem in robotics. In
particular, we achieve that by training multiple agents simultaneously in a
physics-based simulation environment, utilizing an Advantage Actor-Critic
algorithm coupled with a deep neural network. The developed online policy
enables these agents to push obstacles in ways that are not limited to axial
alignments, adapt to unforeseen changes in obstacl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.16209v4' target='_blank'>C-MCTS: Safe Planning with Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Dinesh Parthasarathy, Georgios Kontes, Axel Plinge, Christopher Mutschler</p>
<p><strong>Summary:</strong> The Constrained Markov Decision Process (CMDP) formulation allows to solve
safety-critical decision making tasks that are subject to constraints. While
CMDPs have been extensively studied in the Reinforcement Learning literature,
little attention has been given to sampling-based planning algorithms such as
MCTS for solving them. Previous approaches perform conservatively with respect
to costs as they avoid constraint violations by using Monte Carlo cost
estimates that suffer from high variance. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.07647v1' target='_blank'>Multi-Robot Motion Planning: A Learning-Based Artificial Potential Field
  Solution</a></h2>
<p><strong>Authors:</strong> Dengyu Zhang, Guobin Zhu, Qingrui Zhang</p>
<p><strong>Summary:</strong> Motion planning is a crucial aspect of robot autonomy as it involves
identifying a feasible motion path to a destination while taking into
consideration various constraints, such as input, safety, and performance
constraints, without violating either system or environment boundaries. This
becomes particularly challenging when multiple robots run without
communication, which compromises their real-time efficiency, safety, and
performance. In this paper, we present a learning-based potential field...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.04040v1' target='_blank'>Meta-Policy Learning over Plan Ensembles for Robust Articulated Object
  Manipulation</a></h2>
<p><strong>Authors:</strong> Constantinos Chamzas, Caelan Garrett, Balakumar Sundaralingam, Lydia E. Kavraki, Dieter Fox</p>
<p><strong>Summary:</strong> Recent work has shown that complex manipulation skills, such as pushing or
pouring, can be learned through state-of-the-art learning based techniques,
such as Reinforcement Learning (RL). However, these methods often have high
sample-complexity, are susceptible to domain changes, and produce unsafe
motions that a robot should not perform. On the other hand, purely geometric
model-based planning can produce complex behaviors that satisfy all the
geometric constraints of the robot but might not be...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.10701v3' target='_blank'>Theory of Mind for Multi-Agent Collaboration via Large Language Models</a></h2>
<p><strong>Authors:</strong> Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara</p>
<p><strong>Summary:</strong> While Large Language Models (LLMs) have demonstrated impressive
accomplishments in both reasoning and planning, their abilities in multi-agent
collaborations remains largely unexplored. This study evaluates LLM-based
agents in a multi-agent cooperative text game with Theory of Mind (ToM)
inference tasks, comparing their performance with Multi-Agent Reinforcement
Learning (MARL) and planning-based baselines. We observed evidence of emergent
collaborative behaviors and high-order Theory of Mind ca...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.06297v1' target='_blank'>Dynamic Q-planning for Online UAV Path Planning in Unknown and Complex
  Environments</a></h2>
<p><strong>Authors:</strong> Lidia Gianne Souza da Rocha, Kenny Anderson Queiroz Caldas, Marco Henrique Terra, Fabio Ramos, Kelen Cristiane Teixeira Vivaldini</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicles need an online path planning capability to move in
high-risk missions in unknown and complex environments to complete them safely.
However, many algorithms reported in the literature may not return reliable
trajectories to solve online problems in these scenarios. The Q-Learning
algorithm, a Reinforcement Learning Technique, can generate trajectories in
real-time and has demonstrated fast and reliable results. This technique,
however, has the disadvantage of defining the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.14877v1' target='_blank'>TEeVTOL: Balancing Energy and Time Efficiency in eVTOL Aircraft Path
  Planning Across City-Scale Wind Fields</a></h2>
<p><strong>Authors:</strong> Songyang Liu, Shuai Li, Haochen Li, Weizi Li, Jindong Tan</p>
<p><strong>Summary:</strong> Electric vertical-takeoff and landing (eVTOL) aircraft, recognized for their
maneuverability and flexibility, offer a promising alternative to our
transportation system. However, the operational effectiveness of these aircraft
faces many challenges, such as the delicate balance between energy and time
efficiency, stemming from unpredictable environmental factors, including wind
fields. Mathematical modeling-based approaches have been adopted to plan
aircraft flight path in urban wind fields with...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.16478v4' target='_blank'>Real-World Evaluation of two Cooperative Intersection Management
  Approaches</a></h2>
<p><strong>Authors:</strong> Marvin Klimke, Max Bastian Mertens, Benjamin V√∂lz, Michael Buchholz</p>
<p><strong>Summary:</strong> Cooperative maneuver planning promises to significantly improve traffic
efficiency at unsignalized intersections by leveraging connected automated
vehicles. Previous works on this topic have been mostly developed for
completely automated traffic in a simple simulated environment. In contrast,
our previously introduced planning approaches are specifically designed to
handle real-world mixed traffic. The two methods are based on multi-scenario
prediction and graph-based reinforcement learning, res...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.04028v1' target='_blank'>Contrastive Sparse Autoencoders for Interpreting Planning of
  Chess-Playing Agents</a></h2>
<p><strong>Authors:</strong> Yoann Poupart</p>
<p><strong>Summary:</strong> AI led chess systems to a superhuman level, yet these systems heavily rely on
black-box algorithms. This is unsustainable in ensuring transparency to the
end-user, particularly when these systems are responsible for sensitive
decision-making. Recent interpretability work has shown that the inner
representations of Deep Neural Networks (DNNs) were fathomable and contained
human-understandable concepts. Yet, these methods are seldom contextualised and
are often based on a single hidden state, whic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.11455v2' target='_blank'>Adaptive Reinforcement Learning Planning: Harnessing Large Language
  Models for Complex Information Extraction</a></h2>
<p><strong>Authors:</strong> Zepeng Ding, Ruiyang Ke, Wenhao Huang, Guochao Jiang, Yanda Li, Deqing Yang, Jiaqing Liang</p>
<p><strong>Summary:</strong> Existing research on large language models (LLMs) shows that they can solve
information extraction tasks through multi-step planning. However, their
extraction behavior on complex sentences and tasks is unstable, emerging issues
such as false positives and missing elements. We observe that decomposing
complex extraction tasks and extracting them step by step can effectively
improve LLMs' performance, and the extraction orders of entities significantly
affect the final results of LLMs. This paper...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.11984v1' target='_blank'>Online Pareto-Optimal Decision-Making for Complex Tasks using Active
  Inference</a></h2>
<p><strong>Authors:</strong> Peter Amorese, Shohei Wakayama, Nisar Ahmed, Morteza Lahijanian</p>
<p><strong>Summary:</strong> When a robot autonomously performs a complex task, it frequently must balance
competing objectives while maintaining safety. This becomes more difficult in
uncertain environments with stochastic outcomes. Enhancing transparency in the
robot's behavior and aligning with user preferences are also crucial. This
paper introduces a novel framework for multi-objective reinforcement learning
that ensures safe task execution, optimizes trade-offs between objectives, and
adheres to user preferences. The ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.14562v3' target='_blank'>DROP: Dexterous Reorientation via Online Planning</a></h2>
<p><strong>Authors:</strong> Albert H. Li, Preston Culbertson, Vince Kurtz, Aaron D. Ames</p>
<p><strong>Summary:</strong> Achieving human-like dexterity is a longstanding challenge in robotics, in
part due to the complexity of planning and control for contact-rich systems. In
reinforcement learning (RL), one popular approach has been to use
massively-parallelized, domain-randomized simulations to learn a policy offline
over a vast array of contact conditions, allowing robust sim-to-real transfer.
Inspired by recent advances in real-time parallel simulation, this work
considers instead the viability of online planni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.18361v3' target='_blank'>iWalker: Imperative Visual Planning for Walking Humanoid Robot</a></h2>
<p><strong>Authors:</strong> Xiao Lin, Yuhao Huang, Taimeng Fu, Xiaobin Xiong, Chen Wang</p>
<p><strong>Summary:</strong> Humanoid robots, with the potential to perform a broad range of tasks in
environments designed for humans, have been deemed crucial for the basis of
general AI agents. When talking about planning and controlling, although
traditional models and task-specific methods have been extensively studied over
the past few decades, they are inadequate for achieving the flexibility and
versatility needed for general autonomy. Learning approaches, especially
reinforcement learning, are powerful and popular ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.02389v1' target='_blank'>Diffusion Meets Options: Hierarchical Generative Skill Composition for
  Temporally-Extended Tasks</a></h2>
<p><strong>Authors:</strong> Zeyu Feng, Hao Luan, Kevin Yuchen Ma, Harold Soh</p>
<p><strong>Summary:</strong> Safe and successful deployment of robots requires not only the ability to
generate complex plans but also the capacity to frequently replan and correct
execution errors. This paper addresses the challenge of long-horizon trajectory
planning under temporally extended objectives in a receding horizon manner. To
this end, we propose DOPPLER, a data-driven hierarchical framework that
generates and updates plans based on instruction specified by linear temporal
logic (LTL). Our method decomposes temp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.09505v1' target='_blank'>HG2P: Hippocampus-inspired High-reward Graph and Model-Free Q-Gradient
  Penalty for Path Planning and Motion Control</a></h2>
<p><strong>Authors:</strong> Haoran Wang, Yaoru Sun, Zeshen Tang</p>
<p><strong>Summary:</strong> Goal-conditioned hierarchical reinforcement learning (HRL) decomposes complex
reaching tasks into a sequence of simple subgoal-conditioned tasks, showing
significant promise for addressing long-horizon planning in large-scale
environments. This paper bridges the goal-conditioned HRL based on graph-based
planning to brain mechanisms, proposing a hippocampus-striatum-like
dual-controller hypothesis. Inspired by the brain mechanisms of organisms
(i.e., the high-reward preferences observed in hippoc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.06128v1' target='_blank'>Research on reinforcement learning based warehouse robot navigation
  algorithm in complex warehouse layout</a></h2>
<p><strong>Authors:</strong> Keqin Li, Lipeng Liu, Jiajing Chen, Dezhi Yu, Xiaofan Zhou, Ming Li, Congyu Wang, Zhao Li</p>
<p><strong>Summary:</strong> In this paper, how to efficiently find the optimal path in complex warehouse
layout and make real-time decision is a key problem. This paper proposes a new
method of Proximal Policy Optimization (PPO) and Dijkstra's algorithm, Proximal
policy-Dijkstra (PP-D). PP-D method realizes efficient strategy learning and
real-time decision making through PPO, and uses Dijkstra algorithm to plan the
global optimal path, thus ensuring high navigation accuracy and significantly
improving the efficiency of pa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.07760v1' target='_blank'>Navigation with QPHIL: Quantizing Planner for Hierarchical Implicit
  Q-Learning</a></h2>
<p><strong>Authors:</strong> Alexi Canesse, Mathieu Petitbois, Ludovic Denoyer, Sylvain Lamprier, R√©my Portelas</p>
<p><strong>Summary:</strong> Offline Reinforcement Learning (RL) has emerged as a powerful alternative to
imitation learning for behavior modeling in various domains, particularly in
complex navigation tasks. An existing challenge with Offline RL is the
signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to
errors in value estimates. Towards this, multiple works have demonstrated the
advantage of hierarchical offline RL methods, which decouples high-level path
planning from low-level path following. In ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.18299v1' target='_blank'>Model-Free RL Agents Demonstrate System 1-Like Intentionality</a></h2>
<p><strong>Authors:</strong> Hal Ashton, Matija Franklin</p>
<p><strong>Summary:</strong> This paper argues that model-free reinforcement learning (RL) agents, while
lacking explicit planning mechanisms, exhibit behaviours that can be analogised
to System 1 ("thinking fast") processes in human cognition. Unlike model-based
RL agents, which operate akin to System 2 ("thinking slow") reasoning by
leveraging internal representations for planning, model-free agents react to
environmental stimuli without anticipatory modelling. We propose a novel
framework linking the dichotomy of System ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.06401v1' target='_blank'>Habitizing Diffusion Planning for Efficient and Effective Decision
  Making</a></h2>
<p><strong>Authors:</strong> Haofei Lu, Yifei Shen, Dongsheng Li, Junliang Xing, Dongqi Han</p>
<p><strong>Summary:</strong> Diffusion models have shown great promise in decision-making, also known as
diffusion planning. However, the slow inference speeds limit their potential
for broader real-world applications. Here, we introduce Habi, a general
framework that transforms powerful but slow diffusion planning models into fast
decision-making models, which mimics the cognitive process in the brain that
costly goal-directed behavior gradually transitions to efficient habitual
behavior with repetitive practice. Even usin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.10477v1' target='_blank'>Knowledge Integration Strategies in Autonomous Vehicle Prediction and
  Planning: A Comprehensive Survey</a></h2>
<p><strong>Authors:</strong> Kumar Manas, Adrian Paschke</p>
<p><strong>Summary:</strong> This comprehensive survey examines the integration of knowledge-based
approaches into autonomous driving systems, with a focus on trajectory
prediction and planning. We systematically review methodologies for
incorporating domain knowledge, traffic rules, and commonsense reasoning into
these systems, spanning purely symbolic representations to hybrid
neuro-symbolic architectures. In particular, we analyze recent advancements in
formal logic and differential logic programming, reinforcement learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1712.02441v1' target='_blank'>A Novel Model for Arbitration between Planning and Habitual Control
  Systems</a></h2>
<p><strong>Authors:</strong> Farzaneh S. Fard, Thomas P. Trappenberg</p>
<p><strong>Summary:</strong> It is well established that humans decision making and instrumental control
uses multiple systems, some which use habitual action selection and some which
require deliberate planning. Deliberate planning systems use predictions of
action-outcomes using an internal model of the agent's environment, while
habitual action selection systems learn to automate by repeating previously
rewarded actions. Habitual control is computationally efficient but may be
inflexible in changing environments. Convers...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.08246v3' target='_blank'>Learning Heuristic Selection with Dynamic Algorithm Configuration</a></h2>
<p><strong>Authors:</strong> David Speck, Andr√© Biedenkapp, Frank Hutter, Robert Mattm√ºller, Marius Lindauer</p>
<p><strong>Summary:</strong> A key challenge in satisficing planning is to use multiple heuristics within
one heuristic search. An aggregation of multiple heuristic estimates, for
example by taking the maximum, has the disadvantage that bad estimates of a
single heuristic can negatively affect the whole search. Since the performance
of a heuristic varies from instance to instance, approaches such as algorithm
selection can be successfully applied. In addition, alternating between
multiple heuristics during the search makes ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.05161v1' target='_blank'>Comparative Analysis of Agent-Oriented Task Assignment and Path Planning
  Algorithms Applied to Drone Swarms</a></h2>
<p><strong>Authors:</strong> Rohith Gandhi Ganesan, Samantha Kappagoda, Giuseppe Loianno, David K. A. Mordecai</p>
<p><strong>Summary:</strong> Autonomous drone swarms are a burgeoning technology with significant
applications in the field of mapping, inspection, transportation and
monitoring. To complete a task, each drone has to accomplish a sub-goal within
the context of the overall task at hand and navigate through the environment by
avoiding collision with obstacles and with other agents in the environment. In
this work, we choose the task of optimal coverage of an environment with drone
swarms where the global knowledge of the goal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.11488v1' target='_blank'>Transfer of Deep Reactive Policies for MDP Planning</a></h2>
<p><strong>Authors:</strong> Aniket Bajpai, Sankalp Garg, Mausam</p>
<p><strong>Summary:</strong> Domain-independent probabilistic planners input an MDP description in a
factored representation language such as PPDDL or RDDL, and exploit the
specifics of the representation for faster planning. Traditional algorithms
operate on each problem instance independently, and good methods for
transferring experience from policies of other instances of a domain to a new
instance do not exist. Recently, researchers have begun exploring the use of
deep reactive policies, trained via deep reinforcement l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.05556v3' target='_blank'>Model-Based Offline Planning</a></h2>
<p><strong>Authors:</strong> Arthur Argenson, Gabriel Dulac-Arnold</p>
<p><strong>Summary:</strong> Offline learning is a key part of making reinforcement learning (RL) useable
in real systems. Offline RL looks at scenarios where there is data from a
system's operation, but no direct access to the system when learning a policy.
Recent work on training RL policies from offline data has shown results both
with model-free policies learned directly from the data, or with planning on
top of learnt models of the data. Model-free policies tend to be more
performant, but are more opaque, harder to com...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.11376v2' target='_blank'>Cooperative Behavior Planning for Automated Driving using Graph Neural
  Networks</a></h2>
<p><strong>Authors:</strong> Marvin Klimke, Benjamin V√∂lz, Michael Buchholz</p>
<p><strong>Summary:</strong> Urban intersections are prone to delays and inefficiencies due to static
precedence rules and occlusions limiting the view on prioritized traffic.
Existing approaches to improve traffic flow, widely known as automatic
intersection management systems, are mostly based on non-learning reservation
schemes or optimization algorithms. Machine learning-based techniques show
promising results in planning for a single ego vehicle. This work proposes to
leverage machine learning algorithms to optimize tr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.10033v1' target='_blank'>Skill-based Multi-objective Reinforcement Learning of Industrial Robot
  Tasks with Planning and Knowledge Integration</a></h2>
<p><strong>Authors:</strong> Matthias Mayr, Faseeh Ahmad, Konstantinos Chatzilygeroudis, Luigi Nardi, Volker Krueger</p>
<p><strong>Summary:</strong> In modern industrial settings with small batch sizes it should be easy to set
up a robot system for a new task. Strategies exist, e.g. the use of skills, but
when it comes to handling forces and torques, these systems often fall short.
We introduce an approach that provides a combination of task-level planning
with targeted learning of scenario-specific parameters for skill-based systems.
We propose the following pipeline: (1) the user provides a task goal in the
planning language PDDL, (2) a pl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.00831v1' target='_blank'>Adaptive Task Planning for Large-Scale Robotized Warehouses</a></h2>
<p><strong>Authors:</strong> Dingyuan Shi, Yongxin Tong, Zimu Zhou, Ke Xu, Wenzhe Tan, Hongbo Li</p>
<p><strong>Summary:</strong> Robotized warehouses are deployed to automatically distribute millions of
items brought by the massive logistic orders from e-commerce. A key to
automated item distribution is to plan paths for robots, also known as task
planning, where each task is to deliver racks with items to pickers for
processing and then return the rack back. Prior solutions are unfit for
large-scale robotized warehouses due to the inflexibility to time-varying item
arrivals and the low efficiency for high throughput. In ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.11940v1' target='_blank'>World Value Functions: Knowledge Representation for Learning and
  Planning</a></h2>
<p><strong>Authors:</strong> Geraud Nangue Tasse, Benjamin Rosman, Steven James</p>
<p><strong>Summary:</strong> We propose world value functions (WVFs), a type of goal-oriented general
value function that represents how to solve not just a given task, but any
other goal-reaching task in an agent's environment. This is achieved by
equipping an agent with an internal goal space defined as all the world states
where it experiences a terminal transition. The agent can then modify the
standard task rewards to define its own reward function, which provably drives
it to learn how to achieve all reachable interna...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.00249v4' target='_blank'>BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned
  Approximations</a></h2>
<p><strong>Authors:</strong> Robert J. Moss, Anthony Corso, Jef Caers, Mykel J. Kochenderfer</p>
<p><strong>Summary:</strong> Real-world planning problems, including autonomous driving and sustainable
energy applications like carbon storage and resource exploration, have recently
been modeled as partially observable Markov decision processes (POMDPs) and
solved using approximate methods. To solve high-dimensional POMDPs in practice,
state-of-the-art methods use online planning with problem-specific heuristics
to reduce planning horizons and make the problems tractable. Algorithms that
learn approximations to replace he...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.13552v2' target='_blank'>On Solving the Rubik's Cube with Domain-Independent Planners Using
  Standard Representations</a></h2>
<p><strong>Authors:</strong> Bharath Muppasani, Vishal Pallagani, Biplav Srivastava, Forest Agostinelli</p>
<p><strong>Summary:</strong> Rubik's Cube (RC) is a well-known and computationally challenging puzzle that
has motivated AI researchers to explore efficient alternative representations
and problem-solving methods. The ideal situation for planning here is that a
problem be solved optimally and efficiently represented in a standard notation
using a general-purpose solver and heuristics. The fastest solver today for RC
is DeepCubeA with a custom representation, and another approach is with
Scorpion planner with State-Action-Sp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.16710v1' target='_blank'>Learning whom to trust in navigation: dynamically switching between
  classical and neural planning</a></h2>
<p><strong>Authors:</strong> Sombit Dey, Assem Sadek, Gianluca Monaci, Boris Chidlovskii, Christian Wolf</p>
<p><strong>Summary:</strong> Navigation of terrestrial robots is typically addressed either with
localization and mapping (SLAM) followed by classical planning on the
dynamically created maps, or by machine learning (ML), often through end-to-end
training with reinforcement learning (RL) or imitation learning (IL). Recently,
modular designs have achieved promising results, and hybrid algorithms that
combine ML with classical planning have been proposed. Existing methods
implement these combinations with hand-crafted functio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.10275v3' target='_blank'>Optimizing Crowd-Aware Multi-Agent Path Finding through Local
  Communication with Graph Neural Networks</a></h2>
<p><strong>Authors:</strong> Phu Pham, Aniket Bera</p>
<p><strong>Summary:</strong> Multi-Agent Path Finding (MAPF) in crowded environments presents a
challenging problem in motion planning, aiming to find collision-free paths for
all agents in the system. MAPF finds a wide range of applications in various
domains, including aerial swarms, autonomous warehouse robotics, and
self-driving vehicles. Current approaches to MAPF generally fall into two main
categories: centralized and decentralized planning. Centralized planning
suffers from the curse of dimensionality when the numbe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.00262v2' target='_blank'>Plug-and-Play Policy Planner for Large Language Model Powered Dialogue
  Agents</a></h2>
<p><strong>Authors:</strong> Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, Tat-Seng Chua</p>
<p><strong>Summary:</strong> Proactive dialogues serve as a practical yet challenging dialogue problem in
the era of large language models (LLMs), where the dialogue policy planning is
the key to improving the proactivity of LLMs. Most existing studies enable the
dialogue policy planning of LLMs using various prompting schemes or iteratively
enhance this capability in handling the given case with verbal AI feedback.
However, these approaches are either bounded by the policy planning capability
of the frozen LLMs or hard to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01534v1' target='_blank'>Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon
  Robotics Tasks</a></h2>
<p><strong>Authors:</strong> Murtaza Dalal, Tarun Chiruvolu, Devendra Chaplot, Ruslan Salakhutdinov</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) have been shown to be capable of performing
high-level planning for long-horizon robotics tasks, yet existing methods
require access to a pre-defined skill library (e.g. picking, placing, pulling,
pushing, navigating). However, LLM planning does not address how to design or
learn those behaviors, which remains challenging particularly in long-horizon
settings. Furthermore, for many tasks of interest, the robot needs to be able
to adjust its behavior in a fine-grained...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.00451v1' target='_blank'>Task Planning for Object Rearrangement in Multi-room Environments</a></h2>
<p><strong>Authors:</strong> Karan Mirakhor, Sourav Ghosh, Dipanjan Das, Brojeshwar Bhowmick</p>
<p><strong>Summary:</strong> Object rearrangement in a multi-room setup should produce a reasonable plan
that reduces the agent's overall travel and the number of steps. Recent
state-of-the-art methods fail to produce such plans because they rely on
explicit exploration for discovering unseen objects due to partial
observability and a heuristic planner to sequence the actions for
rearrangement. This paper proposes a novel hierarchical task planner to
efficiently plan a sequence of actions to discover unseen objects and rear...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.14088v1' target='_blank'>ReaLHF: Optimized RLHF Training for Large Language Models through
  Parameter Reallocation</a></h2>
<p><strong>Authors:</strong> Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu</p>
<p><strong>Summary:</strong> Reinforcement Learning from Human Feedback (RLHF) stands as a pivotal
technique in empowering large language model (LLM) applications. Since RLHF
involves diverse computational workloads and intricate dependencies among
multiple LLMs, directly adopting parallelization techniques from supervised
training can result in sub-optimal performance. To overcome this limitation, we
propose a novel approach named parameter ReaLlocation, which dynamically
redistributes LLM parameters in the cluster and ada...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.14655v1' target='_blank'>HYPERmotion: Learning Hybrid Behavior Planning for Autonomous
  Loco-manipulation</a></h2>
<p><strong>Authors:</strong> Jin Wang, Rui Dai, Weijie Wang, Luca Rossini, Francesco Ruscelli, Nikos Tsagarakis</p>
<p><strong>Summary:</strong> Enabling robots to autonomously perform hybrid motions in diverse
environments can be beneficial for long-horizon tasks such as material
handling, household chores, and work assistance. This requires extensive
exploitation of intrinsic motion capabilities, extraction of affordances from
rich environmental information, and planning of physical interaction behaviors.
Despite recent progress has demonstrated impressive humanoid whole-body control
abilities, they struggle to achieve versatility and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02797v1' target='_blank'>Solving Motion Planning Tasks with a Scalable Generative Model</a></h2>
<p><strong>Authors:</strong> Yihan Hu, Siqi Chai, Zhening Yang, Jingyu Qian, Kun Li, Wenxin Shao, Haichao Zhang, Wei Xu, Qiang Liu</p>
<p><strong>Summary:</strong> As autonomous driving systems being deployed to millions of vehicles, there
is a pressing need of improving the system's scalability, safety and reducing
the engineering cost. A realistic, scalable, and practical simulator of the
driving world is highly desired. In this paper, we present an efficient
solution based on generative models which learns the dynamics of the driving
scenes. With this model, we can not only simulate the diverse futures of a
given driving scenario but also generate a var...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.18569v3' target='_blank'>PP-TIL: Personalized Planning for Autonomous Driving with Instance-based
  Transfer Imitation Learning</a></h2>
<p><strong>Authors:</strong> Fangze Lin, Ying He, Fei Yu</p>
<p><strong>Summary:</strong> Personalized motion planning holds significant importance within urban
automated driving, catering to the unique requirements of individual users.
Nevertheless, prior endeavors have frequently encountered difficulties in
simultaneously addressing two crucial aspects: personalized planning within
intricate urban settings and enhancing planning performance through data
utilization. The challenge arises from the expensive and limited nature of user
data, coupled with the scene state space tending t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.24205v1' target='_blank'>Zonal RL-RRT: Integrated RL-RRT Path Planning with Collision Probability
  and Zone Connectivity</a></h2>
<p><strong>Authors:</strong> AmirMohammad Tahmasbi, MohammadSaleh Faghfoorian, Saeed Khodaygan, Aniket Bera</p>
<p><strong>Summary:</strong> Path planning in high-dimensional spaces poses significant challenges,
particularly in achieving both time efficiency and a fair success rate. To
address these issues, we introduce a novel path-planning algorithm, Zonal
RL-RRT, that leverages kd-tree partitioning to segment the map into zones while
addressing zone connectivity, ensuring seamless transitions between zones. By
breaking down the complex environment into multiple zones and using Q-learning
as the high-level decision-maker, our algor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.14584v1' target='_blank'>Simulation-Free Hierarchical Latent Policy Planning for Proactive
  Dialogues</a></h2>
<p><strong>Authors:</strong> Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin</p>
<p><strong>Summary:</strong> Recent advancements in proactive dialogues have garnered significant
attention, particularly for more complex objectives (e.g. emotion support and
persuasion). Unlike traditional task-oriented dialogues, proactive dialogues
demand advanced policy planning and adaptability, requiring rich scenarios and
comprehensive policy repositories to develop such systems. However, existing
approaches tend to rely on Large Language Models (LLMs) for user simulation and
online learning, leading to biases that ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.10141v1' target='_blank'>Enhancing UAV Path Planning Efficiency Through Accelerated Learning</a></h2>
<p><strong>Authors:</strong> Joseanne Viana, Boris Galkin, Lester Ho, Holger Claussen</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields
such as surveillance, reconnaissance, and telecommunications. This study aims
to develop a learning algorithm for the path planning of UAV wireless
communication relays, which can reduce storage requirements and accelerate Deep
Reinforcement Learning (DRL) convergence. Assuming the system possesses terrain
maps of the area and can estimate user locations using localization algorithms
or direct GPS reporting, it can inpu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.14819v1' target='_blank'>Learning from Reward-Free Offline Data: A Case for Planning with Latent
  Dynamics Models</a></h2>
<p><strong>Authors:</strong> Vlad Sobal, Wancong Zhang, Kynghyun Cho, Randall Balestriero, Tim G. J. Rudner, Yann LeCun</p>
<p><strong>Summary:</strong> A long-standing goal in AI is to build agents that can solve a variety of
tasks across different environments, including previously unseen ones. Two
dominant approaches tackle this challenge: (i) reinforcement learning (RL),
which learns policies through trial and error, and (ii) optimal control, which
plans actions using a learned or known dynamics model. However, their relative
strengths and weaknesses remain underexplored in the setting where agents must
learn from offline trajectories withou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.00354v1' target='_blank'>UAV Trajectory Planning in Wireless Sensor Networks for Energy
  Consumption Minimization by Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Botao Zhu, Ebrahim Bedeer, Ha H. Nguyen, Robert Barton, Jerome Henry</p>
<p><strong>Summary:</strong> Unmanned aerial vehicles (UAVs) have emerged as a promising candidate
solution for data collection of large-scale wireless sensor networks (WSNs). In
this paper, we investigate a UAV-aided WSN, where cluster heads (CHs) receive
data from their member nodes, and a UAV is dispatched to collect data from CHs
along the planned trajectory. We aim to minimize the total energy consumption
of the UAV-WSN system in a complete round of data collection. Toward this end,
we formulate the energy consumption ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.12589v3' target='_blank'>A deep reinforcement learning model for predictive maintenance planning
  of road assets: Integrating LCA and LCCA</a></h2>
<p><strong>Authors:</strong> Moein Latifi, Fateme Golivand Darvishvand, Omid Khandel, Mobin Latifi Nowsoud</p>
<p><strong>Summary:</strong> Road maintenance planning is an integral part of road asset management. One
of the main challenges in Maintenance and Rehabilitation (M&R) practices is to
determine maintenance type and timing. This research proposes a framework using
Reinforcement Learning (RL) based on the Long Term Pavement Performance (LTPP)
database to determine the type and timing of M&R practices. A predictive DNN
model is first developed in the proposed algorithm, which serves as the
Environment for the RL algorithm. For...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.03466v4' target='_blank'>Reward-Respecting Subtasks for Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Richard S. Sutton, Marlos C. Machado, G. Zacharias Holland, David Szepesvari, Finbarr Timbers, Brian Tanner, Adam White</p>
<p><strong>Summary:</strong> To achieve the ambitious goals of artificial intelligence, reinforcement
learning must include planning with a model of the world that is abstract in
state and time. Deep learning has made progress with state abstraction, but
temporal abstraction has rarely been used, despite extensively developed theory
based on the options framework. One reason for this is that the space of
possible options is immense, and the methods previously proposed for option
discovery do not take into account how the op...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.07181v2' target='_blank'>Quantum algorithms applied to satellite mission planning for Earth
  observation</a></h2>
<p><strong>Authors:</strong> Serge Rainjonneau, Igor Tokarev, Sergei Iudin, Saaketh Rayaprolu, Karan Pinto, Daria Lemtiuzhnikova, Miras Koblan, Egor Barashov, Mo Kordzanganeh, Markus Pflitsch, Alexey Melnikov</p>
<p><strong>Summary:</strong> Earth imaging satellites are a crucial part of our everyday lives that enable
global tracking of industrial activities. Use cases span many applications,
from weather forecasting to digital maps, carbon footprint tracking, and
vegetation monitoring. However, there are limitations; satellites are difficult
to manufacture, expensive to maintain, and tricky to launch into orbit.
Therefore, satellites must be employed efficiently. This poses a challenge
known as the satellite mission planning proble...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.00569v1' target='_blank'>Human-Like Autonomous Car-Following Model with Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Meixin Zhu, Xuesong Wang, Yinhai Wang</p>
<p><strong>Summary:</strong> This study proposes a framework for human-like autonomous car-following
planning based on deep reinforcement learning (deep RL). Historical driving
data are fed into a simulation environment where an RL agent learns from trial
and error interactions based on a reward function that signals how much the
agent deviates from the empirical data. Through these interactions, an optimal
policy, or car-following model that maps in a human-like way from speed,
relative speed between a lead and following v...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.08690v2' target='_blank'>Replay Buffer with Local Forgetting for Adapting to Local Environment
  Changes in Deep Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Harm van Seijen, Sarath Chandar</p>
<p><strong>Summary:</strong> One of the key behavioral characteristics used in neuroscience to determine
whether the subject of study -- be it a rodent or a human -- exhibits
model-based learning is effective adaptation to local changes in the
environment, a particular form of adaptivity that is the focus of this work. In
reinforcement learning, however, recent work has shown that modern deep
model-based reinforcement-learning (MBRL) methods adapt poorly to local
environment changes. An explanation for this mismatch is that...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.05271v1' target='_blank'>Automaton-Guided Curriculum Generation for Reinforcement Learning Agents</a></h2>
<p><strong>Authors:</strong> Yash Shukla, Abhishek Kulkarni, Robert Wright, Alvaro Velasquez, Jivko Sinapov</p>
<p><strong>Summary:</strong> Despite advances in Reinforcement Learning, many sequential decision making
tasks remain prohibitively expensive and impractical to learn. Recently,
approaches that automatically generate reward functions from logical task
specifications have been proposed to mitigate this issue; however, they scale
poorly on long-horizon tasks (i.e., tasks where the agent needs to perform a
series of correct actions to reach the goal state, considering future
transitions while choosing an action). Employing a c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.07440v2' target='_blank'>Optimizing Memory Mapping Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Pengming Wang, Mikita Sazanovich, Berkin Ilbeyi, Phitchaya Mangpo Phothilimthana, Manish Purohit, Han Yang Tay, Ng√¢n V≈©, Miaosen Wang, Cosmin Paduraru, Edouard Leurent, Anton Zhernov, Po-Sen Huang, Julian Schrittwieser, Thomas Hubert, Robert Tung, Paula Kurylowicz, Kieran Milan, Oriol Vinyals, Daniel J. Mankowitz</p>
<p><strong>Summary:</strong> Resource scheduling and allocation is a critical component of many high
impact systems ranging from congestion control to cloud computing. Finding more
optimal solutions to these problems often has significant impact on resource
and time savings, reducing device wear-and-tear, and even potentially improving
carbon emissions. In this paper, we focus on a specific instance of a
scheduling problem, namely the memory mapping problem that occurs during
compilation of machine learning programs: That i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.00715v4' target='_blank'>Adapting Open-Source Large Language Models for Cost-Effective,
  Expert-Level Clinical Note Generation with On-Policy Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun</p>
<p><strong>Summary:</strong> Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have
demonstrated promising capabilities in clinical text summarization tasks.
However, due to patient data privacy concerns and computational costs, many
healthcare providers prefer using small, locally-hosted models over external
generic LLMs. This study presents a comprehensive domain- and task-specific
adaptation process for the open-source LLaMA-2 13 billion parameter model,
enabling it to generate high-quality clinical notes...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.14995v1' target='_blank'>Reinforcement Learning for Ultrasound Image Analysis A Comprehensive
  Review of Advances and Applications</a></h2>
<p><strong>Authors:</strong> Maha Ezzelarab, Midhila Madhusoodanan, Shrimanti Ghosh, Geetika Vadali, Jacob Jaremko, Abhilash Hareendranathan</p>
<p><strong>Summary:</strong> Over the last decade, the use of machine learning (ML) approaches in
medicinal applications has increased manifold. Most of these approaches are
based on deep learning, which aims to learn representations from grid data
(like medical images). However, reinforcement learning (RL) applications in
medicine are relatively less explored. Medical applications often involve a
sequence of subtasks that form a diagnostic pipeline, and RL is uniquely suited
to optimize over such sequential decision-making...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1303.3163v3' target='_blank'>A Greedy Approximation of Bayesian Reinforcement Learning with Probably
  Optimistic Transition Model</a></h2>
<p><strong>Authors:</strong> Kenji Kawaguchi, Mauricio Araya</p>
<p><strong>Summary:</strong> Bayesian Reinforcement Learning (RL) is capable of not only incorporating
domain knowledge, but also solving the exploration-exploitation dilemma in a
natural way. As Bayesian RL is intractable except for special cases, previous
work has proposed several approximation methods. However, these methods are
usually too sensitive to parameter values, and finding an acceptable parameter
setting is practically impossible in many applications. In this paper, we
propose a new algorithm that greedily appr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1605.04070v1' target='_blank'>A Reinforcement Learning System to Encourage Physical Activity in
  Diabetes Patients</a></h2>
<p><strong>Authors:</strong> Irit Hochberg, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, Elad Yom-Tov</p>
<p><strong>Summary:</strong> Regular physical activity is known to be beneficial to people suffering from
diabetes type 2. Nevertheless, most such people are sedentary. Smartphones
create new possibilities for helping people to adhere to their physical
activity goals, through continuous monitoring and communication, coupled with
personalized feedback.
  We provided 27 sedentary diabetes type 2 patients with a smartphone-based
pedometer and a personal plan for physical activity. Patients were sent SMS
messages to encourage p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1607.07558v5' target='_blank'>Learning to Prevent Monocular SLAM Failure using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vignesh Prasad, Karmesh Yadav, Rohitashva Singh Saurabh, Swapnil Daga, Nahas Pareekutty, K. Madhava Krishna, Balaraman Ravindran, Brojeshwar Bhowmick</p>
<p><strong>Summary:</strong> Monocular SLAM refers to using a single camera to estimate robot ego motion
while building a map of the environment. While Monocular SLAM is a well studied
problem, automating Monocular SLAM by integrating it with trajectory planning
frameworks is particularly challenging. This paper presents a novel formulation
based on Reinforcement Learning (RL) that generates fail safe trajectories
wherein the SLAM generated outputs do not deviate largely from their true
values. Quintessentially, the RL fram...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1709.10082v3' target='_blank'>Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Pinxin Long, Tingxiang Fan, Xinyi Liao, Wenxi Liu, Hao Zhang, Jia Pan</p>
<p><strong>Summary:</strong> Developing a safe and efficient collision avoidance policy for multiple
robots is challenging in the decentralized scenarios where each robot generate
its paths without observing other robots' states and intents. While other
distributed multi-robot collision avoidance systems exist, they often require
extracting agent-level features to plan a local collision-free action, which
can be computationally prohibitive and not robust. More importantly, in
practice the performance of these methods are mu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1801.05500v1' target='_blank'>Cellular-Connected UAVs over 5G: Deep Reinforcement Learning for
  Interference Management</a></h2>
<p><strong>Authors:</strong> Ursula Challita, Walid Saad, Christian Bettstetter</p>
<p><strong>Summary:</strong> In this paper, an interference-aware path planning scheme for a network of
cellular-connected unmanned aerial vehicles (UAVs) is proposed. In particular,
each UAV aims at achieving a tradeoff between maximizing energy efficiency and
minimizing both wireless latency and the interference level caused on the
ground network along its path. The problem is cast as a dynamic game among
UAVs. To solve this game, a deep reinforcement learning algorithm, based on
echo state network (ESN) cells, is propose...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.00198v1' target='_blank'>Learning to Run challenge: Synthesizing physiologically accurate motion
  using deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> ≈Åukasz Kidzi≈Ñski, Sharada P. Mohanty, Carmichael Ong, Jennifer L. Hicks, Sean F. Carroll, Sergey Levine, Marcel Salath√©, Scott L. Delp</p>
<p><strong>Summary:</strong> Synthesizing physiologically-accurate human movement in a variety of
conditions can help practitioners plan surgeries, design experiments, or
prototype assistive devices in simulated environments, reducing time and costs
and improving treatment outcomes. Because of the large and complex solution
spaces of biomechanical models, current methods are constrained to specific
movements and models, requiring careful design of a controller and hindering
many possible applications. We sought to discover ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.08010v4' target='_blank'>Where Do You Think You're Going?: Inferring Beliefs about Dynamics from
  Behavior</a></h2>
<p><strong>Authors:</strong> Siddharth Reddy, Anca D. Dragan, Sergey Levine</p>
<p><strong>Summary:</strong> Inferring intent from observed behavior has been studied extensively within
the frameworks of Bayesian inverse planning and inverse reinforcement learning.
These methods infer a goal or reward function that best explains the actions of
the observed agent, typically a human demonstrator. Another agent can use this
inferred intent to predict, imitate, or assist the human user. However, a
central assumption in inverse reinforcement learning is that the demonstrator
is close to optimal. While models...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.03228v1' target='_blank'>Automatic View Planning with Multi-scale Deep Reinforcement Learning
  Agents</a></h2>
<p><strong>Authors:</strong> Amir Alansary, Loic Le Folgoc, Ghislain Vaillant, Ozan Oktay, Yuanwei Li, Wenjia Bai, Jonathan Passerat-Palmbach, Ricardo Guerrero, Konstantinos Kamnitsas, Benjamin Hou, Steven McDonagh, Ben Glocker, Bernhard Kainz, Daniel Rueckert</p>
<p><strong>Summary:</strong> We propose a fully automatic method to find standardized view planes in 3D
image acquisitions. Standard view images are important in clinical practice as
they provide a means to perform biometric measurements from similar anatomical
regions. These views are often constrained to the native orientation of a 3D
image acquisition. Navigating through target anatomy to find the required view
plane is tedious and operator-dependent. For this task, we employ a multi-scale
reinforcement learning (RL) age...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.09318v4' target='_blank'>Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to
  Reach New Goals</a></h2>
<p><strong>Authors:</strong> Vikas Dhiman, Shurjo Banerjee, Jeffrey M. Siskind, Jason J. Corso</p>
<p><strong>Summary:</strong> Consider mutli-goal tasks that involve static environments and dynamic goals.
Examples of such tasks, such as goal-directed navigation and pick-and-place in
robotics, abound. Two types of Reinforcement Learning (RL) algorithms are used
for such tasks: model-free or model-based. Each of these approaches has
limitations. Model-free RL struggles to transfer learned information when the
goal location changes, but achieves high asymptotic accuracy in single goal
tasks. Model-based RL can transfer lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.04290v3' target='_blank'>Vehicular Edge Computing via Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Qi Qi, Zhanyu Ma</p>
<p><strong>Summary:</strong> The smart vehicles construct Vehicle of Internet which can execute various
intelligent services. Although the computation capability of the vehicle is
limited, multi-type of edge computing nodes provide heterogeneous resources for
vehicular services.When offloading the complicated service to the vehicular
edge computing node, the decision should consider numerous factors.The
offloading decision work mostly formulate the decision to a resource scheduling
problem with single or multiple objective ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.04859v3' target='_blank'>Reinforcement Learning for Integer Programming: Learning to Cut</a></h2>
<p><strong>Authors:</strong> Yunhao Tang, Shipra Agrawal, Yuri Faenza</p>
<p><strong>Summary:</strong> Integer programming (IP) is a general optimization framework widely
applicable to a variety of unstructured and structured problems arising in,
e.g., scheduling, production planning, and graph optimization. As IP models
many provably hard to solve problems, modern IP solvers rely on many
heuristics. These heuristics are usually human-designed, and naturally prone to
suboptimality. The goal of this work is to show that the performance of those
solvers can be greatly enhanced using reinforcement l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.04787v2' target='_blank'>MAT: Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Bohan Wu, Iretiayo Akinola, Jacob Varley, Peter Allen</p>
<p><strong>Summary:</strong> Vision-based grasping systems typically adopt an open-loop execution of a
planned grasp. This policy can fail due to many reasons, including ubiquitous
calibration error. Recovery from a failed grasp is further complicated by
visual occlusion, as the hand is usually occluding the vision sensor as it
attempts another open-loop regrasp. This work presents MAT, a tactile
closed-loop method capable of realizing grasps provided by a coarse initial
positioning of the hand above an object. Our algorith...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06493v1' target='_blank'>Flight Controller Synthesis Via Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> William Koch</p>
<p><strong>Summary:</strong> Traditional control methods are inadequate in many deployment settings
involving control of Cyber-Physical Systems (CPS). In such settings, CPS
controllers must operate and respond to unpredictable interactions, conditions,
or failure modes. Dealing with such unpredictability requires the use of
executive and cognitive control functions that allow for planning and
reasoning. Motivated by the sport of drone racing, this dissertation addresses
these concerns for state-of-the-art flight control by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06710v3' target='_blank'>Driving in Dense Traffic with Model-Free Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Dhruv Mauria Saxena, Sangjae Bae, Alireza Nakhaei, Kikuo Fujimura, Maxim Likhachev</p>
<p><strong>Summary:</strong> Traditional planning and control methods could fail to find a feasible
trajectory for an autonomous vehicle to execute amongst dense traffic on roads.
This is because the obstacle-free volume in spacetime is very small in these
scenarios for the vehicle to drive through. However, that does not mean the
task is infeasible since human drivers are known to be able to drive amongst
dense traffic by leveraging the cooperativeness of other drivers to open a gap.
The traditional methods fail to take in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.04175v1' target='_blank'>Multi-Agent Connected Autonomous Driving using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Praveen Palanisamy</p>
<p><strong>Summary:</strong> The capability to learn and adapt to changes in the driving environment is
crucial for developing autonomous driving systems that are scalable beyond
geo-fenced operational design domains. Deep Reinforcement Learning (RL)
provides a promising and scalable framework for developing adaptive learning
based solutions. Deep RL methods usually model the problem as a (Partially
Observable) Markov Decision Process in which an agent acts in a stationary
environment to learn an optimal behavior policy. Ho...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.06486v1' target='_blank'>Automated Augmentation with Reinforcement Learning and GANs for Robust
  Identification of Traffic Signs using Front Camera Images</a></h2>
<p><strong>Authors:</strong> Sohini Roy Chowdhury, Lars Tornberg, Robin Halvfordsson, Jonatan Nordh, Adam Suhren Gustafsson, Joel Wall, Mattias Westerberg, Adam Wirehed, Louis Tilloy, Zhanying Hu, Haoyuan Tan, Meng Pan, Jonas Sjoberg</p>
<p><strong>Summary:</strong> Traffic sign identification using camera images from vehicles plays a
critical role in autonomous driving and path planning. However, the front
camera images can be distorted due to blurriness, lighting variations and
vandalism which can lead to degradation of detection performances. As a
solution, machine learning models must be trained with data from multiple
domains, and collecting and labeling more data in each new domain is time
consuming and expensive. In this work, we present an end-to-en...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.05780v1' target='_blank'>Reinforcement-Learning based Portfolio Management with Augmented Asset
  Movement Prediction States</a></h2>
<p><strong>Authors:</strong> Yunan Ye, Hengzhi Pei, Boxin Wang, Pin-Yu Chen, Yada Zhu, Jun Xiao, Bo Li</p>
<p><strong>Summary:</strong> Portfolio management (PM) is a fundamental financial planning task that aims
to achieve investment goals such as maximal profits or minimal risks. Its
decision process involves continuous derivation of valuable information from
various data sources and sequential decision optimization, which is a
prospective research direction for reinforcement learning (RL). In this paper,
we propose SARL, a novel State-Augmented RL framework for PM. Our framework
aims to address two unique challenges in financ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.12361v2' target='_blank'>Sub-Goal Trees -- a Framework for Goal-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Tom Jurgenson, Or Avner, Edward Groshev, Aviv Tamar</p>
<p><strong>Summary:</strong> Many AI problems, in robotics and other domains, are goal-based, essentially
seeking trajectories leading to various goal states. Reinforcement learning
(RL), building on Bellman's optimality equation, naturally optimizes for a
single goal, yet can be made multi-goal by augmenting the state with the goal.
Instead, we propose a new RL framework, derived from a dynamic programming
equation for the all pairs shortest path (APSP) problem, which naturally solves
multi-goal queries. We show that this ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.01976v1' target='_blank'>Distributed Adaptive Reinforcement Learning: A Method for Optimal
  Routing</a></h2>
<p><strong>Authors:</strong> Salar Rahili, Benjamin Riviere, Soon-Jo Chung</p>
<p><strong>Summary:</strong> In this paper, a learning-based optimal transportation algorithm for
autonomous taxis and ridesharing vehicles is presented. The goal is to design a
mechanism to solve the routing problem for multiple autonomous vehicles and
multiple customers in order to maximize the transportation company's profit. As
a result, each vehicle selects the customer whose request maximizes the
company's profit in the long run. To solve this problem, the system is modeled
as a Markov Decision Process (MDP) using pas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.05951v3' target='_blank'>MOReL : Model-Based Offline Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims</p>
<p><strong>Summary:</strong> In offline reinforcement learning (RL), the goal is to learn a highly
rewarding policy based solely on a dataset of historical interactions with the
environment. The ability to train RL policies offline can greatly expand the
applicability of RL, its data efficiency, and its experimental velocity. Prior
work in offline RL has been confined almost exclusively to model-free RL
approaches. In this work, we present MOReL, an algorithmic framework for
model-based offline RL. This framework consists o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.01107v1' target='_blank'>Model-Based Reinforcement Learning with Value-Targeted Regression</a></h2>
<p><strong>Authors:</strong> Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, Lin F. Yang</p>
<p><strong>Summary:</strong> This paper studies model-based reinforcement learning (RL) for regret
minimization. We focus on finite-horizon episodic RL where the transition model
$P$ belongs to a known family of models $\mathcal{P}$, a special case of which
is when models in $\mathcal{P}$ take the form of linear mixtures: $P_{\theta} =
\sum_{i=1}^{d} \theta_{i}P_{i}$. We propose a model based RL algorithm that is
based on optimism principle: In each episode, the set of models that are
`consistent' with the data collected is...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.01380v1' target='_blank'>Deep reinforcement learning driven inspection and maintenance planning
  under incomplete information and constraints</a></h2>
<p><strong>Authors:</strong> C. P. Andriotis, K. G. Papakonstantinou</p>
<p><strong>Summary:</strong> Determination of inspection and maintenance policies for minimizing long-term
risks and costs in deteriorating engineering environments constitutes a complex
optimization problem. Major computational challenges include the (i) curse of
dimensionality, due to exponential scaling of state/action set cardinalities
with the number of components; (ii) curse of history, related to exponentially
growing decision-trees with the number of decision-steps; (iii) presence of
state uncertainties, induced by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.01722v1' target='_blank'>Deep Reinforcement Learning for Collaborative Edge Computing in
  Vehicular Networks</a></h2>
<p><strong>Authors:</strong> Mushu Li, Jie Gao, Lian Zhao, Xuemin Shen</p>
<p><strong>Summary:</strong> Mobile edge computing (MEC) is a promising technology to support
mission-critical vehicular applications, such as intelligent path planning and
safety applications. In this paper, a collaborative edge computing framework is
developed to reduce the computing service latency and improve service
reliability for vehicular networks. First, a task partition and scheduling
algorithm (TPSA) is proposed to decide the workload allocation and schedule the
execution order of the tasks offloaded to the edge ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.06797v5' target='_blank'>Reinforcement Learning Based Temporal Logic Control with Maximum
  Probabilistic Satisfaction</a></h2>
<p><strong>Authors:</strong> Mingyu Cai, Shaoping Xiao, Baoluo Li, Zhiliang Li, Zhen Kan</p>
<p><strong>Summary:</strong> This paper presents a model-free reinforcement learning (RL) algorithm to
synthesize a control policy that maximizes the satisfaction probability of
linear temporal logic (LTL) specifications. Due to the consideration of
environment and motion uncertainties, we model the robot motion as a
probabilistic labeled Markov decision process with unknown transition
probabilities and unknown probabilistic label functions. The LTL task
specification is converted to a limit deterministic generalized B\"uch...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.08600v2' target='_blank'>Robot Navigation in Constrained Pedestrian Environments using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Claudia P√©rez-D'Arpino, Can Liu, Patrick Goebel, Roberto Mart√≠n-Mart√≠n, Silvio Savarese</p>
<p><strong>Summary:</strong> Navigating fluently around pedestrians is a necessary capability for mobile
robots deployed in human environments, such as buildings and homes. While
research on social navigation has focused mainly on the scalability with the
number of pedestrians in open spaces, typical indoor environments present the
additional challenge of constrained spaces such as corridors and doorways that
limit maneuverability and influence patterns of pedestrian interaction. We
present an approach based on reinforcemen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.10903v1' target='_blank'>Visual Navigation in Real-World Indoor Environments Using End-to-End
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jon√°≈° Kulh√°nek, Erik Derner, Robert Babu≈°ka</p>
<p><strong>Summary:</strong> Visual navigation is essential for many applications in robotics, from
manipulation, through mobile robotics to automated driving. Deep reinforcement
learning (DRL) provides an elegant map-free approach integrating image
processing, localization, and planning in one module, which can be trained and
therefore optimized for a given environment. However, to date, DRL-based visual
navigation was validated exclusively in simulation, where the simulator
provides information that is not available in th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.13303v1' target='_blank'>Trajectory-wise Multiple Choice Learning for Dynamics Generalization in
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Younggyo Seo, Kimin Lee, Ignasi Clavera, Thanard Kurutach, Jinwoo Shin, Pieter Abbeel</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL) has shown great potential in various
control tasks in terms of both sample-efficiency and final performance.
However, learning a generalizable dynamics model robust to changes in dynamics
remains a challenge since the target transition dynamics follow a multi-modal
distribution. In this paper, we present a new model-based RL algorithm, coined
trajectory-wise multiple choice learning, that learns a multi-headed dynamics
model for dynamics generalization. Th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.10200v1' target='_blank'>Exact Reduction of Huge Action Spaces in General Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sultan Javed Majeed, Marcus Hutter</p>
<p><strong>Summary:</strong> The reinforcement learning (RL) framework formalizes the notion of learning
with interactions. Many real-world problems have large state-spaces and/or
action-spaces such as in Go, StarCraft, protein folding, and robotics or are
non-Markovian, which cause significant challenges to RL algorithms. In this
work we address the large action-space problem by sequentializing actions,
which can reduce the action-space size significantly, even down to two actions
at the expense of an increased planning ho...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.01774v1' target='_blank'>An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor
  Robot Navigation</a></h2>
<p><strong>Authors:</strong> Kaushik Balakrishnan, Punarjay Chakravarty, Shubham Shrivastava</p>
<p><strong>Summary:</strong> Training robots to navigate diverse environments is a challenging problem as
it involves the confluence of several different perception tasks such as
mapping and localization, followed by optimal path-planning and control.
Recently released photo-realistic simulators such as Habitat allow for the
training of networks that output control actions directly from perception:
agents use Deep Reinforcement Learning (DRL) to regress directly from the
camera image to a control output in an end-to-end fas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.05325v2' target='_blank'>Learning Kinematic Feasibility for Mobile Manipulation through Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Daniel Honerkamp, Tim Welschehold, Abhinav Valada</p>
<p><strong>Summary:</strong> Mobile manipulation tasks remain one of the critical challenges for the
widespread adoption of autonomous robots in both service and industrial
scenarios. While planning approaches are good at generating feasible whole-body
robot trajectories, they struggle with dynamic environments as well as the
incorporation of constraints given by the task and the environment. On the
other hand, dynamic motion models in the action space struggle with generating
kinematically feasible trajectories for mobile ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.07282v1' target='_blank'>Rule-Based Reinforcement Learning for Efficient Robot Navigation with
  Space Reduction</a></h2>
<p><strong>Authors:</strong> Yuanyang Zhu, Zhi Wang, Chunlin Chen, Daoyi Dong</p>
<p><strong>Summary:</strong> For real-world deployments, it is critical to allow robots to navigate in
complex environments autonomously. Traditional methods usually maintain an
internal map of the environment, and then design several simple rules, in
conjunction with a localization and planning approach, to navigate through the
internal map. These approaches often involve a variety of assumptions and prior
knowledge. In contrast, recent reinforcement learning (RL) methods can provide
a model-free, self-learning mechanism a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.02039v4' target='_blank'>Offline Reinforcement Learning as One Big Sequence Modeling Problem</a></h2>
<p><strong>Authors:</strong> Michael Janner, Qiyang Li, Sergey Levine</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is typically concerned with estimating stationary
policies or single-step models, leveraging the Markov property to factorize
problems in time. However, we can also view RL as a generic sequence modeling
problem, with the goal being to produce a sequence of actions that leads to a
sequence of high rewards. Viewed in this way, it is tempting to consider
whether high-capacity sequence prediction models that work well in other
domains, such as natural-language processing...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.14130v1' target='_blank'>Continuous Control with Deep Reinforcement Learning for Autonomous
  Vessels</a></h2>
<p><strong>Authors:</strong> Nader Zare, Bruno Brandoli, Mahtab Sarvmaili, Amilcar Soares, Stan Matwin</p>
<p><strong>Summary:</strong> Maritime autonomous transportation has played a crucial role in the
globalization of the world economy. Deep Reinforcement Learning (DRL) has been
applied to automatic path planning to simulate vessel collision avoidance
situations in open seas. End-to-end approaches that learn complex mappings
directly from the input have poor generalization to reach the targets in
different environments. In this work, we present a new strategy called
state-action rotation to improve agent's performance in unse...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.05120v3' target='_blank'>TERP: Reliable Planning in Uneven Outdoor Environments using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kasun Weerakoon, Adarsh Jagan Sathyamoorthy, Utsav Patel, Dinesh Manocha</p>
<p><strong>Summary:</strong> We present a novel method for reliable robot navigation in uneven outdoor
terrains. Our approach employs a novel fully-trained Deep Reinforcement
Learning (DRL) network that uses elevation maps of the environment, robot pose,
and goal as inputs to compute an attention mask of the environment. The
attention mask is used to identify reduced stability regions in the elevation
map and is computed using channel and spatial attention modules and a novel
reward function. We continuously compute and upd...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.06609v1' target='_blank'>DSDF: An approach to handle stochastic agents in collaborative
  multi-agent reinforcement learning</a></h2>
<p><strong>Authors:</strong> Satheesh K. Perepu, Kaushik Dey</p>
<p><strong>Summary:</strong> Multi-Agent reinforcement learning has received lot of attention in recent
years and have applications in many different areas. Existing methods involving
Centralized Training and Decentralized execution, attempts to train the agents
towards learning a pattern of coordinated actions to arrive at optimal joint
policy. However if some agents are stochastic to varying degrees of
stochasticity, the above methods often fail to converge and provides poor
coordination among agents. In this paper we sho...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.11661v3' target='_blank'>Deep Reinforcement Learning-Based Long-Range Autonomous Valet Parking
  for Smart Cities</a></h2>
<p><strong>Authors:</strong> Muhammad Khalid, Liang Wang, Kezhi Wang, Cunhua Pan, Nauman Aslam, Yue Cao</p>
<p><strong>Summary:</strong> In this paper, to reduce the congestion rate at the city center and increase
the quality of experience (QoE) of each user, the framework of long-range
autonomous valet parking (LAVP) is presented, where an Autonomous Vehicle (AV)
is deployed in the city, which can pick up, drop off users at their required
spots, and then drive to the car park out of city center autonomously. In this
framework, we aim to minimize the overall distance of the AV, while guarantee
all users are served, i.e., picking ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1610.03295v1' target='_blank'>Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua</p>
<p><strong>Summary:</strong> Autonomous driving is a multi-agent setting where the host vehicle must apply
sophisticated negotiation skills with other road users when overtaking, giving
way, merging, taking left and right turns and while pushing ahead in
unstructured urban roadways. Since there are many possible scenarios, manually
tackling all possible cases will likely yield a too simplistic policy.
Moreover, one must balance between unexpected behavior of other
drivers/pedestrians and at the same time not to be too defen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.00444v3' target='_blank'>Inverse Reinforcement Learning via Nonparametric Spatio-Temporal Subgoal
  Modeling</a></h2>
<p><strong>Authors:</strong> Adrian ≈†o≈°iƒá, Elmar Rueckert, Jan Peters, Abdelhak M. Zoubir, Heinz Koeppl</p>
<p><strong>Summary:</strong> Advances in the field of inverse reinforcement learning (IRL) have led to
sophisticated inference frameworks that relax the original modeling assumption
of observing an agent behavior that reflects only a single intention. Instead
of learning a global behavioral model, recent IRL methods divide the
demonstration data into parts, to account for the fact that different
trajectories may correspond to different intentions, e.g., because they were
generated by different domain experts. In this work, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.07225v1' target='_blank'>Integrating kinematics and environment context into deep inverse
  reinforcement learning for predicting off-road vehicle trajectories</a></h2>
<p><strong>Authors:</strong> Yanfu Zhang, Wenshan Wang, Rogerio Bonatti, Daniel Maturana, Sebastian Scherer</p>
<p><strong>Summary:</strong> Predicting the motion of a mobile agent from a third-person perspective is an
important component for many robotics applications, such as autonomous
navigation and tracking. With accurate motion prediction of other agents,
robots can plan for more intelligent behaviors to achieve specified objectives,
instead of acting in a purely reactive way. Previous work addresses motion
prediction by either only filtering kinematics, or using hand-designed and
learned representations of the environment. Ins...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.01241v2' target='_blank'>Centerline Depth World Reinforcement Learning-based Left Atrial
  Appendage Orifice Localization</a></h2>
<p><strong>Authors:</strong> Walid Abdullah Al, Il Dong Yun, Eun Ju Chun</p>
<p><strong>Summary:</strong> Left atrial appendage (LAA) closure (LAAC) is a minimally invasive
implant-based method to prevent cardiovascular stroke in patients with
non-valvular atrial fibrillation. Assessing the LAA orifice in preoperative CT
angiography plays a crucial role in choosing an appropriate LAAC implant size
and a proper C-arm angulation. However, accurate orifice localization is hard
because of the high anatomic variation of LAA, and unclear position and
orientation of the orifice in available CT views. Deep ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.02435v2' target='_blank'>Self-Adapting Goals Allow Transfer of Predictive Models to New Tasks</a></h2>
<p><strong>Authors:</strong> Kai Olav Ellefsen, Jim Torresen</p>
<p><strong>Summary:</strong> A long-standing challenge in Reinforcement Learning is enabling agents to
learn a model of their environment which can be transferred to solve other
problems in a world with the same underlying rules. One reason this is
difficult is the challenge of learning accurate models of an environment. If
such a model is inaccurate, the agent's plans and actions will likely be
sub-optimal, and likely lead to the wrong outcomes. Recent progress in
model-based reinforcement learning has improved the ability...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.04371v3' target='_blank'>A Multi-Agent Reinforcement Learning Approach For Safe and Efficient
  Behavior Planning Of Connected Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Songyang Han, Shanglin Zhou, Jiangwei Wang, Lynn Pepin, Caiwen Ding, Jie Fu, Fei Miao</p>
<p><strong>Summary:</strong> The recent advancements in wireless technology enable connected autonomous
vehicles (CAVs) to gather information about their environment by
vehicle-to-vehicle (V2V) communication. In this work, we design an
information-sharing-based multi-agent reinforcement learning (MARL) framework
for CAVs, to take advantage of the extra information when making decisions to
improve traffic efficiency and safety. The safe actor-critic algorithm we
propose has two new techniques: the truncated Q-function and sa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.04427v1' target='_blank'>Transfer Reinforcement Learning under Unobserved Contextual Information</a></h2>
<p><strong>Authors:</strong> Yan Zhang, Michael M. Zavlanos</p>
<p><strong>Summary:</strong> In this paper, we study a transfer reinforcement learning problem where the
state transitions and rewards are affected by the environmental context.
Specifically, we consider a demonstrator agent that has access to a
context-aware policy and can generate transition and reward data based on that
policy. These data constitute the experience of the demonstrator. Then, the
goal is to transfer this experience, excluding the underlying contextual
information, to a learner agent that does not have acce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.06651v4' target='_blank'>A Text-based Deep Reinforcement Learning Framework for Interactive
  Recommendation</a></h2>
<p><strong>Authors:</strong> Chaoyang Wang, Zhiqiang Guo, Jianjun Li, Peng Pan, Guohui Li</p>
<p><strong>Summary:</strong> Due to its nature of learning from dynamic interactions and planning for
long-run performance, reinforcement learning (RL) recently has received much
attention in interactive recommender systems (IRSs). IRSs usually face the
large discrete action space problem, which makes most of the existing RL-based
recommendation methods inefficient. Moreover, data sparsity is another
challenging problem that most IRSs are confronted with. While the textual
information like reviews and descriptions is less s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.11935v1' target='_blank'>The Variational Bandwidth Bottleneck: Stochastic Evaluation on an
  Information Budget</a></h2>
<p><strong>Authors:</strong> Anirudh Goyal, Yoshua Bengio, Matthew Botvinick, Sergey Levine</p>
<p><strong>Summary:</strong> In many applications, it is desirable to extract only the relevant
information from complex input data, which involves making a decision about
which input features are relevant. The information bottleneck method formalizes
this as an information-theoretic optimization problem by maintaining an optimal
tradeoff between compression (throwing away irrelevant input information), and
predicting the target. In many problem settings, including the reinforcement
learning problems we consider in this wor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.14288v3' target='_blank'>Actor-Critic Reinforcement Learning for Control with Stability Guarantee</a></h2>
<p><strong>Authors:</strong> Minghao Han, Lixian Zhang, Jun Wang, Wei Pan</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) and its integration with deep learning have
achieved impressive performance in various robotic control tasks, ranging from
motion planning and navigation to end-to-end visual manipulation. However,
stability is not guaranteed in model-free RL by solely using data. From a
control-theoretic perspective, stability is the most important property for any
control system, since it is closely related to safety, robustness, and
reliability of robotic systems. In this paper, we...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.06622v1' target='_blank'>Cautious Adaptation For Reinforcement Learning in Safety-Critical
  Settings</a></h2>
<p><strong>Authors:</strong> Jesse Zhang, Brian Cheung, Chelsea Finn, Sergey Levine, Dinesh Jayaraman</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) in real-world safety-critical target settings
like urban driving is hazardous, imperiling the RL agent, other agents, and the
environment. To overcome this difficulty, we propose a "safety-critical
adaptation" task setting: an agent first trains in non-safety-critical "source"
environments such as in a simulator, before it adapts to the target environment
where failures carry heavy costs. We propose a solution approach, CARL, that
builds on the intuition that prior ex...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.07971v2' target='_blank'>Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, Peter Duerr</p>
<p><strong>Summary:</strong> Autonomous car racing is a major challenge in robotics. It raises fundamental
problems for classical approaches such as planning minimum-time trajectories
under uncertain dynamics and controlling the car at the limits of its handling.
Besides, the requirement of minimizing the lap time, which is a sparse
objective, and the difficulty of collecting training data from human experts
have also hindered researchers from directly applying learning-based approaches
to solve the problem. In the present ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.08111v4' target='_blank'>Reward Maximisation through Discrete Active Inference</a></h2>
<p><strong>Authors:</strong> Lancelot Da Costa, Noor Sajid, Thomas Parr, Karl Friston, Ryan Smith</p>
<p><strong>Summary:</strong> Active inference is a probabilistic framework for modelling the behaviour of
biological and artificial agents, which derives from the principle of
minimising free energy. In recent years, this framework has successfully been
applied to a variety of situations where the goal was to maximise reward,
offering comparable and sometimes superior performance to alternative
approaches. In this paper, we clarify the connection between reward
maximisation and active inference by demonstrating how and when...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.14280v2' target='_blank'>Learning to swim in potential flow</a></h2>
<p><strong>Authors:</strong> Yusheng Jiao, Feng Ling, Sina Heydari, Nicolas Heess, Josh Merel, Eva Kanso</p>
<p><strong>Summary:</strong> Fish swim by undulating their bodies. These propulsive motions require
coordinated shape changes of a body that interacts with its fluid environment,
but the specific shape coordination that leads to robust turning and swimming
motions remains unclear. To address the problem of underwater motion planning,
we propose a simple model of a three-link fish swimming in a potential flow
environment and we use model-free reinforcement learning for shape control. We
arrive at optimal shape changes for tw...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.03506v1' target='_blank'>The Value Equivalence Principle for Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Christopher Grimm, Andr√© Barreto, Satinder Singh, David Silver</p>
<p><strong>Summary:</strong> Learning models of the environment from data is often viewed as an essential
component to building intelligent reinforcement learning (RL) agents. The
common practice is to separate the learning of the model from its use, by
constructing a model of the environment's dynamics that correctly predicts the
observed state transitions. In this paper we argue that the limited
representational resources of model-based RL agents are better used to build
models that are directly useful for value-based pla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.14430v1' target='_blank'>Deep Reinforcement Learning for Crowdsourced Urban Delivery: System
  States Characterization, Heuristics-guided Action Choice, and
  Rule-Interposing Integration</a></h2>
<p><strong>Authors:</strong> Tanvir Ahamed, Bo Zou, Nahid Parvez Farazi, Theja Tulabandhula</p>
<p><strong>Summary:</strong> This paper investigates the problem of assigning shipping requests to ad hoc
couriers in the context of crowdsourced urban delivery. The shipping requests
are spatially distributed each with a limited time window between the earliest
time for pickup and latest time for delivery. The ad hoc couriers, termed
crowdsourcees, also have limited time availability and carrying capacity. We
propose a new deep reinforcement learning (DRL)-based approach to tackling this
assignment problem. A deep Q networ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.06274v3' target='_blank'>Hedging of Financial Derivative Contracts via Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Oleg Szehr</p>
<p><strong>Summary:</strong> The construction of approximate replication strategies for pricing and
hedging of derivative contracts in incomplete markets is a key problem of
financial engineering. Recently Reinforcement Learning algorithms for hedging
under realistic market conditions have attracted significant interest. While
research in the derivatives area mostly focused on variations of $Q$-learning,
in artificial intelligence Monte Carlo Tree Search is the recognized
state-of-the-art method for various planning problem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.06632v1' target='_blank'>Deep Reinforcement Learning for Backup Strategies against Adversaries</a></h2>
<p><strong>Authors:</strong> Pascal Debus, Nicolas M√ºller, Konstantin B√∂ttinger</p>
<p><strong>Summary:</strong> Many defensive measures in cyber security are still dominated by heuristics,
catalogs of standard procedures, and best practices. Considering the case of
data backup strategies, we aim towards mathematically modeling the underlying
threat models and decision problems. By formulating backup strategies in the
language of stochastic processes, we can translate the challenge of finding
optimal defenses into a reinforcement learning problem. This enables us to
train autonomous agents that learn to op...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.12432v1' target='_blank'>Deep Reinforcement Learning for Safe Landing Site Selection with
  Concurrent Consideration of Divert Maneuvers</a></h2>
<p><strong>Authors:</strong> Keidai Iiyama, Kento Tomita, Bhavi A. Jagatia, Tatsuwaki Nakagawa, Koki Ho</p>
<p><strong>Summary:</strong> This research proposes a new integrated framework for identifying safe
landing locations and planning in-flight divert maneuvers. The state-of-the-art
algorithms for landing zone selection utilize local terrain features such as
slopes and roughness to judge the safety and priority of the landing point.
However, when there are additional chances of observation and diverting in the
future, these algorithms are not able to evaluate the safety of the decision
itself to target the selected landing po...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.14718v3' target='_blank'>Increasing the Efficiency of Policy Learning for Autonomous Vehicles by
  Multi-Task Representation Learning</a></h2>
<p><strong>Authors:</strong> Eshagh Kargar, Ville Kyrki</p>
<p><strong>Summary:</strong> Driving in a dynamic, multi-agent, and complex urban environment is a
difficult task requiring a complex decision-making policy. The learning of such
a policy requires a state representation that can encode the entire
environment. Mid-level representations that encode a vehicle's environment as
images have become a popular choice. Still, they are quite high-dimensional,
limiting their use in data-hungry approaches such as reinforcement learning. In
this article, we propose to learn a low-dimensi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.00854v2' target='_blank'>Emotional Contagion-Aware Deep Reinforcement Learning for Antagonistic
  Crowd Simulation</a></h2>
<p><strong>Authors:</strong> Pei Lv, Qingqing Yu, Boya Xu, Chaochao Li, Bing Zhou, Mingliang Xu</p>
<p><strong>Summary:</strong> The antagonistic behavior in the crowd usually exacerbates the seriousness of
the situation in sudden riots, where the antagonistic emotional contagion and
behavioral decision making play very important roles. However, the complex
mechanism of antagonistic emotion influencing decision making, especially in
the environment of sudden confrontation, has not yet been explored very
clearly. In this paper, we propose an Emotional contagion-aware Deep
reinforcement learning model for Antagonistic Crowd...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.08587v1' target='_blank'>Adaptive ABAC Policy Learning: A Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Leila Karimi, Mai Abdelhakim, James Joshi</p>
<p><strong>Summary:</strong> With rapid advances in computing systems, there is an increasing demand for
more effective and efficient access control (AC) approaches. Recently,
Attribute Based Access Control (ABAC) approaches have been shown to be
promising in fulfilling the AC needs of such emerging complex computing
environments. An ABAC model grants access to a requester based on attributes of
entities in a system and an authorization policy; however, its generality and
flexibility come with a higher cost. Further, increa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.14780v1' target='_blank'>Procedural Content Generation: Better Benchmarks for Transfer
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Matthias M√ºller-Brockhausen, Mike Preuss, Aske Plaat</p>
<p><strong>Summary:</strong> The idea of transfer in reinforcement learning (TRL) is intriguing: being
able to transfer knowledge from one problem to another problem without learning
everything from scratch. This promises quicker learning and learning more
complex methods. To gain an insight into the field and to detect emerging
trends, we performed a database search. We note a surprisingly late adoption of
deep learning that starts in 2018. The introduction of deep learning has not
yet solved the greatest challenge of TRL:...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.01689v2' target='_blank'>Restless and Uncertain: Robust Policies for Restless Bandits via Deep
  Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jackson A. Killian, Lily Xu, Arpita Biswas, Milind Tambe</p>
<p><strong>Summary:</strong> We introduce robustness in \textit{restless multi-armed bandits} (RMABs), a
popular model for constrained resource allocation among independent stochastic
processes (arms). Nearly all RMAB techniques assume stochastic dynamics are
precisely known. However, in many real-world settings, dynamics are estimated
with significant \emph{uncertainty}, e.g., via historical data, which can lead
to bad outcomes if ignored. To address this, we develop an algorithm to compute
minimax regret -- robust policie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.04924v1' target='_blank'>Distributed Deep Reinforcement Learning for Intelligent Traffic
  Monitoring with a Team of Aerial Robots</a></h2>
<p><strong>Authors:</strong> Behzad Khamidehi, Elvino S. Sousa</p>
<p><strong>Summary:</strong> This paper studies the traffic monitoring problem in a road network using a
team of aerial robots. The problem is challenging due to two main reasons.
First, the traffic events are stochastic, both temporally and spatially.
Second, the problem has a non-homogeneous structure as the traffic events
arrive at different locations of the road network at different rates.
Accordingly, some locations require more visits by the robots compared to other
locations. To address these issues, we define an unc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.04462v3' target='_blank'>Deep Reinforcement Learning for Demand Driven Services in Logistics and
  Transportation Systems: A Survey</a></h2>
<p><strong>Authors:</strong> Zefang Zong, Jingwei Wang, Tao Feng, Tong Xia, Depeng Jin, Yong Li</p>
<p><strong>Summary:</strong> Recent technology development brings the boom of numerous new Demand-Driven
Services (DDS) into urban lives, including ridesharing, on-demand delivery,
express systems and warehousing. In DDS, a service loop is an elemental
structure, including its service worker, the service providers and
corresponding service targets. The service workers should transport either
people or parcels from the providers to the target locations. Various planning
tasks within DDS can thus be classified into two indivi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.06394v2' target='_blank'>Reward-Free Model-Based Reinforcement Learning with Linear Function
  Approximation</a></h2>
<p><strong>Authors:</strong> Weitong Zhang, Dongruo Zhou, Quanquan Gu</p>
<p><strong>Summary:</strong> We study the model-based reward-free reinforcement learning with linear
function approximation for episodic Markov decision processes (MDPs). In this
setting, the agent works in two phases. In the exploration phase, the agent
interacts with the environment and collects samples without the reward. In the
planning phase, the agent is given a specific reward function and uses samples
collected from the exploration phase to learn a good policy. We propose a new
provably efficient algorithm, called U...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.10083v4' target='_blank'>Contrastive Active Inference</a></h2>
<p><strong>Authors:</strong> Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt</p>
<p><strong>Summary:</strong> Active inference is a unifying theory for perception and action resting upon
the idea that the brain maintains an internal model of the world by minimizing
free energy. From a behavioral perspective, active inference agents can be seen
as self-evidencing beings that act to fulfill their optimistic predictions,
namely preferred outcomes or goals. In contrast, reinforcement learning
requires human-designed rewards to accomplish any desired outcome. Although
active inference could provide a more na...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.10441v1' target='_blank'>Feedback Linearization of Car Dynamics for Racing via Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Michael Estrada, Sida Li, Xiangyu Cai</p>
<p><strong>Summary:</strong> Through the method of Learning Feedback Linearization, we seek to learn a
linearizing controller to simplify the process of controlling a car to race
autonomously. A soft actor-critic approach is used to learn a decoupling matrix
and drift vector that effectively correct for errors in a hand-designed
linearizing controller. The result is an exactly linearizing controller that
can be used to enable the well-developed theory of linear systems to design
path planning and tracking schemes that are e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.12422v1' target='_blank'>A Differentiable Newton-Euler Algorithm for Real-World Robotics</a></h2>
<p><strong>Authors:</strong> Michael Lutter, Johannes Silberbauer, Joe Watson, Jan Peters</p>
<p><strong>Summary:</strong> Obtaining dynamics models is essential for robotics to achieve accurate
model-based controllers and simulators for planning. The dynamics models are
typically obtained using model specification of the manufacturer or simple
numerical methods such as linear regression. However, this approach does not
guarantee physically plausible parameters and can only be applied to kinematic
chains consisting of rigid bodies. In this article, we describe a
differentiable simulator that can be used to identify ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.15481v1' target='_blank'>Brick-by-Brick: Combinatorial Construction with Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Hyunsoo Chung, Jungtaek Kim, Boris Knyazev, Jinhwi Lee, Graham W. Taylor, Jaesik Park, Minsu Cho</p>
<p><strong>Summary:</strong> Discovering a solution in a combinatorial space is prevalent in many
real-world problems but it is also challenging due to diverse complex
constraints and the vast number of possible combinations. To address such a
problem, we introduce a novel formulation, combinatorial construction, which
requires a building agent to assemble unit primitives (i.e., LEGO bricks)
sequentially -- every connection between two bricks must follow a fixed rule,
while no bricks mutually overlap. To construct a target ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.00278v3' target='_blank'>A Decentralized Reinforcement Learning Framework for Efficient Passage
  of Emergency Vehicles</a></h2>
<p><strong>Authors:</strong> Haoran Su, Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty</p>
<p><strong>Summary:</strong> Emergency vehicles (EMVs) play a critical role in a city's response to
time-critical events such as medical emergencies and fire outbreaks. The
existing approaches to reduce EMV travel time employ route optimization and
traffic signal pre-emption without accounting for the coupling between route
these two subproblems. As a result, the planned route often becomes suboptimal.
In addition, these approaches also do not focus on minimizing disruption to the
overall traffic flow. To address these issu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.02258v1' target='_blank'>Multi-Agent Deep Reinforcement Learning For Optimising Energy Efficiency
  of Fixed-Wing UAV Cellular Access Points</a></h2>
<p><strong>Authors:</strong> Boris Galkin, Babatunji Omoniwa, Ivana Dusparic</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicles (UAVs) promise to become an intrinsic part of next
generation communications, as they can be deployed to provide wireless
connectivity to ground users to supplement existing terrestrial networks. The
majority of the existing research into the use of UAV access points for
cellular coverage considers rotary-wing UAV designs (i.e. quadcopters).
However, we expect fixed-wing UAVs to be more appropriate for connectivity
purposes in scenarios where long flight times are necess...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.05567v1' target='_blank'>VeSoNet: Traffic-Aware Content Caching for Vehicular Social Networks
  based on Path Planning and Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Nyothiri Aung, Sahraoui Dhelim, Liming Chen, Wenyin Zhang, Abderrahmane Lakas, Huansheng Ning</p>
<p><strong>Summary:</strong> Vehicular social networking is an emerging application of the promising
Internet of Vehicles (IoV) which aims to achieve the seamless integration of
vehicular networks and social networks. However, the unique characteristics of
vehicular networks such as high mobility and frequent communication
interruptions make content delivery to end-users under strict delay constrains
an extremely challenging task. In this paper, we propose a social-aware
vehicular edge computing architecture that solves the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.11188v3' target='_blank'>Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement
  Learning with Actor Rectification</a></h2>
<p><strong>Authors:</strong> Ling Pan, Longbo Huang, Tengyu Ma, Huazhe Xu</p>
<p><strong>Summary:</strong> Conservatism has led to significant progress in offline reinforcement
learning (RL) where an agent learns from pre-collected datasets. However, as
many real-world scenarios involve interaction among multiple agents, it is
important to resolve offline RL in the multi-agent setting. Given the recent
success of transferring online RL algorithms to the multi-agent setting, one
may expect that offline RL algorithms will also transfer to multi-agent
settings directly. Surprisingly, we empirically obse...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.00333v1' target='_blank'>Joint Cluster Head Selection and Trajectory Planning in UAV-Aided IoT
  Networks by Reinforcement Learning with Sequential Model</a></h2>
<p><strong>Authors:</strong> Botao Zhu, Ebrahim Bedeer, Ha H. Nguyen, Robert Barton, Jerome Henry</p>
<p><strong>Summary:</strong> Employing unmanned aerial vehicles (UAVs) has attracted growing interests and
emerged as the state-of-the-art technology for data collection in
Internet-of-Things (IoT) networks. In this paper, with the objective of
minimizing the total energy consumption of the UAV-IoT system, we formulate the
problem of jointly designing the UAV's trajectory and selecting cluster heads
in the IoT network as a constrained combinatorial optimization problem which is
classified as NP-hard and challenging to solve...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.07415v2' target='_blank'>Stochastic Planner-Actor-Critic for Unsupervised Deformable Image
  Registration</a></h2>
<p><strong>Authors:</strong> Ziwei Luo, Jing Hu, Xin Wang, Shu Hu, Bin Kong, Youbing Yin, Qi Song, Xi Wu, Siwei Lyu</p>
<p><strong>Summary:</strong> Large deformations of organs, caused by diverse shapes and nonlinear shape
changes, pose a significant challenge for medical image registration.
Traditional registration methods need to iteratively optimize an objective
function via a specific deformation model along with meticulous parameter
tuning, but which have limited capabilities in registering images with large
deformations. While deep learning-based methods can learn the complex mapping
from input images to their respective deformation f...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.04651v1' target='_blank'>Multi-echelon Supply Chains with Uncertain Seasonal Demands and Lead
  Times Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Julio C√©sar Alves, Geraldo Robson Mateus</p>
<p><strong>Summary:</strong> We address the problem of production planning and distribution in
multi-echelon supply chains. We consider uncertain demands and lead times which
makes the problem stochastic and non-linear. A Markov Decision Process
formulation and a Non-linear Programming model are presented. As a sequential
decision-making problem, Deep Reinforcement Learning (RL) is a possible
solution approach. This type of technique has gained a lot of attention from
Artificial Intelligence and Optimization communities in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.11964v1' target='_blank'>Dynamic Temporal Reconciliation by Reinforcement learning</a></h2>
<p><strong>Authors:</strong> Himanshi Charotia, Abhishek Garg, Gaurav Dhama, Naman Maheshwari</p>
<p><strong>Summary:</strong> Planning based on long and short term time series forecasts is a common
practice across many industries. In this context, temporal aggregation and
reconciliation techniques have been useful in improving forecasts, reducing
model uncertainty, and providing a coherent forecast across different time
horizons. However, an underlying assumption spanning all these techniques is
the complete availability of data across all levels of the temporal hierarchy,
while this offers mathematical convenience but...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.12231v5' target='_blank'>Overcoming Exploration: Deep Reinforcement Learning for Continuous
  Control in Cluttered Environments from Temporal Logic Specifications</a></h2>
<p><strong>Authors:</strong> Mingyu Cai, Erfan Aasi, Calin Belta, Cristian-Ioan Vasile</p>
<p><strong>Summary:</strong> Model-free continuous control for robot navigation tasks using Deep
Reinforcement Learning (DRL) that relies on noisy policies for exploration is
sensitive to the density of rewards. In practice, robots are usually deployed
in cluttered environments, containing many obstacles and narrow passageways.
Designing dense effective rewards is challenging, resulting in exploration
issues during training. Such a problem becomes even more serious when tasks are
described using temporal logic specification...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.02404v2' target='_blank'>Model-Free Reinforcement Learning for Symbolic Automata-encoded
  Objectives</a></h2>
<p><strong>Authors:</strong> Anand Balakrishnan, Stefan Jak≈°iƒá, Edgar A. Aguilar, Dejan Niƒçkoviƒá, Jyotirmoy V. Deshmukh</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is a popular approach for robotic path planning
in uncertain environments. However, the control policies trained for an RL
agent crucially depend on user-defined, state-based reward functions. Poorly
designed rewards can lead to policies that do get maximal rewards but fail to
satisfy desired task objectives or are unsafe. There are several examples of
the use of formal languages such as temporal logics and automata to specify
high-level task specifications for robots...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.12028v1' target='_blank'>Evolutionary Multi-Objective Reinforcement Learning Based Trajectory
  Control and Task Offloading in UAV-Assisted Mobile Edge Computing</a></h2>
<p><strong>Authors:</strong> Fuhong Song, Huanlai Xing, Xinhan Wang, Shouxi Luo, Penglin Dai, Zhiwen Xiao, Bowen Zhao</p>
<p><strong>Summary:</strong> This paper studies the trajectory control and task offloading (TCTO) problem
in an unmanned aerial vehicle (UAV)-assisted mobile edge computing system,
where a UAV flies along a planned trajectory to collect computation tasks from
smart devices (SDs). We consider a scenario that SDs are not directly connected
by the base station (BS) and the UAV has two roles to play: MEC server or
wireless relay. The UAV makes task offloading decisions online, in which the
collected tasks can be executed locall...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.12597v1' target='_blank'>Context-Hierarchy Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Wei Gao, David Hsu, Wee Sun Lee</p>
<p><strong>Summary:</strong> An inverse reinforcement learning (IRL) agent learns to act intelligently by
observing expert demonstrations and learning the expert's underlying reward
function. Although learning the reward functions from demonstrations has
achieved great success in various tasks, several other challenges are mostly
ignored. Firstly, existing IRL methods try to learn the reward function from
scratch without relying on any prior knowledge. Secondly, traditional IRL
methods assume the reward functions are homoge...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.13436v1' target='_blank'>Neural-Progressive Hedging: Enforcing Constraints in Reinforcement
  Learning with Stochastic Programming</a></h2>
<p><strong>Authors:</strong> Supriyo Ghosh, Laura Wynter, Shiau Hong Lim, Duc Thien Nguyen</p>
<p><strong>Summary:</strong> We propose a framework, called neural-progressive hedging (NP), that
leverages stochastic programming during the online phase of executing a
reinforcement learning (RL) policy. The goal is to ensure feasibility with
respect to constraints and risk-based objectives such as conditional
value-at-risk (CVaR) during the execution of the policy, using probabilistic
models of the state transitions to guide policy adjustments. The framework is
particularly amenable to the class of sequential resource al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.04120v2' target='_blank'>Graph-based Reinforcement Learning meets Mixed Integer Programs: An
  application to 3D robot assembly discovery</a></h2>
<p><strong>Authors:</strong> Niklas Funk, Svenja Menzenbach, Georgia Chalvatzaki, Jan Peters</p>
<p><strong>Summary:</strong> Robot assembly discovery is a challenging problem that lives at the
intersection of resource allocation and motion planning. The goal is to combine
a predefined set of objects to form something new while considering task
execution with the robot-in-the-loop. In this work, we tackle the problem of
building arbitrary, predefined target structures entirely from scratch using a
set of Tetris-like building blocks and a robotic manipulator. Our novel
hierarchical approach aims at efficiently decomposi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.05973v1' target='_blank'>Imitation and Adaptation Based on Consistency: A Quadruped Robot
  Imitates Animals from Videos Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Qingfeng Yao, Jilong Wang, Shuyu Yang, Cong Wang, Hongyin Zhang, Qifeng Zhang, Donglin Wang</p>
<p><strong>Summary:</strong> The essence of quadrupeds' movements is the movement of the center of
gravity, which has a pattern in the action of quadrupeds. However, the gait
motion planning of the quadruped robot is time-consuming. Animals in nature can
provide a large amount of gait information for robots to learn and imitate.
Common methods learn animal posture with a motion capture system or numerous
motion data points. In this paper, we propose a video imitation adaptation
network (VIAN) that can imitate the action of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.13733v2' target='_blank'>Blocks Assemble! Learning to Assemble with Large-Scale Structured
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Seyed Kamyar Seyed Ghasemipour, Daniel Freeman, Byron David, Shixiang Shane Gu, Satoshi Kataoka, Igor Mordatch</p>
<p><strong>Summary:</strong> Assembly of multi-part physical structures is both a valuable end product for
autonomous robotics, as well as a valuable diagnostic task for open-ended
training of embodied intelligent agents. We introduce a naturalistic
physics-based environment with a set of connectable magnet blocks inspired by
children's toy kits. The objective is to assemble blocks into a succession of
target blueprints. Despite the simplicity of this objective, the compositional
nature of building diverse blueprints from a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.17275v1' target='_blank'>DiffSkill: Skill Abstraction from Differentiable Physics for Deformable
  Object Manipulations with Tools</a></h2>
<p><strong>Authors:</strong> Xingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B. Tenenbaum, David Held, Chuang Gan</p>
<p><strong>Summary:</strong> We consider the problem of sequential robotic manipulation of deformable
objects using tools. Previous works have shown that differentiable physics
simulators provide gradients to the environment state and help trajectory
optimization to converge orders of magnitude faster than model-free
reinforcement learning algorithms for deformable object manipulation. However,
such gradient-based trajectory optimization typically requires access to the
full simulator states and can only solve short-horizon...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.08594v1' target='_blank'>Multi-UAV Collision Avoidance using Multi-Agent Reinforcement Learning
  with Counterfactual Credit Assignment</a></h2>
<p><strong>Authors:</strong> Shuangyao Huang, Haibo Zhang, Zhiyi Huang</p>
<p><strong>Summary:</strong> Multi-UAV collision avoidance is a challenging task for UAV swarm
applications due to the need of tight cooperation among swarm members for
collision-free path planning. Centralized Training with Decentralized Execution
(CTDE) in Multi-Agent Reinforcement Learning is a promising method for
multi-UAV collision avoidance, in which the key challenge is to effectively
learn decentralized policies that can maximize a global reward cooperatively.
We propose a new multi-agent critic-actor learning sche...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.14133v1' target='_blank'>Network Topology Optimization via Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zhuoran Li, Xing Wang, Ling Pan, Lin Zhu, Zhendong Wang, Junlan Feng, Chao Deng, Longbo Huang</p>
<p><strong>Summary:</strong> Topology impacts important network performance metrics, including link
utilization, throughput and latency, and is of central importance to network
operators. However, due to the combinatorial nature of network topology, it is
extremely difficult to obtain an optimal solution, especially since topology
planning in networks also often comes with management-specific constraints. As
a result, local optimization with hand-tuned heuristic methods from human
experts are often adopted in practice. Yet,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.08827v1' target='_blank'>World Value Functions: Knowledge Representation for Multitask
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Geraud Nangue Tasse, Steven James, Benjamin Rosman</p>
<p><strong>Summary:</strong> An open problem in artificial intelligence is how to learn and represent
knowledge that is sufficient for a general agent that needs to solve multiple
tasks in a given world. In this work we propose world value functions (WVFs),
which are a type of general value function with mastery of the world - they
represent not only how to solve a given task, but also how to solve any other
goal-reaching task. To achieve this, we equip the agent with an internal goal
space defined as all the world states w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.06011v1' target='_blank'>Reinforcement Learning-based Placement of Charging Stations in Urban
  Road Networks</a></h2>
<p><strong>Authors:</strong> Leonie von Wahl, Nicolas Tempelmeier, Ashutosh Sao, Elena Demidova</p>
<p><strong>Summary:</strong> The transition from conventional mobility to electromobility largely depends
on charging infrastructure availability and optimal placement.This paper
examines the optimal placement of charging stations in urban areas. We maximise
the charging infrastructure supply over the area and minimise waiting, travel,
and charging times while setting budget constraints. Moreover, we include the
possibility of charging vehicles at home to obtain a more refined estimation of
the actual charging demand throug...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.09743v2' target='_blank'>Guided Safe Shooting: model based reinforcement learning with safety
  constraints</a></h2>
<p><strong>Authors:</strong> Giuseppe Paolo, Jonas Gonzalez-Billandon, Albert Thomas, Bal√°zs K√©gl</p>
<p><strong>Summary:</strong> In the last decade, reinforcement learning successfully solved complex
control tasks and decision-making problems, like the Go board game. Yet, there
are few success stories when it comes to deploying those algorithms to
real-world scenarios. One of the reasons is the lack of guarantees when dealing
with and avoiding unsafe states, a fundamental requirement in critical control
engineering systems. In this paper, we introduce Guided Safe Shooting (GuSS), a
model-based RL approach that can learn t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.10598v2' target='_blank'>A deep inverse reinforcement learning approach to route choice modeling
  with context-dependent rewards</a></h2>
<p><strong>Authors:</strong> Zhan Zhao, Yuebing Liang</p>
<p><strong>Summary:</strong> Route choice modeling is a fundamental task in transportation planning and
demand forecasting. Classical methods generally adopt the discrete choice model
(DCM) framework with linear utility functions and high-level route
characteristics. While several recent studies have started to explore the
applicability of deep learning for route choice modeling, they are limited to
path-based models with relatively simple model architectures and relying on
predefined choice sets. Existing link-based models...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.12542v2' target='_blank'>Value-Consistent Representation Learning for Data-Efficient
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yang Yue, Bingyi Kang, Zhongwen Xu, Gao Huang, Shuicheng Yan</p>
<p><strong>Summary:</strong> Deep reinforcement learning (RL) algorithms suffer severe performance
degradation when the interaction data is scarce, which limits their real-world
application. Recently, visual representation learning has been shown to be
effective and promising for boosting sample efficiency in RL. These methods
usually rely on contrastive learning and data augmentation to train a
transition model for state prediction, which is different from how the model is
used in RL--performing value-based planning. Accor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.08056v1' target='_blank'>Federated Deep Reinforcement Learning for RIS-Assisted Indoor
  Multi-Robot Communication Systems</a></h2>
<p><strong>Authors:</strong> Ruyu Luo, Wanli Ni, Hui Tian, Julian Cheng</p>
<p><strong>Summary:</strong> Indoor multi-robot communications face two key challenges: one is the severe
signal strength degradation caused by blockages (e.g., walls) and the other is
the dynamic environment caused by robot mobility. To address these issues, we
consider the reconfigurable intelligent surface (RIS) to overcome the signal
blockage and assist the trajectory design among multiple robots. Meanwhile, the
non-orthogonal multiple access (NOMA) is adopted to cope with the scarcity of
spectrum and enhance the connec...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.12584v2' target='_blank'>Socially Fair Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Debmalya Mandal, Jiarui Gan</p>
<p><strong>Summary:</strong> We consider the problem of episodic reinforcement learning where there are
multiple stakeholders with different reward functions. Our goal is to output a
policy that is socially fair with respect to different reward functions. Prior
works have proposed different objectives that a fair policy must optimize
including minimum welfare, and generalized Gini welfare. We first take an
axiomatic view of the problem, and propose four axioms that any such fair
objective must satisfy. We show that the Nash...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.00853v4' target='_blank'>TarGF: Learning Target Gradient Field to Rearrange Objects without
  Explicit Goal Specification</a></h2>
<p><strong>Authors:</strong> Mingdong Wu, Fangwei Zhong, Yulong Xia, Hao Dong</p>
<p><strong>Summary:</strong> Object Rearrangement is to move objects from an initial state to a goal
state. Here, we focus on a more practical setting in object rearrangement,
i.e., rearranging objects from shuffled layouts to a normative target
distribution without explicit goal specification. However, it remains
challenging for AI agents, as it is hard to describe the target distribution
(goal specification) for reward engineering or collect expert trajectories as
demonstrations. Hence, it is infeasible to directly employ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.14548v2' target='_blank'>Offline Reinforcement Learning via High-Fidelity Generative Behavior
  Modeling</a></h2>
<p><strong>Authors:</strong> Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, Jun Zhu</p>
<p><strong>Summary:</strong> In offline reinforcement learning, weighted regression is a common method to
ensure the learned policy stays close to the behavior policy and to prevent
selecting out-of-sample actions. In this work, we show that due to the limited
distributional expressivity of policy models, previous methods might still
select unseen actions during training, which deviates from their initial
motivation. To address this problem, we adopt a generative approach by
decoupling the learned policy into two parts: an ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.14781v1' target='_blank'>Learning Parsimonious Dynamics for Generalization in Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Tankred Saanum, Eric Schulz</p>
<p><strong>Summary:</strong> Humans are skillful navigators: We aptly maneuver through new places, realize
when we are back at a location we have seen before, and can even conceive of
shortcuts that go through parts of our environments we have never visited.
Current methods in model-based reinforcement learning on the other hand
struggle with generalizing about environment dynamics out of the training
distribution. We argue that two principles can help bridge this gap: latent
learning and parsimonious dynamics. Humans tend ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.11604v3' target='_blank'>Horizon-Free and Variance-Dependent Reinforcement Learning for Latent
  Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Runlong Zhou, Ruosong Wang, Simon S. Du</p>
<p><strong>Summary:</strong> We study regret minimization for reinforcement learning (RL) in Latent Markov
Decision Processes (LMDPs) with context in hindsight. We design a novel
model-based algorithmic framework which can be instantiated with both a
model-optimistic and a value-optimistic solver. We prove an
$\tilde{O}(\sqrt{\mathsf{Var}^\star M \Gamma S A K})$ regret bound where
$\tilde{O}$ hides logarithm factors, $M$ is the number of contexts, $S$ is the
number of states, $A$ is the number of actions, $K$ is the number ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.13383v1' target='_blank'>Evaluating Long-Term Memory in 3D Mazes</a></h2>
<p><strong>Authors:</strong> Jurgis Pasukonis, Timothy Lillicrap, Danijar Hafner</p>
<p><strong>Summary:</strong> Intelligent agents need to remember salient information to reason in
partially-observed environments. For example, agents with a first-person view
should remember the positions of relevant objects even if they go out of view.
Similarly, to effectively navigate through rooms agents need to remember the
floor plan of how rooms are connected. However, most benchmark tasks in
reinforcement learning do not test long-term memory in agents, slowing down
progress in this important research direction. In...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.08998v1' target='_blank'>Data-pooling Reinforcement Learning for Personalized Healthcare
  Intervention</a></h2>
<p><strong>Authors:</strong> Xinyun Chen, Pengyi Shi, Shanwen Pu</p>
<p><strong>Summary:</strong> Motivated by the emerging needs of personalized preventative intervention in
many healthcare applications, we consider a multi-stage, dynamic
decision-making problem in the online setting with unknown model parameters. To
deal with the pervasive issue of small sample size in personalized planning, we
develop a novel data-pooling reinforcement learning (RL) algorithm based on a
general perturbed value iteration framework. Our algorithm adaptively pools
historical data, with three main innovations...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.10585v1' target='_blank'>Prediction-aware and Reinforcement Learning based Altruistic Cooperative
  Driving</a></h2>
<p><strong>Authors:</strong> Rodolfo Valiente, Mahdi Razzaghpour, Behrad Toghi, Ghayoor Shah, Yaser P. Fallah</p>
<p><strong>Summary:</strong> Autonomous vehicle (AV) navigation in the presence of Human-driven vehicles
(HVs) is challenging, as HVs continuously update their policies in response to
AVs. In order to navigate safely in the presence of complex AV-HV social
interactions, the AVs must learn to predict these changes. Humans are capable
of navigating such challenging social interaction settings because of their
intrinsic knowledge about other agents behaviors and use that to forecast what
might happen in the future. Inspired by...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.15589v3' target='_blank'>Inapplicable Actions Learning for Knowledge Transfer in Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Leo Ardon, Alberto Pozanco, Daniel Borrajo, Sumitra Ganesh</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) algorithms are known to scale poorly to
environments with many available actions, requiring numerous samples to learn
an optimal policy. The traditional approach of considering the same fixed
action space in every possible state implies that the agent must understand,
while also learning to maximize its reward, to ignore irrelevant actions such
as $\textit{inapplicable actions}$ (i.e. actions that have no effect on the
environment when performed in a given state). Kno...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.06001v1' target='_blank'>Reinforcement Learning and Tree Search Methods for the Unit Commitment
  Problem</a></h2>
<p><strong>Authors:</strong> Patrick de Mars</p>
<p><strong>Summary:</strong> The unit commitment (UC) problem, which determines operating schedules of
generation units to meet demand, is a fundamental task in power systems
operation. Existing UC methods using mixed-integer programming are not
well-suited to highly stochastic systems. Approaches which more rigorously
account for uncertainty could yield large reductions in operating costs by
reducing spinning reserve requirements; operating power stations at higher
efficiencies; and integrating greater volumes of variable ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.12767v1' target='_blank'>Streaming Traffic Flow Prediction Based on Continuous Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Yanan Xiao, Minyu Liu, Zichen Zhang, Lu Jiang, Minghao Yin, Jianan Wang</p>
<p><strong>Summary:</strong> Traffic flow prediction is an important part of smart transportation. The
goal is to predict future traffic conditions based on historical data recorded
by sensors and the traffic network. As the city continues to build, parts of
the transportation network will be added or modified. How to accurately predict
expanding and evolving long-term streaming networks is of great significance.
To this end, we propose a new simulation-based criterion that considers
teaching autonomous agents to mimic sens...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.14849v1' target='_blank'>Symbolic Visual Reinforcement Learning: A Scalable Framework with
  Object-Level Abstraction and Differentiable Expression Search</a></h2>
<p><strong>Authors:</strong> Wenqing Zheng, S P Sharan, Zhiwen Fan, Kevin Wang, Yihan Xi, Zhangyang Wang</p>
<p><strong>Summary:</strong> Learning efficient and interpretable policies has been a challenging task in
reinforcement learning (RL), particularly in the visual RL setting with complex
scenes. While neural networks have achieved competitive performance, the
resulting policies are often over-parameterized black boxes that are difficult
to interpret and deploy efficiently. More recent symbolic RL frameworks have
shown that high-level domain-specific programming logic can be designed to
handle both policy learning and symboli...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.04195v2' target='_blank'>Orbit: A Unified Simulation Framework for Interactive Robot Learning
  Environments</a></h2>
<p><strong>Authors:</strong> Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, Animesh Garg</p>
<p><strong>Summary:</strong> We present Orbit, a unified and modular framework for robot learning powered
by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently
create robotic environments with photo-realistic scenes and high-fidelity rigid
and deformable body simulation. With Orbit, we provide a suite of benchmark
tasks of varying difficulty -- from single-stage cabinet opening and cloth
folding to multi-stage tasks such as room reorganization. To support working
with diverse observations and action spac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.06863v1' target='_blank'>A reinforcement learning path planning approach for range-only
  underwater target localization with autonomous vehicles</a></h2>
<p><strong>Authors:</strong> Ivan Masmitja, Mario Martin, Kakani Katija, Spartacus Gomariz, Joan Navarro</p>
<p><strong>Summary:</strong> Underwater target localization using range-only and single-beacon (ROSB)
techniques with autonomous vehicles has been used recently to improve the
limitations of more complex methods, such as long baseline and ultra-short
baseline systems. Nonetheless, in ROSB target localization methods, the
trajectory of the tracking vehicle near the localized target plays an important
role in obtaining the best accuracy of the predicted target position. Here, we
investigate a Reinforcement Learning (RL) appro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.08502v1' target='_blank'>Plan To Predict: Learning an Uncertainty-Foreseeing Model for
  Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zifan Wu, Chao Yu, Chen Chen, Jianye Hao, Hankz Hankui Zhuo</p>
<p><strong>Summary:</strong> In Model-based Reinforcement Learning (MBRL), model learning is critical
since an inaccurate model can bias policy learning via generating misleading
samples. However, learning an accurate model can be difficult since the policy
is continually updated and the induced distribution over visited states used
for model learning shifts accordingly. Prior methods alleviate this issue by
quantifying the uncertainty of model-generated samples. However, these methods
only quantify the uncertainty passivel...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.02506v2' target='_blank'>Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking
  Job Shop Problem Using Graph Neural Network and Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vivian W. H. Wong, Sang Hun Kim, Junyoung Park, Jinkyoo Park, Kincho H. Law</p>
<p><strong>Summary:</strong> The interrupting swap-allowed blocking job shop problem (ISBJSSP) is a
complex scheduling problem that is able to model many manufacturing planning
and logistics applications realistically by addressing both the lack of storage
capacity and unforeseen production interruptions. Subjected to random
disruptions due to machine malfunction or maintenance, industry production
settings often choose to adopt dispatching rules to enable adaptive, real-time
re-scheduling, rather than traditional methods t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.03811v2' target='_blank'>Environment Transformer and Policy Optimization for Model-Based Offline
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Pengqin Wang, Meixin Zhu, Shaojie Shen</p>
<p><strong>Summary:</strong> Interacting with the actual environment to acquire data is often costly and
time-consuming in robotic tasks. Model-based offline reinforcement learning
(RL) provides a feasible solution. On the one hand, it eliminates the
requirements of interaction with the actual environment. On the other hand, it
learns the transition dynamics and reward function from the offline datasets
and generates simulated rollouts to accelerate training. Previous model-based
offline RL methods adopt probabilistic ensem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.10859v1' target='_blank'>Improved Sample Complexity for Reward-free Reinforcement Learning under
  Low-rank MDPs</a></h2>
<p><strong>Authors:</strong> Yuan Cheng, Ruiquan Huang, Jing Yang, Yingbin Liang</p>
<p><strong>Summary:</strong> In reward-free reinforcement learning (RL), an agent explores the environment
first without any reward information, in order to achieve certain learning
goals afterwards for any given reward. In this paper we focus on reward-free RL
under low-rank MDP models, in which both the representation and linear weight
vectors are unknown. Although various algorithms have been proposed for
reward-free low-rank MDPs, the corresponding sample complexity is still far
from being satisfactory. In this work, we...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.12289v1' target='_blank'>Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian
  Interactions using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Qiming Ye, Yuxiang Feng, Jose Javier Escribano Macias, Marc Stettler, Panagiotis Angeloudis</p>
<p><strong>Summary:</strong> The deployment of Autonomous Vehicles (AVs) poses considerable challenges and
unique opportunities for the design and management of future urban road
infrastructure. In light of this disruptive transformation, the Right-Of-Way
(ROW) composition of road space has the potential to be renewed. Design
approaches and intelligent control models have been proposed to address this
problem, but we lack an operational framework that can dynamically generate ROW
plans for AVs and pedestrians in response to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.12354v1' target='_blank'>Deep Reinforcement Learning for Localizability-Enhanced Navigation in
  Dynamic Human Environments</a></h2>
<p><strong>Authors:</strong> Yuan Chen, Quecheng Qiu, Xiangyu Liu, Guangda Chen, Shunyi Yao, Jie Peng, Jianmin Ji, Yanyong Zhang</p>
<p><strong>Summary:</strong> Reliable localization is crucial for autonomous robots to navigate
efficiently and safely. Some navigation methods can plan paths with high
localizability (which describes the capability of acquiring reliable
localization). By following these paths, the robot can access the sensor
streams that facilitate more accurate location estimation results by the
localization algorithms. However, most of these methods require prior knowledge
and struggle to adapt to unseen scenarios or dynamic changes. To ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.03998v1' target='_blank'>Evolving Reinforcement Learning Environment to Minimize Learner's
  Achievable Reward: An Application on Hardening Active Directory Systems</a></h2>
<p><strong>Authors:</strong> Diksha Goel, Aneta Neumann, Frank Neumann, Hung Nguyen, Mingyu Guo</p>
<p><strong>Summary:</strong> We study a Stackelberg game between one attacker and one defender in a
configurable environment. The defender picks a specific environment
configuration. The attacker observes the configuration and attacks via
Reinforcement Learning (RL trained against the observed environment). The
defender's goal is to find the environment with minimum achievable reward for
the attacker. We apply Evolutionary Diversity Optimization (EDO) to generate
diverse population of environments for training. Environments...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.12046v3' target='_blank'>When to Replan? An Adaptive Replanning Strategy for Autonomous
  Navigation using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kohei Honda, Ryo Yonetani, Mai Nishimura, Tadashi Kozuno</p>
<p><strong>Summary:</strong> The hierarchy of global and local planners is one of the most commonly
utilized system designs in autonomous robot navigation. While the global
planner generates a reference path from the current to goal locations based on
the pre-built map, the local planner produces a kinodynamic trajectory to
follow the reference path while avoiding perceived obstacles. To account for
unforeseen or dynamic obstacles not present on the pre-built map, ``when to
replan'' the reference path is critical for the su...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.13090v1' target='_blank'>Model Extraction Attacks Against Reinforcement Learning Based
  Controllers</a></h2>
<p><strong>Authors:</strong> Momina Sajid, Yanning Shen, Yasser Shoukry</p>
<p><strong>Summary:</strong> We introduce the problem of model-extraction attacks in cyber-physical
systems in which an attacker attempts to estimate (or extract) the feedback
controller of the system. Extracting (or estimating) the controller provides an
unmatched edge to attackers since it allows them to predict the future control
actions of the system and plan their attack accordingly. Hence, it is important
to understand the ability of the attackers to perform such an attack. In this
paper, we focus on the setting when ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.04180v3' target='_blank'>Train a Real-world Local Path Planner in One Hour via Partially
  Decoupled Reinforcement Learning and Vectorized Diversity</a></h2>
<p><strong>Authors:</strong> Jinghao Xin, Jinwoo Kim, Zhi Li, Ning Li</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning (DRL) has exhibited efficacy in resolving the
Local Path Planning (LPP) problem. However, such application in the real world
is immensely limited due to the deficient training efficiency and
generalization capability of DRL. To alleviate these two issues, a solution
named Color is proposed, which consists of an Actor-Sharer-Learner (ASL)
training framework and a mobile robot-oriented simulator Sparrow. Specifically,
the ASL intends to improve the training efficiency o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.08359v1' target='_blank'>Horizon-free Reinforcement Learning in Adversarial Linear Mixture MDPs</a></h2>
<p><strong>Authors:</strong> Kaixuan Ji, Qingyue Zhao, Jiafan He, Weitong Zhang, Quanquan Gu</p>
<p><strong>Summary:</strong> Recent studies have shown that episodic reinforcement learning (RL) is no
harder than bandits when the total reward is bounded by $1$, and proved regret
bounds that have a polylogarithmic dependence on the planning horizon $H$.
However, it remains an open question that if such results can be carried over
to adversarial RL, where the reward is adversarially chosen at each episode. In
this paper, we answer this question affirmatively by proposing the first
horizon-free policy search algorithm. To ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.08844v2' target='_blank'>RL4F: Generating Natural Language Feedback with Reinforcement Learning
  for Repairing Model Outputs</a></h2>
<p><strong>Authors:</strong> Afra Feyza Aky√ºrek, Ekin Aky√ºrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, Niket Tandon</p>
<p><strong>Summary:</strong> Despite their unprecedented success, even the largest language models make
mistakes. Similar to how humans learn and improve using feedback, previous work
proposed providing language models with natural language feedback to guide them
in repairing their outputs. Because human-generated critiques are expensive to
obtain, researchers have devised learned critique generators in lieu of human
critics while assuming one can train downstream models to utilize generated
feedback. However, this approach...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.10192v1' target='_blank'>Curriculum Learning in Job Shop Scheduling using Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Constantin Waubert de Puiseau, Hasan Tercan, Tobias Meisen</p>
<p><strong>Summary:</strong> Solving job shop scheduling problems (JSSPs) with a fixed strategy, such as a
priority dispatching rule, may yield satisfactory results for several problem
instances but, nevertheless, insufficient results for others. From this
single-strategy perspective finding a near optimal solution to a specific JSSP
varies in difficulty even if the machine setup remains the same. A recent
intensively researched and promising method to deal with difficulty variability
is Deep Reinforcement Learning (DRL), w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.10865v2' target='_blank'>Semantically Aligned Task Decomposition in Multi-Agent Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, Hongyuan Zha</p>
<p><strong>Summary:</strong> The difficulty of appropriately assigning credit is particularly heightened
in cooperative MARL with sparse reward, due to the concurrent time and
structural scales involved. Automatic subgoal generation (ASG) has recently
emerged as a viable MARL approach inspired by utilizing subgoals in
intrinsically motivated reinforcement learning. However, end-to-end learning of
complex task planning from sparse rewards without prior knowledge, undoubtedly
requires massive training samples. Moreover, the d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.11206v1' target='_blank'>LIMA: Less Is More for Alignment</a></h2>
<p><strong>Authors:</strong> Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy</p>
<p><strong>Summary:</strong> Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.14816v2' target='_blank'>Provable Offline Preference-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D. Lee, Wen Sun</p>
<p><strong>Summary:</strong> In this paper, we investigate the problem of offline Preference-based
Reinforcement Learning (PbRL) with human feedback where feedback is available
in the form of preference between trajectory pairs rather than explicit
rewards. Our proposed algorithm consists of two main steps: (1) estimate the
implicit reward using Maximum Likelihood Estimation (MLE) with general function
approximation from offline data and (2) solve a distributionally robust
planning problem over a confidence set around the M...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.18246v2' target='_blank'>Provable and Practical: Efficient Exploration in Reinforcement Learning
  via Langevin Monte Carlo</a></h2>
<p><strong>Authors:</strong> Haque Ishfaq, Qingfeng Lan, Pan Xu, A. Rupam Mahmood, Doina Precup, Anima Anandkumar, Kamyar Azizzadenesheli</p>
<p><strong>Summary:</strong> We present a scalable and effective exploration strategy based on Thompson
sampling for reinforcement learning (RL). One of the key shortcomings of
existing Thompson sampling algorithms is the need to perform a Gaussian
approximation of the posterior distribution, which is not a good surrogate in
most practical settings. We instead directly sample the Q function from its
posterior distribution, by using Langevin Monte Carlo, an efficient type of
Markov Chain Monte Carlo (MCMC) method. Our method...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.00603v2' target='_blank'>Safe Offline Reinforcement Learning with Real-Time Budget Constraints</a></h2>
<p><strong>Authors:</strong> Qian Lin, Bo Tang, Zifan Wu, Chao Yu, Shangqin Mao, Qianlong Xie, Xingxing Wang, Dong Wang</p>
<p><strong>Summary:</strong> Aiming at promoting the safe real-world deployment of Reinforcement Learning
(RL), research on safe RL has made significant progress in recent years.
However, most existing works in the literature still focus on the online
setting where risky violations of the safety budget are likely to be incurred
during training. Besides, in many real-world applications, the learned policy
is required to respond to dynamically determined safety budgets (i.e.,
constraint threshold) in real time. In this paper,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.01243v2' target='_blank'>Efficient Reinforcement Learning with Impaired Observability: Learning
  to Act with Delayed and Missing State Observations</a></h2>
<p><strong>Authors:</strong> Minshuo Chen, Jie Meng, Yu Bai, Yinyu Ye, H. Vincent Poor, Mengdi Wang</p>
<p><strong>Summary:</strong> In real-world reinforcement learning (RL) systems, various forms of {\it
impaired observability} can complicate matters. These situations arise when an
agent is unable to observe the most recent state of the system due to latency
or lossy channels, yet the agent must still make real-time decisions. This
paper introduces a theoretical investigation into efficient RL in control
systems where agents must act with delayed and missing state observations. We
present algorithms and establish near-optim...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.11171v3' target='_blank'>Sim-to-real transfer of active suspension control using deep
  reinforcement learning</a></h2>
<p><strong>Authors:</strong> Viktor Wiberg, Erik Wallin, Arvid F√§lldin, Tobias Semberg, Morgan Rossander, Eddie Wadbro, Martin Servin</p>
<p><strong>Summary:</strong> We explore sim-to-real transfer of deep reinforcement learning controllers
for a heavy vehicle with active suspensions designed for traversing rough
terrain. While related research primarily focuses on lightweight robots with
electric motors and fast actuation, this study uses a forestry vehicle with a
complex hydraulic driveline and slow actuation. We simulate the vehicle using
multibody dynamics and apply system identification to find an appropriate set
of simulation parameters. We then train ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.08082v1' target='_blank'>POMDP inference and robust solution via deep reinforcement learning: An
  application to railway optimal maintenance</a></h2>
<p><strong>Authors:</strong> Giacomo Arcieri, Cyprien Hoelzl, Oliver Schwery, Daniel Straub, Konstantinos G. Papakonstantinou, Eleni Chatzi</p>
<p><strong>Summary:</strong> Partially Observable Markov Decision Processes (POMDPs) can model complex
sequential decision-making problems under stochastic and uncertain
environments. A main reason hindering their broad adoption in real-world
applications is the lack of availability of a suitable POMDP model or a
simulator thereof. Available solution algorithms, such as Reinforcement
Learning (RL), require the knowledge of the transition dynamics and the
observation generating process, which are often unknown and non-trivia...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.11432v1' target='_blank'>An Analysis of Multi-Agent Reinforcement Learning for Decentralized
  Inventory Control Systems</a></h2>
<p><strong>Authors:</strong> Marwan Mousa, Damien van de Berg, Niki Kotecha, Ehecatl Antonio del Rio-Chanona, Max Mowbray</p>
<p><strong>Summary:</strong> Most solutions to the inventory management problem assume a centralization of
information that is incompatible with organisational constraints in real supply
chain networks. The inventory management problem is a well-known planning
problem in operations research, concerned with finding the optimal re-order
policy for nodes in a supply chain. While many centralized solutions to the
problem exist, they are not applicable to real-world supply chains made up of
independent entities. The problem can ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.06594v1' target='_blank'>CoverNav: Cover Following Navigation Planning in Unstructured Outdoor
  Environment with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Anjan Basak, Derrik E. Asher</p>
<p><strong>Summary:</strong> Autonomous navigation in offroad environments has been extensively studied in
the robotics field. However, navigation in covert situations where an
autonomous vehicle needs to remain hidden from outside observers remains an
underexplored area. In this paper, we propose a novel Deep Reinforcement
Learning (DRL) based algorithm, called CoverNav, for identifying covert and
navigable trajectories with minimal cost in offroad terrains and jungle
environments in the presence of observers. CoverNav foc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.14947v2' target='_blank'>Improving Generalization in Reinforcement Learning Training Regimes for
  Social Robot Navigation</a></h2>
<p><strong>Authors:</strong> Adam Sigal, Hsiu-Chin Lin, AJung Moon</p>
<p><strong>Summary:</strong> In order for autonomous mobile robots to navigate in human spaces, they must
abide by our social norms. Reinforcement learning (RL) has emerged as an
effective method to train sequential decision-making policies that are able to
respect these norms. However, a large portion of existing work in the field
conducts both RL training and testing in simplistic environments. This limits
the generalization potential of these models to unseen environments, and the
meaningfulness of their reported results...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.06420v1' target='_blank'>Verifiable Reinforcement Learning Systems via Compositionality</a></h2>
<p><strong>Authors:</strong> Cyrus Neary, Aryaman Singh Samyal, Christos Verginis, Murat Cubuktepe, Ufuk Topcu</p>
<p><strong>Summary:</strong> We propose a framework for verifiable and compositional reinforcement
learning (RL) in which a collection of RL subsystems, each of which learns to
accomplish a separate subtask, are composed to achieve an overall task. The
framework consists of a high-level model, represented as a parametric Markov
decision process, which is used to plan and analyze compositions of subsystems,
and of the collection of low-level subsystems themselves. The subsystems are
implemented as deep RL agents operating un...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.01380v2' target='_blank'>Pessimistic Nonlinear Least-Squares Value Iteration for Offline
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Qiwei Di, Heyang Zhao, Jiafan He, Quanquan Gu</p>
<p><strong>Summary:</strong> Offline reinforcement learning (RL), where the agent aims to learn the
optimal policy based on the data collected by a behavior policy, has attracted
increasing attention in recent years. While offline RL with linear function
approximation has been extensively studied with optimal results achieved under
certain assumptions, many works shift their interest to offline RL with
non-linear function approximation. However, limited works on offline RL with
non-linear function approximation have instanc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.07378v3' target='_blank'>GARL: Genetic Algorithm-Augmented Reinforcement Learning to Detect
  Violations in Marker-Based Autonomous Landing Systems</a></h2>
<p><strong>Authors:</strong> Linfeng Liang, Yao Deng, Kye Morton, Valtteri Kallinen, Alice James, Avishkar Seth, Endrowednes Kuantama, Subhas Mukhopadhyay, Richard Han, Xi Zheng</p>
<p><strong>Summary:</strong> Automated Uncrewed Aerial Vehicle (UAV) landing is crucial for autonomous UAV
services such as monitoring, surveying, and package delivery. It involves
detecting landing targets, perceiving obstacles, planning collision-free paths,
and controlling UAV movements for safe landing. Failures can lead to
significant losses, necessitating rigorous simulation-based testing for safety.
Traditional offline testing methods, limited to static environments and
predefined trajectories, may miss violation cas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.10943v2' target='_blank'>Reaching the Limit in Autonomous Racing: Optimal Control versus
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yunlong Song, Angel Romero, Matthias Mueller, Vladlen Koltun, Davide Scaramuzza</p>
<p><strong>Summary:</strong> A central question in robotics is how to design a control system for an agile
mobile robot. This paper studies this question systematically, focusing on a
challenging setting: autonomous drone racing. We show that a neural network
controller trained with reinforcement learning (RL) outperformed optimal
control (OC) methods in this setting. We then investigated which fundamental
factors have contributed to the success of RL or have limited OC. Our study
indicates that the fundamental advantage of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.18966v1' target='_blank'>Spacecraft Autonomous Decision-Planning for Collision Avoidance: a
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Nicolas Bourriez, Adrien Loizeau, Adam F. Abdin</p>
<p><strong>Summary:</strong> The space environment around the Earth is becoming increasingly populated by
both active spacecraft and space debris. To avoid potential collision events,
significant improvements in Space Situational Awareness (SSA) activities and
Collision Avoidance (CA) technologies are allowing the tracking and maneuvering
of spacecraft with increasing accuracy and reliability. However, these
procedures still largely involve a high level of human intervention to make the
necessary decisions. For an increasin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.19331v1' target='_blank'>AdapINT: A Flexible and Adaptive In-Band Network Telemetry System Based
  on Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Penghui Zhang, Hua Zhang, Yibo Pi, Zijian Cao, Jingyu Wang, Jianxin Liao</p>
<p><strong>Summary:</strong> In-band Network Telemetry (INT) has emerged as a promising network
measurement technology. However, existing network telemetry systems lack the
flexibility to meet diverse telemetry requirements and are also difficult to
adapt to dynamic network environments. In this paper, we propose AdapINT, a
versatile and adaptive in-band network telemetry framework assisted by
dual-timescale probes, including long-period auxiliary probes (APs) and
short-period dynamic probes (DPs). Technically, the APs coll...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.00855v2' target='_blank'>A Multi-Agent Reinforcement Learning Framework for Evaluating the U.S.
  Ending the HIV Epidemic Plan</a></h2>
<p><strong>Authors:</strong> Dinesh Sharma, Ankit Shah, Chaitra Gopalappa</p>
<p><strong>Summary:</strong> Human immunodeficiency virus (HIV) is a major public health concern in the
United States, with about 1.2 million people living with HIV and 35,000 newly
infected each year. There are considerable geographical disparities in HIV
burden and care access across the U.S. The 2019 Ending the HIV Epidemic (EHE)
initiative aims to reduce new infections by 90% by 2030, by improving coverage
of diagnoses, treatment, and prevention interventions and prioritizing
jurisdictions with high HIV prevalence. Iden...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.09233v1' target='_blank'>Neural Packing: from Visual Sensing to Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Juzhan Xu, Minglun Gong, Hao Zhang, Hui Huang, Ruizhen Hu</p>
<p><strong>Summary:</strong> We present a novel learning framework to solve the transport-and-packing
(TAP) problem in 3D. It constitutes a full solution pipeline from partial
observations of input objects via RGBD sensing and recognition to final box
placement, via robotic motion planning, to arrive at a compact packing in a
target container. The technical core of our method is a neural network for TAP,
trained via reinforcement learning (RL), to solve the NP-hard combinatorial
optimization problem. Our network simultaneou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.00198v2' target='_blank'>Optimal Attack and Defense for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jeremy McMahan, Young Wu, Xiaojin Zhu, Qiaomin Xie</p>
<p><strong>Summary:</strong> To ensure the usefulness of Reinforcement Learning (RL) in real systems, it
is crucial to ensure they are robust to noise and adversarial attacks. In
adversarial RL, an external attacker has the power to manipulate the victim
agent's interaction with the environment. We study the full class of online
manipulation attacks, which include (i) state attacks, (ii) observation attacks
(which are a generalization of perceived-state attacks), (iii) action attacks,
and (iv) reward attacks. We show the at...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.07216v1' target='_blank'>Learning from Interaction: User Interface Adaptation using Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Daniel Gaspar-Figueiredo</p>
<p><strong>Summary:</strong> The continuous adaptation of software systems to meet the evolving needs of
users is very important for enhancing user experience (UX). User interface (UI)
adaptation, which involves adjusting the layout, navigation, and content
presentation based on user preferences and contextual conditions, plays an
important role in achieving this goal. However, suggesting the right adaptation
at the right time and in the right place remains a challenge in order to make
it valuable for the end-user. To tackl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.00916v1' target='_blank'>Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mohamad Abed El Rahman Hammoud, Naila Raboudi, Edriss S. Titi, Omar Knio, Ibrahim Hoteit</p>
<p><strong>Summary:</strong> Data assimilation (DA) plays a pivotal role in diverse applications, ranging
from climate predictions and weather forecasts to trajectory planning for
autonomous vehicles. A prime example is the widely used ensemble Kalman filter
(EnKF), which relies on linear updates to minimize variance among the ensemble
of forecast states. Recent advancements have seen the emergence of deep
learning approaches in this domain, primarily within a supervised learning
framework. However, the adaptability of such...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.01481v1' target='_blank'>Optimizing UAV-UGV Coalition Operations: A Hybrid Clustering and
  Multi-Agent Reinforcement Learning Approach for Path Planning in Obstructed
  Environment</a></h2>
<p><strong>Authors:</strong> Shamyo Brotee, Farhan Kabir, Md. Abdur Razzaque, Palash Roy, Md. Mamun-Or-Rashid, Md. Rafiul Hassan, Mohammad Mehedi Hassan</p>
<p><strong>Summary:</strong> One of the most critical applications undertaken by coalitions of Unmanned
Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is reaching
predefined targets by following the most time-efficient routes while avoiding
collisions. Unfortunately, UAVs are hampered by limited battery life, and UGVs
face challenges in reachability due to obstacles and elevation variations.
Existing literature primarily focuses on one-to-one coalitions, which
constrains the efficiency of reaching targets. In th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.05969v1' target='_blank'>Spatial-Aware Deep Reinforcement Learning for the Traveling Officer
  Problem</a></h2>
<p><strong>Authors:</strong> Niklas Strau√ü, Matthias Schubert</p>
<p><strong>Summary:</strong> The traveling officer problem (TOP) is a challenging stochastic optimization
task. In this problem, a parking officer is guided through a city equipped with
parking sensors to fine as many parking offenders as possible. A major
challenge in TOP is the dynamic nature of parking offenses, which randomly
appear and disappear after some time, regardless of whether they have been
fined. Thus, solutions need to dynamically adjust to currently fineable parking
offenses while also planning ahead to incr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.11118v1' target='_blank'>Meta Reinforcement Learning for Strategic IoT Deployments Coverage in
  Disaster-Response UAV Swarms</a></h2>
<p><strong>Authors:</strong> Marwan Dhuheir, Aiman Erbad, Ala Al-Fuqaha</p>
<p><strong>Summary:</strong> In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the
attention of researchers in academia and industry for their potential use in
critical emergency applications, such as providing wireless services to ground
users and collecting data from areas affected by disasters, due to their
advantages in terms of maneuverability and movement flexibility. The UAVs'
limited resources, energy budget, and strict mission completion time have posed
challenges in adopting UAVs for these applicati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.12455v1' target='_blank'>Multi-agent deep reinforcement learning with centralized training and
  decentralized execution for transportation infrastructure management</a></h2>
<p><strong>Authors:</strong> M. Saifullah, K. G. Papakonstantinou, C. P. Andriotis, S. M. Stoffels</p>
<p><strong>Summary:</strong> We present a multi-agent Deep Reinforcement Learning (DRL) framework for
managing large transportation infrastructure systems over their life-cycle.
Life-cycle management of such engineering systems is a computationally
intensive task, requiring appropriate sequential inspection and maintenance
decisions able to reduce long-term risks and costs, while dealing with
different uncertainties and constraints that lie in high-dimensional spaces. To
date, static age- or condition-based maintenance meth...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.17914v1' target='_blank'>Attention Graph for Multi-Robot Social Navigation with Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Erwan Escudie, Laetitia Matignon, Jacques Saraydaryan</p>
<p><strong>Summary:</strong> Learning robot navigation strategies among pedestrian is crucial for domain
based applications. Combining perception, planning and prediction allows us to
model the interactions between robots and pedestrians, resulting in impressive
outcomes especially with recent approaches based on deep reinforcement learning
(RL). However, these works do not consider multi-robot scenarios. In this
paper, we present MultiSoc, a new method for learning multi-agent socially
aware navigation strategies using RL....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.01874v1' target='_blank'>The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement
  Learning and Large Language Models</a></h2>
<p><strong>Authors:</strong> Moschoula Pternea, Prerna Singh, Abir Chakraborty, Yagna Oruganti, Mirco Milletari, Sayli Bapat, Kebei Jiang</p>
<p><strong>Summary:</strong> In this work, we review research studies that combine Reinforcement Learning
(RL) and Large Language Models (LLMs), two areas that owe their momentum to the
development of deep neural networks. We propose a novel taxonomy of three main
classes based on the way that the two model types interact with each other. The
first class, RL4LLM, includes studies where RL is leveraged to improve the
performance of LLMs on tasks related to Natural Language Processing. L4LLM is
divided into two sub-categories...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.05066v1' target='_blank'>Exploration Without Maps via Zero-Shot Out-of-Distribution Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Shathushan Sivashangaran, Apoorva Khairnar, Azim Eskandarian</p>
<p><strong>Summary:</strong> Operation of Autonomous Mobile Robots (AMRs) of all forms that include
wheeled ground vehicles, quadrupeds and humanoids in dynamically changing GPS
denied environments without a-priori maps, exclusively using onboard sensors,
is an unsolved problem that has potential to transform the economy, and vastly
improve humanity's capabilities with improvements to agriculture,
manufacturing, disaster response, military and space exploration. Conventional
AMR automation approaches are modularized into pe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.06694v1' target='_blank'>Scaling Intelligent Agents in Combat Simulations for Wargaming</a></h2>
<p><strong>Authors:</strong> Scotty Black, Christian Darken</p>
<p><strong>Summary:</strong> Remaining competitive in future conflicts with technologically-advanced
competitors requires us to accelerate our research and development in
artificial intelligence (AI) for wargaming. More importantly, leveraging
machine learning for intelligent combat behavior development will be key to one
day achieving superhuman performance in this domain--elevating the quality and
accelerating the speed of our decisions in future wars. Although deep
reinforcement learning (RL) continues to show promising ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.08502v2' target='_blank'>Provable Traffic Rule Compliance in Safe Reinforcement Learning on the
  Open Sea</a></h2>
<p><strong>Authors:</strong> Hanna Krasowski, Matthias Althoff</p>
<p><strong>Summary:</strong> For safe operation, autonomous vehicles have to obey traffic rules that are
set forth in legal documents formulated in natural language. Temporal logic is
a suitable concept to formalize such traffic rules. Still, temporal logic rules
often result in constraints that are hard to solve using optimization-based
motion planners. Reinforcement learning (RL) is a promising method to find
motion plans for autonomous vehicles. However, vanilla RL algorithms are based
on random exploration and do not au...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.16801v2' target='_blank'>Craftax: A Lightning-Fast Benchmark for Open-Ended Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel Coward, Jakob Foerster</p>
<p><strong>Summary:</strong> Benchmarks play a crucial role in the development and analysis of
reinforcement learning (RL) algorithms. We identify that existing benchmarks
used for research into open-ended learning fall into one of two categories.
Either they are too slow for meaningful research to be performed without
enormous computational resources, like Crafter, NetHack and Minecraft, or they
are not complex enough to pose a significant challenge, like Minigrid and
Procgen. To remedy this, we first present Craftax-Class...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.03848v1' target='_blank'>Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Zifan Xu, Amir Hossain Raj, Xuesu Xiao, Peter Stone</p>
<p><strong>Summary:</strong> Recent advances of locomotion controllers utilizing deep reinforcement
learning (RL) have yielded impressive results in terms of achieving rapid and
robust locomotion across challenging terrain, such as rugged rocks, non-rigid
ground, and slippery surfaces. However, while these controllers primarily
address challenges underneath the robot, relatively little research has
investigated legged mobility through confined 3D spaces, such as narrow tunnels
or irregular voids, which impose all-around con...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.05787v1' target='_blank'>Scaling Team Coordination on Graphs with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Manshi Limbu, Zechen Hu, Xuan Wang, Daigo Shishika, Xuesu Xiao</p>
<p><strong>Summary:</strong> This paper studies Reinforcement Learning (RL) techniques to enable team
coordination behaviors in graph environments with support actions among
teammates to reduce the costs of traversing certain risky edges in a
centralized manner. While classical approaches can solve this non-standard
multi-agent path planning problem by converting the original Environment Graph
(EG) into a Joint State Graph (JSG) to implicitly incorporate the support
actions, those methods do not scale well to large graphs a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.20016v2' target='_blank'>EnCoMP: Enhanced Covert Maneuver Planning with Adaptive Threat-Aware
  Visibility Estimation using Offline Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Jade Freeman, Timothy Gregory, Theron T. Trout</p>
<p><strong>Summary:</strong> Autonomous robots operating in complex environments face the critical
challenge of identifying and utilizing environmental cover for covert
navigation to minimize exposure to potential threats. We propose EnCoMP, an
enhanced navigation framework that integrates offline reinforcement learning
and our novel Adaptive Threat-Aware Visibility Estimation (ATAVE) algorithm to
enable robots to navigate covertly and efficiently in diverse outdoor settings.
ATAVE is a dynamic probabilistic threat modeling...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.00282v3' target='_blank'>Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,
  Taxonomy, and Methods</a></h2>
<p><strong>Authors:</strong> Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, Yun Li</p>
<p><strong>Summary:</strong> With extensive pre-trained knowledge and high-level general capabilities,
large language models (LLMs) emerge as a promising avenue to augment
reinforcement learning (RL) in aspects such as multi-task learning, sample
efficiency, and high-level task planning. In this survey, we provide a
comprehensive review of the existing literature in LLM-enhanced RL and
summarize its characteristics compared to conventional RL methods, aiming to
clarify the research scope and directions for future studies. U...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.00383v2' target='_blank'>Learning Tactile Insertion in the Real World</a></h2>
<p><strong>Authors:</strong> Daniel Palenicek, Theo Gruner, Tim Schneider, Alina B√∂hm, Janis Lenz, Inga Pfenning, Eric Kr√§mer, Jan Peters</p>
<p><strong>Summary:</strong> Humans have exceptional tactile sensing capabilities, which they can leverage
to solve challenging, partially observable tasks that cannot be solved from
visual observation alone. Research in tactile sensing attempts to unlock this
new input modality for robots. Lately, these sensors have become cheaper and,
thus, widely available. At the same time, the question of how to integrate them
into control loops is still an active area of research, with central challenges
being partial observability an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01693v2' target='_blank'>Adversarial Attacks on Reinforcement Learning Agents for Command and
  Control</a></h2>
<p><strong>Authors:</strong> Ahaan Dabholkar, James Z. Hare, Mark Mittrick, John Richardson, Nicholas Waytowich, Priya Narayanan, Saurabh Bagchi</p>
<p><strong>Summary:</strong> Given the recent impact of Deep Reinforcement Learning in training agents to
win complex games like StarCraft and DoTA(Defense Of The Ancients) - there has
been a surge in research for exploiting learning based techniques for
professional wargaming, battlefield simulation and modeling. Real time strategy
games and simulators have become a valuable resource for operational planning
and military research. However, recent work has shown that such learning based
approaches are highly susceptible to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.02425v1' target='_blank'>Learning Robot Soccer from Egocentric Vision with Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Dhruva Tirumala, Markus Wulfmeier, Ben Moran, Sandy Huang, Jan Humplik, Guy Lever, Tuomas Haarnoja, Leonard Hasenclever, Arunkumar Byravan, Nathan Batchelor, Neil Sreendra, Kushal Patel, Marlon Gwira, Francesco Nori, Martin Riedmiller, Nicolas Heess</p>
<p><strong>Summary:</strong> We apply multi-agent deep reinforcement learning (RL) to train end-to-end
robot soccer policies with fully onboard computation and sensing via egocentric
RGB vision. This setting reflects many challenges of real-world robotics,
including active perception, agile full-body control, and long-horizon planning
in a dynamic, partially-observable, multi-agent domain. We rely on large-scale,
simulation-based data generation to obtain complex behaviors from egocentric
vision which can be successfully tr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.05422v1' target='_blank'>Diffusion-based Reinforcement Learning for Dynamic UAV-assisted Vehicle
  Twins Migration in Vehicular Metaverses</a></h2>
<p><strong>Authors:</strong> Yongju Tong, Jiawen Kang, Junlong Chen, Minrui Xu, Gaolei Li, Weiting Zhang, Xincheng Yan</p>
<p><strong>Summary:</strong> Air-ground integrated networks can relieve communication pressure on ground
transportation networks and provide 6G-enabled vehicular Metaverses services
offloading in remote areas with sparse RoadSide Units (RSUs) coverage and
downtown areas where users have a high demand for vehicular services. Vehicle
Twins (VTs) are the digital twins of physical vehicles to enable more immersive
and realistic vehicular services, which can be offloaded and updated on RSU, to
manage and provide vehicular Metave...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.09755v1' target='_blank'>Mix Q-learning for Lane Changing: A Collaborative Decision-Making Method
  in Multi-Agent Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xiaojun Bi, Mingjie He, Yiwen Sun</p>
<p><strong>Summary:</strong> Lane-changing decisions, which are crucial for autonomous vehicle path
planning, face practical challenges due to rule-based constraints and limited
data. Deep reinforcement learning has become a major research focus due to its
advantages in data acquisition and interpretability. However, current models
often overlook collaboration, which affects not only impacts overall traffic
efficiency but also hinders the vehicle's own normal driving in the long run.
To address the aforementioned issue, thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.13434v1' target='_blank'>Tactile Aware Dynamic Obstacle Avoidance in Crowded Environment with
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Yung Chuen Ng, Qi Wen, Lim, Chun Ye Tan, Zhen Hao Gan, Meng Yee, Chuah</p>
<p><strong>Summary:</strong> Mobile robots operating in crowded environments require the ability to
navigate among humans and surrounding obstacles efficiently while adhering to
safety standards and socially compliant mannerisms. This scale of the robot
navigation problem may be classified as both a local path planning and
trajectory optimization problem. This work presents an array of force sensors
that act as a tactile layer to complement the use of a LiDAR for the purpose of
inducing awareness of contact with any surroun...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.00007v1' target='_blank'>Graph Neural Networks and Reinforcement Learning for Proactive
  Application Image Placement</a></h2>
<p><strong>Authors:</strong> Antonios Makris, Theodoros Theodoropoulos, Evangelos Psomakelis, Emanuele Carlini, Matteo Mordacchini, Patrizio Dazzi, Konstantinos Tserpes</p>
<p><strong>Summary:</strong> The shift from Cloud Computing to a Cloud-Edge continuum presents new
opportunities and challenges for data-intensive and interactive applications.
Edge computing has garnered a lot of attention from both industry and academia
in recent years, emerging as a key enabler for meeting the increasingly strict
demands of Next Generation applications. In Edge computing the computations are
placed closer to the end-users, to facilitate low-latency and high-bandwidth
applications and services. However, t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.06494v4' target='_blank'>DiffPhyCon: A Generative Approach to Control Complex Physical Systems</a></h2>
<p><strong>Authors:</strong> Long Wei, Peiyan Hu, Ruiqi Feng, Haodong Feng, Yixuan Du, Tao Zhang, Rui Wang, Yue Wang, Zhi-Ming Ma, Tailin Wu</p>
<p><strong>Summary:</strong> Controlling the evolution of complex physical systems is a fundamental task
across science and engineering. Classical techniques suffer from limited
applicability or huge computational costs. On the other hand, recent deep
learning and reinforcement learning-based approaches often struggle to optimize
long-term control sequences under the constraints of system dynamics. In this
work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class
of method to address the physical syste...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.07747v1' target='_blank'>HGFF: A Deep Reinforcement Learning Framework for Lifetime Maximization
  in Wireless Sensor Networks</a></h2>
<p><strong>Authors:</strong> Xiaoxu Han, Xin Mu, Jinghui Zhong</p>
<p><strong>Summary:</strong> Planning the movement of the sink to maximize the lifetime in wireless sensor
networks is an essential problem of great research challenge and practical
value. Many existing mobile sink techniques based on mathematical programming
or heuristics have demonstrated the feasibility of the task. Nevertheless, the
huge computation consumption or the over-reliance on human knowledge can result
in relatively low performance. In order to balance the need for high-quality
solutions with the goal of minimi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.08932v2' target='_blank'>Deep Attention Driven Reinforcement Learning (DAD-RL) for Autonomous
  Decision-Making in Dynamic Environment</a></h2>
<p><strong>Authors:</strong> Jayabrata Chowdhury, Venkataramanan Shivaraman, Sumit Dangi, Suresh Sundaram, P. B. Sujit</p>
<p><strong>Summary:</strong> Autonomous Vehicle (AV) decision making in urban environments is inherently
challenging due to the dynamic interactions with surrounding vehicles. For safe
planning, AV must understand the weightage of various spatiotemporal
interactions in a scene. Contemporary works use colossal transformer
architectures to encode interactions mainly for trajectory prediction,
resulting in increased computational complexity. To address this issue without
compromising spatiotemporal understanding and performanc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.10971v1' target='_blank'>Walking the Values in Bayesian Inverse Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ondrej Bajgar, Alessandro Abate, Konstantinos Gatsis, Michael A. Osborne</p>
<p><strong>Summary:</strong> The goal of Bayesian inverse reinforcement learning (IRL) is recovering a
posterior distribution over reward functions using a set of demonstrations from
an expert optimizing for a reward unknown to the learner. The resulting
posterior over rewards can then be used to synthesize an apprentice policy that
performs well on the same or a similar task. A key challenge in Bayesian IRL is
bridging the computational gap between the hypothesis space of possible rewards
and the likelihood, often defined ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.13734v1' target='_blank'>Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion
  Models: A Tutorial and Review</a></h2>
<p><strong>Authors:</strong> Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, Sergey Levine</p>
<p><strong>Summary:</strong> This tutorial provides a comprehensive survey of methods for fine-tuning
diffusion models to optimize downstream reward functions. While diffusion
models are widely known to provide excellent generative modeling capability,
practical applications in domains such as biology require generating samples
that maximize some desired metric (e.g., translation efficiency in RNA, docking
score in molecules, stability in protein). In these cases, the diffusion model
can be optimized not only to generate re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.15041v1' target='_blank'>Earth Observation Satellite Scheduling with Graph Neural Networks</a></h2>
<p><strong>Authors:</strong> Antoine Jacquet, Guillaume Infantes, Nicolas Meuleau, Emmanuel Benazera, St√©phanie Roussel, Vincent Baudoui, Jonathan Guerra</p>
<p><strong>Summary:</strong> The Earth Observation Satellite Planning (EOSP) is a difficult optimization
problem with considerable practical interest. A set of requested observations
must be scheduled on an agile Earth observation satellite while respecting
constraints on their visibility window, as well as maneuver constraints that
impose varying delays between successive observations. In addition, the problem
is largely oversubscribed: there are much more candidate observations than what
can possibly be achieved. Therefor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.03429v1' target='_blank'>Reinforcement Learning Approach to Optimizing Profilometric Sensor
  Trajectories for Surface Inspection</a></h2>
<p><strong>Authors:</strong> Sara Roos-Hoefgeest, Mario Roos-Hoefgeest, Ignacio Alvarez, Rafael C. Gonz√°lez</p>
<p><strong>Summary:</strong> High-precision surface defect detection in manufacturing is essential for
ensuring quality control. Laser triangulation profilometric sensors are key to
this process, providing detailed and accurate surface measurements over a line.
To achieve a complete and precise surface scan, accurate relative motion
between the sensor and the workpiece is required. It is crucial to control the
sensor pose to maintain optimal distance and relative orientation to the
surface. It is also important to ensure un...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.07341v1' target='_blank'>Online Decision MetaMorphFormer: A Casual Transformer-Based
  Reinforcement Learning Framework of Universal Embodied Intelligence</a></h2>
<p><strong>Authors:</strong> Luo Ji, Runji Lin</p>
<p><strong>Summary:</strong> Interactive artificial intelligence in the motion control field is an
interesting topic, especially when universal knowledge is adaptive to multiple
tasks and universal environments. Despite there being increasing efforts in the
field of Reinforcement Learning (RL) with the aid of transformers, most of them
might be limited by the offline training pipeline, which prohibits exploration
and generalization abilities. To address this limitation, we propose the
framework of Online Decision MetaMorphF...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.11852v1' target='_blank'>XP-MARL: Auxiliary Prioritization in Multi-Agent Reinforcement Learning
  to Address Non-Stationarity</a></h2>
<p><strong>Authors:</strong> Jianye Xu, Omar Sobhy, Bassam Alrifaee</p>
<p><strong>Summary:</strong> Non-stationarity poses a fundamental challenge in Multi-Agent Reinforcement
Learning (MARL), arising from agents simultaneously learning and altering their
policies. This creates a non-stationary environment from the perspective of
each individual agent, often leading to suboptimal or even unconverged learning
outcomes. We propose an open-source framework named XP-MARL, which augments
MARL with auxiliary prioritization to address this challenge in cooperative
settings. XP-MARL is 1) founded upon...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.15866v2' target='_blank'>Multi-UAV Pursuit-Evasion with Online Planning in Unknown Environments
  by Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang</p>
<p><strong>Summary:</strong> Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key
challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL)
has demonstrated potential in modeling cooperative behaviors, but most RL-based
approaches remain constrained to simplified simulations with limited dynamics
or fixed scenarios. Previous attempts to deploy RL policy to real-world
pursuit-evasion are largely restricted to two-dimensional scenarios, such as
ground vehicles or UAVs at fixed al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.17922v1' target='_blank'>Navigation in a simplified Urban Flow through Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Federica Tonti, Jean Rabault, Ricardo Vinuesa</p>
<p><strong>Summary:</strong> The increasing number of unmanned aerial vehicles (UAVs) in urban
environments requires a strategy to minimize their environmental impact, both
in terms of energy efficiency and noise reduction. In order to reduce these
concerns, novel strategies for developing prediction models and optimization of
flight planning, for instance through deep reinforcement learning (DRL), are
needed. Our goal is to develop DRL algorithms capable of enabling the
autonomous navigation of UAVs in urban environments, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.20005v1' target='_blank'>Enhancing Battery Storage Energy Arbitrage with Deep Reinforcement
  Learning and Time-Series Forecasting</a></h2>
<p><strong>Authors:</strong> Manuel Sage, Joshua Campbell, Yaoyao Fiona Zhao</p>
<p><strong>Summary:</strong> Energy arbitrage is one of the most profitable sources of income for battery
operators, generating revenues by buying and selling electricity at different
prices. Forecasting these revenues is challenging due to the inherent
uncertainty of electricity prices. Deep reinforcement learning (DRL) emerged in
recent years as a promising tool, able to cope with uncertainty by training on
large quantities of historical data. However, without access to future
electricity prices, DRL agents can only react...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.20907v1' target='_blank'>Combining Deep Reinforcement Learning with a Jerk-Bounded Trajectory
  Generator for Kinematically Constrained Motion Planning</a></h2>
<p><strong>Authors:</strong> Seyed Adel Alizadeh Kolagar, Mehdi Heydari Shahna, Jouni Mattila</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) is emerging as a promising method for
adaptive robotic motion and complex task automation, effectively addressing the
limitations of traditional control methods. However, ensuring safety throughout
both the learning process and policy deployment remains a key challenge due to
the risky exploration inherent in DRL, as well as the discrete nature of
actions taken at intervals. These discontinuities, despite being part of a
continuous action space, can lead to abru...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.07514v2' target='_blank'>Robust Offline Reinforcement Learning for Non-Markovian Decision
  Processes</a></h2>
<p><strong>Authors:</strong> Ruiquan Huang, Yingbin Liang, Jing Yang</p>
<p><strong>Summary:</strong> Distributionally robust offline reinforcement learning (RL) aims to find a
policy that performs the best under the worst environment within an uncertainty
set using an offline dataset collected from a nominal model. While recent
advances in robust RL focus on Markov decision processes (MDPs), robust
non-Markovian RL is limited to planning problem where the transitions in the
uncertainty set are known. In this paper, we study the learning problem of
robust offline non-Markovian RL. Specifically, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.08299v3' target='_blank'>DNN Task Assignment in UAV Networks: A Generative AI Enhanced
  Multi-Agent Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Xin Tang, Qian Chen, Wenjie Weng, Binhan Liao, Jiacheng Wang, Xianbin Cao, Xiaohuan Li</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment
capabilities, prompting the development of UAVs for various application
scenarios within the Internet of Things (IoT). The unique capabilities of UAVs
give rise to increasingly critical and complex tasks in uncertain and
potentially harsh environments. The substantial amount of data generated from
these applications necessitates processing and analysis through deep neural
networks (DNNs). However, UAVs encounter challe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.10175v2' target='_blank'>The Surprising Ineffectiveness of Pre-Trained Visual Representations for
  Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Moritz Schneider, Robert Krug, Narunas Vaskevicius, Luigi Palmieri, Joschka Boedecker</p>
<p><strong>Summary:</strong> Visual Reinforcement Learning (RL) methods often require extensive amounts of
data. As opposed to model-free RL, model-based RL (MBRL) offers a potential
solution with efficient data utilization through planning. Additionally, RL
lacks generalization capabilities for real-world tasks. Prior work has shown
that incorporating pre-trained visual representations (PVRs) enhances sample
efficiency and generalization. While PVRs have been extensively studied in the
context of model-free RL, their poten...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.02271v1' target='_blank'>Securing Integrated Sensing and Communication Against a Mobile
  Adversary: A Stackelberg Game with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Milad Tatar Mamaghani, Xiangyun Zhou, Nan Yang, A. Lee Swindlehurst</p>
<p><strong>Summary:</strong> In this paper, we study a secure integrated sensing and communication (ISAC)
system employing a full-duplex base station with sensing capabilities against a
mobile proactive adversarial target$\unicode{x2014}$a malicious unmanned aerial
vehicle (M-UAV). We develop a game-theoretic model to enhance communication
security, radar sensing accuracy, and power efficiency. The interaction between
the legitimate network and the mobile adversary is formulated as a
non-cooperative Stackelberg game (NSG), ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.14377v1' target='_blank'>Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone
  Flight</a></h2>
<p><strong>Authors:</strong> Angel Romero, Ashwin Shenai, Ismail Geles, Elie Aljalbout, Davide Scaramuzza</p>
<p><strong>Summary:</strong> Autonomous drone racing has risen as a challenging robotic benchmark for
testing the limits of learning, perception, planning, and control. Expert human
pilots are able to agilely fly a drone through a race track by mapping the
real-time feed from a single onboard camera directly to control commands.
Recent works in autonomous drone racing attempting direct pixel-to-commands
control policies (without explicit state estimation) have relied on either
intermediate representations that simplify the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.01268v1' target='_blank'>Resilient UAV Trajectory Planning via Few-Shot Meta-Offline
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Eslam Eldeeb, Hirley Alves</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has been a promising essence in future 5G-beyond
and 6G systems. Its main advantage lies in its robust model-free
decision-making in complex and large-dimension wireless environments. However,
most existing RL frameworks rely on online interaction with the environment,
which might not be feasible due to safety and cost concerns. Another problem
with online RL is the lack of scalability of the designed algorithm with
dynamic or new environments. This work proposes a no...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.01465v1' target='_blank'>Embrace Collisions: Humanoid Shadowing for Deployable Contact-Agnostics
  Motions</a></h2>
<p><strong>Authors:</strong> Ziwen Zhuang, Hang Zhao</p>
<p><strong>Summary:</strong> Previous humanoid robot research works treat the robot as a bipedal mobile
manipulation platform, where only the feet and hands contact the environment.
However, we humans use all body parts to interact with the world, e.g., we sit
in chairs, get up from the ground, or roll on the floor. Contacting the
environment using body parts other than feet and hands brings significant
challenges in both model-predictive control and reinforcement learning-based
methods. An unpredictable contact sequence ma...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.05524v2' target='_blank'>Physically Embedded Planning Problems: New Challenges for Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Mehdi Mirza, Andrew Jaegle, Jonathan J. Hunt, Arthur Guez, Saran Tunyasuvunakool, Alistair Muldal, Th√©ophane Weber, Peter Karkus, S√©bastien Racani√®re, Lars Buesing, Timothy Lillicrap, Nicolas Heess</p>
<p><strong>Summary:</strong> Recent work in deep reinforcement learning (RL) has produced algorithms
capable of mastering challenging games such as Go, chess, or shogi. In these
works the RL agent directly observes the natural state of the game and controls
that state directly with its actions. However, when humans play such games,
they do not just reason about the moves but also interact with their physical
environment. They understand the state of the game by looking at the physical
board in front of them and modify it by...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.08999v2' target='_blank'>PassGoodPool: Joint Passengers and Goods Fleet Management with
  Reinforcement Learning aided Pricing, Matching, and Route Planning</a></h2>
<p><strong>Authors:</strong> Kaushik Manchella, Marina Haliem, Vaneet Aggarwal, Bharat Bhargava</p>
<p><strong>Summary:</strong> The ubiquitous growth of mobility-on-demand services for passenger and goods
delivery has brought various challenges and opportunities within the realm of
transportation systems. As a result, intelligent transportation systems are
being developed to maximize operational profitability, user convenience, and
environmental sustainability. The growth of last mile deliveries alongside
ridesharing calls for an efficient and cohesive system that transports both
passengers and goods. Existing methods ad...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02670v4' target='_blank'>Robust Adversarial Attacks Detection based on Explainable Deep
  Reinforcement Learning For UAV Guidance and Planning</a></h2>
<p><strong>Authors:</strong> Thomas Hickling, Nabil Aouf, Phillippa Spencer</p>
<p><strong>Summary:</strong> The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents
operating in public are increasing. Adopting AI-based techniques and, more
specifically, Deep Learning (DL) approaches to control and guide these UAVs can
be beneficial in terms of performance but can add concerns regarding the safety
of those techniques and their vulnerability against adversarial attacks.
Confusion in the agent's decision-making process caused by these attacks can
seriously affect the safety of the UAV. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.01861v1' target='_blank'>Online Shielding for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Bettina K√∂nighofer, Julian Rudolf, Alexander Palmisano, Martin Tappler, Roderick Bloem</p>
<p><strong>Summary:</strong> Besides the recent impressive results on reinforcement learning (RL), safety
is still one of the major research challenges in RL. RL is a machine-learning
approach to determine near-optimal policies in Markov decision processes
(MDPs). In this paper, we consider the setting where the safety-relevant
fragment of the MDP together with a temporal logic safety specification is
given and many safety violations can be avoided by planning ahead a short time
into the future. We propose an approach for o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.02476v5' target='_blank'>Deep Reinforcement Learning for Traveling Purchaser Problems</a></h2>
<p><strong>Authors:</strong> Haofeng Yuan, Rongping Zhu, Wanlu Yang, Shiji Song, Keyou You, Wei Fan, C. L. Philip Chen</p>
<p><strong>Summary:</strong> The traveling purchaser problem (TPP) is an important combinatorial
optimization problem with broad applications. Due to the coupling between
routing and purchasing, existing works on TPPs commonly address route
construction and purchase planning simultaneously, which, however, leads to
exact methods with high computational cost and heuristics with sophisticated
design but limited performance. In sharp contrast, we propose a novel approach
based on deep reinforcement learning (DRL), which addres...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.17794v3' target='_blank'>LNS2+RL: Combining Multi-Agent Reinforcement Learning with Large
  Neighborhood Search in Multi-Agent Path Finding</a></h2>
<p><strong>Authors:</strong> Yutong Wang, Tanishq Duhan, Jiaoyang Li, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> Multi-Agent Path Finding (MAPF) is a critical component of logistics and
warehouse management, which focuses on planning collision-free paths for a team
of robots in a known environment. Recent work introduced a novel MAPF approach,
LNS2, which proposed to repair a quickly obtained set of infeasible paths via
iterative replanning, by relying on a fast, yet lower-quality, prioritized
planning (PP) algorithm. At the same time, there has been a recent push for
Multi-Agent Reinforcement Learning (MA...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1312.0286v2' target='_blank'>Efficient Learning and Planning with Compressed Predictive States</a></h2>
<p><strong>Authors:</strong> William L. Hamilton, Mahdi Milani Fard, Joelle Pineau</p>
<p><strong>Summary:</strong> Predictive state representations (PSRs) offer an expressive framework for
modelling partially observable systems. By compactly representing systems as
functions of observable quantities, the PSR learning approach avoids using
local-minima prone expectation-maximization and instead employs a globally
optimal moment-based algorithm. Moreover, since PSRs do not require a
predetermined latent state structure as an input, they offer an attractive
framework for model-based reinforcement learning when ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1608.04996v1' target='_blank'>Open Problem: Approximate Planning of POMDPs in the class of Memoryless
  Policies</a></h2>
<p><strong>Authors:</strong> Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar</p>
<p><strong>Summary:</strong> Planning plays an important role in the broad class of decision theory.
Planning has drawn much attention in recent work in the robotics and sequential
decision making areas. Recently, Reinforcement Learning (RL), as an
agent-environment interaction problem, has brought further attention to
planning methods. Generally in RL, one can assume a generative model, e.g.
graphical models, for the environment, and then the task for the RL agent is to
learn the model parameters and find the optimal strat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.08761v1' target='_blank'>Distributed Policy Iteration for Scalable Approximation of Cooperative
  Multi-Agent Policies</a></h2>
<p><strong>Authors:</strong> Thomy Phan, Kyrill Schmid, Lenz Belzner, Thomas Gabor, Sebastian Feld, Claudia Linnhoff-Popien</p>
<p><strong>Summary:</strong> Decision making in multi-agent systems (MAS) is a great challenge due to
enormous state and joint action spaces as well as uncertainty, making
centralized control generally infeasible. Decentralized control offers better
scalability and robustness but requires mechanisms to coordinate on joint tasks
and to avoid conflicts. Common approaches to learn decentralized policies for
cooperative MAS suffer from non-stationarity and lacking credit assignment,
which can lead to unstable and uncoordinated ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.10099v1' target='_blank'>DynoPlan: Combining Motion Planning and Deep Neural Network based
  Controllers for Safe HRL</a></h2>
<p><strong>Authors:</strong> Daniel Angelov, Yordan Hristov, Subramanian Ramamoorthy</p>
<p><strong>Summary:</strong> Many realistic robotics tasks are best solved compositionally, through
control architectures that sequentially invoke primitives and achieve error
correction through the use of loops and conditionals taking the system back to
alternative earlier states. Recent end-to-end approaches to task learning
attempt to directly learn a single controller that solves an entire task, but
this has been difficult for complex control tasks that would have otherwise
required a diversity of local primitive moves,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.07532v2' target='_blank'>Online Bayesian Goal Inference for Boundedly-Rational Planning Agents</a></h2>
<p><strong>Authors:</strong> Tan Zhi-Xuan, Jordyn L. Mann, Tom Silver, Joshua B. Tenenbaum, Vikash K. Mansinghka</p>
<p><strong>Summary:</strong> People routinely infer the goals of others by observing their actions over
time. Remarkably, we can do so even when those actions lead to failure,
enabling us to assist others when we detect that they might not achieve their
goals. How might we endow machines with similar capabilities? Here we present
an architecture capable of inferring an agent's goals online from both optimal
and non-optimal sequences of actions. Our architecture models agents as
boundedly-rational planners that interleave se...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.07635v2' target='_blank'>Learning Robotic Assembly from CAD</a></h2>
<p><strong>Authors:</strong> Garrett Thomas, Melissa Chien, Aviv Tamar, Juan Aparicio Ojea, Pieter Abbeel</p>
<p><strong>Summary:</strong> In this work, motivated by recent manufacturing trends, we investigate
autonomous robotic assembly. Industrial assembly tasks require contact-rich
manipulation skills, which are challenging to acquire using classical control
and motion planning approaches. Consequently, robot controllers for assembly
domains are presently engineered to solve a particular task, and cannot easily
handle variations in the product or environment. Reinforcement learning (RL) is
a promising approach for autonomously a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.00988v1' target='_blank'>Behavior Planning of Autonomous Cars with Social Perception</a></h2>
<p><strong>Authors:</strong> Liting Sun, Wei Zhan, Ching-Yao Chan, Masayoshi Tomizuka</p>
<p><strong>Summary:</strong> Autonomous cars have to navigate in dynamic environment which can be full of
uncertainties. The uncertainties can come either from sensor limitations such
as occlusions and limited sensor range, or from probabilistic prediction of
other road participants, or from unknown social behavior in a new area. To
safely and efficiently drive in the presence of these uncertainties, the
decision-making and planning modules of autonomous cars should intelligently
utilize all available information and approp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.09014v3' target='_blank'>Learning Hybrid Object Kinematics for Efficient Hierarchical Planning
  Under Uncertainty</a></h2>
<p><strong>Authors:</strong> Ajinkya Jain, Scott Niekum</p>
<p><strong>Summary:</strong> Sudden changes in the dynamics of robotic tasks, such as contact with an
object or the latching of a door, are often viewed as inconvenient
discontinuities that make manipulation difficult. However, when these
transitions are well-understood, they can be leveraged to reduce uncertainty or
aid manipulation---for example, wiggling a screw to determine if it is fully
inserted or not. Current model-free reinforcement learning approaches require
large amounts of data to learn to leverage such dynamic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.00370v1' target='_blank'>PlaNet of the Bayesians: Reconsidering and Improving Deep Planning
  Network by Incorporating Bayesian Inference</a></h2>
<p><strong>Authors:</strong> Masashi Okada, Norio Kosaka, Tadahiro Taniguchi</p>
<p><strong>Summary:</strong> In the present paper, we propose an extension of the Deep Planning Network
(PlaNet), also referred to as PlaNet of the Bayesians (PlaNet-Bayes). There has
been a growing demand in model predictive control (MPC) in partially observable
environments in which complete information is unavailable because of, for
example, lack of expensive sensors. PlaNet is a promising solution to realize
such latent MPC, as it is used to train state-space models via model-based
reinforcement learning (MBRL) and to c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.09854v1' target='_blank'>Generalized Inverse Planning: Learning Lifted non-Markovian Utility for
  Generalizable Task Representation</a></h2>
<p><strong>Authors:</strong> Sirui Xie, Feng Gao, Song-Chun Zhu</p>
<p><strong>Summary:</strong> In searching for a generalizable representation of temporally extended tasks,
we spot two necessary constituents: the utility needs to be non-Markovian to
transfer temporal relations invariant to a probability shift, the utility also
needs to be lifted to abstract out specific grounding objects. In this work, we
study learning such utility from human demonstrations. While inverse
reinforcement learning (IRL) has been accepted as a general framework of
utility learning, its fundamental formulatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00898v1' target='_blank'>A Novel Automated Curriculum Strategy to Solve Hard Sokoban Planning
  Instances</a></h2>
<p><strong>Authors:</strong> Dieqiao Feng, Carla P. Gomes, Bart Selman</p>
<p><strong>Summary:</strong> In recent years, we have witnessed tremendous progress in deep reinforcement
learning (RL) for tasks such as Go, Chess, video games, and robot control.
Nevertheless, other combinatorial domains, such as AI planning, still pose
considerable challenges for RL approaches. The key difficulty in those domains
is that a positive reward signal becomes {\em exponentially rare} as the
minimal solution length increases. So, an RL approach loses its training
signal. There has been promising recent progress...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00910v1' target='_blank'>AI based Algorithms of Path Planning, Navigation and Control for Mobile
  Ground Robots and UAVs</a></h2>
<p><strong>Authors:</strong> Jian Zhang</p>
<p><strong>Summary:</strong> As the demands of autonomous mobile robots are increasing in recent years,
the requirement of the path planning/navigation algorithm should not be content
with the ability to reach the target without any collisions, but also should
try to achieve possible optimal or suboptimal path from the initial position to
the target according to the robot's constrains in practice. This report
investigates path planning and control strategies for mobile robots with
machine learning techniques, including grou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.05734v3' target='_blank'>Learning Efficient Multi-Agent Cooperative Visual Exploration</a></h2>
<p><strong>Authors:</strong> Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu</p>
<p><strong>Summary:</strong> We tackle the problem of cooperative visual exploration where multiple agents
need to jointly explore unseen regions as fast as possible based on visual
signals. Classical planning-based methods often suffer from expensive
computation overhead at each step and a limited expressiveness of complex
cooperation strategy. By contrast, reinforcement learning (RL) has recently
become a popular paradigm for tackling this challenge due to its modeling
capability of arbitrarily complex strategies and mini...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.10658v1' target='_blank'>Q-Learning Based Energy-Efficient Network Planning in IP-over-EON</a></h2>
<p><strong>Authors:</strong> Pramit Biswas, Md Shahbaz Akhtar, Aneek Adhya, Sriparna Saha, Sudhan Majhi</p>
<p><strong>Summary:</strong> During network planning phase, optimal network planning implemented through
efficient resource allocation and static traffic demand provisioning in
IP-over-elastic optical network (IP-over-EON) is significantly challenging
compared with the fixed-grid wavelength division multiplexing (WDM) network due
to increased flexibility in IP-over-EON. Mathematical optimization models used
for this purpose may not provide solution for large networks due to large
computational complexity. In this regard, a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.15052v2' target='_blank'>Learning Minimum-Time Flight in Cluttered Environments</a></h2>
<p><strong>Authors:</strong> Robert Penicka, Yunlong Song, Elia Kaufmann, Davide Scaramuzza</p>
<p><strong>Summary:</strong> We tackle the problem of minimum-time flight for a quadrotor through a
sequence of waypoints in the presence of obstacles while exploiting the full
quadrotor dynamics. Early works relied on simplified dynamics or polynomial
trajectory representations that did not exploit the full actuator potential of
the quadrotor, and, thus, resulted in suboptimal solutions. Recent works can
plan minimum-time trajectories; yet, the trajectories are executed with control
methods that do not account for obstacle...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.02464v1' target='_blank'>A Learning System for Motion Planning of Free-Float Dual-Arm Space
  Manipulator towards Non-Cooperative Object</a></h2>
<p><strong>Authors:</strong> Shengjie Wang, Yuxue Cao, Xiang Zheng, Tao Zhang</p>
<p><strong>Summary:</strong> Recent years have seen the emergence of non-cooperative objects in space,
like failed satellites and space junk. These objects are usually operated or
collected by free-float dual-arm space manipulators. Thanks to eliminating the
difficulties of modeling and manual parameter-tuning, reinforcement learning
(RL) methods have shown a more promising sign in the trajectory planning of
space manipulators. Although previous studies demonstrate their effectiveness,
they cannot be applied in tracking dyn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.10291v3' target='_blank'>Efficient Planning in a Compact Latent Action Space</a></h2>
<p><strong>Authors:</strong> Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rockt√§schel, Edward Grefenstette, Yuandong Tian</p>
<p><strong>Summary:</strong> Planning-based reinforcement learning has shown strong performance in tasks
in discrete and low-dimensional continuous action spaces. However, planning
usually brings significant computational overhead for decision-making, and
scaling such methods to high-dimensional action spaces remains challenging. To
advance efficient planning for high-dimensional continuous control, we propose
Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent
action codes with a state-conditional VQ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.03570v1' target='_blank'>Combining Planning, Reasoning and Reinforcement Learning to solve
  Industrial Robot Tasks</a></h2>
<p><strong>Authors:</strong> Matthias Mayr, Faseeh Ahmad, Konstantinos Chatzilygeroudis, Luigi Nardi, Volker Krueger</p>
<p><strong>Summary:</strong> One of today's goals for industrial robot systems is to allow fast and easy
provisioning for new tasks. Skill-based systems that use planning and knowledge
representation have long been one possible answer to this. However, especially
with contact-rich robot tasks that need careful parameter settings, such
reasoning techniques can fall short if the required knowledge not adequately
modeled. We show an approach that provides a combination of task-level planning
and reasoning with targeted learnin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.03358v1' target='_blank'>Cost-Effective Two-Stage Network Slicing for Edge-Cloud Orchestrated
  Vehicular Networks</a></h2>
<p><strong>Authors:</strong> Wen Wu, Kaige Qu, Peng Yang, Ning Zhang, Xuemin, Shen, Weihua Zhuang</p>
<p><strong>Summary:</strong> In this paper, we study a network slicing problem for edge-cloud orchestrated
vehicular networks, in which the edge and cloud servers are orchestrated to
process computation tasks for reducing network slicing cost while satisfying
the quality of service requirements. We propose a two-stage network slicing
framework, which consists of 1) network planning stage in a large timescale to
perform slice deployment, edge resource provisioning, and cloud resource
provisioning, and 2) network operation st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.03126v2' target='_blank'>Viewpoint Push Planning for Mapping of Unknown Confined Spaces</a></h2>
<p><strong>Authors:</strong> Nils Dengler, Sicong Pan, Vamsi Kalagaturu, Rohit Menon, Murad Dawood, Maren Bennewitz</p>
<p><strong>Summary:</strong> Viewpoint planning is an important task in any application where objects or
scenes need to be viewed from different angles to achieve sufficient coverage.
The mapping of confined spaces such as shelves is an especially challenging
task since objects occlude each other and the scene can only be observed from
the front, posing limitations on the possible viewpoints. In this paper, we
propose a deep reinforcement learning framework that generates promising views
aiming at reducing the map entropy. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.10165v2' target='_blank'>Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs</a></h2>
<p><strong>Authors:</strong> Junkai Zhang, Weitong Zhang, Quanquan Gu</p>
<p><strong>Summary:</strong> We study reward-free reinforcement learning (RL) with linear function
approximation, where the agent works in two phases: (1) in the exploration
phase, the agent interacts with the environment but cannot access the reward;
and (2) in the planning phase, the agent is given a reward function and is
expected to find a near-optimal policy based on samples collected in the
exploration phase. The sample complexities of existing reward-free algorithms
have a polynomial dependence on the planning horizo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.00732v1' target='_blank'>Leveraging Predictive Models for Adaptive Sampling of Spatiotemporal
  Fluid Processes</a></h2>
<p><strong>Authors:</strong> Sandeep Manjanna, Tom Z. Jiahao, M. Ani Hsieh</p>
<p><strong>Summary:</strong> Persistent monitoring of a spatiotemporal fluid process requires data
sampling and predictive modeling of the process being monitored. In this paper
we present PASST algorithm: Predictive-model based Adaptive Sampling of a
Spatio-Temporal process. PASST is an adaptive robotic sampling algorithm that
leverages predictive models to efficiently and persistently monitor a fluid
process in a given region of interest. Our algorithm makes use of the
predictions from a learned prediction model to plan a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.01468v3' target='_blank'>Probing the Multi-turn Planning Capabilities of LLMs via 20 Question
  Games</a></h2>
<p><strong>Authors:</strong> Yizhe Zhang, Jiarui Lu, Navdeep Jaitly</p>
<p><strong>Summary:</strong> Large language models (LLMs) are effective at answering questions that are
clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.05104v3' target='_blank'>An Autonomous Driving Model Integrated with BEV-V2X Perception, Fusion
  Prediction of Motion and Occupancy, and Driving Planning, in Complex Traffic
  Intersections</a></h2>
<p><strong>Authors:</strong> Fukang Li, Wenlin Ou, Kunpeng Gao, Yuwen Pang, Yifei Li, Henry Fan</p>
<p><strong>Summary:</strong> The comprehensiveness of vehicle-to-everything (V2X) recognition enriches and
holistically shapes the global Birds-Eye-View (BEV) perception, incorporating
rich semantics and integrating driving scene information, thereby serving
features of vehicle state prediction, decision-making and driving planning.
Utilizing V2X message sets to form BEV map proves to be an effective perception
method for connected and automated vehicles (CAVs). Specifically, Map Msg.
(MAP), Signal Phase And Timing (SPAT) a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.05784v1' target='_blank'>Graph-based Prediction and Planning Policy Network (GP3Net) for scalable
  self-driving in dynamic environments using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jayabrata Chowdhury, Venkataramanan Shivaraman, Suresh Sundaram, P B Sujit</p>
<p><strong>Summary:</strong> Recent advancements in motion planning for Autonomous Vehicles (AVs) show
great promise in using expert driver behaviors in non-stationary driving
environments. However, learning only through expert drivers needs more
generalizability to recover from domain shifts and near-failure scenarios due
to the dynamic behavior of traffic participants and weather conditions. A deep
Graph-based Prediction and Planning Policy Network (GP3Net) framework is
proposed for non-stationary environments that encode...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.07226v1' target='_blank'>Stitching Sub-Trajectories with Conditional Diffusion Model for
  Goal-Conditioned Offline RL</a></h2>
<p><strong>Authors:</strong> Sungyoon Kim, Yunseon Choi, Daiki E. Matsunaga, Kee-Eung Kim</p>
<p><strong>Summary:</strong> Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an
important problem in RL that focuses on acquiring diverse goal-oriented skills
solely from pre-collected behavior datasets. In this setting, the reward
feedback is typically absent except when the goal is achieved, which makes it
difficult to learn policies especially from a finite dataset of suboptimal
behaviors. In addition, realistic scenarios involve long-horizon planning,
which necessitates the extraction of useful skills ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.11658v3' target='_blank'>Dynamic planning in hierarchical active inference</a></h2>
<p><strong>Authors:</strong> Matteo Priorelli, Ivilin Peev Stoianov</p>
<p><strong>Summary:</strong> By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behaviors could be explained in terms of active
inference - either as discrete decision-m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.10454v2' target='_blank'>Partially Observable Task and Motion Planning with Uncertainty and Risk
  Awareness</a></h2>
<p><strong>Authors:</strong> Aidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka, Joshua Tenenbaum, Tom√°s Lozano-P√©rez, Leslie Pack Kaelbling</p>
<p><strong>Summary:</strong> Integrated task and motion planning (TAMP) has proven to be a valuable
approach to generalizable long-horizon robotic manipulation and navigation
problems. However, the typical TAMP problem formulation assumes full
observability and deterministic action effects. These assumptions limit the
ability of the planner to gather information and make decisions that are
risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness
(TAMPURA) that is capable of efficiently solving long-hor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.14314v2' target='_blank'>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</a></h2>
<p><strong>Authors:</strong> Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li</p>
<p><strong>Summary:</strong> Grounding the reasoning ability of large language models (LLMs) for embodied
tasks is challenging due to the complexity of the physical world. Especially,
LLM planning for multi-agent collaboration requires communication of agents or
credit assignment as the feedback to re-adjust the proposed plans and achieve
effective coordination. However, existing methods that overly rely on physical
verification or self-reflection suffer from excessive and inefficient querying
of LLMs. In this paper, we pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.08002v2' target='_blank'>Efficient Adaptation in Mixed-Motive Environments via Hierarchical
  Opponent Modeling and Planning</a></h2>
<p><strong>Authors:</strong> Yizhe Huang, Anji Liu, Fanqi Kong, Yaodong Yang, Song-Chun Zhu, Xue Feng</p>
<p><strong>Summary:</strong> Despite the recent successes of multi-agent reinforcement learning (MARL)
algorithms, efficiently adapting to co-players in mixed-motive environments
remains a significant challenge. One feasible approach is to hierarchically
model co-players' behavior based on inferring their characteristics. However,
these methods often encounter difficulties in efficient reasoning and
utilization of inferred information. To address these issues, we propose
Hierarchical Opponent modeling and Planning (HOP), a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.10403v1' target='_blank'>Cooperative Reward Shaping for Multi-Agent Pathfinding</a></h2>
<p><strong>Authors:</strong> Zhenyu Song, Ronghao Zheng, Senlin Zhang, Meiqin Liu</p>
<p><strong>Summary:</strong> The primary objective of Multi-Agent Pathfinding (MAPF) is to plan efficient
and conflict-free paths for all agents. Traditional multi-agent path planning
algorithms struggle to achieve efficient distributed path planning for multiple
agents. In contrast, Multi-Agent Reinforcement Learning (MARL) has been
demonstrated as an effective approach to achieve this objective. By modeling
the MAPF problem as a MARL problem, agents can achieve efficient path planning
and collision avoidance through distr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.13567v3' target='_blank'>Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation</a></h2>
<p><strong>Authors:</strong> Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Pascal Mettes, Fabio Galasso</p>
<p><strong>Summary:</strong> Autonomous robots are increasingly becoming a strong fixture in social
environments. Effective crowd navigation requires not only safe yet fast
planning, but should also enable interpretability and computational efficiency
for working in real-time on embedded devices. In this work, we advocate for
hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav.
Different from conventional reinforcement learning-based crowd navigation
methods, Hyp2Nav leverages the intrinsic properties of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.14185v1' target='_blank'>DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework
  Based on Large Language Models</a></h2>
<p><strong>Authors:</strong> Ziai Zhou, Bin Zhou, Hao Liu</p>
<p><strong>Summary:</strong> Real-time dynamic path planning in complex traffic environments presents
challenges, such as varying traffic volumes and signal wait times. Traditional
static routing algorithms like Dijkstra and A* compute shortest paths but often
fail under dynamic conditions. Recent Reinforcement Learning (RL) approaches
offer improvements but tend to focus on local optima, risking dead-ends or
boundary issues. This paper proposes a novel approach based on causal inference
for real-time dynamic path planning,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.09441v2' target='_blank'>PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for
  Quadrupedal Robot Locomotion</a></h2>
<p><strong>Authors:</strong> Aditya Shirwatkar, Naman Saxena, Kishore Chandra, Shishir Kolathaya</p>
<p><strong>Summary:</strong> A core strength of Model Predictive Control (MPC) for quadrupedal locomotion
has been its ability to enforce constraints and provide interpretability of the
sequence of commands over the horizon. However, despite being able to plan, MPC
struggles to scale with task complexity, often failing to achieve robust
behavior on rapidly changing surfaces. On the other hand, model-free
Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains,
showing emergent motions but inherently l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.11253v1' target='_blank'>Are Expressive Models Truly Necessary for Offline RL?</a></h2>
<p><strong>Authors:</strong> Guan Wang, Haoyi Niu, Jianxiong Li, Li Jiang, Jianming Hu, Xianyuan Zhan</p>
<p><strong>Summary:</strong> Among various branches of offline reinforcement learning (RL) methods,
goal-conditioned supervised learning (GCSL) has gained increasing popularity as
it formulates the offline RL problem as a sequential modeling task, therefore
bypassing the notoriously difficult credit assignment challenge of value
learning in conventional RL paradigm. Sequential modeling, however, requires
capturing accurate dynamics across long horizons in trajectory data to ensure
reasonable policy performance. To meet this...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.13072v2' target='_blank'>AdaWM: Adaptive World Model based Planning for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, Junshan Zhang</p>
<p><strong>Summary:</strong> World model based reinforcement learning (RL) has emerged as a promising
approach for autonomous driving, which learns a latent dynamics model and uses
it to train a planning policy. To speed up the learning process, the
pretrain-finetune paradigm is often used, where online RL is initialized by a
pretrained model and a policy learned offline. However, naively performing such
initialization in RL may result in dramatic performance degradation during the
online interactions in the new task. To ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.05453v1' target='_blank'>LLM-Powered Decentralized Generative Agents with Adaptive Hierarchical
  Knowledge Graph for Cooperative Planning</a></h2>
<p><strong>Authors:</strong> Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, Carlee Joe-Wong</p>
<p><strong>Summary:</strong> Developing intelligent agents for long-term cooperation in dynamic open-world
scenarios is a major challenge in multi-agent systems. Traditional Multi-agent
Reinforcement Learning (MARL) frameworks like centralized training
decentralized execution (CTDE) struggle with scalability and flexibility. They
require centralized long-term planning, which is difficult without custom
reward functions, and face challenges in processing multi-modal data. CTDE
approaches also assume fixed cooperation strateg...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1206.5287v1' target='_blank'>Policy Iteration for Relational MDPs</a></h2>
<p><strong>Authors:</strong> Chenggang Wang, Roni Khardon</p>
<p><strong>Summary:</strong> Relational Markov Decision Processes are a useful abstraction for complex
reinforcement learning problems and stochastic planning problems. Recent work
developed representation schemes and algorithms for planning in such problems
using the value iteration algorithm. However, exact versions of more complex
algorithms, including policy iteration, have not been developed or analyzed.
The paper investigates this potential and makes several contributions. First we
observe two anomalies for relational...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1512.06789v1' target='_blank'>Information-Theoretic Bounded Rationality</a></h2>
<p><strong>Authors:</strong> Pedro A. Ortega, Daniel A. Braun, Justin Dyer, Kee-Eung Kim, Naftali Tishby</p>
<p><strong>Summary:</strong> Bounded rationality, that is, decision-making and planning under resource
limitations, is widely regarded as an important open problem in artificial
intelligence, reinforcement learning, computational neuroscience and economics.
This paper offers a consolidated presentation of a theory of bounded
rationality based on information-theoretic ideas. We provide a conceptual
justification for using the free energy functional as the objective function
for characterizing bounded-rational decisions. This...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1602.04875v3' target='_blank'>POMDP-lite for Robust Robot Planning under Uncertainty</a></h2>
<p><strong>Authors:</strong> Min Chen, Emilio Frazzoli, David Hsu, Wee Sun Lee</p>
<p><strong>Summary:</strong> The partially observable Markov decision process (POMDP) provides a
principled general model for planning under uncertainty. However, solving a
general POMDP is computationally intractable in the worst case. This paper
introduces POMDP-lite, a subclass of POMDPs in which the hidden state variables
are constant or only change deterministically. We show that a POMDP-lite is
equivalent to a set of fully observable Markov decision processes indexed by a
hidden parameter and is useful for modeling a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1605.08478v1' target='_blank'>Model-Free Imitation Learning with Policy Optimization</a></h2>
<p><strong>Authors:</strong> Jonathan Ho, Jayesh K. Gupta, Stefano Ermon</p>
<p><strong>Summary:</strong> In imitation learning, an agent learns how to behave in an environment with
an unknown cost function by mimicking expert demonstrations. Existing imitation
learning algorithms typically involve solving a sequence of planning or
reinforcement learning problems. Such algorithms are therefore not directly
applicable to large, high-dimensional environments, and their performance can
significantly degrade if the planning problems are not solved to optimality.
Under the apprenticeship learning formali...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1609.05140v2' target='_blank'>The Option-Critic Architecture</a></h2>
<p><strong>Authors:</strong> Pierre-Luc Bacon, Jean Harb, Doina Precup</p>
<p><strong>Summary:</strong> Temporal abstraction is key to scaling up learning and planning in
reinforcement learning. While planning with temporally extended actions is well
understood, creating such abstractions autonomously from data has remained
challenging. We tackle this problem in the framework of options [Sutton, Precup
& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options
and propose a new option-critic architecture capable of learning both the
internal policies and the termination condition...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.06471v1' target='_blank'>Multi-Timescale, Gradient Descent, Temporal Difference Learning with
  Linear Options</a></h2>
<p><strong>Authors:</strong> Peeyush Kumar, Doina Precup</p>
<p><strong>Summary:</strong> Deliberating on large or continuous state spaces have been long standing
challenges in reinforcement learning. Temporal Abstraction have somewhat made
this possible, but efficiently planing using temporal abstraction still remains
an issue. Moreover using spatial abstractions to learn policies for various
situations at once while using temporal abstraction models is an open problem.
We propose here an efficient algorithm which is convergent under linear
function approximation while planning usin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1707.03497v2' target='_blank'>Value Prediction Network</a></h2>
<p><strong>Authors:</strong> Junhyuk Oh, Satinder Singh, Honglak Lee</p>
<p><strong>Summary:</strong> This paper proposes a novel deep reinforcement learning (RL) architecture,
called Value Prediction Network (VPN), which integrates model-free and
model-based RL methods into a single neural network. In contrast to typical
model-based RL methods, VPN learns a dynamics model whose abstract states are
trained to make option-conditional predictions of future values (discounted sum
of rewards) rather than of future observations. Our experimental results show
that VPN has several advantages over both ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1712.05812v6' target='_blank'>Occam's razor is insufficient to infer the preferences of irrational
  agents</a></h2>
<p><strong>Authors:</strong> Stuart Armstrong, S√∂ren Mindermann</p>
<p><strong>Summary:</strong> Inverse reinforcement learning (IRL) attempts to infer human rewards or
preferences from observed behavior. Since human planning systematically
deviates from rationality, several approaches have been tried to account for
specific human shortcomings. However, the general problem of inferring the
reward function of an agent of unknown rationality has received little
attention. Unlike the well-known ambiguity problems in IRL, this one is
practically relevant but cannot be resolved by observing the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1802.09180v1' target='_blank'>Cuttlefish: A Lightweight Primitive for Adaptive Query Processing</a></h2>
<p><strong>Authors:</strong> Tomer Kaftan, Magdalena Balazinska, Alvin Cheung, Johannes Gehrke</p>
<p><strong>Summary:</strong> Modern data processing applications execute increasingly sophisticated
analysis that requires operations beyond traditional relational algebra. As a
result, operators in query plans grow in diversity and complexity. Designing
query optimizer rules and cost models to choose physical operators for all of
these novel logical operators is impractical. To address this challenge, we
develop Cuttlefish, a new primitive for adaptively processing online query
plans that explores candidate physical operat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.11199v2' target='_blank'>Value Propagation Networks</a></h2>
<p><strong>Authors:</strong> Nantas Nardelli, Gabriel Synnaeve, Zeming Lin, Pushmeet Kohli, Philip H. S. Torr, Nicolas Usunier</p>
<p><strong>Summary:</strong> We present Value Propagation (VProp), a set of parameter-efficient
differentiable planning modules built on Value Iteration which can successfully
be trained using reinforcement learning to solve unseen tasks, has the
capability to generalize to larger map sizes, and can learn to navigate in
dynamic environments. We show that the modules enable learning to plan when the
environment also includes stochastic elements, providing a cost-efficient
learning system to build low-level size-invariant pla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.02095v1' target='_blank'>Space Navigator: a Tool for the Optimization of Collision Avoidance
  Maneuvers</a></h2>
<p><strong>Authors:</strong> Leonid Gremyachikh, Dmitrii Dubov, Nikita Kazeev, Andrey Kulibaba, Andrey Skuratov, Anton Tereshkin, Andrey Ustyuzhanin, Lubov Shiryaeva, Sergej Shishkin</p>
<p><strong>Summary:</strong> The number of space objects will grow several times in a few years due to the
planned launches of constellations of thousands microsatellites. It leads to a
significant increase in the threat of satellite collisions. Spacecraft must
undertake collision avoidance maneuvers to mitigate the risk. According to
publicly available information, conjunction events are now manually handled by
operators on the Earth. The manual maneuver planning requires qualified
personnel and will be impractical for con...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.03218v3' target='_blank'>Planning With Uncertain Specifications (PUnS)</a></h2>
<p><strong>Authors:</strong> Ankit Shah, Shen Li, Julie Shah</p>
<p><strong>Summary:</strong> Reward engineering is crucial to high performance in reinforcement learning
systems. Prior research into reward design has largely focused on Markovian
functions representing the reward. While there has been research into
expressing non-Markov rewards as linear temporal logic (LTL) formulas, this has
focused on task specifications directly defined by the user. However, in many
real-world applications, task specifications are ambiguous, and can only be
expressed as a belief over LTL formulas. In ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.13165v3' target='_blank'>Relational Graph Learning for Crowd Navigation</a></h2>
<p><strong>Authors:</strong> Changan Chen, Sha Hu, Payam Nikdel, Greg Mori, Manolis Savva</p>
<p><strong>Summary:</strong> We present a relational graph learning approach for robotic crowd navigation
using model-based deep reinforcement learning that plans actions by looking
into the future. Our approach reasons about the relations between all agents
based on their latent features and uses a Graph Convolutional Network to encode
higher-order interactions in each agent's state representation, which is
subsequently leveraged for state prediction and value estimation. The ability
to predict human motion allows us to pe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.09996v3' target='_blank'>Uncertainty-sensitive Learning and Planning with Ensembles</a></h2>
<p><strong>Authors:</strong> Piotr Mi≈Ço≈õ, ≈Åukasz Kuci≈Ñski, Konrad Czechowski, Piotr Kozakowski, Maciek Klimek</p>
<p><strong>Summary:</strong> We propose a reinforcement learning framework for discrete environments in
which an agent makes both strategic and tactical decisions. The former
manifests itself through the use of value function, while the latter is powered
by a tree search planner. These tools complement each other. The planning
module performs a local \textit{what-if} analysis, which allows to avoid
tactical pitfalls and boost backups of the value function. The value function,
being global in nature, compensates for inherent...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.00735v2' target='_blank'>Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based
  Plans</a></h2>
<p><strong>Authors:</strong> Nachiket Deo, Mohan M. Trivedi</p>
<p><strong>Summary:</strong> We address the problem of forecasting pedestrian and vehicle trajectories in
unknown environments, conditioned on their past motion and scene structure.
Trajectory forecasting is a challenging problem due to the large variation in
scene structure and the multimodal distribution of future trajectories. Unlike
prior approaches that directly learn one-to-many mappings from observed context
to multiple future trajectories, we propose to condition trajectory forecasts
on plans sampled from a grid bas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.00786v3' target='_blank'>Intelligent Roundabout Insertion using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Alessandro Paolo Capasso, Giulio Bacchiani, Daniele Molinari</p>
<p><strong>Summary:</strong> An important topic in the autonomous driving research is the development of
maneuver planning systems. Vehicles have to interact and negotiate with each
other so that optimal choices, in terms of time and safety, are taken. For this
purpose, we present a maneuver planning module able to negotiate the entering
in busy roundabouts. The proposed module is based on a neural network trained
to predict when and how entering the roundabout throughout the whole duration
of the maneuver. Our model is tra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.04032v2' target='_blank'>POPCORN: Partially Observed Prediction COnstrained ReiNforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Joseph Futoma, Michael C. Hughes, Finale Doshi-Velez</p>
<p><strong>Summary:</strong> Many medical decision-making tasks can be framed as partially observed Markov
decision processes (POMDPs). However, prevailing two-stage approaches that
first learn a POMDP and then solve it often fail because the model that best
fits the data may not be well suited for planning. We introduce a new
optimization objective that (a) produces both high-performing policies and
high-quality generative models, even when some observations are irrelevant for
planning, and (b) does so in batch off-policy ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.09228v1' target='_blank'>Context-aware Distribution of Fog Applications Using Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Nan Wang, Blesson Varghese</p>
<p><strong>Summary:</strong> Fog computing is an emerging paradigm that aims to meet the increasing
computation demands arising from the billions of devices connected to the
Internet. Offloading services of an application from the Cloud to the edge of
the network can improve the overall Quality-of-Service (QoS) of the application
since it can process data closer to user devices. Diverse Fog nodes ranging
from Wi-Fi routers to mini-clouds with varying resource capabilities makes it
challenging to determine which services of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.10268v1' target='_blank'>Path Planning for UAV-Mounted Mobile Edge Computing with Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Q. Liu, L. Shi, L. Sun, J. Li, M. Ding, F. Shu</p>
<p><strong>Summary:</strong> In this letter, we study an unmanned aerial vehicle (UAV)-mounted mobile edge
computing network, where the UAV executes computational tasks offloaded from
mobile terminal users (TUs) and the motion of each TU follows a Gauss-Markov
random model. To ensure the quality-of-service (QoS) of each TU, the UAV with
limited energy dynamically plans its trajectory according to the locations of
mobile TUs. Towards this end, we formulate the problem as a Markov decision
process, wherein the UAV trajectory ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.07381v2' target='_blank'>Spatial Concept-Based Navigation with Human Speech Instructions via
  Probabilistic Inference on Bayesian Generative Model</a></h2>
<p><strong>Authors:</strong> Akira Taniguchi, Yoshinobu Hagiwara, Tadahiro Taniguchi, Tetsunari Inamura</p>
<p><strong>Summary:</strong> Robots are required to not only learn spatial concepts autonomously but also
utilize such knowledge for various tasks in a domestic environment. Spatial
concept represents a multimodal place category acquired from the robot's
spatial experience including vision, speech-language, and self-position. The
aim of this study is to enable a mobile robot to perform navigational tasks
with human speech instructions, such as `Go to the kitchen', via probabilistic
inference on a Bayesian generative model u...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.11775v4' target='_blank'>SACBP: Belief Space Planning for Continuous-Time Dynamical Systems via
  Stochastic Sequential Action Control</a></h2>
<p><strong>Authors:</strong> Haruki Nishimura, Mac Schwager</p>
<p><strong>Summary:</strong> We propose a novel belief space planning technique for continuous dynamics by
viewing the belief system as a hybrid dynamical system with time-driven
switching. Our approach is based on the perturbation theory of differential
equations and extends Sequential Action Control to stochastic dynamics. The
resulting algorithm, which we name SACBP, does not require discretization of
spaces or time and synthesizes control signals in near real-time. SACBP is an
anytime algorithm that can handle general p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.03863v1' target='_blank'>Learning hierarchical behavior and motion planning for autonomous
  driving</a></h2>
<p><strong>Authors:</strong> Jingke Wang, Yue Wang, Dongkun Zhang, Yezhou Yang, Rong Xiong</p>
<p><strong>Summary:</strong> Learning-based driving solution, a new branch for autonomous driving, is
expected to simplify the modeling of driving by learning the underlying
mechanisms from data. To improve the tactical decision-making for
learning-based driving solution, we introduce hierarchical behavior and motion
planning (HBMP) to explicitly model the behavior in learning-based solution.
Due to the coupled action space of behavior and motion, it is challenging to
solve HBMP problem using reinforcement learning (RL) for...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.10190v1' target='_blank'>Learning to Track Dynamic Targets in Partially Known Environments</a></h2>
<p><strong>Authors:</strong> Heejin Jeong, Hamed Hassani, Manfred Morari, Daniel D. Lee, George J. Pappas</p>
<p><strong>Summary:</strong> We solve active target tracking, one of the essential tasks in autonomous
systems, using a deep reinforcement learning (RL) approach. In this problem, an
autonomous agent is tasked with acquiring information about targets of
interests using its onboard sensors. The classical challenges in this problem
are system model dependence and the difficulty of computing
information-theoretic cost functions for a long planning horizon. RL provides
solutions for these challenges as the length of its effecti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.05838v1' target='_blank'>Control as Hybrid Inference</a></h2>
<p><strong>Authors:</strong> Alexander Tschantz, Beren Millidge, Anil K. Seth, Christopher L. Buckley</p>
<p><strong>Summary:</strong> The field of reinforcement learning can be split into model-based and
model-free methods. Here, we unify these approaches by casting model-free
policy optimisation as amortised variational inference, and model-based
planning as iterative variational inference, within a `control as hybrid
inference' (CHI) framework. We present an implementation of CHI which naturally
mediates the balance between iterative and amortised inference. Using a
didactic experiment, we demonstrate that the proposed algor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.13363v2' target='_blank'>Learning Compositional Neural Programs for Continuous Control</a></h2>
<p><strong>Authors:</strong> Thomas Pierrot, Nicolas Perrin, Feryal Behbahani, Alexandre Laterre, Olivier Sigaud, Karim Beguir, Nando de Freitas</p>
<p><strong>Summary:</strong> We propose a novel solution to challenging sparse-reward, continuous control
problems that require hierarchical planning at multiple levels of abstraction.
Our solution, dubbed AlphaNPI-X, involves three separate stages of learning.
First, we use off-policy reinforcement learning algorithms with experience
replay to learn a set of atomic goal-conditioned policies, which can be easily
repurposed for many tasks. Second, we learn self-models describing the effect
of the atomic policies on the envir...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.14464v1' target='_blank'>Disentangled Planning and Control in Vision Based Robotics via Reward
  Machines</a></h2>
<p><strong>Authors:</strong> Alberto Camacho, Jacob Varley, Deepali Jain, Atil Iscen, Dmitry Kalashnikov</p>
<p><strong>Summary:</strong> In this work we augment a Deep Q-Learning agent with a Reward Machine (DQRM)
to increase speed of learning vision-based policies for robot tasks, and
overcome some of the limitations of DQN that prevent it from converging to
good-quality policies. A reward machine (RM) is a finite state machine that
decomposes a task into a discrete planning graph and equips the agent with a
reward function to guide it toward task completion. The reward machine can be
used for both reward shaping, and informing ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.02635v1' target='_blank'>qRRT: Quality-Biased Incremental RRT for Optimal Motion Planning in
  Non-Holonomic Systems</a></h2>
<p><strong>Authors:</strong> Nahas Pareekutty, Francis James, Balaraman Ravindran, Suril V. Shah</p>
<p><strong>Summary:</strong> This paper presents a sampling-based method for optimal motion planning in
non-holonomic systems in the absence of known cost functions. It uses the
principle of learning through experience to deduce the cost-to-go of regions
within the workspace. This cost information is used to bias an incremental
graph-based search algorithm that produces solution trajectories. Iterative
improvement of cost information and search biasing produces solutions that are
proven to be asymptotically optimal. The pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.06303v1' target='_blank'>Learning and Planning in Complex Action Spaces</a></h2>
<p><strong>Authors:</strong> Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, David Silver</p>
<p><strong>Summary:</strong> Many important real-world problems have action spaces that are
high-dimensional, continuous or both, making full enumeration of all possible
actions infeasible. Instead, only small subsets of actions can be sampled for
the purpose of policy evaluation and improvement. In this paper, we propose a
general framework to reason in a principled way about policy evaluation and
improvement over such sampled action subsets. This sample-based policy
iteration framework can in principle be applied to any r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.09203v1' target='_blank'>Learning from Demonstration without Demonstrations</a></h2>
<p><strong>Authors:</strong> Tom Blau, Gilad Francis, Philippe Morere</p>
<p><strong>Summary:</strong> State-of-the-art reinforcement learning (RL) algorithms suffer from high
sample complexity, particularly in the sparse reward case. A popular strategy
for mitigating this problem is to learn control policies by imitating a set of
expert demonstrations. The drawback of such approaches is that an expert needs
to produce demonstrations, which may be costly in practice. To address this
shortcoming, we propose Probabilistic Planning for Demonstration Discovery
(P2D2), a technique for automatically di...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.05143v3' target='_blank'>Bundled Gradients through Contact via Randomized Smoothing</a></h2>
<p><strong>Authors:</strong> H. J. Terry Suh, Tao Pang, Russ Tedrake</p>
<p><strong>Summary:</strong> The empirical success of derivative-free methods in reinforcement learning
for planning through contact seems at odds with the perceived fragility of
classical gradient-based optimization methods in these domains. What is causing
this gap, and how might we use the answer to improve gradient-based methods? We
believe a stochastic formulation of dynamics is one crucial ingredient. We use
tools from randomized smoothing to analyze sampling-based approximations of the
gradient, and formalize such ap...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.08490v2' target='_blank'>Integrating Deep Reinforcement and Supervised Learning to Expedite
  Indoor Mapping</a></h2>
<p><strong>Authors:</strong> Elchanan Zwecher, Eran Iceland, Sean R. Levy, Shmuel Y. Hayoun, Oren Gal, Ariel Barel</p>
<p><strong>Summary:</strong> The challenge of mapping indoor environments is addressed. Typical heuristic
algorithms for solving the motion planning problem are frontier-based methods,
that are especially effective when the environment is completely unknown.
However, in cases where prior statistical data on the environment's
architectonic features is available, such algorithms can be far from optimal.
Furthermore, their calculation time may increase substantially as more areas
are exposed. In this paper we propose two means...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.08973v1' target='_blank'>Hierarchical Policy for Non-prehensile Multi-object Rearrangement with
  Deep Reinforcement Learning and Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Fan Bai, Fei Meng, Jianbang Liu, Jiankun Wang, Max Q. -H. Meng</p>
<p><strong>Summary:</strong> Non-prehensile multi-object rearrangement is a robotic task of planning
feasible paths and transferring multiple objects to their predefined target
poses without grasping. It needs to consider how each object reaches the target
and the order of object movement, which significantly deepens the complexity of
the problem. To address these challenges, we propose a hierarchical policy to
divide and conquer for non-prehensile multi-object rearrangement. In the
high-level policy, guided by a designed p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.13978v1' target='_blank'>Identifying Reasoning Flaws in Planning-Based RL Using Tree Explanations</a></h2>
<p><strong>Authors:</strong> Kin-Ho Lam, Zhengxian Lin, Jed Irvine, Jonathan Dodge, Zeyad T Shureih, Roli Khanna, Minsuk Kahng, Alan Fern</p>
<p><strong>Summary:</strong> Enabling humans to identify potential flaws in an agent's decision making is
an important Explainable AI application. We consider identifying such flaws in
a planning-based deep reinforcement learning (RL) agent for a complex real-time
strategy game. In particular, the agent makes decisions via tree search using a
learned model and evaluation function over interpretable states and actions.
This gives the potential for humans to identify flaws at the level of reasoning
steps in the tree, even if ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1610.02847v1' target='_blank'>Situational Awareness by Risk-Conscious Skills</a></h2>
<p><strong>Authors:</strong> Daniel J. Mankowitz, Aviv Tamar, Shie Mannor</p>
<p><strong>Summary:</strong> Hierarchical Reinforcement Learning has been previously shown to speed up the
convergence rate of RL planning algorithms as well as mitigate feature-based
model misspecification (Mankowitz et. al. 2016a,b, Bacon 2015). To do so, it
utilizes hierarchical abstractions, also known as skills -- a type of
temporally extended action (Sutton et. al. 1999) to plan at a higher level,
abstracting away from the lower-level details. We incorporate risk sensitivity,
also referred to as Situational Awareness ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.00297v1' target='_blank'>Q-CP: Learning Action Values for Cooperative Planning</a></h2>
<p><strong>Authors:</strong> Francesco Riccio, Roberto Capobianco, Daniele Nardi</p>
<p><strong>Summary:</strong> Research on multi-robot systems has demonstrated promising results in
manifold applications and domains. Still, efficiently learning an effective
robot behaviors is very difficult, due to unstructured scenarios, high
uncertainties, and large state dimensionality (e.g. hyper-redundant and groups
of robot). To alleviate this problem, we present Q-CP a cooperative model-based
reinforcement learning algorithm, which exploits action values to both (1)
guide the exploration of the state space and (2) ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.00429v2' target='_blank'>Learning Human-Aware Path Planning with Fully Convolutional Networks</a></h2>
<p><strong>Authors:</strong> No√© P√©rez-Higueras, Fernando Caballero, Luis Merino</p>
<p><strong>Summary:</strong> This work presents an approach to learn path planning for robot social
navigation by demonstration. We make use of Fully Convolutional Neural Networks
(FCNs) to learn from expert's path demonstrations a map that marks a feasible
path to the goal as a classification problem. The use of FCNs allows us to
overcome the problem of manually designing/identifying the cost-map and
relevant features for the task of robot navigation. The method makes use of
optimal Rapidly-exploring Random Tree planner (R...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.11556v2' target='_blank'>Efficient and Trustworthy Social Navigation Via Explicit and Implicit
  Robot-Human Communication</a></h2>
<p><strong>Authors:</strong> Yuhang Che, Allison M. Okamura, Dorsa Sadigh</p>
<p><strong>Summary:</strong> In this paper, we present a planning framework that uses a combination of
implicit (robot motion) and explicit (visual/audio/haptic feedback)
communication during mobile robot navigation. First, we developed a model that
approximates both continuous movements and discrete behavior modes in human
navigation, considering the effects of implicit and explicit communication on
human decision making. The model approximates the human as an optimal agent,
with a reward function obtained through inverse ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.12612v2' target='_blank'>Learning Navigation Subroutines from Egocentric Videos</a></h2>
<p><strong>Authors:</strong> Ashish Kumar, Saurabh Gupta, Jitendra Malik</p>
<p><strong>Summary:</strong> Planning at a higher level of abstraction instead of low level torques
improves the sample efficiency in reinforcement learning, and computational
efficiency in classical planning. We propose a method to learn such
hierarchical abstractions, or subroutines from egocentric video data of experts
performing tasks. We learn a self-supervised inverse model on small amounts of
random interaction data to pseudo-label the expert egocentric videos with agent
actions. Visuomotor subroutines are acquired f...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.00030v2' target='_blank'>Policy-Aware Model Learning for Policy Gradient Methods</a></h2>
<p><strong>Authors:</strong> Romina Abachi, Mohammad Ghavamzadeh, Amir-massoud Farahmand</p>
<p><strong>Summary:</strong> This paper considers the problem of learning a model in model-based
reinforcement learning (MBRL). We examine how the planning module of an MBRL
algorithm uses the model, and propose that the model learning module should
incorporate the way the planner is going to use the model. This is in contrast
to conventional model learning approaches, such as those based on maximum
likelihood estimate, that learn a predictive model of the environment without
explicitly considering the interaction of the mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.01891v2' target='_blank'>Adaptive Online Distributed Optimal Control of Very-Large-Scale Robotic
  Systems</a></h2>
<p><strong>Authors:</strong> Pingping Zhu, Chang Liu, Silvia Ferrari</p>
<p><strong>Summary:</strong> This paper presents an adaptive online distributed optimal control approach
that is applicable to optimal planning for very-large-scale robotics systems in
highly uncertain environments. This approach is developed based on the optimal
mass transport theory. It is also viewed as an online reinforcement learning
and approximate dynamic programming approach in the Wasserstein-GMM space,
where a novel value functional is defined based on the probability density
functions of robots and the time-varyi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.10962v1' target='_blank'>What is the Reward for Handwriting? -- Handwriting Generation by
  Imitation Learning</a></h2>
<p><strong>Authors:</strong> Keisuke Kanda, Brian Kenji Iwana, Seiichi Uchida</p>
<p><strong>Summary:</strong> Analyzing the handwriting generation process is an important issue and has
been tackled by various generation models, such as kinematics based models and
stochastic models. In this study, we use a reinforcement learning (RL)
framework to realize handwriting generation with the careful future planning
ability. In fact, the handwriting process of human beings is also supported by
their future planning ability; for example, the ability is necessary to
generate a closed trajectory like '0' because a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.03125v1' target='_blank'>LBGP: Learning Based Goal Planning for Autonomous Following in Front</a></h2>
<p><strong>Authors:</strong> Payam Nikdel, Richard Vaughan, Mo Chen</p>
<p><strong>Summary:</strong> This paper investigates a hybrid solution which combines deep reinforcement
learning (RL) and classical trajectory planning for the following in front
application. Here, an autonomous robot aims to stay ahead of a person as the
person freely walks around. Following in front is a challenging problem as the
user's intended trajectory is unknown and needs to be estimated, explicitly or
implicitly, by the robot. In addition, the robot needs to find a feasible way
to safely navigate ahead of human tr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.09812v1' target='_blank'>Deep Latent Competition: Learning to Race Using Visual Control Policies
  in Latent Space</a></h2>
<p><strong>Authors:</strong> Wilko Schwarting, Tim Seyde, Igor Gilitschenski, Lucas Liebenwein, Ryan Sander, Sertac Karaman, Daniela Rus</p>
<p><strong>Summary:</strong> Learning competitive behaviors in multi-agent settings such as racing
requires long-term reasoning about potential adversarial interactions. This
paper presents Deep Latent Competition (DLC), a novel reinforcement learning
algorithm that learns competitive visual control policies through self-play in
imagination. The DLC agent imagines multi-agent interaction sequences in the
compact latent space of a learned world model that combines a joint transition
function with opponent viewpoint predictio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.10098v2' target='_blank'>Reward Signal Design for Autonomous Racing</a></h2>
<p><strong>Authors:</strong> Benjamin Evans, Herman A. Engelbrecht, Hendrik W. Jordaan</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has shown to be a valuable tool in training
neural networks for autonomous motion planning. The application of RL to a
specific problem is dependent on a reward signal to quantify how good or bad a
certain action is. This paper addresses the problem of reward signal design for
robotic control in the context of local planning for autonomous racing. We aim
to design reward signals that are able to perform well in multiple, competing,
continuous metrics. Three different ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.04088v1' target='_blank'>PEARL: Parallelized Expert-Assisted Reinforcement Learning for Scene
  Rearrangement Planning</a></h2>
<p><strong>Authors:</strong> Hanqing Wang, Zan Wang, Wei Liang, Lap-Fai Yu</p>
<p><strong>Summary:</strong> Scene Rearrangement Planning (SRP) is an interior task proposed recently. The
previous work defines the action space of this task with handcrafted
coarse-grained actions that are inflexible to be used for transforming scene
arrangement and intractable to be deployed in practice. Additionally, this new
task lacks realistic indoor scene rearrangement data to feed popular
data-hungry learning approaches and meet the needs of quantitative evaluation.
To address these problems, we propose a fine-grai...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.01068v1' target='_blank'>Time-based Dynamic Controllability of Disjunctive Temporal Networks with
  Uncertainty: A Tree Search Approach with Graph Neural Network Guidance</a></h2>
<p><strong>Authors:</strong> Kevin Osanlou, Jeremy Frank, J. Benton, Andrei Bursuc, Christophe Guettier, Eric Jacopin, Tristan Cazenave</p>
<p><strong>Summary:</strong> Scheduling in the presence of uncertainty is an area of interest in
artificial intelligence due to the large number of applications. We study the
problem of dynamic controllability (DC) of disjunctive temporal networks with
uncertainty (DTNU), which seeks a strategy to satisfy all constraints in
response to uncontrollable action durations. We introduce a more restricted,
stronger form of controllability than DC for DTNUs, time-based dynamic
controllability (TDC), and present a tree search approa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.01295v2' target='_blank'>MBDP: A Model-based Approach to Achieve both Robustness and Sample
  Efficiency via Double Dropout Planning</a></h2>
<p><strong>Authors:</strong> Wanpeng Zhang, Xi Xiao, Yao Yao, Mingzhe Chen, Dijun Luo</p>
<p><strong>Summary:</strong> Model-based reinforcement learning is a widely accepted solution for solving
excessive sample demands. However, the predictions of the dynamics models are
often not accurate enough, and the resulting bias may incur catastrophic
decisions due to insufficient robustness. Therefore, it is highly desired to
investigate how to improve the robustness of model-based RL algorithms while
maintaining high sampling efficiency. In this paper, we propose Model-Based
Double-dropout Planning (MBDP) to balance ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.03213v1' target='_blank'>Temporally Abstract Partial Models</a></h2>
<p><strong>Authors:</strong> Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, Doina Precup</p>
<p><strong>Summary:</strong> Humans and animals have the ability to reason and make predictions about
different courses of action at many time scales. In reinforcement learning,
option models (Sutton, Precup \& Singh, 1999; Precup, 2000) provide the
framework for this kind of temporally abstract prediction and reasoning.
Natural intelligent agents are also able to focus their attention on courses of
action that are relevant or feasible in a given situation, sometimes termed
affordable actions. In this paper, we define a not...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.12245v1' target='_blank'>Active Inference for Stochastic Control</a></h2>
<p><strong>Authors:</strong> Aswin Paul, Noor Sajid, Manoj Gopalkrishnan, Adeel Razi</p>
<p><strong>Summary:</strong> Active inference has emerged as an alternative approach to control problems
given its intuitive (probabilistic) formalism. However, despite its theoretical
utility, computational implementations have largely been restricted to
low-dimensional, deterministic settings. This paper highlights that this is a
consequence of the inability to adequately model stochastic transition
dynamics, particularly when an extensive policy (i.e., action trajectory) space
must be evaluated during planning. Fortunate...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00760v1' target='_blank'>AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for
  Dynamic Crowded Environment</a></h2>
<p><strong>Authors:</strong> Huifeng Guan, Yuan Gao, Min Zhao, Yong Yang, Fuqin Deng, Tin Lun Lam</p>
<p><strong>Summary:</strong> Multi-agent path finding in dynamic crowded environments is of great academic
and practical value for multi-robot systems in the real world. To improve the
effectiveness and efficiency of communication and learning process during path
planning in dynamic crowded environments, we introduce an algorithm called
Attention and BicNet based Multi-agent path planning with effective
reinforcement (AB-Mapper)under the actor-critic reinforcement learning
framework. In this framework, on the one hand, we u...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.12080v1' target='_blank'>C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks</a></h2>
<p><strong>Authors:</strong> Tianjun Zhang, Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, Joseph E. Gonzalez</p>
<p><strong>Summary:</strong> Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range
of domains, including navigation and manipulation, but learning to reach
distant goals remains a central challenge to the field. Learning to reach such
goals is particularly hard without any offline data, expert demonstrations, and
reward shaping. In this paper, we propose an algorithm to solve the distant
goal-reaching task by using search at training time to automatically generate a
curriculum of intermediate states. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.12840v1' target='_blank'>Self-Consistent Models and Values</a></h2>
<p><strong>Authors:</strong> Gregory Farquhar, Kate Baumli, Zita Marinho, Angelos Filos, Matteo Hessel, Hado van Hasselt, David Silver</p>
<p><strong>Summary:</strong> Learned models of the environment provide reinforcement learning (RL) agents
with flexible ways of making predictions about the environment. In particular,
models enable planning, i.e. using more computation to improve value functions
or policies, without requiring additional environment interactions. In this
work, we investigate a way of augmenting model-based RL, by additionally
encouraging a learned model and value function to be jointly
\emph{self-consistent}. Our approach differs from class...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.06803v1' target='_blank'>Two steps to risk sensitivity</a></h2>
<p><strong>Authors:</strong> Chris Gagne, Peter Dayan</p>
<p><strong>Summary:</strong> Distributional reinforcement learning (RL) -- in which agents learn about all
the possible long-term consequences of their actions, and not just the expected
value -- is of great recent interest. One of the most important affordances of
a distributional view is facilitating a modern, measured, approach to risk when
outcomes are not completely certain. By contrast, psychological and
neuroscientific investigations into decision making under risk have utilized a
variety of more venerable theoretica...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.03871v1' target='_blank'>Combining Learning-based Locomotion Policy with Model-based Manipulation
  for Legged Mobile Manipulators</a></h2>
<p><strong>Authors:</strong> Yuntao Ma, Farbod Farshidian, Takahiro Miki, Joonho Lee, Marco Hutter</p>
<p><strong>Summary:</strong> Deep reinforcement learning produces robust locomotion policies for legged
robots over challenging terrains. To date, few studies have leveraged
model-based methods to combine these locomotion skills with the precise control
of manipulators. Here, we incorporate external dynamics plans into
learning-based locomotion policies for mobile manipulation. We train the base
policy by applying a random wrench sequence on the robot base in simulation and
adding the noisified wrench sequence prediction to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.03634v2' target='_blank'>Multi-Agent Path Finding with Prioritized Communication Learning</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Hongjun Chen, Bo Jin, Wenzhe Tan, Hongyuan Zha, Xiangfeng Wang</p>
<p><strong>Summary:</strong> Multi-agent pathfinding (MAPF) has been widely used to solve large-scale
real-world problems, e.g., automation warehouses. The learning-based, fully
decentralized framework has been introduced to alleviate real-time problems and
simultaneously pursue optimal planning policy. However, existing methods might
generate significantly more vertex conflicts (or collisions), which lead to a
low success rate or more makespan. In this paper, we propose a PrIoritized
COmmunication learning method (PICO), w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.00352v1' target='_blank'>Affordance Learning from Play for Sample-Efficient Policy Learning</a></h2>
<p><strong>Authors:</strong> Jessica Borja-Diaz, Oier Mees, Gabriel Kalweit, Lukas Hermann, Joschka Boedecker, Wolfram Burgard</p>
<p><strong>Summary:</strong> Robots operating in human-centered environments should have the ability to
understand how objects function: what can be done with each object, where this
interaction may occur, and how the object is used to achieve a goal. To this
end, we propose a novel approach that extracts a self-supervised visual
affordance model from human teleoperated play data and leverages it to enable
efficient policy learning and motion planning. We combine model-based planning
with model-free deep reinforcement learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.02381v1' target='_blank'>Where to Look Next: Learning Viewpoint Recommendations for Informative
  Trajectory Planning</a></h2>
<p><strong>Authors:</strong> Max Lodel, Bruno Brito, √Ålvaro Serra-G√≥mez, Laura Ferranti, Robert Babu≈°ka, Javier Alonso-Mora</p>
<p><strong>Summary:</strong> Search missions require motion planning and navigation methods for
information gathering that continuously replan based on new observations of the
robot's surroundings. Current methods for information gathering, such as Monte
Carlo Tree Search, are capable of reasoning over long horizons, but they are
computationally expensive. An alternative for fast online execution is to
train, offline, an information gathering policy, which indirectly reasons about
the information value of new observations. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.12922v2' target='_blank'>Horizon-Free Reinforcement Learning in Polynomial Time: the Power of
  Stationary Policies</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Xiangyang Ji, Simon S. Du</p>
<p><strong>Summary:</strong> This paper gives the first polynomial-time algorithm for tabular Markov
Decision Processes (MDP) that enjoys a regret bound \emph{independent on the
planning horizon}. Specifically, we consider tabular MDP with $S$ states, $A$
actions, a planning horizon $H$, total reward bounded by $1$, and the agent
plays for $K$ episodes. We design an algorithm that achieves an
$O\left(\mathrm{poly}(S,A,\log K)\sqrt{K}\right)$ regret in contrast to
existing bounds which either has an additional $\mathrm{polyl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.10044v3' target='_blank'>Towards biologically plausible Dreaming and Planning in recurrent
  spiking networks</a></h2>
<p><strong>Authors:</strong> Cristiano Capone, Pier Stanislao Paolucci</p>
<p><strong>Summary:</strong> Humans and animals can learn new skills after practicing for a few hours,
while current reinforcement learning algorithms require a large amount of data
to achieve good performances. Recent model-based approaches show promising
results by reducing the number of necessary interactions with the environment
to learn a desirable policy. However, these methods require biological
implausible ingredients, such as the detailed storage of older experiences, and
long periods of offline learning. The optim...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.10464v1' target='_blank'>Synthesis from Satisficing and Temporal Goals</a></h2>
<p><strong>Authors:</strong> Suguman Bansal, Lydia Kavraki, Moshe Y. Vardi, Andrew Wells</p>
<p><strong>Summary:</strong> Reactive synthesis from high-level specifications that combine hard
constraints expressed in Linear Temporal Logic LTL with soft constraints
expressed by discounted-sum (DS) rewards has applications in planning and
reinforcement learning. An existing approach combines techniques from LTL
synthesis with optimization for the DS rewards but has failed to yield a sound
algorithm. An alternative approach combining LTL synthesis with satisficing DS
rewards (rewards that achieve a threshold) is sound a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.09897v1' target='_blank'>Successor Representation Active Inference</a></h2>
<p><strong>Authors:</strong> Beren Millidge, Christopher L Buckley</p>
<p><strong>Summary:</strong> Recent work has uncovered close links between between classical reinforcement
learning algorithms, Bayesian filtering, and Active Inference which lets us
understand value functions in terms of Bayesian posteriors. An alternative, but
less explored, model-free RL algorithm is the successor representation, which
expresses the value function in terms of a successor matrix of expected future
state occupancies. In this paper, we derive the probabilistic interpretation of
the successor representation ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.12644v2' target='_blank'>Learning Bipedal Walking On Planned Footsteps For Humanoid Robots</a></h2>
<p><strong>Authors:</strong> Rohan Pratap Singh, Mehdi Benallegue, Mitsuharu Morisawa, Rafael Cisneros, Fumio Kanehiro</p>
<p><strong>Summary:</strong> Deep reinforcement learning (RL) based controllers for legged robots have
demonstrated impressive robustness for walking in different environments for
several robot platforms. To enable the application of RL policies for humanoid
robots in real-world settings, it is crucial to build a system that can achieve
robust walking in any direction, on 2D and 3D terrains, and be controllable by
a user-command. In this paper, we tackle this problem by learning a policy to
follow a given step sequence. The...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.01776v1' target='_blank'>Indoor Path Planning for Multiple Unmanned Aerial Vehicles via
  Curriculum Learning</a></h2>
<p><strong>Authors:</strong> Jongmin Park, Kwansik Park</p>
<p><strong>Summary:</strong> Multi-agent reinforcement learning was performed in this study for indoor
path planning of two unmanned aerial vehicles (UAVs). Each UAV performed the
task of moving as fast as possible from a randomly paired initial position to a
goal position in an environment with obstacles. To minimize training time and
prevent the damage of UAVs, learning was performed by simulation. Considering
the non-stationary characteristics of the multi-agent environment wherein the
optimal behavior varies based on th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.11097v1' target='_blank'>Learning Agile Flight Maneuvers: Deep SE(3) Motion Planning and Control
  for Quadrotors</a></h2>
<p><strong>Authors:</strong> Yixiao Wang, Bingheng Wang, Shenning Zhang, Han Wei Sia, Lin Zhao</p>
<p><strong>Summary:</strong> Agile flights of autonomous quadrotors in cluttered environments require
constrained motion planning and control subject to translational and rotational
dynamics. Traditional model-based methods typically demand complicated design
and heavy computation. In this paper, we develop a novel deep reinforcement
learning-based method that tackles the challenging task of flying through a
dynamic narrow gate. We design a model predictive controller with its adaptive
tracking references parameterized by a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.14123v1' target='_blank'>Zero-Shot Retargeting of Learned Quadruped Locomotion Policies Using
  Hybrid Kinodynamic Model Predictive Control</a></h2>
<p><strong>Authors:</strong> He Li, Tingnan Zhang, Wenhao Yu, Patrick M. Wensing</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) has witnessed great strides for quadruped
locomotion, with continued progress in the reliable sim-to-real transfer of
policies. However, it remains a challenge to reuse a policy on another robot,
which could save time for retraining. In this work, we present a framework for
zero-shot policy retargeting wherein diverse motor skills can be transferred
between robots of different shapes and sizes. The new framework centers on a
planning-and-control pipeline that systemat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.00552v3' target='_blank'>Occlusion-Aware Crowd Navigation Using People as Sensors</a></h2>
<p><strong>Authors:</strong> Ye-Ji Mun, Masha Itkina, Shuijing Liu, Katherine Driggs-Campbell</p>
<p><strong>Summary:</strong> Autonomous navigation in crowded spaces poses a challenge for mobile robots
due to the highly dynamic, partially observable environment. Occlusions are
highly prevalent in such settings due to a limited sensor field of view and
obstructing human agents. Previous work has shown that observed interactive
behaviors of human agents can be used to estimate potential obstacles despite
occlusions. We propose integrating such social inference techniques into the
planning pipeline. We use a variational a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.02087v1' target='_blank'>Bilinear Exponential Family of MDPs: Frequentist Regret Bound with
  Tractable Exploration and Planning</a></h2>
<p><strong>Authors:</strong> Reda Ouhamma, Debabrota Basu, Odalric-Ambrym Maillard</p>
<p><strong>Summary:</strong> We study the problem of episodic reinforcement learning in continuous
state-action spaces with unknown rewards and transitions. Specifically, we
consider the setting where the rewards and transitions are modeled using
parametric bilinear exponential families. We propose an algorithm, BEF-RLSVI,
that a) uses penalized maximum likelihood estimators to learn the unknown
parameters, b) injects a calibrated Gaussian noise in the parameter of rewards
to ensure exploration, and c) leverages linearity o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.15839v1' target='_blank'>Continuous Neural Algorithmic Planners</a></h2>
<p><strong>Authors:</strong> Yu He, Petar Veliƒçkoviƒá, Pietro Li√≤, Andreea Deac</p>
<p><strong>Summary:</strong> Neural algorithmic reasoning studies the problem of learning algorithms with
neural networks, especially with graph architectures. A recent proposal, XLVIN,
reaps the benefits of using a graph neural network that simulates the value
iteration algorithm in deep reinforcement learning agents. It allows model-free
planning without access to privileged information about the environment, which
is usually unavailable. However, XLVIN only supports discrete action spaces,
and is hence nontrivially appli...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.01607v1' target='_blank'>A Hierarchical Approach for Strategic Motion Planning in Autonomous
  Racing</a></h2>
<p><strong>Authors:</strong> Rudolf Reiter, Jasper Hoffmann, Joschka Boedecker, Moritz Diehl</p>
<p><strong>Summary:</strong> We present an approach for safe trajectory planning, where a strategic task
related to autonomous racing is learned sample-efficient within a simulation
environment. A high-level policy, represented as a neural network, outputs a
reward specification that is used within the cost function of a parametric
nonlinear model predictive controller (NMPC). By including constraints and
vehicle kinematics in the NLP, we are able to guarantee safe and feasible
trajectories related to the used model. Compar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.02909v1' target='_blank'>Scalable Planning and Learning Framework Development for Swarm-to-Swarm
  Engagement Problems</a></h2>
<p><strong>Authors:</strong> Umut Demir, A. Sadik Satir, Gulay Goktas Sever, Cansu Yikilmaz, Nazim Kemal Ure</p>
<p><strong>Summary:</strong> Development of guidance, navigation and control frameworks/algorithms for
swarms attracted significant attention in recent years. That being said,
algorithms for planning swarm allocations/trajectories for engaging with enemy
swarms is largely an understudied problem. Although small-scale scenarios can
be addressed with tools from differential game theory, existing approaches fail
to scale for large-scale multi-agent pursuit evasion (PE) scenarios. In this
work, we propose a reinforcement learni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.13758v2' target='_blank'>Learning, Fast and Slow: A Goal-Directed Memory-Based Approach for
  Dynamic Environments</a></h2>
<p><strong>Authors:</strong> John Chong Min Tan, Mehul Motani</p>
<p><strong>Summary:</strong> Model-based next state prediction and state value prediction are slow to
converge. To address these challenges, we do the following: i) Instead of a
neural network, we do model-based planning using a parallel memory retrieval
system (which we term the slow mechanism); ii) Instead of learning state
values, we guide the agent's actions using goal-directed exploration, by using
a neural network to choose the next action given the current state and the goal
state (which we term the fast mechanism). ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.04387v2' target='_blank'>Catch Planner: Catching High-Speed Targets in the Flight</a></h2>
<p><strong>Authors:</strong> Huan Yu, Pengqin Wang, Jin Wang, Jialin Ji, Zhi Zheng, Jie Tu, Guodong Lu, Jun Meng, Meixin Zhu, Shaojie Shen, Fei Gao</p>
<p><strong>Summary:</strong> Catching high-speed targets in the flight is a complex and typical highly
dynamic task. In this paper, we propose Catch Planner, a planning-with-decision
scheme for catching. For sequential decision making, we propose a policy search
method based on deep reinforcement learning. In order to make catching adaptive
and flexible, we propose a trajectory optimization method to jointly optimize
the highly coupled catching time and terminal state while considering the
dynamic feasibility and safety. We...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.08856v1' target='_blank'>On the Benefits of Leveraging Structural Information in Planning Over
  the Learned Model</a></h2>
<p><strong>Authors:</strong> Jiajun Shen, Kananart Kuwaranancharoen, Raid Ayoub, Pietro Mercati, Shreyas Sundaram</p>
<p><strong>Summary:</strong> Model-based Reinforcement Learning (RL) integrates learning and planning and
has received increasing attention in recent years. However, learning the model
can incur a significant cost (in terms of sample complexity), due to the need
to obtain a sufficient number of samples for each state-action pair. In this
paper, we investigate the benefits of leveraging structural information about
the system in terms of reducing sample complexity. Specifically, we consider
the setting where the transition p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.14272v1' target='_blank'>Learning to Operate in Open Worlds by Adapting Planning Models</a></h2>
<p><strong>Authors:</strong> Wiktor Piotrowski, Roni Stern, Yoni Sher, Jacob Le, Matthew Klenk, Johan deKleer, Shiwali Mohan</p>
<p><strong>Summary:</strong> Planning agents are ill-equipped to act in novel situations in which their
domain model no longer accurately represents the world. We introduce an
approach for such agents operating in open worlds that detects the presence of
novelties and effectively adapts their domain models and consequent action
selection. It uses observations of action execution and measures their
divergence from what is expected, according to the environment model, to infer
existence of a novelty. Then, it revises the mode...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.10590v2' target='_blank'>A Review of Symbolic, Subsymbolic and Hybrid Methods for Sequential
  Decision Making</a></h2>
<p><strong>Authors:</strong> Carlos N√∫√±ez-Molina, Pablo Mesejo, Juan Fern√°ndez-Olivares</p>
<p><strong>Summary:</strong> In the field of Sequential Decision Making (SDM), two paradigms have
historically vied for supremacy: Automated Planning (AP) and Reinforcement
Learning (RL). In the spirit of reconciliation, this article reviews AP, RL and
hybrid methods (e.g., novel learn to plan techniques) for solving Sequential
Decision Processes (SDPs), focusing on their knowledge representation:
symbolic, subsymbolic, or a combination. Additionally, it also covers methods
for learning the SDP structure. Finally, we compar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.11941v4' target='_blank'>Efficient Dynamics Modeling in Interactive Environments with Koopman
  Theory</a></h2>
<p><strong>Authors:</strong> Arnab Kumar Mondal, Siba Smarak Panigrahi, Sai Rajeswar, Kaleem Siddiqi, Siamak Ravanbakhsh</p>
<p><strong>Summary:</strong> The accurate modeling of dynamics in interactive environments is critical for
successful long-range prediction. Such a capability could advance Reinforcement
Learning (RL) and Planning algorithms, but achieving it is challenging.
Inaccuracies in model estimates can compound, resulting in increased errors
over long horizons. We approach this problem from the lens of Koopman theory,
where the nonlinear dynamics of the environment can be linearized in a
high-dimensional latent space. This allows us...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.06758v1' target='_blank'>Layered controller synthesis for dynamic multi-agent systems</a></h2>
<p><strong>Authors:</strong> Emily Clement, Nicolas Perrin-Gilbert, Philipp Schlehuber-Caissier</p>
<p><strong>Summary:</strong> In this paper we present a layered approach for multi-agent control problem,
decomposed into three stages, each building upon the results of the previous
one. First, a high-level plan for a coarse abstraction of the system is
computed, relying on parametric timed automata augmented with stopwatches as
they allow to efficiently model simplified dynamics of such systems. In the
second stage, the high-level plan, based on SMT-formulation, mainly handles the
combinatorial aspects of the problem, pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.10085v3' target='_blank'>A Decision Making Framework for Recommended Maintenance of Road Segments</a></h2>
<p><strong>Authors:</strong> Haoyu Sun, Yan Yan</p>
<p><strong>Summary:</strong> Due to limited budgets allocated for road maintenance projects in various
countries, road management departments face difficulties in making scientific
maintenance decisions. This paper aims to provide road management departments
with more scientific decision tools and evidence. The framework proposed in
this paper mainly has the following four innovative points: 1) Predicting
pavement performance deterioration levels of road sections as decision basis
rather than accurately predicting specific ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.02754v1' target='_blank'>Pre- and post-contact policy decomposition for non-prehensile
  manipulation with zero-shot sim-to-real transfer</a></h2>
<p><strong>Authors:</strong> Minchan Kim, Junhyek Han, Jaehyung Kim, Beomjoon Kim</p>
<p><strong>Summary:</strong> We present a system for non-prehensile manipulation that require a
significant number of contact mode transitions and the use of environmental
contacts to successfully manipulate an object to a target location. Our method
is based on deep reinforcement learning which, unlike state-of-the-art planning
algorithms, does not require apriori knowledge of the physical parameters of
the object or environment such as friction coefficients or centers of mass. The
planning time is reduced to the simple fe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.15079v2' target='_blank'>Towards High Efficient Long-horizon Planning with Expert-guided
  Motion-encoding Tree Search</a></h2>
<p><strong>Authors:</strong> Tong Zhou, Erli Lyu, Jiaole Wang, Guangdu Cen, Ziqi Zha, Senmao Qi, Max Q. -H. Meng</p>
<p><strong>Summary:</strong> Autonomous driving holds promise for increased safety, optimized traffic
management, and a new level of convenience in transportation. While model-based
reinforcement learning approaches such as MuZero enables long-term planning,
the exponentially increase of the number of search nodes as the tree goes
deeper significantly effect the searching efficiency. To deal with this
problem, in this paper we proposed the expert-guided motion-encoding tree
search (EMTS) algorithm. EMTS extends the MuZero a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.16397v3' target='_blank'>Uncertainty-Aware Decision Transformer for Stochastic Driving
  Environments</a></h2>
<p><strong>Authors:</strong> Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao</p>
<p><strong>Summary:</strong> Offline Reinforcement Learning (RL) enables policy learning without active
interactions, making it especially appealing for self-driving tasks. Recent
successes of Transformers inspire casting offline RL as sequence modeling,
which, however, fails in stochastic environments with incorrect assumptions
that identical actions can consistently achieve the same goal. In this paper,
we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in
stochastic driving environments without ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.00971v1' target='_blank'>BeBOP -- Combining Reactive Planning and Bayesian Optimization to Solve
  Robotic Manipulation Tasks</a></h2>
<p><strong>Authors:</strong> Jonathan Styrud, Matthias Mayr, Erik Hellsten, Volker Krueger, Christian Smith</p>
<p><strong>Summary:</strong> Robotic systems for manipulation tasks are increasingly expected to be easy
to configure for new tasks. While in the past, robot programs were often
written statically and tuned manually, the current, faster transition times
call for robust, modular and interpretable solutions that also allow a robotic
system to learn how to perform a task. We propose the method Behavior-based
Bayesian Optimization and Planning (BeBOP) that combines two approaches for
generating behavior trees: we build the stru...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.04159v2' target='_blank'>Amortized Network Intervention to Steer the Excitatory Point Processes</a></h2>
<p><strong>Authors:</strong> Zitao Song, Wendi Ren, Shuang Li</p>
<p><strong>Summary:</strong> Excitatory point processes (i.e., event flows) occurring over dynamic graphs
(i.e., evolving topologies) provide a fine-grained model to capture how
discrete events may spread over time and space. How to effectively steer the
event flows by modifying the dynamic graph structures presents an interesting
problem, motivated by curbing the spread of infectious diseases through
strategically locking down cities to mitigating traffic congestion via traffic
light optimization. To address the intricacie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.17019v1' target='_blank'>Conditionally Combining Robot Skills using Large Language Models</a></h2>
<p><strong>Authors:</strong> K. R. Zentner, Ryan Julian, Brian Ichter, Gaurav S. Sukhatme</p>
<p><strong>Summary:</strong> This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call "Language-World," which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.04726v1' target='_blank'>Social Motion Prediction with Cognitive Hierarchies</a></h2>
<p><strong>Authors:</strong> Wentao Zhu, Jason Qin, Yuke Lou, Hang Ye, Xiaoxuan Ma, Hai Ci, Yizhou Wang</p>
<p><strong>Summary:</strong> Humans exhibit a remarkable capacity for anticipating the actions of others
and planning their own actions accordingly. In this study, we strive to
replicate this ability by addressing the social motion prediction problem. We
introduce a new benchmark, a novel formulation, and a cognition-inspired
framework. We present Wusi, a 3D multi-person motion dataset under the context
of team sports, which features intense and strategic human interactions and
diverse pose distributions. By reformulating t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.05596v1' target='_blank'>LLM Augmented Hierarchical Agents</a></h2>
<p><strong>Authors:</strong> Bharat Prakash, Tim Oates, Tinoosh Mohsenin</p>
<p><strong>Summary:</strong> Solving long-horizon, temporally-extended tasks using Reinforcement Learning
(RL) is challenging, compounded by the common practice of learning without
prior knowledge (or tabula rasa learning). Humans can generate and execute
plans with temporally-extended actions and quickly learn to perform new tasks
because we almost never solve problems from scratch. We want autonomous agents
to have this same ability. Recently, LLMs have been shown to encode a
tremendous amount of knowledge about the world...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.18636v1' target='_blank'>End-to-end Autonomous Driving using Deep Learning: A Systematic Review</a></h2>
<p><strong>Authors:</strong> Apoorv Singh</p>
<p><strong>Summary:</strong> End-to-end autonomous driving is a fully differentiable machine learning
system that takes raw sensor input data and other metadata as prior information
and directly outputs the ego vehicle's control signals or planned trajectories.
This paper attempts to systematically review all recent Machine Learning-based
techniques to perform this end-to-end task, including, but not limited to,
object detection, semantic scene understanding, object tracking, trajectory
predictions, trajectory planning, veh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.02478v1' target='_blank'>RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum
  Handoffs, Disconnectivity, and Energy Consumption</a></h2>
<p><strong>Authors:</strong> Nesrine Cherif, Wael Jaafar, Halim Yanikomeroglu, Abbas Yongacoglu</p>
<p><strong>Summary:</strong> Unmanned aerial vehicle (UAV) is a promising technology for last-mile cargo
delivery. However, the limited on-board battery capacity, cellular
unreliability, and frequent handoffs in the airspace are the main obstacles to
unleash its full potential. Given that existing cellular networks were
primarily designed to service ground users, re-utilizing the same architecture
for highly mobile aerial users, e.g., cargo-UAVs, is deemed challenging.
Indeed, to ensure a safe delivery using cargo-UAVs, it ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.03223v1' target='_blank'>Hierarchical RL-Guided Large-scale Navigation of a Snake Robot</a></h2>
<p><strong>Authors:</strong> Shuo Jiang, Adarsh Salagame, Alireza Ramezani, Lawson Wong</p>
<p><strong>Summary:</strong> Classical snake robot control leverages mimicking snake-like gaits tuned for
specific environments. However, to operate adaptively in unstructured
environments, gait generation must be dynamically scheduled. In this work, we
present a four-layer hierarchical control scheme to enable the snake robot to
navigate freely in large-scale environments. The proposed model decomposes
navigation into global planning, local planning, gait generation, and gait
tracking. Using reinforcement learning (RL) and...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.00006v3' target='_blank'>Building Open-Ended Embodied Agent via Language-Policy Bidirectional
  Adaptation</a></h2>
<p><strong>Authors:</strong> Shaopeng Zhai, Jie Wang, Tianyi Zhang, Fuxian Huang, Qi Zhang, Ming Zhou, Jing Hou, Yu Qiao, Yu Liu</p>
<p><strong>Summary:</strong> Building embodied agents on integrating Large Language Models (LLMs) and
Reinforcement Learning (RL) have revolutionized human-AI interaction:
researchers can now leverage language instructions to plan decision-making for
open-ended tasks. However, existing research faces challenges in meeting the
requirement of open-endedness. They typically either train LLM/RL models to
adapt to a fixed counterpart, limiting exploration of novel skills and
hindering the efficacy of human-AI interaction. To thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.03546v1' target='_blank'>NovelGym: A Flexible Ecosystem for Hybrid Planning and Learning Agents
  Designed for Open Worlds</a></h2>
<p><strong>Authors:</strong> Shivam Goel, Yichen Wei, Panagiotis Lymperopoulos, Klara Chura, Matthias Scheutz, Jivko Sinapov</p>
<p><strong>Summary:</strong> As AI agents leave the lab and venture into the real world as autonomous
vehicles, delivery robots, and cooking robots, it is increasingly necessary to
design and comprehensively evaluate algorithms that tackle the ``open-world''.
To this end, we introduce NovelGym, a flexible and adaptable ecosystem designed
to simulate gridworld environments, serving as a robust platform for
benchmarking reinforcement learning (RL) and hybrid planning and learning
agents in open-world contexts. The modular arc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.11977v3' target='_blank'>Adaptive Motion Planning for Multi-fingered Functional Grasp via Force
  Feedback</a></h2>
<p><strong>Authors:</strong> Dongying Tian, Xiangbo Lin, Yi Sun</p>
<p><strong>Summary:</strong> Enabling multi-fingered robots to grasp and manipulate objects with
human-like dexterity is especially challenging during the dynamic, continuous
hand-object interactions. Closed-loop feedback control is essential for
dexterous hands to dynamically finetune hand poses when performing precise
functional grasps. This work proposes an adaptive motion planning method based
on deep reinforcement learning to adjust grasping poses according to real-time
feedback from joint torques from pre-grasp to goa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.17835v2' target='_blank'>Simplifying Latent Dynamics with Softly State-Invariant World Models</a></h2>
<p><strong>Authors:</strong> Tankred Saanum, Peter Dayan, Eric Schulz</p>
<p><strong>Summary:</strong> To solve control problems via model-based reasoning or planning, an agent
needs to know how its actions affect the state of the world. The actions an
agent has at its disposal often change the state of the environment in
systematic ways. However, existing techniques for world modelling do not
guarantee that the effect of actions are represented in such systematic ways.
We introduce the Parsimonious Latent Space Model (PLSM), a world model that
regularizes the latent dynamics to make the effect o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.08772v3' target='_blank'>Optimal Task Assignment and Path Planning using Conflict-Based Search
  with Precedence and Temporal Constraints</a></h2>
<p><strong>Authors:</strong> Yu Quan Chong, Jiaoyang Li, Katia Sycara</p>
<p><strong>Summary:</strong> The Multi-Agent Path Finding (MAPF) problem entails finding collision-free
paths for a set of agents, guiding them from their start to goal locations.
However, MAPF does not account for several practical task-related constraints.
For example, agents may need to perform actions at goal locations with specific
execution times, adhering to predetermined orders and timeframes. Moreover,
goal assignments may not be predefined for agents, and the optimization
objective may lack an explicit definition....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.17967v1' target='_blank'>Imitation-regularized Optimal Transport on Networks: Provable Robustness
  and Application to Logistics Planning</a></h2>
<p><strong>Authors:</strong> Koshi Oishi, Yota Hashizume, Tomohiko Jimbo, Hirotaka Kaji, Kenji Kashima</p>
<p><strong>Summary:</strong> Network systems form the foundation of modern society, playing a critical
role in various applications. However, these systems are at significant risk of
being adversely affected by unforeseen circumstances, such as disasters.
Considering this, there is a pressing need for research to enhance the
robustness of network systems. Recently, in reinforcement learning, the
relationship between acquiring robustness and regularizing entropy has been
identified. Additionally, imitation learning is used w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.15301v2' target='_blank'>Planning with a Learned Policy Basis to Optimally Solve Complex Tasks</a></h2>
<p><strong>Authors:</strong> Guillermo Infante, David Kuric, Anders Jonsson, Vicen√ß G√≥mez, Herke van Hoof</p>
<p><strong>Summary:</strong> Conventional reinforcement learning (RL) methods can successfully solve a
wide range of sequential decision problems. However, learning policies that can
generalize predictably across multiple tasks in a setting with non-Markovian
reward specifications is a challenging problem. We propose to use successor
features to learn a policy basis so that each (sub)policy in it solves a
well-defined subproblem. In a task described by a finite state automaton (FSA)
that involves the same set of subproblems...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.03888v2' target='_blank'>A proximal policy optimization based intelligent home solar management</a></h2>
<p><strong>Authors:</strong> Kode Creer, Imitiaz Parvez</p>
<p><strong>Summary:</strong> In the smart grid, the prosumers can sell unused electricity back to the
power grid, assuming the prosumers own renewable energy sources and storage
units. The maximizing of their profits under a dynamic electricity market is a
problem that requires intelligent planning. To address this, we propose a
framework based on Proximal Policy Optimization (PPO) using recurrent rewards.
By using the information about the rewards modeled effectively with PPO to
maximize our objective, we were able to get ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.04292v5' target='_blank'>Conversational Disease Diagnosis via External Planner-Controlled Large
  Language Models</a></h2>
<p><strong>Authors:</strong> Zhoujian Sun, Cheng Luo, Ziyi Liu, Zhengxing Huang</p>
<p><strong>Summary:</strong> The development of large language models (LLMs) has brought unprecedented
possibilities for artificial intelligence (AI) based medical diagnosis.
However, the application perspective of LLMs in real diagnostic scenarios is
still unclear because they are not adept at collecting patient data
proactively. This study presents a LLM-based diagnostic system that enhances
planning capabilities by emulating doctors. Our system involves two external
planners to handle planning tasks. The first planner em...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.08855v1' target='_blank'>WROOM: An Autonomous Driving Approach for Off-Road Navigation</a></h2>
<p><strong>Authors:</strong> Dvij Kalaria, Shreya Sharma, Sarthak Bhagat, Haoru Xue, John M. Dolan</p>
<p><strong>Summary:</strong> Off-road navigation is a challenging problem both at the planning level to
get a smooth trajectory and at the control level to avoid flipping over,
hitting obstacles, or getting stuck at a rough patch. There have been several
recent works using classical approaches involving depth map prediction followed
by smooth trajectory planning and using a controller to track it. We design an
end-to-end reinforcement learning (RL) system for an autonomous vehicle in
off-road environments using a custom-des...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.10042v1' target='_blank'>Optimizing Search and Rescue UAV Connectivity in Challenging Terrain
  through Multi Q-Learning</a></h2>
<p><strong>Authors:</strong> Mohammed M. H. Qazzaz, Syed A. R. Zaidi, Desmond C. McLernon, Abdelaziz Salama, Aubida A. Al-Hameed</p>
<p><strong>Summary:</strong> Using Unmanned Aerial Vehicles (UAVs) in Search and rescue operations (SAR)
to navigate challenging terrain while maintaining reliable communication with
the cellular network is a promising approach. This paper suggests a novel
technique employing a reinforcement learning multi Q-learning algorithm to
optimize UAV connectivity in such scenarios. We introduce a Strategic Planning
Agent for efficient path planning and collision awareness and a Real-time
Adaptive Agent to maintain optimal connectio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.17272v2' target='_blank'>DPN: Decoupling Partition and Navigation for Neural Solvers of Min-max
  Vehicle Routing Problems</a></h2>
<p><strong>Authors:</strong> Zhi Zheng, Shunyu Yao, Zhenkun Wang, Xialiang Tong, Mingxuan Yuan, Ke Tang</p>
<p><strong>Summary:</strong> The min-max vehicle routing problem (min-max VRP) traverses all given
customers by assigning several routes and aims to minimize the length of the
longest route. Recently, reinforcement learning (RL)-based sequential planning
methods have exhibited advantages in solving efficiency and optimality.
However, these methods fail to exploit the problem-specific properties in
learning representations, resulting in less effective features for decoding
optimal routes. This paper considers the sequential ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.18703v2' target='_blank'>Bridging the Gap between Partially Observable Stochastic Games and
  Sparse POMDP Methods</a></h2>
<p><strong>Authors:</strong> Tyler Becker, Zachary Sunberg</p>
<p><strong>Summary:</strong> Many real-world decision problems involve the interaction of multiple
self-interested agents with limited sensing ability. The partially observable
stochastic game (POSG) provides a mathematical framework for modeling these
problems, however solving a POSG requires difficult reasoning over two critical
factors: (1) information revealed by partial observations and (2) decisions
other agents make. In the single agent case, partially observable Markov
decision process (POMDP) planning can efficient...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.19045v2' target='_blank'>To RL or not to RL? An Algorithmic Cheat-Sheet for AI-Based Radio
  Resource Management</a></h2>
<p><strong>Authors:</strong> Lorenzo Maggi, Matthew Andrews, Ryo Koblitz</p>
<p><strong>Summary:</strong> Several Radio Resource Management (RRM) use cases can be framed as sequential
decision planning problems, where an agent (the base station, typically) makes
decisions that influence the network utility and state. While Reinforcement
Learning (RL) in its general form can address this scenario, it is known to be
sample inefficient. Following the principle of Occam's razor, we argue that the
choice of the solution technique for RRM should be guided by questions such as,
"Is it a short or long-term ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.00490v2' target='_blank'>Research on the Application of Computer Vision Based on Deep Learning in
  Autonomous Driving Technology</a></h2>
<p><strong>Authors:</strong> Jingyu Zhang, Jin Cao, Jinghao Chang, Xinjin Li, Houze Liu, Zhenglin Li</p>
<p><strong>Summary:</strong> This research aims to explore the application of deep learning in autonomous
driving computer vision technology and its impact on improving system
performance. By using advanced technologies such as convolutional neural
networks (CNN), multi-task joint learning methods, and deep reinforcement
learning, this article analyzes in detail the application of deep learning in
image recognition, real-time target tracking and classification, environment
perception and decision support, and path planning ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.03200v1' target='_blank'>Adaptive Distance Functions via Kelvin Transformation</a></h2>
<p><strong>Authors:</strong> Rafael I. Cabral Muchacho, Florian T. Pokorny</p>
<p><strong>Summary:</strong> The term safety in robotics is often understood as a synonym for avoidance.
Although this perspective has led to progress in path planning and reactive
control, a generalization of this perspective is necessary to include task
semantics relevant to contact-rich manipulation tasks, especially during
teleoperation and to ensure the safety of learned policies.
  We introduce the semantics-aware distance function and a corresponding
computational method based on the Kelvin Transformation. The semant...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.15225v1' target='_blank'>Deep UAV Path Planning with Assured Connectivity in Dense Urban Setting</a></h2>
<p><strong>Authors:</strong> Jiyong Oh, Syed M. Raza, Lusungu J. Mwasinga, Moonseong Kim, Hyunseung Choo</p>
<p><strong>Summary:</strong> Unmanned Ariel Vehicle (UAV) services with 5G connectivity is an emerging
field with numerous applications. Operator-controlled UAV flights and manual
static flight configurations are major limitations for the wide adoption of
scalability of UAV services. Several services depend on excellent UAV
connectivity with a cellular network and maintaining it is challenging in
predetermined flight paths. This paper addresses these limitations by proposing
a Deep Reinforcement Learning (DRL) framework for...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.06931v1' target='_blank'>A Unified Approach to Multi-task Legged Navigation: Temporal Logic Meets
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jesse Jiang, Samuel Coogan, Ye Zhao</p>
<p><strong>Summary:</strong> This study examines the problem of hopping robot navigation planning to
achieve simultaneous goal-directed and environment exploration tasks. We
consider a scenario in which the robot has mandatory goal-directed tasks
defined using Linear Temporal Logic (LTL) specifications as well as optional
exploration tasks represented using a reward function. Additionally, there
exists uncertainty in the robot dynamics which results in motion perturbation.
We first propose an abstraction of 3D hopping robot...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.07086v2' target='_blank'>Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks
  with Large Language Models</a></h2>
<p><strong>Authors:</strong> Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, Nick Haber</p>
<p><strong>Summary:</strong> Multi-agent reinforcement learning (MARL) methods struggle with the
non-stationarity of multi-agent systems and fail to adaptively learn online
when tested with novel agents. Here, we leverage large language models (LLMs)
to create an autonomous agent that can handle these challenges. Our agent,
Hypothetical Minds, consists of a cognitively-inspired architecture, featuring
modular components for perception, memory, and hierarchical planning over two
levels of abstraction. We introduce the Theory...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.01639v2' target='_blank'>Coordinating Planning and Tracking in Layered Control Policies via
  Actor-Critic Learning</a></h2>
<p><strong>Authors:</strong> Fengjun Yang, Nikolai Matni</p>
<p><strong>Summary:</strong> We propose a reinforcement learning (RL)-based algorithm to jointly train (1)
a trajectory planner and (2) a tracking controller in a layered control
architecture. Our algorithm arises naturally from a rewrite of the underlying
optimal control problem that lends itself to an actor-critic learning approach.
By explicitly learning a \textit{dual} network to coordinate the interaction
between the planning and tracking layers, we demonstrate the ability to achieve
an effective consensus between the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.03768v1' target='_blank'>HDPlanner: Advancing Autonomous Deployments in Unknown Environments
  through Hierarchical Decision Networks</a></h2>
<p><strong>Authors:</strong> Jingsong Liang, Yuhong Cao, Yixiao Ma, Hanqi Zhao, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> In this paper, we introduce HDPlanner, a deep reinforcement learning (DRL)
based framework designed to tackle two core and challenging tasks for mobile
robots: autonomous exploration and navigation, where the robot must optimize
its trajectory adaptively to achieve the task objective through continuous
interactions in unknown environments. Specifically, HDPlanner relies on novel
hierarchical attention networks to empower the robot to reason about its belief
across multiple spatial scales and seq...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.14826v3' target='_blank'>ToolPlanner: A Tool Augmented LLM for Multi Granularity Instructions
  with Path Planning and Feedback</a></h2>
<p><strong>Authors:</strong> Qinzhuo Wu, Wei Liu, Jian Luan, Bin Wang</p>
<p><strong>Summary:</strong> Recently, tool-augmented LLMs have gained increasing attention. Given an
instruction, tool-augmented LLMs can interact with various external tools in
multiple rounds and provide a final answer. However, previous LLMs were trained
on overly detailed instructions, which included API names or parameters, while
real users would not explicitly mention these API details. This leads to a gap
between trained LLMs and real-world scenarios. In addition, most works ignore
whether the interaction process fo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.05364v1' target='_blank'>Diffusion Model Predictive Control</a></h2>
<p><strong>Authors:</strong> Guangyao Zhou, Sivaramakrishnan Swaminathan, Rajkumar Vasudeva Raju, J. Swaroop Guntupalli, Wolfgang Lehrach, Joseph Ortiz, Antoine Dedieu, Miguel L√°zaro-Gredilla, Kevin Murphy</p>
<p><strong>Summary:</strong> We propose Diffusion Model Predictive Control (D-MPC), a novel MPC approach
that learns a multi-step action proposal and a multi-step dynamics model, both
using diffusion models, and combines them for use in online MPC. On the popular
D4RL benchmark, we show performance that is significantly better than existing
model-based offline planning methods using MPC and competitive with
state-of-the-art (SOTA) model-based and model-free reinforcement learning
methods. We additionally illustrate D-MPC's ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.23156v1' target='_blank'>VisualPredicator: Learning Abstract World Models with Neuro-Symbolic
  Predicates for Robot Planning</a></h2>
<p><strong>Authors:</strong> Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, Jo√£o F. Henriques, Kevin Ellis</p>
<p><strong>Summary:</strong> Broadly intelligent agents should form task-specific abstractions that
selectively expose the essential elements of a task, while abstracting away the
complexity of the raw sensorimotor space. In this work, we present
Neuro-Symbolic Predicates, a first-order abstraction language that combines the
strengths of symbolic and neural knowledge representations. We outline an
online algorithm for inventing such predicates and learning abstract world
models. We compare our approach to hierarchical reinf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.17137v1' target='_blank'>Self-reconfiguration Strategies for Space-distributed Spacecraft</a></h2>
<p><strong>Authors:</strong> Tianle Liu, Zhixiang Wang, Yongwei Zhang, Ziwei Wang, Zihao Liu, Yizhai Zhang, Panfeng Huang</p>
<p><strong>Summary:</strong> This paper proposes a distributed on-orbit spacecraft assembly algorithm,
where future spacecraft can assemble modules with different functions on orbit
to form a spacecraft structure with specific functions. This form of spacecraft
organization has the advantages of reconfigurability, fast mission response and
easy maintenance. Reasonable and efficient on-orbit self-reconfiguration
algorithms play a crucial role in realizing the benefits of distributed
spacecraft. This paper adopts the framewor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.05528v1' target='_blank'>AI Planning: A Primer and Survey (Preliminary Report)</a></h2>
<p><strong>Authors:</strong> Dillon Z. Chen, Pulkit Verma, Siddharth Srivastava, Michael Katz, Sylvie Thi√©baux</p>
<p><strong>Summary:</strong> Automated decision-making is a fundamental topic that spans multiple
sub-disciplines in AI: reinforcement learning (RL), AI planning (AP),
foundation models, and operations research, among others. Despite recent
efforts to ``bridge the gaps'' between these communities, there remain many
insights that have not yet transcended the boundaries. Our goal in this paper
is to provide a brief and non-exhaustive primer on ideas well-known in AP, but
less so in other sub-disciplines. We do so by introduci...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.10599v1' target='_blank'>Advances in Transformers for Robotic Applications: A Review</a></h2>
<p><strong>Authors:</strong> Nikunj Sanghai, Nik Bear Brown</p>
<p><strong>Summary:</strong> The introduction of Transformers architecture has brought about significant
breakthroughs in Deep Learning (DL), particularly within Natural Language
Processing (NLP). Since their inception, Transformers have outperformed many
traditional neural network architectures due to their "self-attention"
mechanism and their scalability across various applications. In this paper, we
cover the use of Transformers in Robotics. We go through recent advances and
trends in Transformer architectures and examin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.03286v1' target='_blank'>Conditional Prediction by Simulation for Automated Driving</a></h2>
<p><strong>Authors:</strong> Fabian Konstantinidis, Moritz Sackmann, Ulrich Hofmann, Christoph Stiller</p>
<p><strong>Summary:</strong> Modular automated driving systems commonly handle prediction and planning as
sequential, separate tasks, thereby prohibiting cooperative maneuvers. To
enable cooperative planning, this work introduces a prediction model that
models the conditional dependencies between trajectories. For this, predictions
are generated by a microscopic traffic simulation, with the individual traffic
participants being controlled by a realistic behavior model trained via
Adversarial Inverse Reinforcement Learning. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.06813v1' target='_blank'>Policy Guided Tree Search for Enhanced LLM Reasoning</a></h2>
<p><strong>Authors:</strong> Yang Li</p>
<p><strong>Summary:</strong> Despite their remarkable capabilities, large language models often struggle
with tasks requiring complex reasoning and planning. While existing approaches
like Chain-of-Thought prompting and tree search techniques show promise, they
are limited by their reliance on predefined heuristics and computationally
expensive exploration strategies. We propose Policy-Guided Tree Search (PGTS),
a framework that combines reinforcement learning with structured tree
exploration to efficiently navigate reasoni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.10148v1' target='_blank'>Cooperative Multi-Agent Planning with Adaptive Skill Synthesis</a></h2>
<p><strong>Authors:</strong> Zhiyuan Li, Wenshuai Zhao, Joni Pajarinen</p>
<p><strong>Summary:</strong> Despite much progress in training distributed artificial intelligence (AI),
building cooperative multi-agent systems with multi-agent reinforcement
learning (MARL) faces challenges in sample efficiency, interpretability, and
transferability. Unlike traditional learning-based methods that require
extensive interaction with the environment, large language models (LLMs)
demonstrate remarkable capabilities in zero-shot planning and complex
reasoning. However, existing LLM-based approaches heavily re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.08029v1' target='_blank'>Planning to Learn: A Novel Algorithm for Active Learning during
  Model-Based Planning</a></h2>
<p><strong>Authors:</strong> Rowan Hodson, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, Ryan Smith</p>
<p><strong>Summary:</strong> Active Inference is a recent framework for modeling planning under
uncertainty. Empirical and theoretical work have now begun to evaluate the
strengths and weaknesses of this approach and how it might be improved. A
recent extension - the sophisticated inference (SI) algorithm - improves
performance on multi-step planning problems through recursive decision tree
search. However, little work to date has been done to compare SI to other
established planning algorithms. SI was also developed with a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.01705v4' target='_blank'>Learning to Accelerate by the Methods of Step-size Planning</a></h2>
<p><strong>Authors:</strong> Hengshuai Yao</p>
<p><strong>Summary:</strong> Gradient descent is slow to converge for ill-conditioned problems and
non-convex problems. An important technique for acceleration is step-size
adaptation. The first part of this paper contains a detailed review of
step-size adaptation methods, including Polyak step-size, L4, LossGrad, Adam,
IDBD, and Hypergradient descent, and the relation of step-size adaptation to
meta-gradient methods. In the second part of this paper, we propose a new class
of methods of accelerating gradient descent that h...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.12827v5' target='_blank'>Entity Abstraction in Visual Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Rishi Veerapaneni, John D. Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu, Joshua B. Tenenbaum, Sergey Levine</p>
<p><strong>Summary:</strong> This paper tests the hypothesis that modeling a scene in terms of entities
and their local interactions, as opposed to modeling the scene globally,
provides a significant benefit in generalizing to physical tasks in a
combinatorial space the learner has not encountered before. We present
object-centric perception, prediction, and planning (OP3), which to the best of
our knowledge is the first fully probabilistic entity-centric dynamic latent
variable framework for model-based reinforcement learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.01755v2' target='_blank'>A Distributed Model-Free Ride-Sharing Approach for Joint Matching,
  Pricing, and Dispatching using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Marina Haliem, Ganapathy Mani, Vaneet Aggarwal, Bharat Bhargava</p>
<p><strong>Summary:</strong> Significant development of ride-sharing services presents a plethora of
opportunities to transform urban mobility by providing personalized and
convenient transportation while ensuring efficiency of large-scale ride
pooling. However, a core problem for such services is route planning for each
driver to fulfill the dynamically arriving requests while satisfying given
constraints. Current models are mostly limited to static routes with only two
rides per vehicle (optimally) or three (with heuristi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.08015v2' target='_blank'>Applicability and Challenges of Deep Reinforcement Learning for
  Satellite Frequency Plan Design</a></h2>
<p><strong>Authors:</strong> Juan Jose Garau Luis, Edward Crawley, Bruce Cameron</p>
<p><strong>Summary:</strong> The study and benchmarking of Deep Reinforcement Learning (DRL) models has
become a trend in many industries, including aerospace engineering and
communications. Recent studies in these fields propose these kinds of models to
address certain complex real-time decision-making problems in which classic
approaches do not meet time requirements or fail to obtain optimal solutions.
While the good performance of DRL models has been proved for specific use cases
or scenarios, most studies do not discus...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.04603v1' target='_blank'>Models, Pixels, and Rewards: Evaluating Design Trade-offs in Visual
  Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Mohammad Babaeizadeh, Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn, Sergey Levine, Dumitru Erhan</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) methods have shown strong sample
efficiency and performance across a variety of tasks, including when faced with
high-dimensional visual observations. These methods learn to predict the
environment dynamics and expected reward from interaction and use this
predictive model to plan and perform the task. However, MBRL methods vary in
their fundamental design choices, and there is no strong consensus in the
literature on how these design decisions affect pe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.10290v2' target='_blank'>Integrated Decision and Control: Towards Interpretable and
  Computationally Efficient Driving Intelligence</a></h2>
<p><strong>Authors:</strong> Yang Guan, Yangang Ren, Qi Sun, Shengbo Eben Li, Haitong Ma, Jingliang Duan, Yifan Dai, Bo Cheng</p>
<p><strong>Summary:</strong> Decision and control are core functionalities of high-level automated
vehicles. Current mainstream methods, such as functionality decomposition and
end-to-end reinforcement learning (RL), either suffer high time complexity or
poor interpretability and adaptability on real-world autonomous driving tasks.
In this paper, we present an interpretable and computationally efficient
framework called integrated decision and control (IDC) for automated vehicles,
which decomposes the driving task into stat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.07746v1' target='_blank'>CEM-GD: Cross-Entropy Method with Gradient Descent Planner for
  Model-Based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kevin Huang, Sahin Lale, Ugo Rosolia, Yuanyuan Shi, Anima Anandkumar</p>
<p><strong>Summary:</strong> Current state-of-the-art model-based reinforcement learning algorithms use
trajectory sampling methods, such as the Cross-Entropy Method (CEM), for
planning in continuous control settings. These zeroth-order optimizers require
sampling a large number of trajectory rollouts to select an optimal action,
which scales poorly for large prediction horizons or high dimensional action
spaces. First-order methods that use the gradients of the rewards with respect
to the actions as an update can mitigate ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.14176v1' target='_blank'>DayDreamer: World Models for Physical Robot Learning</a></h2>
<p><strong>Authors:</strong> Philipp Wu, Alejandro Escontrela, Danijar Hafner, Ken Goldberg, Pieter Abbeel</p>
<p><strong>Summary:</strong> To solve tasks in complex environments, robots need to learn from experience.
Deep reinforcement learning is a common approach to robot learning but requires
a large amount of trial and error to learn, limiting its deployment in the
physical world. As a consequence, many advances in robot learning rely on
simulators. On the other hand, learning inside of simulators fails to capture
the complexity of the real world, is prone to simulator inaccuracies, and the
resulting behaviors do not adapt to c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.03516v4' target='_blank'>Neuroevolution is a Competitive Alternative to Reinforcement Learning
  for Skill Discovery</a></h2>
<p><strong>Authors:</strong> Felix Chalumeau, Raphael Boige, Bryan Lim, Valentin Mac√©, Maxime Allard, Arthur Flajolet, Antoine Cully, Thomas Pierrot</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for
training neural policies to solve complex control tasks. However, these
policies tend to be overfit to the exact specifications of the task and
environment they were trained on, and thus do not perform well when conditions
deviate slightly or when composed hierarchically to solve even more complex
tasks. Recent work has shown that training a mixture of policies, as opposed to
a single one, that are driven to explore differen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.03604v8' target='_blank'>Enabling Intelligent Interactions between an Agent and an LLM: A
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu</p>
<p><strong>Summary:</strong> Large language models (LLMs) encode a vast amount of world knowledge acquired
from massive text datasets. Recent studies have demonstrated that LLMs can
assist an embodied agent in solving complex sequential decision making tasks by
providing high-level instructions. However, interactions with LLMs can be
time-consuming. In many practical scenarios, it requires a significant amount
of storage space that can only be deployed on remote cloud servers.
Additionally, using commercial LLMs can be cost...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.08705v3' target='_blank'>Partially Observable Multi-Agent Reinforcement Learning with Information
  Sharing</a></h2>
<p><strong>Authors:</strong> Xiangyu Liu, Kaiqing Zhang</p>
<p><strong>Summary:</strong> We study provable multi-agent reinforcement learning (RL) in the general
framework of partially observable stochastic games (POSGs). To circumvent the
known hardness results and the use of computationally intractable oracles, we
advocate leveraging the potential \emph{information-sharing} among agents, a
common practice in empirical multi-agent RL, and a standard model for
multi-agent control systems with communications. We first establish several
computational complexity results to justify the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.02902v1' target='_blank'>Sim-to-Real Transfer for Mobile Robots with Reinforcement Learning: from
  NVIDIA Isaac Sim to Gazebo and Real ROS 2 Robots</a></h2>
<p><strong>Authors:</strong> Sahar Salimpour, Jorge Pe√±a-Queralta, Diego Paez-Granados, Jukka Heikkonen, Tomi Westerlund</p>
<p><strong>Summary:</strong> Unprecedented agility and dexterous manipulation have been demonstrated with
controllers based on deep reinforcement learning (RL), with a significant
impact on legged and humanoid robots. Modern tooling and simulation platforms,
such as NVIDIA Isaac Sim, have been enabling such advances. This article
focuses on demonstrating the applications of Isaac in local planning and
obstacle avoidance as one of the most fundamental ways in which a mobile robot
interacts with its environments. Although the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.08020v1' target='_blank'>Cooperative Patrol Routing: Optimizing Urban Crime Surveillance through
  Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Juan Palma-Borda, Eduardo Guzm√°n, Mar√≠a-Victoria Belmonte</p>
<p><strong>Summary:</strong> The effective design of patrol strategies is a difficult and complex problem,
especially in medium and large areas. The objective is to plan, in a
coordinated manner, the optimal routes for a set of patrols in a given area, in
order to achieve maximum coverage of the area, while also trying to minimize
the number of patrols. In this paper, we propose a multi-agent reinforcement
learning (MARL) model, based on a decentralized partially observable Markov
decision process, to plan unpredictable pat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1205.2651v1' target='_blank'>Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal
  Decision Making</a></h2>
<p><strong>Authors:</strong> Mark Crowley, John Nelson, David L Poole</p>
<p><strong>Summary:</strong> We introduce a challenging real-world planning problem where actions must be
taken at each location in a spatial area at each point in time. We use forestry
planning as the motivating application. In Large Scale Spatial-Temporal (LSST)
planning problems, the state and action spaces are defined as the
cross-products of many local state and action spaces spread over a large
spatial area such as a city or forest. These problems possess state
uncertainty, have complex utility functions involving spa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1908.00722v3' target='_blank'>Learning to combine primitive skills: A step towards versatile robotic
  manipulation</a></h2>
<p><strong>Authors:</strong> Robin Strudel, Alexander Pashevich, Igor Kalevatykh, Ivan Laptev, Josef Sivic, Cordelia Schmid</p>
<p><strong>Summary:</strong> Manipulation tasks such as preparing a meal or assembling furniture remain
highly challenging for robotics and vision. Traditional task and motion
planning (TAMP) methods can solve complex tasks but require full state
observability and are not adapted to dynamic scene changes. Recent learning
methods can operate directly on visual inputs but typically require many
demonstrations and/or task-specific reward engineering. In this work we aim to
overcome previous limitations and propose a reinforcem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.06193v3' target='_blank'>Exploration by Maximizing R√©nyi Entropy for Reward-Free RL Framework</a></h2>
<p><strong>Authors:</strong> Chuheng Zhang, Yuanying Cai, Longbo Huang, Jian Li</p>
<p><strong>Summary:</strong> Exploration is essential for reinforcement learning (RL). To face the
challenges of exploration, we consider a reward-free RL framework that
completely separates exploration from exploitation and brings new challenges
for exploration algorithms. In the exploration phase, the agent learns an
exploratory policy by interacting with a reward-free environment and collects a
dataset of transitions by executing the policy. In the planning phase, the
agent computes a good policy for any reward function ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.08140v1' target='_blank'>POMP: Pomcp-based Online Motion Planning for active visual search in
  indoor environments</a></h2>
<p><strong>Authors:</strong> Yiming Wang, Francesco Giuliari, Riccardo Berra, Alberto Castellini, Alessio Del Bue, Alessandro Farinelli, Marco Cristani, Francesco Setti</p>
<p><strong>Summary:</strong> In this paper we focus on the problem of learning an optimal policy for
Active Visual Search (AVS) of objects in known indoor environments with an
online setup. Our POMP method uses as input the current pose of an agent (e.g.
a robot) and a RGB-D frame. The task is to plan the next move that brings the
agent closer to the target object. We model this problem as a Partially
Observable Markov Decision Process solved by a Monte-Carlo planning approach.
This allows us to make decisions on the next m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.02613v2' target='_blank'>Planning with Learned Dynamic Model for Unsupervised Point Cloud
  Registration</a></h2>
<p><strong>Authors:</strong> Haobo Jiang, Jin Xie, Jianjun Qian, Jian Yang</p>
<p><strong>Summary:</strong> Point cloud registration is a fundamental problem in 3D computer vision. In
this paper, we cast point cloud registration into a planning problem in
reinforcement learning, which can seek the transformation between the source
and target point clouds through trial and error. By modeling the point cloud
registration process as a Markov decision process (MDP), we develop a latent
dynamic model of point clouds, consisting of a transformation network and
evaluation network. The transformation network ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.05533v3' target='_blank'>Efficient Local Planning with Linear Function Approximation</a></h2>
<p><strong>Authors:</strong> Dong Yin, Botao Hao, Yasin Abbasi-Yadkori, Nevena Laziƒá, Csaba Szepesv√°ri</p>
<p><strong>Summary:</strong> We study query and computationally efficient planning algorithms with linear
function approximation and a simulator. We assume that the agent only has local
access to the simulator, meaning that the agent can only query the simulator at
states that have been visited before. This setting is more practical than many
prior works on reinforcement learning with a generative model. We propose two
algorithms, named confident Monte Carlo least square policy iteration
(Confident MC-LSPI) and confident Mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.04238v1' target='_blank'>Improving Kinodynamic Planners for Vehicular Navigation with Learned
  Goal-Reaching Controllers</a></h2>
<p><strong>Authors:</strong> Aravind Sivaramakrishnan, Edgar Granados, Seth Karten, Troy McMahon, Kostas E. Bekris</p>
<p><strong>Summary:</strong> This paper aims to improve the path quality and computational efficiency of
sampling-based kinodynamic planners for vehicular navigation. It proposes a
learning framework for identifying promising controls during the expansion
process of sampling-based planners. Given a dynamics model, a reinforcement
learning process is trained offline to return a low-cost control that reaches a
local goal state (i.e., a waypoint) in the absence of obstacles. By focusing on
the system's dynamics and not knowing...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.05442v1' target='_blank'>Neural Algorithmic Reasoners are Implicit Planners</a></h2>
<p><strong>Authors:</strong> Andreea Deac, Petar Veliƒçkoviƒá, Ognjen Milinkoviƒá, Pierre-Luc Bacon, Jian Tang, Mladen Nikoliƒá</p>
<p><strong>Summary:</strong> Implicit planning has emerged as an elegant technique for combining learned
models of the world with end-to-end model-free reinforcement learning. We study
the class of implicit planners inspired by value iteration, an algorithm that
is guaranteed to yield perfect policies in fully-specified tabular
environments. We find that prior approaches either assume that the environment
is provided in such a tabular form -- which is highly restrictive -- or infer
"local neighbourhoods" of states to run va...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.07105v1' target='_blank'>Integrated Path Planning and Tracking Control of Marine Current Turbine
  in Uncertain Ocean Environments</a></h2>
<p><strong>Authors:</strong> Arezoo Hasankhani, Ertugrul Baris Ondes, Yufei Tang, Cornel Sultan, James VanZwieten</p>
<p><strong>Summary:</strong> This paper presents an integrated path planning and tracking control of
marine hydrokinetic energy harvesting devices. To address the highly nonlinear
and uncertain oceanic environment, the path planner is designed based on a
reinforcement learning (RL) approach by fully exploring the historical ocean
current profiles. The planner will search for a path to optimize a chosen cost
criterion, such as maximizing the total harvested energy for a given time.
Model predictive control (MPC) is then util...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.10007v2' target='_blank'>Online Grounding of Symbolic Planning Domains in Unknown Environments</a></h2>
<p><strong>Authors:</strong> Leonardo Lamanna, Luciano Serafini, Alessandro Saetti, Alfonso Gerevini, Paolo Traverso</p>
<p><strong>Summary:</strong> If a robotic agent wants to exploit symbolic planning techniques to achieve
some goal, it must be able to properly ground an abstract planning domain in
the environment in which it operates. However, if the environment is initially
unknown by the agent, the agent needs to explore it and discover the salient
aspects of the environment needed to reach its goals. Namely, the agent has to
discover: (i) the objects present in the environment, (ii) the properties of
these objects and their relations, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.13404v1' target='_blank'>Abstractions of General Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sultan J. Majeed</p>
<p><strong>Summary:</strong> The field of artificial intelligence (AI) is devoted to the creation of
artificial decision-makers that can perform (at least) on par with the human
counterparts on a domain of interest. Unlike the agents in traditional AI, the
agents in artificial general intelligence (AGI) are required to replicate human
intelligence in almost every domain of interest. Moreover, an AGI agent should
be able to achieve this without (virtually any) further changes, retraining, or
fine-tuning of the parameters. Th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.13003v1' target='_blank'>Decision Making in Non-Stationary Environments with Policy-Augmented
  Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Geoffrey Pettet, Ayan Mukhopadhyay, Abhishek Dubey</p>
<p><strong>Summary:</strong> Decision-making under uncertainty (DMU) is present in many important
problems. An open challenge is DMU in non-stationary environments, where the
dynamics of the environment can change over time. Reinforcement Learning (RL),
a popular approach for DMU problems, learns a policy by interacting with a
model of the environment offline. Unfortunately, if the environment changes the
policy can become stale and take sub-optimal actions, and relearning the policy
for the updated environment takes time a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.02390v2' target='_blank'>Learning Pneumatic Non-Prehensile Manipulation with a Mobile Blower</a></h2>
<p><strong>Authors:</strong> Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz, Thomas Funkhouser</p>
<p><strong>Summary:</strong> We investigate pneumatic non-prehensile manipulation (i.e., blowing) as a
means of efficiently moving scattered objects into a target receptacle. Due to
the chaotic nature of aerodynamic forces, a blowing controller must (i)
continually adapt to unexpected changes from its actions, (ii) maintain
fine-grained control, since the slightest misstep can result in large
unintended consequences (e.g., scatter objects already in a pile), and (iii)
infer long-range plans (e.g., move the robot to strategi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.02092v1' target='_blank'>Learning Abstract and Transferable Representations for Planning</a></h2>
<p><strong>Authors:</strong> Steven James, Benjamin Rosman, George Konidaris</p>
<p><strong>Summary:</strong> We are concerned with the question of how an agent can acquire its own
representations from sensory data. We restrict our focus to learning
representations for long-term planning, a class of problems that
state-of-the-art learning methods are unable to solve. We propose a framework
for autonomously learning state abstractions of an agent's environment, given a
set of skills. Importantly, these abstractions are task-independent, and so can
be reused to solve new tasks. We demonstrate how an agent...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.06002v1' target='_blank'>Learning Generalized Policies Without Supervision Using GNNs</a></h2>
<p><strong>Authors:</strong> Simon St√•hlberg, Blai Bonet, Hector Geffner</p>
<p><strong>Summary:</strong> We consider the problem of learning generalized policies for classical
planning domains using graph neural networks from small instances represented
in lifted STRIPS. The problem has been considered before but the proposed
neural architectures are complex and the results are often mixed. In this work,
we use a simple and general GNN architecture and aim at obtaining crisp
experimental results and a deeper understanding: either the policy greedy in
the learned value function achieves close to 100...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02039v2' target='_blank'>Beyond Value: CHECKLIST for Testing Inferences in Planning-Based RL</a></h2>
<p><strong>Authors:</strong> Kin-Ho Lam, Delyar Tabatabai, Jed Irvine, Donald Bertucci, Anita Ruangrotsakun, Minsuk Kahng, Alan Fern</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) agents are commonly evaluated via their expected
value over a distribution of test scenarios. Unfortunately, this evaluation
approach provides limited evidence for post-deployment generalization beyond
the test distribution. In this paper, we address this limitation by extending
the recent CheckList testing methodology from natural language processing to
planning-based RL. Specifically, we consider testing RL agents that make
decisions via online tree search using a l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.14298v1' target='_blank'>Left Heavy Tails and the Effectiveness of the Policy and Value Networks
  in DNN-based best-first search for Sokoban Planning</a></h2>
<p><strong>Authors:</strong> Dieqiao Feng, Carla Gomes, Bart Selman</p>
<p><strong>Summary:</strong> Despite the success of practical solvers in various NP-complete domains such
as SAT and CSP as well as using deep reinforcement learning to tackle
two-player games such as Go, certain classes of PSPACE-hard planning problems
have remained out of reach. Even carefully designed domain-specialized solvers
can fail quickly due to the exponential search space on hard instances. Recent
works that combine traditional search methods, such as best-first search and
Monte Carlo tree search, with Deep Neura...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.04642v1' target='_blank'>Exploration via Planning for Information about the Optimal Trajectory</a></h2>
<p><strong>Authors:</strong> Viraj Mehta, Ian Char, Joseph Abbate, Rory Conlin, Mark D. Boyer, Stefano Ermon, Jeff Schneider, Willie Neiswanger</p>
<p><strong>Summary:</strong> Many potential applications of reinforcement learning (RL) are stymied by the
large numbers of samples required to learn an effective policy. This is
especially true when applying RL to real-world control tasks, e.g. in the
sciences or robotics, where executing a policy in the environment is costly. In
popular RL algorithms, agents typically explore either by adding stochasticity
to a reward-maximizing policy or by attempting to gather maximal information
about environment dynamics without takin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.13755v3' target='_blank'>Retrosynthetic Planning with Dual Value Networks</a></h2>
<p><strong>Authors:</strong> Guoqing Liu, Di Xue, Shufang Xie, Yingce Xia, Austin Tripp, Krzysztof Maziarz, Marwin Segler, Tao Qin, Zongzhang Zhang, Tie-Yan Liu</p>
<p><strong>Summary:</strong> Retrosynthesis, which aims to find a route to synthesize a target molecule
from commercially available starting materials, is a critical task in drug
discovery and materials design. Recently, the combination of ML-based
single-step reaction predictors with multi-step planners has led to promising
results. However, the single-step predictors are mostly trained offline to
optimize the single-step accuracy, without considering complete routes. Here,
we leverage reinforcement learning (RL) to improv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.10311v1' target='_blank'>Understanding the effect of varying amounts of replay per step</a></h2>
<p><strong>Authors:</strong> Animesh Kumar Paul, Videh Raj Nema</p>
<p><strong>Summary:</strong> Model-based reinforcement learning uses models to plan, where the predictions
and policies of an agent can be improved by using more computation without
additional data from the environment, thereby improving sample efficiency.
However, learning accurate estimates of the model is hard. Subsequently, the
natural question is whether we can get similar benefits as planning with
model-free methods. Experience replay is an essential component of many
model-free algorithms enabling sample-efficient le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.00561v1' target='_blank'>Model-free Motion Planning of Autonomous Agents for Complex Tasks in
  Partially Observable Environments</a></h2>
<p><strong>Authors:</strong> Junchao Li, Mingyu Cai, Zhen Kan, Shaoping Xiao</p>
<p><strong>Summary:</strong> Motion planning of autonomous agents in partially known environments with
incomplete information is a challenging problem, particularly for complex
tasks. This paper proposes a model-free reinforcement learning approach to
address this problem. We formulate motion planning as a probabilistic-labeled
partially observable Markov decision process (PL-POMDP) problem and use linear
temporal logic (LTL) to express the complex task. The LTL formula is then
converted to a limit-deterministic generalized...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.18258v2' target='_blank'>Maximize to Explore: One Objective Function Fusing Estimation, Planning,
  and Exploration</a></h2>
<p><strong>Authors:</strong> Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran Wang</p>
<p><strong>Summary:</strong> In online reinforcement learning (online RL), balancing exploration and
exploitation is crucial for finding an optimal policy in a sample-efficient
way. To achieve this, existing sample-efficient online RL algorithms typically
consist of three components: estimation, planning, and exploration. However, in
order to cope with general function approximators, most of them involve
impractical algorithmic components to incentivize exploration, such as
optimization within data-dependent level-sets or c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.00845v2' target='_blank'>BitE : Accelerating Learned Query Optimization in a Mixed-Workload
  Environment</a></h2>
<p><strong>Authors:</strong> Yuri Kim, Yewon Choi, Yujung Gil, Sanghee Lee, Heesik Shin, Jaehyok Chong</p>
<p><strong>Summary:</strong> Although the many efforts to apply deep reinforcement learning to query
optimization in recent years, there remains room for improvement as query
optimizers are complex entities that require hand-designed tuning of workloads
and datasets. Recent research present learned query optimizations results
mostly in bulks of single workloads which focus on picking up the unique traits
of the specific workload. This proves to be problematic in scenarios where the
different characteristics of multiple work...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.00867v1' target='_blank'>IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive
  Control</a></h2>
<p><strong>Authors:</strong> Rohan Chitnis, Yingchen Xu, Bobak Hashemi, Lucas Lehnert, Urun Dogan, Zheqing Zhu, Olivier Delalleau</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL) has shown great promise due to its
sample efficiency, but still struggles with long-horizon sparse-reward tasks,
especially in offline settings where the agent learns from a fixed dataset. We
hypothesize that model-based RL agents struggle in these environments due to a
lack of long-term planning capabilities, and that planning in a temporally
abstract model of the environment can alleviate this issue. In this paper, we
make two key contributions: 1) we in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.11551v2' target='_blank'>IMP-MARL: a Suite of Environments for Large-scale Infrastructure
  Management Planning via MARL</a></h2>
<p><strong>Authors:</strong> Pascal Leroy, Pablo G. Morato, Jonathan Pisane, Athanasios Kolios, Damien Ernst</p>
<p><strong>Summary:</strong> We introduce IMP-MARL, an open-source suite of multi-agent reinforcement
learning (MARL) environments for large-scale Infrastructure Management Planning
(IMP), offering a platform for benchmarking the scalability of cooperative MARL
methods in real-world engineering applications. In IMP, a multi-component
engineering system is subject to a risk of failure due to its components'
damage condition. Specifically, each agent plans inspections and repairs for a
specific system component, aiming to min...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.17030v1' target='_blank'>SkiROS2: A skill-based Robot Control Platform for ROS</a></h2>
<p><strong>Authors:</strong> Matthias Mayr, Francesco Rovida, Volker Krueger</p>
<p><strong>Summary:</strong> The need for autonomous robot systems in both the service and the industrial
domain is larger than ever. In the latter, the transition to small batches or
even "batch size 1" in production created a need for robot control system
architectures that can provide the required flexibility. Such architectures
must not only have a sufficient knowledge integration framework. It must also
support autonomous mission execution and allow for interchangeability and
interoperability between different tasks an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.03903v2' target='_blank'>LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination
  Abilities in Large Language Models</a></h2>
<p><strong>Authors:</strong> Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang</p>
<p><strong>Summary:</strong> The emergent reasoning and Theory of Mind (ToM) abilities demonstrated by
Large Language Models (LLMs) make them promising candidates for developing
coordination agents. In this study, we introduce a new LLM-Coordination
Benchmark aimed at a detailed analysis of LLMs within the context of Pure
Coordination Games, where participating agents need to cooperate for the most
gain. This benchmark evaluates LLMs through two distinct tasks: (1)
\emph{Agentic Coordination}, where LLMs act as proactive pa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.04547v1' target='_blank'>Deep Learning Based Active Spatial Channel Gain Prediction Using a Swarm
  of Unmanned Aerial Vehicles</a></h2>
<p><strong>Authors:</strong> Enes Krijestorac, Danijela Cabric</p>
<p><strong>Summary:</strong> Prediction of wireless channel gain (CG) across space is a necessary tool for
many important wireless network design problems. In this paper, we develop
prediction methods that use environment-specific features, namely building maps
and CG measurements, to achieve high prediction accuracy. We assume that
measurements are collected using a swarm of coordinated unmanned aerial
vehicles (UAVs). We develop novel active prediction approaches which consist of
both methods for UAV path planning for opt...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.07220v2' target='_blank'>COPlanner: Plan to Roll Out Conservatively but to Explore Optimistically
  for Model-Based RL</a></h2>
<p><strong>Authors:</strong> Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, Furong Huang</p>
<p><strong>Summary:</strong> Dyna-style model-based reinforcement learning contains two phases: model
rollouts to generate sample for policy learning and real environment
exploration using current policy for dynamics model learning. However, due to
the complex real-world environment, it is inevitable to learn an imperfect
dynamics model with model prediction error, which can further mislead policy
learning and result in sub-optimal solutions. In this paper, we propose
$\texttt{COPlanner}$, a planning-driven framework for mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.00252v1' target='_blank'>Active Neural Topological Mapping for Multi-Agent Exploration</a></h2>
<p><strong>Authors:</strong> Xinyi Yang, Yuxiang Yang, Chao Yu, Jiayu Chen, Jingchen Yu, Haibing Ren, Huazhong Yang, Yu Wang</p>
<p><strong>Summary:</strong> This paper investigates the multi-agent cooperative exploration problem,
which requires multiple agents to explore an unseen environment via sensory
signals in a limited time. A popular approach to exploration tasks is to
combine active mapping with planning. Metric maps capture the details of the
spatial representation, but are with high communication traffic and may vary
significantly between scenarios, resulting in inferior generalization.
Topological maps are a promising alternative as they ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.02522v2' target='_blank'>MASP: Scalable GNN-based Planning for Multi-Agent Navigation</a></h2>
<p><strong>Authors:</strong> Xinyi Yang, Xinting Yang, Chao Yu, Jiayu Chen, Wenbo Ding, Huazhong Yang, Yu Wang</p>
<p><strong>Summary:</strong> We investigate multi-agent navigation tasks, where multiple agents need to
reach initially unassigned goals in a limited time. Classical planning-based
methods suffer from expensive computation overhead at each step and offer
limited expressiveness for complex cooperation strategies. In contrast,
reinforcement learning (RL) has recently become a popular approach for
addressing this issue. However, RL struggles with low data efficiency and
cooperation when directly exploring (nearly) optimal poli...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.08406v3' target='_blank'>Transition Constrained Bayesian Optimization via Markov Decision
  Processes</a></h2>
<p><strong>Authors:</strong> Jose Pablo Folch, Calvin Tsay, Robert M Lee, Behrang Shafei, Weronika Ormaniec, Andreas Krause, Mark van der Wilk, Ruth Misener, Mojm√≠r Mutn√Ω</p>
<p><strong>Summary:</strong> Bayesian optimization is a methodology to optimize black-box functions.
Traditionally, it focuses on the setting where you can arbitrarily query the
search space. However, many real-life problems do not offer this flexibility;
in particular, the search space of the next query may depend on previous ones.
Example challenges arise in the physical sciences in the form of local movement
constraints, required monotonicity in certain variables, and transitions
influencing the accuracy of measurements....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.12589v2' target='_blank'>FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal
  Footstep Planning and Forecasting</a></h2>
<p><strong>Authors:</strong> Cl√©ment Gaspard, Gr√©goire Passault, M√©lodie Daniel, Olivier Ly</p>
<p><strong>Summary:</strong> Designing a humanoid locomotion controller is challenging and classically
split up in sub-problems. Footstep planning is one of those, where the sequence
of footsteps is defined. Even in simpler environments, finding a minimal
sequence, or even a feasible sequence, yields a complex optimization problem.
In the literature, this problem is usually addressed by search-based algorithms
(e.g. variants of A*). However, such approaches are either computationally
expensive or rely on hand-crafted tuning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.15383v2' target='_blank'>Generating Code World Models with Large Language Models Guided by Monte
  Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Nicola Dainese, Matteo Merler, Minttu Alakuijala, Pekka Marttinen</p>
<p><strong>Summary:</strong> In this work we consider Code World Models, world models generated by a Large
Language Model (LLM) in the form of Python code for model-based Reinforcement
Learning (RL). Calling code instead of LLMs for planning has potential to be
more precise, reliable, interpretable, and extremely efficient. However,
writing appropriate Code World Models requires the ability to understand
complex instructions, to generate exact code with non-trivial logic and to
self-debug a long program with feedback from u...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.08653v1' target='_blank'>BaSeNet: A Learning-based Mobile Manipulator Base Pose Sequence Planning
  for Pickup Tasks</a></h2>
<p><strong>Authors:</strong> Lakshadeep Naik, Sinan Kalkan, Sune L. S√∏rensen, Mikkel B. Kj√¶rgaard, Norbert Kr√ºger</p>
<p><strong>Summary:</strong> In many applications, a mobile manipulator robot is required to grasp a set
of objects distributed in space. This may not be feasible from a single base
pose and the robot must plan the sequence of base poses for grasping all
objects, minimizing the total navigation and grasping time. This is a
Combinatorial Optimization problem that can be solved using exact methods,
which provide optimal solutions but are computationally expensive, or
approximate methods, which offer computationally efficient ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.10667v2' target='_blank'>UniZero: Generalized and Efficient Planning with Scalable Latent World
  Models</a></h2>
<p><strong>Authors:</strong> Yuan Pu, Yazhe Niu, Zhenjie Yang, Jiyuan Ren, Hongsheng Li, Yu Liu</p>
<p><strong>Summary:</strong> Learning predictive world models is crucial for enhancing the planning
capabilities of reinforcement learning (RL) agents. Recently, MuZero-style
algorithms, leveraging the value equivalence principle and Monte Carlo Tree
Search (MCTS), have achieved superhuman performance in various domains.
However, these methods struggle to scale in heterogeneous scenarios with
diverse dependencies and task variability. To overcome these limitations, we
introduce UniZero, a novel approach that employs a modul...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.12532v1' target='_blank'>Towards Collaborative Intelligence: Propagating Intentions and Reasoning
  for Multi-Agent Coordination with Large Language Models</a></h2>
<p><strong>Authors:</strong> Xihe Qiu, Haoyu Wang, Xiaoyu Tan, Chao Qu, Yujie Xiong, Yuan Cheng, Yinghui Xu, Wei Chu, Yuan Qi</p>
<p><strong>Summary:</strong> Effective collaboration in multi-agent systems requires communicating goals
and intentions between agents. Current agent frameworks often suffer from
dependencies on single-agent execution and lack robust inter-module
communication, frequently leading to suboptimal multi-agent reinforcement
learning (MARL) policies and inadequate task coordination. To address these
challenges, we present a framework for training large language models (LLMs) as
collaborative agents to enable coordinated behaviors...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.17683v1' target='_blank'>RL-augmented MPC Framework for Agile and Robust Bipedal Footstep
  Locomotion Planning and Control</a></h2>
<p><strong>Authors:</strong> Seung Hyeon Bang, Carlos Arribalzaga Jov√©, Luis Sentis</p>
<p><strong>Summary:</strong> This paper proposes an online bipedal footstep planning strategy that
combines model predictive control (MPC) and reinforcement learning (RL) to
achieve agile and robust bipedal maneuvers. While MPC-based foot placement
controllers have demonstrated their effectiveness in achieving dynamic
locomotion, their performance is often limited by the use of simplified models
and assumptions. To address this challenge, we develop a novel foot placement
controller that leverages a learned policy to bridge...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.01584v3' target='_blank'>GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS</a></h2>
<p><strong>Authors:</strong> Saman Kazemkhani, Aarav Pandya, Daphne Cornelisse, Brennan Shacklett, Eugene Vinitsky</p>
<p><strong>Summary:</strong> Multi-agent learning algorithms have been successful at generating superhuman
planning in various games but have had limited impact on the design of deployed
multi-agent planners. A key bottleneck in applying these techniques to
multi-agent planning is that they require billions of steps of experience. To
enable the study of multi-agent planning at scale, we present GPUDrive.
GPUDrive is a GPU-accelerated, multi-agent simulator built on top of the
Madrona Game Engine capable of generating over a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.13139v3' target='_blank'>Optimally Solving Simultaneous-Move Dec-POMDPs: The Sequential Central
  Planning Approach</a></h2>
<p><strong>Authors:</strong> Johan Peralez, Aur√®lien Delage, Jacopo Castellini, Rafael F. Cunha, Jilles S. Dibangoye</p>
<p><strong>Summary:</strong> The centralized training for decentralized execution paradigm emerged as the
state-of-the-art approach to $\epsilon$-optimally solving decentralized
partially observable Markov decision processes. However, scalability remains a
significant issue. This paper presents a novel and more scalable alternative,
namely the sequential-move centralized training for decentralized execution.
This paradigm further pushes the applicability of the Bellman's principle of
optimality, raising three new properties...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.06985v1' target='_blank'>Enhancing Cross-domain Pre-Trained Decision Transformers with Adaptive
  Attention</a></h2>
<p><strong>Authors:</strong> Wenhao Zhao, Qiushui Xu, Linjie Xu, Lei Song, Jinyu Wang, Chunlai Zhou, Jiang Bian</p>
<p><strong>Summary:</strong> Recently, the pre-training of decision transformers (DT) using a different
domain, such as natural language text, has generated significant attention in
offline reinforcement learning (Offline RL). Although this cross-domain
pre-training approach achieves superior performance compared to training from
scratch in environments required short-term planning ability, the mechanisms by
which pre-training benefits the fine-tuning phase remain unclear. Furthermore,
we point out that the cross-domain pre...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.19829v1' target='_blank'>Generalizability of Graph Neural Networks for Decentralized Unlabeled
  Motion Planning</a></h2>
<p><strong>Authors:</strong> Shreyas Muthusamy, Damian Owerko, Charilaos I. Kanatsoulis, Saurav Agarwal, Alejandro Ribeiro</p>
<p><strong>Summary:</strong> Unlabeled motion planning involves assigning a set of robots to target
locations while ensuring collision avoidance, aiming to minimize the total
distance traveled. The problem forms an essential building block for
multi-robot systems in applications such as exploration, surveillance, and
transportation. We address this problem in a decentralized setting where each
robot knows only the positions of its $k$-nearest robots and $k$-nearest
targets. This scenario combines elements of combinatorial a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.12650v1' target='_blank'>Neural-Network-Driven Reward Prediction as a Heuristic: Advancing
  Q-Learning for Mobile Robot Path Planning</a></h2>
<p><strong>Authors:</strong> Yiming Ji, Kaijie Yun, Yang Liu, Zongwu Xie, Hong Liu</p>
<p><strong>Summary:</strong> Q-learning is a widely used reinforcement learning technique for solving path
planning problems. It primarily involves the interaction between an agent and
its environment, enabling the agent to learn an optimal strategy that maximizes
cumulative rewards. Although many studies have reported the effectiveness of
Q-learning, it still faces slow convergence issues in practical applications.
To address this issue, we propose the NDR-QL method, which utilizes neural
network outputs as heuristic infor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.09290v1' target='_blank'>Interoceptive Robots for Convergent Shared Control in Collaborative
  Construction Work</a></h2>
<p><strong>Authors:</strong> Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat</p>
<p><strong>Summary:</strong> Building autonomous mobile robots (AMRs) with optimized efficiency and
adaptive capabilities-able to respond to changing task demands and dynamic
environments-is a strongly desired goal for advancing construction robotics.
Such robots can play a critical role in enabling automation, reducing
operational carbon footprints, and supporting modular construction processes.
Inspired by the adaptive autonomy of living organisms, we introduce
interoception, which centers on the robot's internal state re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.03804v3' target='_blank'>Model-Based Reinforcement Learning with a Generative Model is Minimax
  Optimal</a></h2>
<p><strong>Authors:</strong> Alekh Agarwal, Sham Kakade, Lin F. Yang</p>
<p><strong>Summary:</strong> This work considers the sample and computational complexity of obtaining an
$\epsilon$-optimal policy in a discounted Markov Decision Process (MDP), given
only access to a generative model. In this work, we study the effectiveness of
the most natural plug-in approach to model-based planning: we build the maximum
likelihood estimate of the transition model in the MDP from observations and
then find an optimal policy in this empirical MDP. We ask arguably the most
basic and unresolved question in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.13475v2' target='_blank'>LLM should think and action as a human</a></h2>
<p><strong>Authors:</strong> Haun Leung, ZiNan Wang</p>
<p><strong>Summary:</strong> It is popular lately to train large language models to be used as chat
assistants, but in the conversation between the user and the chat assistant,
there are prompts, require multi-turns between the chat assistant and the user.
However, there are a number of issues with the multi-turns conversation: The
response of the chat assistant is prone to errors and can't help users achieve
their goals, and as the number of conversation turns increases, the probability
of errors will also increase; It is ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/cs/0301006v1' target='_blank'>Temporal plannability by variance of the episode length</a></h2>
<p><strong>Authors:</strong> Balint Takacs, Istvan Szita, Andras Lorincz</p>
<p><strong>Summary:</strong> Optimization of decision problems in stochastic environments is usually
concerned with maximizing the probability of achieving the goal and minimizing
the expected episode length. For interacting agents in time-critical
applications, learning of the possibility of scheduling of subtasks (events) or
the full task is an additional relevant issue. Besides, there exist highly
stochastic problems where the actual trajectories show great variety from
episode to episode, but completing the task takes a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/cs/0407016v1' target='_blank'>Learning for Adaptive Real-time Search</a></h2>
<p><strong>Authors:</strong> Vadim Bulitko</p>
<p><strong>Summary:</strong> Real-time heuristic search is a popular model of acting and learning in
intelligent autonomous agents. Learning real-time search agents improve their
performance over time by acquiring and refining a value function guiding the
application of their actions. As computing the perfect value function is
typically intractable, a heuristic approximation is acquired instead. Most
studies of learning in real-time search (and reinforcement learning) assume
that a simple value-function-greedy policy is use...</p>
<hr>
<h2><a href='http://arxiv.org/abs/cs/0411018v1' target='_blank'>Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots</a></h2>
<p><strong>Authors:</strong> Pedro U. Lima, Luis M. M. Custodio</p>
<p><strong>Summary:</strong> This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for "Society of Robots" and "Soccer Robots", the case study
where we are testing our population of r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/0902.0392v2' target='_blank'>Tree Exploration for Bayesian RL Exploration</a></h2>
<p><strong>Authors:</strong> Christos Dimitrakakis</p>
<p><strong>Summary:</strong> Research in reinforcement learning has produced algorithms for optimal
decision making under uncertainty that fall within two main types. The first
employs a Bayesian framework, where optimality improves with increased
computational time. This is because the resulting planning task takes the form
of a dynamic programming problem on a belief tree with an infinite number of
states. The second type employs relatively simple algorithm which are shown to
suffer small regret within a distribution-free...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1202.3699v1' target='_blank'>Learning is planning: near Bayes-optimal reinforcement learning via
  Monte-Carlo tree search</a></h2>
<p><strong>Authors:</strong> John Asmuth, Michael L. Littman</p>
<p><strong>Summary:</strong> Bayes-optimal behavior, while well-defined, is often difficult to achieve.
Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it
is possible to act near-optimally in Markov Decision Processes (MDPs) with very
large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is
equivalent to optimal behavior in the known belief-space MDP, although the size
of this belief-space MDP grows exponentially with the amount of history
retained, and is potentially infinite...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1206.6404v1' target='_blank'>Policy Gradients with Variance Related Risk Criteria</a></h2>
<p><strong>Authors:</strong> Dotan Di Castro, Aviv Tamar, Shie Mannor</p>
<p><strong>Summary:</strong> Managing risk in dynamic decision problems is of cardinal importance in many
fields such as finance and process control. The most common approach to
defining risk is through various variance related criteria such as the Sharpe
Ratio or the standard deviation adjusted reward. It is known that optimizing
many of the variance related risk criteria is NP-hard. In this paper we devise
a framework for local policy gradient style algorithms for reinforcement
learning for variance related criteria. Our ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1301.3630v4' target='_blank'>Behavior Pattern Recognition using A New Representation Model</a></h2>
<p><strong>Authors:</strong> Qifeng Qiao, Peter A. Beling</p>
<p><strong>Summary:</strong> We study the use of inverse reinforcement learning (IRL) as a tool for the
recognition of agents' behavior on the basis of observation of their sequential
decision behavior interacting with the environment. We model the problem faced
by the agents as a Markov decision process (MDP) and model the observed
behavior of the agents in terms of forward planning for the MDP. We use IRL to
learn reward functions and then use these reward functions as the basis for
clustering or classification models. Ex...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1503.01793v1' target='_blank'>Correct-by-synthesis reinforcement learning with temporal logic
  constraints</a></h2>
<p><strong>Authors:</strong> Min Wen, Ruediger Ehlers, Ufuk Topcu</p>
<p><strong>Summary:</strong> We consider a problem on the synthesis of reactive controllers that optimize
some a priori unknown performance criterion while interacting with an
uncontrolled environment such that the system satisfies a given temporal logic
specification. We decouple the problem into two subproblems. First, we extract
a (maximally) permissive strategy for the system, which encodes multiple
(possibly all) ways in which the system can react to the adversarial
environment and satisfy the specifications. Then, we ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1511.07401v2' target='_blank'>MazeBase: A Sandbox for Learning from Games</a></h2>
<p><strong>Authors:</strong> Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, Rob Fergus</p>
<p><strong>Summary:</strong> This paper introduces MazeBase: an environment for simple 2D games, designed
as a sandbox for machine learning approaches to reasoning and planning. Within
it, we create 10 simple games embodying a range of algorithmic tasks (e.g.
if-then statements or set negation). A variety of neural models (fully
connected, convolutional network, memory network) are deployed via
reinforcement learning on these games, with and without a procedurally
generated curriculum. Despite the tasks' simplicity, the per...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1604.05508v3' target='_blank'>Intelligent Agent-Based Stimulation for Testing Robotic Software in
  Human-Robot Interactions</a></h2>
<p><strong>Authors:</strong> Dejanira Araiza-Illan, Anthony G. Pipe, Kerstin Eder</p>
<p><strong>Summary:</strong> The challenges of robotic software testing extend beyond conventional
software testing. Valid, realistic and interesting tests need to be generated
for multiple programs and hardware running concurrently, deployed into dynamic
environments with people. We investigate the use of Belief-Desire-Intention
(BDI) agents as models for test generation, in the domain of human-robot
interaction (HRI) in simulations. These models provide rational agency,
causality, and a reasoning mechanism for planning, w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1612.00380v1' target='_blank'>Playing Doom with SLAM-Augmented Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Shehroze Bhatti, Alban Desmaison, Ondrej Miksik, Nantas Nardelli, N. Siddharth, Philip H. S. Torr</p>
<p><strong>Summary:</strong> A number of recent approaches to policy learning in 2D game domains have been
successful going directly from raw input images to actions. However when
employed in complex 3D environments, they typically suffer from challenges
related to partial observability, combinatorial exploration spaces, path
planning, and a scarcity of rewarding scenarios. Inspired from prior work in
human cognition that indicates how humans employ a variety of semantic concepts
and abstractions (object categories, localis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1612.05628v5' target='_blank'>An Alternative Softmax Operator for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Kavosh Asadi, Michael L. Littman</p>
<p><strong>Summary:</strong> A softmax operator applied to a set of values acts somewhat like the
maximization function and somewhat like an average. In sequential decision
making, softmax is often used in settings where it is necessary to maximize
utility but also to hedge against problems that arise from putting all of one's
weight behind a single maximum utility decision. The Boltzmann softmax operator
is the most commonly used softmax operator in this setting, but we show that
this operator is prone to misbehavior. In t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1701.04113v1' target='_blank'>Near Optimal Behavior via Approximate State Abstraction</a></h2>
<p><strong>Authors:</strong> David Abel, D. Ellis Hershkowitz, Michael L. Littman</p>
<p><strong>Summary:</strong> The combinatorial explosion that plagues planning and reinforcement learning
(RL) algorithms can be moderated using state abstraction. Prohibitively large
task representations can be condensed such that essential information is
preserved, and consequently, solutions are tractably computable. However, exact
abstractions, which treat only fully-identical situations as equivalent, fail
to present opportunities for abstraction in environments where no two
situations are exactly alike. In this work, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.01358v1' target='_blank'>Generalised Discount Functions applied to a Monte-Carlo AImu
  Implementation</a></h2>
<p><strong>Authors:</strong> Sean Lamont, John Aslanides, Jan Leike, Marcus Hutter</p>
<p><strong>Summary:</strong> In recent years, work has been done to develop the theory of General
Reinforcement Learning (GRL). However, there are few examples demonstrating
these results in a concrete way. In particular, there are no examples
demonstrating the known results regarding gener- alised discounting. We have
added to the GRL simulation platform AIXIjs the functionality to assign an
agent arbitrary discount functions, and an environment which can be used to
determine the effect of discounting on an agent's policy....</p>
<hr>
<h2><a href='http://arxiv.org/abs/1703.05423v1' target='_blank'>End-to-end optimization of goal-driven and visually grounded dialogue
  systems</a></h2>
<p><strong>Authors:</strong> Florian Strub, Harm de Vries, Jeremie Mary, Bilal Piot, Aaron Courville, Olivier Pietquin</p>
<p><strong>Summary:</strong> End-to-end design of dialogue systems has recently become a popular research
topic thanks to powerful tools such as encoder-decoder architectures for
sequence-to-sequence learning. Yet, most current approaches cast human-machine
dialogue management as a supervised learning problem, aiming at predicting the
next utterance of a participant given the full history of the dialogue. This
vision is too simplistic to render the intrinsic planning problem inherent to
dialogue as well as its grounded natu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1706.04317v2' target='_blank'>Schema Networks: Zero-shot Transfer with a Generative Causal Model of
  Intuitive Physics</a></h2>
<p><strong>Authors:</strong> Ken Kansky, Tom Silver, David A. M√©ly, Mohamed Eldawy, Miguel L√°zaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix, Dileep George</p>
<p><strong>Summary:</strong> The recent adaptation of deep neural network-based methods to reinforcement
learning and planning domains has yielded remarkable progress on individual
tasks. Nonetheless, progress on task-to-task transfer remains limited. In
pursuit of efficient and robust generalization, we introduce the Schema
Network, an object-oriented generative physics simulator capable of
disentangling multiple causes of events and reasoning backward through causes
to achieve goals. The richly structured architecture of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1708.09020v1' target='_blank'>Learning to Price with Reference Effects</a></h2>
<p><strong>Authors:</strong> Abbas Kazerouni, Benjamin Van Roy</p>
<p><strong>Summary:</strong> As a firm varies the price of a product, consumers exhibit reference effects,
making purchase decisions based not only on the prevailing price but also the
product's price history. We consider the problem of learning such behavioral
patterns as a monopolist releases, markets, and prices products. This context
calls for pricing decisions that intelligently trade off between maximizing
revenue generated by a current product and probing to gain information for
future benefit. Due to dependence on p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1710.00459v2' target='_blank'>Deep Abstract Q-Networks</a></h2>
<p><strong>Authors:</strong> Melrose Roderick, Christopher Grimm, Stefanie Tellex</p>
<p><strong>Summary:</strong> We examine the problem of learning and planning on high-dimensional domains
with long horizons and sparse rewards. Recent approaches have shown great
successes in many Atari 2600 domains. However, domains with long horizons and
sparse rewards, such as Montezuma's Revenge and Venture, remain challenging for
existing methods. Methods using abstraction (Dietterich 2000; Sutton, Precup,
and Singh 1999) have shown to be useful in tackling long-horizon problems. We
combine recent techniques of deep re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1711.07832v1' target='_blank'>Situationally Aware Options</a></h2>
<p><strong>Authors:</strong> Daniel J. Mankowitz, Aviv Tamar, Shie Mannor</p>
<p><strong>Summary:</strong> Hierarchical abstractions, also known as options -- a type of temporally
extended action (Sutton et. al. 1999) that enables a reinforcement learning
agent to plan at a higher level, abstracting away from the lower-level details.
In this work, we learn reusable options whose parameters can vary, encouraging
different behaviors, based on the current situation. In principle, these
behaviors can include vigor, defence or even risk-averseness. These are some
examples of what we refer to in the broade...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1711.10173v2' target='_blank'>Hierarchical Policy Search via Return-Weighted Density Estimation</a></h2>
<p><strong>Authors:</strong> Takayuki Osa, Masashi Sugiyama</p>
<p><strong>Summary:</strong> Learning an optimal policy from a multi-modal reward function is a
challenging problem in reinforcement learning (RL). Hierarchical RL (HRL)
tackles this problem by learning a hierarchical policy, where multiple option
policies are in charge of different strategies corresponding to modes of a
reward function and a gating policy selects the best option for a given
context. Although HRL has been demonstrated to be promising, current
state-of-the-art methods cannot still perform well in complex rea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1712.05474v4' target='_blank'>AI2-THOR: An Interactive 3D Environment for Visual AI</a></h2>
<p><strong>Authors:</strong> Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, Ali Farhadi</p>
<p><strong>Summary:</strong> We introduce The House Of inteRactions (THOR), a framework for visual AI
research, available at http://ai2thor.allenai.org. AI2-THOR consists of near
photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes
and interact with objects to perform tasks. AI2-THOR enables research in many
different domains including but not limited to deep reinforcement learning,
imitation learning, learning by interaction, planning, visual question
answering, unsupervised representation learning,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1801.05039v3' target='_blank'>Global Convergence of Policy Gradient Methods for the Linear Quadratic
  Regulator</a></h2>
<p><strong>Authors:</strong> Maryam Fazel, Rong Ge, Sham M. Kakade, Mehran Mesbahi</p>
<p><strong>Summary:</strong> Direct policy gradient methods for reinforcement learning and continuous
control problems are a popular approach for a variety of reasons: 1) they are
easy to implement without explicit knowledge of the underlying model 2) they
are an "end-to-end" approach, directly optimizing the performance metric of
interest 3) they inherently allow for richly parameterized policies. A notable
drawback is that even in the most basic continuous control problem (that of
linear quadratic regulators), these metho...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.00495v2' target='_blank'>Transferable Pedestrian Motion Prediction Models at Intersections</a></h2>
<p><strong>Authors:</strong> Macheng Shen, Golnaz Habibi, Jonathan P. How</p>
<p><strong>Summary:</strong> One desirable capability of autonomous cars is to accurately predict the
pedestrian motion near intersections for safe and efficient trajectory
planning. We are interested in developing transfer learning algorithms that can
be trained on the pedestrian trajectories collected at one intersection and yet
still provide accurate predictions of the trajectories at another, previously
unseen intersection. We first discussed the feature selection for transferable
pedestrian motion models in general. Fo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.02884v1' target='_blank'>Policy Gradient With Value Function Approximation For Collective
  Multiagent Planning</a></h2>
<p><strong>Authors:</strong> Duc Thien Nguyen, Akshat Kumar, Hoong Chuin Lau</p>
<p><strong>Summary:</strong> Decentralized (PO)MDPs provide an expressive framework for sequential
decision making in a multiagent system. Given their computational complexity,
recent research has focused on tractable yet practical subclasses of
Dec-POMDPs. We address such a subclass called CDEC-POMDP where the collective
behavior of a population of agents affects the joint-reward and environment
dynamics. Our main contribution is an actor-critic (AC) reinforcement learning
method for optimizing CDEC-POMDP policies. Vanilla...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.07855v3' target='_blank'>Subgoal Discovery for Hierarchical Dialogue Policy Learning</a></h2>
<p><strong>Authors:</strong> Da Tang, Xiujun Li, Jianfeng Gao, Chong Wang, Lihong Li, Tony Jebara</p>
<p><strong>Summary:</strong> Developing agents to engage in complex goal-oriented dialogues is challenging
partly because the main learning signals are very sparse in long conversations.
In this paper, we propose a divide-and-conquer approach that discovers and
exploits the hidden structure of the task to enable efficient policy learning.
First, given successful example dialogues, we propose the Subgoal Discovery
Network (SDN) to divide a complex goal-oriented task into a set of simpler
subgoals in an unsupervised fashion. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.08685v3' target='_blank'>Crawling in Rogue's dungeons with (partitioned) A3C</a></h2>
<p><strong>Authors:</strong> Andrea Asperti, Daniele Cortesi, Francesco Sovrano</p>
<p><strong>Summary:</strong> Rogue is a famous dungeon-crawling video-game of the 80ies, the ancestor of
its gender. Rogue-like games are known for the necessity to explore partially
observable and always different randomly-generated labyrinths, preventing any
form of level replay. As such, they serve as a very natural and challenging
task for reinforcement learning, requiring the acquisition of complex,
non-reactive behaviors involving memory and planning. In this article we show
how, exploiting a version of A3C partitione...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1804.09401v2' target='_blank'>Generative Temporal Models with Spatial Memory for Partially Observed
  Environments</a></h2>
<p><strong>Authors:</strong> Marco Fraccaro, Danilo Jimenez Rezende, Yori Zwols, Alexander Pritzel, S. M. Ali Eslami, Fabio Viola</p>
<p><strong>Summary:</strong> In model-based reinforcement learning, generative and temporal models of
environments can be leveraged to boost agent performance, either by tuning the
agent's representations during training or via use as part of an explicit
planning mechanism. However, their application in practice has been limited to
simplistic environments, due to the difficulty of training such models in
larger, potentially partially-observed and 3D environments. In this work we
introduce a novel action-conditioned generati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.02363v2' target='_blank'>Planning and Learning with Stochastic Action Sets</a></h2>
<p><strong>Authors:</strong> Craig Boutilier, Alon Cohen, Amit Daniely, Avinatan Hassidim, Yishay Mansour, Ofer Meshi, Martin Mladenov, Dale Schuurmans</p>
<p><strong>Summary:</strong> In many practical uses of reinforcement learning (RL) the set of actions
available at a given state is a random variable, with realizations governed by
an exogenous stochastic process. Somewhat surprisingly, the foundations for
such sequential decision processes have been unaddressed. In this work, we
formalize and investigate MDPs with stochastic action sets (SAS-MDPs) to
provide these foundations. We show that optimal policies and value functions in
this model have a structure that admits a co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.10129v1' target='_blank'>Dyna Planning using a Feature Based Generative Model</a></h2>
<p><strong>Authors:</strong> Ryan Faulkner, Doina Precup</p>
<p><strong>Summary:</strong> Dyna-style reinforcement learning is a powerful approach for problems where
not much real data is available. The main idea is to supplement real
trajectories, or sequences of sampled states over time, with simulated ones
sampled from a learned model of the environment. However, in large state
spaces, the problem of learning a good generative model of the environment has
been open so far. We propose to use deep belief networks to learn an
environment model for use in Dyna. We present our approach...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.00973v1' target='_blank'>Sequential Test for the Lowest Mean: From Thompson to Murphy Sampling</a></h2>
<p><strong>Authors:</strong> Emilie Kaufmann, Wouter Koolen, Aurelien Garivier</p>
<p><strong>Summary:</strong> Learning the minimum/maximum mean among a finite set of distributions is a
fundamental sub-task in planning, game tree search and reinforcement learning.
We formalize this learning task as the problem of sequentially testing how the
minimum mean among a finite set of distributions compares to a given threshold.
We develop refined non-asymptotic lower bounds, which show that optimality
mandates very different sampling behavior for a low vs high true minimum. We
show that Thompson Sampling and the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.03107v3' target='_blank'>Temporal Difference Variational Auto-Encoder</a></h2>
<p><strong>Authors:</strong> Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, Theophane Weber</p>
<p><strong>Summary:</strong> To act and plan in complex environments, we posit that agents should have a
mental simulator of the world with three characteristics: (a) it should build
an abstract state representing the condition of the world; (b) it should form a
belief which represents uncertainty on the world; (c) it should go beyond
simple step-by-step simulation, and exhibit temporal abstraction. Motivated by
the absence of a model satisfying all these requirements, we propose TD-VAE, a
generative sequence model that lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1807.02187v2' target='_blank'>Encoding Motion Primitives for Autonomous Vehicles using Virtual
  Velocity Constraints and Neural Network Scheduling</a></h2>
<p><strong>Authors:</strong> Mogens Graf Plessen</p>
<p><strong>Summary:</strong> Within the context of trajectory planning for autonomous vehicles this paper
proposes methods for efficient encoding of motion primitives in neural networks
on top of model-based and gradient-free reinforcement learning. It is
distinguished between 5 core aspects: system model, network architecture,
training algorithm, training tasks selection and hardware/software
implementation. For the system model, a kinematic (3-states-2-controls) and a
dynamic (16-states-2-controls) vehicle model are compa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1807.07134v1' target='_blank'>Representational efficiency outweighs action efficiency in human program
  induction</a></h2>
<p><strong>Authors:</strong> Sophia Sanborn, David D. Bourgin, Michael Chang, Thomas L. Griffiths</p>
<p><strong>Summary:</strong> The importance of hierarchically structured representations for tractable
planning has long been acknowledged. However, the questions of how people
discover such abstractions and how to define a set of optimal abstractions
remain open. This problem has been explored in cognitive science in the problem
solving literature and in computer science in hierarchical reinforcement
learning. Here, we emphasize an algorithmic perspective on learning
hierarchical representations in which the objective is t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.05869v1' target='_blank'>Large-scale Interactive Recommendation with Tree-structured Policy
  Gradient</a></h2>
<p><strong>Authors:</strong> Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, Yong Yu</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has recently been introduced to interactive
recommender systems (IRS) because of its nature of learning from dynamic
interactions and planning for long-run performance. As IRS is always with
thousands of items to recommend (i.e., thousands of actions), most existing
RL-based methods, however, fail to handle such a large discrete action space
problem and thus become inefficient. The existing work that tries to deal with
the large discrete action space problem by utiliz...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.07550v1' target='_blank'>Switch-based Active Deep Dyna-Q: Efficient Adaptive Planning for
  Task-Completion Dialogue Policy Learning</a></h2>
<p><strong>Authors:</strong> Yuexin Wu, Xiujun Li, Jingjing Liu, Jianfeng Gao, Yiming Yang</p>
<p><strong>Summary:</strong> Training task-completion dialogue agents with reinforcement learning usually
requires a large number of real user experiences. The Dyna-Q algorithm extends
Q-learning by integrating a world model, and thus can effectively boost
training efficiency using simulated experiences generated by the world model.
The effectiveness of Dyna-Q, however, depends on the quality of the world model
- or implicitly, the pre-specified ratio of real vs. simulated experiences used
for Q-learning. To this end, we ex...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1811.07631v1' target='_blank'>Chat More If You Like: Dynamic Cue Words Planning to Flow Longer
  Conversations</a></h2>
<p><strong>Authors:</strong> Lili Yao, Ruijian Xu, Chao Li, Dongyan Zhao, Rui Yan</p>
<p><strong>Summary:</strong> To build an open-domain multi-turn conversation system is one of the most
interesting and challenging tasks in Artificial Intelligence. Many research
efforts have been dedicated to building such dialogue systems, yet few shed
light on modeling the conversation flow in an ongoing dialogue. Besides, it is
common for people to talk about highly relevant aspects during a conversation.
And the topics are coherent and drift naturally, which demonstrates the
necessity of dialogue flow modeling. To this...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.06576v1' target='_blank'>Towards Physically Safe Reinforcement Learning under Supervision</a></h2>
<p><strong>Authors:</strong> Yinan Zhang, Devin Balkcom, Haoxiang Li</p>
<p><strong>Summary:</strong> This paper addresses the question of how a previously available control
policy $\pi_s$ can be used as a supervisor to more quickly and safely train a
new learned control policy $\pi_L$ for a robot. A weighted average of the
supervisor and learned policies is used during trials, with a heavier weight
initially on the supervisor, in order to allow safe and useful physical trials
while the learned policy is still ineffective. During the process, the weight
is adjusted to favor the learned policy. A...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.02870v1' target='_blank'>Visual search and recognition for robot task execution and monitoring</a></h2>
<p><strong>Authors:</strong> Lorenzo Mauro, Francesco Puja, Simone Grazioso, Valsamis Ntouskos, Marta Sanzari, Edoardo Alati, Fiora Pirri</p>
<p><strong>Summary:</strong> Visual search of relevant targets in the environment is a crucial robot
skill. We propose a preliminary framework for the execution monitor of a robot
task, taking care of the robot attitude to visually searching the environment
for targets involved in the task. Visual search is also relevant to recover
from a failure. The framework exploits deep reinforcement learning to acquire a
"common sense" scene structure and it takes advantage of a deep convolutional
network to detect objects and relevan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.09996v1' target='_blank'>The Termination Critic</a></h2>
<p><strong>Authors:</strong> Anna Harutyunyan, Will Dabney, Diana Borsa, Nicolas Heess, Remi Munos, Doina Precup</p>
<p><strong>Summary:</strong> In this work, we consider the problem of autonomously discovering behavioral
abstractions, or options, for reinforcement learning agents. We propose an
algorithm that focuses on the termination condition, as opposed to -- as is
common -- the policy. The termination condition is usually trained to optimize
a control objective: an option ought to terminate if another has better value.
We offer a different, information-theoretic perspective, and propose that
terminations should focus instead on the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.03674v2' target='_blank'>Learning Self-Game-Play Agents for Combinatorial Optimization Problems</a></h2>
<p><strong>Authors:</strong> Ruiyang Xu, Karl Lieberherr</p>
<p><strong>Summary:</strong> Recent progress in reinforcement learning (RL) using self-game-play has shown
remarkable performance on several board games (e.g., Chess and Go) as well as
video games (e.g., Atari games and Dota2). It is plausible to consider that RL,
starting from zero knowledge, might be able to gradually approximate a winning
strategy after a certain amount of training. In this paper, we explore neural
Monte-Carlo-Tree-Search (neural MCTS), an RL algorithm which has been applied
successfully by DeepMind to p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.09295v1' target='_blank'>DQN with model-based exploration: efficient learning on environments
  with sparse rewards</a></h2>
<p><strong>Authors:</strong> Stephen Zhen Gou, Yuyang Liu</p>
<p><strong>Summary:</strong> We propose Deep Q-Networks (DQN) with model-based exploration, an algorithm
combining both model-free and model-based approaches that explores better and
learns environments with sparse rewards more efficiently. DQN is a
general-purpose, model-free algorithm and has been proven to perform well in a
variety of tasks including Atari 2600 games since it's first proposed by Minh
et el. However, like many other reinforcement learning (RL) algorithms, DQN
suffers from poor sample efficiency when rewar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.11981v3' target='_blank'>Regularizing Trajectory Optimization with Denoising Autoencoders</a></h2>
<p><strong>Authors:</strong> Rinu Boney, Norman Di Palo, Mathias Berglund, Alexander Ilin, Juho Kannala, Antti Rasmus, Harri Valpola</p>
<p><strong>Summary:</strong> Trajectory optimization using a learned model of the environment is one of
the core elements of model-based reinforcement learning. This procedure often
suffers from exploiting inaccuracies of the learned model. We propose to
regularize trajectory optimization by means of a denoising autoencoder that is
trained on the same trajectories as the model of the environment. We show that
the proposed regularization leads to improved planning with both gradient-based
and gradient-free optimizers. We als...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.08857v1' target='_blank'>Deep Neuroevolution of Recurrent and Discrete World Models</a></h2>
<p><strong>Authors:</strong> Sebastian Risi, Kenneth O. Stanley</p>
<p><strong>Summary:</strong> Neural architectures inspired by our own human cognitive system, such as the
recently introduced world models, have been shown to outperform traditional
deep reinforcement learning (RL) methods in a variety of different domains.
Instead of the relatively simple architectures employed in most RL experiments,
world models rely on multiple different neural components that are responsible
for visual information processing, memory, and decision-making. However, so far
the components of these models h...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.01388v1' target='_blank'>How to Build User Simulators to Train RL-based Dialog Systems</a></h2>
<p><strong>Authors:</strong> Weiyan Shi, Kun Qian, Xuewei Wang, Zhou Yu</p>
<p><strong>Summary:</strong> User simulators are essential for training reinforcement learning (RL) based
dialog models. The performance of the simulator directly impacts the RL policy.
However, building a good user simulator that models real user behaviors is
challenging. We propose a method of standardizing user simulator building that
can be used by the community to compare dialog system quality using the same
set of user simulators fairly. We present implementations of six user
simulators trained with different dialog p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.02769v2' target='_blank'>Adaptive Trust Region Policy Optimization: Global Convergence and Faster
  Rates for Regularized MDPs</a></h2>
<p><strong>Authors:</strong> Lior Shani, Yonathan Efroni, Shie Mannor</p>
<p><strong>Summary:</strong> Trust region policy optimization (TRPO) is a popular and empirically
successful policy search algorithm in Reinforcement Learning (RL) in which a
surrogate problem, that restricts consecutive policies to be 'close' to one
another, is iteratively solved. Nevertheless, TRPO has been considered a
heuristic algorithm inspired by Conservative Policy Iteration (CPI). We show
that the adaptive scaling mechanism used in TRPO is in fact the natural "RL
version" of traditional trust-region methods from co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06174v1' target='_blank'>Petri Net Machines for Human-Agent Interaction</a></h2>
<p><strong>Authors:</strong> Christian Dondrup, Ioannis Papaioannou, Oliver Lemon</p>
<p><strong>Summary:</strong> Smart speakers and robots become ever more prevalent in our daily lives.
These agents are able to execute a wide range of tasks and actions and,
therefore, need systems to control their execution. Current state-of-the-art
such as (deep) reinforcement learning, however, requires vast amounts of data
for training which is often hard to come by when interacting with humans. To
overcome this issue, most systems still rely on Finite State Machines. We
introduce Petri Net Machines which present a form...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.01728v2' target='_blank'>Machine learning strategies for path-planning microswimmers in turbulent
  flows</a></h2>
<p><strong>Authors:</strong> Jaya Kumar Alageshan, Akhilesh Kumar Verma, J√©r√©mie Bec, Rahul Pandit</p>
<p><strong>Summary:</strong> We develop an adversarial-reinforcement learning scheme for microswimmers in
statistically homogeneous and isotropic turbulent fluid flows, in both two (2D)
and three dimensions (3D). We show that this scheme allows microswimmers to
find non-trivial paths, which enable them to reach a target on average in less
time than a naive microswimmer, which tries, at any instant of time and at a
given position in space, to swim in the direction of the target. We use
pseudospectral direct numerical simulat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.00617v2' target='_blank'>Explicit Explore-Exploit Algorithms in Continuous State Spaces</a></h2>
<p><strong>Authors:</strong> Mikael Henaff</p>
<p><strong>Summary:</strong> We present a new model-based algorithm for reinforcement learning (RL) which
consists of explicit exploration and exploitation phases, and is applicable in
large or infinite state spaces. The algorithm maintains a set of dynamics
models consistent with current experience and explores by finding policies
which induce high disagreement between their state predictions. It then
exploits using the refined set of models or experience gathered during
exploration. We show that under realizability and op...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.00926v2' target='_blank'>Learning Algorithmic Solutions to Symbolic Planning Tasks with a Neural
  Computer Architecture</a></h2>
<p><strong>Authors:</strong> Daniel Tanneberg, Elmar Rueckert, Jan Peters</p>
<p><strong>Summary:</strong> A key feature of intelligent behavior is the ability to learn abstract
strategies that transfer to unfamiliar problems. Therefore, we present a novel
architecture, based on memory-augmented networks, that is inspired by the von
Neumann and Harvard architectures of modern computers. This architecture
enables the learning of abstract algorithmic solutions via Evolution Strategies
in a reinforcement learning setting. Applied to Sokoban, sliding block puzzle
and robotic manipulation tasks, we show t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.03594v2' target='_blank'>Robo-PlaNet: Learning to Poke in a Day</a></h2>
<p><strong>Authors:</strong> Maxime Chevalier-Boisvert, Guillaume Alain, Florian Golemo, Derek Nowrouzezahrai</p>
<p><strong>Summary:</strong> Recently, the Deep Planning Network (PlaNet) approach was introduced as a
model-based reinforcement learning method that learns environment dynamics
directly from pixel observations. This architecture is useful for learning
tasks in which either the agent does not have access to meaningful states (like
position/velocity of robotic joints) or where the observed states significantly
deviate from the physical state of the agent (which is commonly the case in
low-cost robots in the form of backlash ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1911.07794v5' target='_blank'>Gamma-Nets: Generalizing Value Estimation over Timescale</a></h2>
<p><strong>Authors:</strong> Craig Sherstan, Shibhansh Dohare, James MacGlashan, Johannes G√ºnther, Patrick M. Pilarski</p>
<p><strong>Summary:</strong> We present $\Gamma$-nets, a method for generalizing value function estimation
over timescale. By using the timescale as one of the estimator's inputs we can
estimate value for arbitrary timescales. As a result, the prediction target for
any timescale is available and we are free to train on multiple timescales at
each timestep. Here we empirically evaluate $\Gamma$-nets in the policy
evaluation setting. We first demonstrate the approach on a square wave and then
on a robot arm using linear funct...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.05453v1' target='_blank'>Value-of-Information based Arbitration between Model-based and
  Model-free Control</a></h2>
<p><strong>Authors:</strong> Krishn Bera, Yash Mandilwar, Bapi Raju</p>
<p><strong>Summary:</strong> There have been numerous attempts in explaining the general learning
behaviours using model-based and model-free methods. While the model-based
control is flexible yet computationally expensive in planning, the model-free
control is quick but inflexible. The model-based control is therefore immune
from reward devaluation and contingency degradation. Multiple arbitration
schemes have been suggested to achieve the data efficiency and computational
efficiency of model-based and model-free control r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.02366v2' target='_blank'>What can robotics research learn from computer vision research?</a></h2>
<p><strong>Authors:</strong> Peter Corke, Feras Dayoub, David Hall, John Skinner, Niko S√ºnderhauf</p>
<p><strong>Summary:</strong> The computer vision and robotics research communities are each strong.
However progress in computer vision has become turbo-charged in recent years
due to big data, GPU computing, novel learning algorithms and a very effective
research methodology. By comparison, progress in robotics seems slower. It is
true that robotics came later to exploring the potential of learning -- the
advantages over the well-established body of knowledge in dynamics, kinematics,
planning and control is still being deb...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.06793v1' target='_blank'>Learning Options from Demonstration using Skill Segmentation</a></h2>
<p><strong>Authors:</strong> Matthew Cockcroft, Shahil Mawjee, Steven James, Pravesh Ranchod</p>
<p><strong>Summary:</strong> We present a method for learning options from segmented demonstration
trajectories. The trajectories are first segmented into skills using
nonparametric Bayesian clustering and a reward function for each segment is
then learned using inverse reinforcement learning. From this, a set of inferred
trajectories for the demonstration are generated. Option initiation sets and
termination conditions are learned from these trajectories using the one-class
support vector machine clustering algorithm. We d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.04317v4' target='_blank'>Machine Learning Approaches For Motor Learning: A Short Review</a></h2>
<p><strong>Authors:</strong> Baptiste Caramiaux, Jules Fran√ßoise, Wanyu Liu, T√©o Sanchez, Fr√©d√©ric Bevilacqua</p>
<p><strong>Summary:</strong> Machine learning approaches have seen considerable applications in human
movement modeling, but remain limited for motor learning. Motor learning
requires accounting for motor variability, and poses new challenges as the
algorithms need to be able to differentiate between new movements and variation
of known ones. In this short review, we outline existing machine learning
models for motor learning and their adaptation capabilities. We identify and
describe three types of adaptation: Parameter ad...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.09516v1' target='_blank'>Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation</a></h2>
<p><strong>Authors:</strong> Yaqi Duan, Mengdi Wang</p>
<p><strong>Summary:</strong> This paper studies the statistical theory of batch data reinforcement
learning with function approximation. Consider the off-policy evaluation
problem, which is to estimate the cumulative value of a new target policy from
logged history generated by unknown behavioral policies. We study a
regression-based fitted Q iteration method, and show that it is equivalent to a
model-based method that estimates a conditional mean embedding of the
transition operator. We prove that this method is informatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.11637v1' target='_blank'>Learning Navigation Costs from Demonstration in Partially Observable
  Environments</a></h2>
<p><strong>Authors:</strong> Tianyu Wang, Vikas Dhiman, Nikolay Atanasov</p>
<p><strong>Summary:</strong> This paper focuses on inverse reinforcement learning (IRL) to enable safe and
efficient autonomous navigation in unknown partially observable environments.
The objective is to infer a cost function that explains expert-demonstrated
navigation behavior while relying only on the observations and state-control
trajectory used by the expert. We develop a cost function representation
composed of two parts: a probabilistic occupancy encoder, with recurrent
dependence on the observation sequence, and a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.11963v1' target='_blank'>Plannable Approximations to MDP Homomorphisms: Equivariance under
  Actions</a></h2>
<p><strong>Authors:</strong> Elise van der Pol, Thomas Kipf, Frans A. Oliehoek, Max Welling</p>
<p><strong>Summary:</strong> This work exploits action equivariance for representation learning in
reinforcement learning. Equivariance under actions states that transitions in
the input space are mirrored by equivalent transitions in latent space, while
the map and transition functions should also commute. We introduce a
contrastive loss function that enforces action equivariance on the learned
representations. We prove that when our loss is zero, we have a homomorphism of
a deterministic Markov Decision Process (MDP). Lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.12086v1' target='_blank'>Reinforcement Learning of Risk-Constrained Policies in Markov Decision
  Processes</a></h2>
<p><strong>Authors:</strong> Tomas Brazdil, Krishnendu Chatterjee, Petr Novotny, Jiri Vahala</p>
<p><strong>Summary:</strong> Markov decision processes (MDPs) are the defacto frame-work for sequential
decision making in the presence ofstochastic uncertainty. A classical
optimization criterion forMDPs is to maximize the expected discounted-sum
pay-off, which ignores low probability catastrophic events withhighly negative
impact on the system. On the other hand,risk-averse policies require the
probability of undesirableevents to be below a given threshold, but they do not
accountfor optimization of the expected payoff. W...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.08114v1' target='_blank'>Mutual Information Maximization for Robust Plannable Representations</a></h2>
<p><strong>Authors:</strong> Yiming Ding, Ignasi Clavera, Pieter Abbeel</p>
<p><strong>Summary:</strong> Extending the capabilities of robotics to real-world complex, unstructured
environments requires the need of developing better perception systems while
maintaining low sample complexity. When dealing with high-dimensional state
spaces, current methods are either model-free or model-based based on
reconstruction objectives. The sample inefficiency of the former constitutes a
major barrier for applying them to the real-world. The later, while they
present low sample complexity, they learn latent s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.09645v1' target='_blank'>The Second Type of Uncertainty in Monte Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker</p>
<p><strong>Summary:</strong> Monte Carlo Tree Search (MCTS) efficiently balances exploration and
exploitation in tree search based on count-derived uncertainty. However, these
local visit counts ignore a second type of uncertainty induced by the size of
the subtree below an action. We first show how, due to the lack of this second
uncertainty type, MCTS may completely fail in well-known sparse exploration
problems, known from the reinforcement learning community. We then introduce a
new algorithm, which estimates the size o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.04363v1' target='_blank'>Hallucinating Value: A Pitfall of Dyna-style Planning with Imperfect
  Environment Models</a></h2>
<p><strong>Authors:</strong> Taher Jafferjee, Ehsan Imani, Erin Talvitie, Martha White, Micheal Bowling</p>
<p><strong>Summary:</strong> Dyna-style reinforcement learning (RL) agents improve sample efficiency over
model-free RL agents by updating the value function with simulated experience
generated by an environment model. However, it is often difficult to learn
accurate models of environment dynamics, and even small errors may result in
failure of Dyna agents. In this paper, we investigate one type of model error:
hallucinated states. These are states generated by the model, but that are not
real states of the environment. We ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.05043v2' target='_blank'>Learning Navigation Costs from Demonstration with Semantic Observations</a></h2>
<p><strong>Authors:</strong> Tianyu Wang, Vikas Dhiman, Nikolay Atanasov</p>
<p><strong>Summary:</strong> This paper focuses on inverse reinforcement learning (IRL) for autonomous
robot navigation using semantic observations. The objective is to infer a cost
function that explains demonstrated behavior while relying only on the expert's
observations and state-control trajectory. We develop a map encoder, which
infers semantic class probabilities from the observation sequence, and a cost
encoder, defined as deep neural network over the semantic features. Since the
expert cost is not directly observab...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.09118v2' target='_blank'>$Q$-learning with Logarithmic Regret</a></h2>
<p><strong>Authors:</strong> Kunhe Yang, Lin F. Yang, Simon S. Du</p>
<p><strong>Summary:</strong> This paper presents the first non-asymptotic result showing that a model-free
algorithm can achieve a logarithmic cumulative regret for episodic tabular
reinforcement learning if there exists a strictly positive sub-optimality gap
in the optimal $Q$-function. We prove that the optimistic $Q$-learning studied
in [Jin et al. 2018] enjoys a ${\mathcal{O}}\left(\frac{SA\cdot
\mathrm{poly}\left(H\right)}{\Delta_{\min}}\log\left(SAT\right)\right)$
cumulative regret bound, where $S$ is the number of st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.10277v1' target='_blank'>Active Learning for Nonlinear System Identification with Guarantees</a></h2>
<p><strong>Authors:</strong> Horia Mania, Michael I. Jordan, Benjamin Recht</p>
<p><strong>Summary:</strong> While the identification of nonlinear dynamical systems is a fundamental
building block of model-based reinforcement learning and feedback control, its
sample complexity is only understood for systems that either have discrete
states and actions or for systems that can be identified from data generated by
i.i.d. random inputs. Nonetheless, many interesting dynamical systems have
continuous states and actions and can only be identified through a judicious
choice of inputs. Motivated by practical ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.00391v3' target='_blank'>Convex Regularization in Monte-Carlo Tree Search</a></h2>
<p><strong>Authors:</strong> Tuan Dam, Carlo D'Eramo, Jan Peters, Joni Pajarinen</p>
<p><strong>Summary:</strong> Monte-Carlo planning and Reinforcement Learning (RL) are essential to
sequential decision making. The recent AlphaGo and AlphaZero algorithms have
shown how to successfully combine these two paradigms in order to solve large
scale sequential decision problems. These methodologies exploit a variant of
the well-known UCT algorithm to trade off exploitation of good actions and
exploration of unvisited states, but their empirical success comes at the cost
of poor sample-efficiency and high computati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.01995v2' target='_blank'>Bidirectional Model-based Policy Optimization</a></h2>
<p><strong>Authors:</strong> Hang Lai, Jian Shen, Weinan Zhang, Yong Yu</p>
<p><strong>Summary:</strong> Model-based reinforcement learning approaches leverage a forward dynamics
model to support planning and decision making, which, however, may fail
catastrophically if the model is inaccurate. Although there are several
existing methods dedicated to combating the model error, the potential of the
single forward model is still limited. In this paper, we propose to
additionally construct a backward dynamics model to reduce the reliance on
accuracy in forward model predictions. We develop a novel met...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.07170v2' target='_blank'>Goal-Aware Prediction: Learning to Model What Matters</a></h2>
<p><strong>Authors:</strong> Suraj Nair, Silvio Savarese, Chelsea Finn</p>
<p><strong>Summary:</strong> Learned dynamics models combined with both planning and policy learning
algorithms have shown promise in enabling artificial agents to learn to perform
many diverse tasks with limited supervision. However, one of the fundamental
challenges in using a learned forward dynamics model is the mismatch between
the objective of the learned model (future state reconstruction), and that of
the downstream planner or policy (completing a specified task). This issue is
exacerbated by vision-based control ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.00824v4' target='_blank'>Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds</a></h2>
<p><strong>Authors:</strong> Lirui Wang, Yu Xiang, Wei Yang, Arsalan Mousavian, Dieter Fox</p>
<p><strong>Summary:</strong> 6D robotic grasping beyond top-down bin-picking scenarios is a challenging
task. Previous solutions based on 6D grasp synthesis with robot motion planning
usually operate in an open-loop setting, which are sensitive to grasp synthesis
errors. In this work, we propose a new method for learning closed-loop control
policies for 6D grasping. Our policy takes a segmented point cloud of an object
from an egocentric camera as input, and outputs continuous 6D control actions
of the robot gripper for gra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.03790v1' target='_blank'>Text-based RL Agents with Commonsense Knowledge: New Challenges,
  Environments and Baselines</a></h2>
<p><strong>Authors:</strong> Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, Murray Campbell</p>
<p><strong>Summary:</strong> Text-based games have emerged as an important test-bed for Reinforcement
Learning (RL) research, requiring RL agents to combine grounded language
understanding with sequential decision making. In this paper, we examine the
problem of infusing RL agents with commonsense knowledge. Such knowledge would
allow agents to efficiently act in the world by pruning out implausible
actions, and to perform look-ahead planning to determine how current actions
might affect future world states. We design a new...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.12870v3' target='_blank'>Efficient Learning in Non-Stationary Linear Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Ahmed Touati, Pascal Vincent</p>
<p><strong>Summary:</strong> We study episodic reinforcement learning in non-stationary linear (a.k.a.
low-rank) Markov Decision Processes (MDPs), i.e, both the reward and transition
kernel are linear with respect to a given feature map and are allowed to evolve
either slowly or abruptly over time. For this problem setting, we propose
OPT-WLSVI an optimistic model-free algorithm based on weighted least squares
value iteration which uses exponential weights to smoothly forget data that are
far in the past. We show that our a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.03234v1' target='_blank'>Amortized Q-learning with Model-based Action Proposals for Autonomous
  Driving on Highways</a></h2>
<p><strong>Authors:</strong> Branka Mirchevska, Maria H√ºgle, Gabriel Kalweit, Moritz Werling, Joschka Boedecker</p>
<p><strong>Summary:</strong> Well-established optimization-based methods can guarantee an optimal
trajectory for a short optimization horizon, typically no longer than a few
seconds. As a result, choosing the optimal trajectory for this short horizon
may still result in a sub-optimal long-term solution. At the same time, the
resulting short-term trajectories allow for effective, comfortable and provable
safe maneuvers in a dynamic traffic environment. In this work, we address the
question of how to ensure an optimal long-te...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.09456v2' target='_blank'>Stabilizing Q Learning Via Soft Mellowmax Operator</a></h2>
<p><strong>Authors:</strong> Yaozhong Gan, Zhe Zhang, Xiaoyang Tan</p>
<p><strong>Summary:</strong> Learning complicated value functions in high dimensional state space by
function approximation is a challenging task, partially due to that the
max-operator used in temporal difference updates can theoretically cause
instability for most linear or non-linear approximation schemes. Mellowmax is a
recently proposed differentiable and non-expansion softmax operator that allows
a convergent behavior in learning and planning. Unfortunately, the performance
bound for the fixed point it converges to re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.08137v2' target='_blank'>Hierarchical Human-Motion Prediction and Logic-Geometric Programming for
  Minimal Interference Human-Robot Tasks</a></h2>
<p><strong>Authors:</strong> An T. Le, Philipp Kratzer, Simon Hagenmayer, Marc Toussaint, Jim Mainprice</p>
<p><strong>Summary:</strong> In this paper, we tackle the problem of human-robot coordination in sequences
of manipulation tasks. Our approach integrates hierarchical human motion
prediction with Task and Motion Planning (TAMP). We first devise a hierarchical
motion prediction approach by combining Inverse Reinforcement Learning and
short-term motion prediction using a Recurrent Neural Network. In a second
step, we propose a dynamic version of the TAMP algorithm Logic-Geometric
Programming (LGP). Our version of Dynamic LGP,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.02040v3' target='_blank'>Towards Learning to Play Piano with Dexterous Hands and Touch</a></h2>
<p><strong>Authors:</strong> Huazhe Xu, Yuping Luo, Shaoxiong Wang, Trevor Darrell, Roberto Calandra</p>
<p><strong>Summary:</strong> The virtuoso plays the piano with passion, poetry and extraordinary technical
ability. As Liszt said (a virtuoso)must call up scent and blossom, and breathe
the breath of life. The strongest robots that can play a piano are based on a
combination of specialized robot hands/piano and hardcoded planning algorithms.
In contrast to that, in this paper, we demonstrate how an agent can learn
directly from machine-readable music score to play the piano with dexterous
hands on a simulated piano using re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.03926v1' target='_blank'>Reconciling Rewards with Predictive State Representations</a></h2>
<p><strong>Authors:</strong> Andrea Baisero, Christopher Amato</p>
<p><strong>Summary:</strong> Predictive state representations (PSRs) are models of controlled non-Markov
observation sequences which exhibit the same generative process governing POMDP
observations without relying on an underlying latent state. In that respect, a
PSR is indistinguishable from the corresponding POMDP. However, PSRs
notoriously ignore the notion of rewards, which undermines the general utility
of PSR models for control, planning, or reinforcement learning. Therefore, we
describe a sufficient and necessary acc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.05022v1' target='_blank'>Potential-based Reward Shaping in Sokoban</a></h2>
<p><strong>Authors:</strong> Zhao Yang, Mike Preuss, Aske Plaat</p>
<p><strong>Summary:</strong> Learning to solve sparse-reward reinforcement learning problems is difficult,
due to the lack of guidance towards the goal. But in some problems, prior
knowledge can be used to augment the learning process. Reward shaping is a way
to incorporate prior knowledge into the original reward function in order to
speed up the learning. While previous work has investigated the use of expert
knowledge to generate potential functions, in this work, we study whether we
can use a search algorithm(A*) to aut...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.05767v1' target='_blank'>Computation Rate Maximum for Mobile Terminals in UAV-assisted Wireless
  Powered MEC Networks with Fairness Constraint</a></h2>
<p><strong>Authors:</strong> Xiaoyi Zhou, Liang Huang, Tong Ye, Weiqiang Sun</p>
<p><strong>Summary:</strong> This paper investigates an unmanned aerial vehicle (UAV)-assisted wireless
powered mobile-edge computing (MEC) system, where the UAV powers the mobile
terminals by wireless power transfer (WPT) and provides computation service for
them. We aim to maximize the computation rate of terminals while ensuring
fairness among them. Considering the random trajectories of mobile terminals,
we propose a soft actor-critic (SAC)-based UAV trajectory planning and resource
allocation (SAC-TR) algorithm, which ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1509.02971v6' target='_blank'>Continuous control with deep reinforcement learning</a></h2>
<p><strong>Authors:</strong> Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra</p>
<p><strong>Summary:</strong> We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our alg...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1610.00696v2' target='_blank'>Deep Visual Foresight for Planning Robot Motion</a></h2>
<p><strong>Authors:</strong> Chelsea Finn, Sergey Levine</p>
<p><strong>Summary:</strong> A key challenge in scaling up robot learning to many skills and environments
is removing the need for human supervision, so that robots can collect their
own data and improve their own performance without being limited by the cost of
requesting human feedback. Model-based reinforcement learning holds the promise
of enabling an agent to learn to predict the effects of its actions, which
could provide flexible predictive models for a wide range of tasks and
environments, without detailed human sup...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1803.05156v1' target='_blank'>The 2017 AIBIRDS Competition</a></h2>
<p><strong>Authors:</strong> Matthew Stephenson, Jochen Renz, Xiaoyu Ge, Peng Zhang</p>
<p><strong>Summary:</strong> This paper presents an overview of the sixth AIBIRDS competition, held at the
26th International Joint Conference on Artificial Intelligence. This
competition tasked participants with developing an intelligent agent which can
play the physics-based puzzle game Angry Birds. This game uses a sophisticated
physics engine that requires agents to reason and predict the outcome of
actions with only limited environmental information. Agents entered into this
competition were required to solve a wide as...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.06519v2' target='_blank'>Factorized Machine Self-Confidence for Decision-Making Agents</a></h2>
<p><strong>Authors:</strong> Brett W Israelsen, Nisar R Ahmed, Eric Frew, Dale Lawrence, Brian Argrow</p>
<p><strong>Summary:</strong> Algorithmic assurances from advanced autonomous systems assist human users in
understanding, trusting, and using such systems appropriately. Designing these
systems with the capacity of assessing their own capabilities is one approach
to creating an algorithmic assurance. The idea of `machine self-confidence' is
introduced for autonomous systems. Using a factorization based framework for
self-confidence assessment, one component of self-confidence, called
`solver-quality', is discussed in the co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.11583v4' target='_blank'>Learning Abstract Options</a></h2>
<p><strong>Authors:</strong> Matthew Riemer, Miao Liu, Gerald Tesauro</p>
<p><strong>Summary:</strong> Building systems that autonomously create temporal abstractions from data is
a key challenge in scaling learning and planning in reinforcement learning. One
popular approach for addressing this challenge is the options framework (Sutton
et al., 1999). However, only recently in (Bacon et al., 2017) was a policy
gradient theorem derived for online learning of general purpose options in an
end to end fashion. In this work, we extend previous work on this topic that
only focuses on learning a two-le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.12162v5' target='_blank'>Model-Based Active Exploration</a></h2>
<p><strong>Authors:</strong> Pranav Shyam, Wojciech Ja≈õkowski, Faustino Gomez</p>
<p><strong>Summary:</strong> Efficient exploration is an unsolved problem in Reinforcement Learning which
is usually addressed by reactively rewarding the agent for fortuitously
encountering novel situations. This paper introduces an efficient active
exploration algorithm, Model-Based Active eXploration (MAX), which uses an
ensemble of forward models to plan to observe novel events. This is carried out
by optimizing agent behaviour with respect to a measure of novelty derived from
the Bayesian perspective of exploration, wh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.13400v3' target='_blank'>Differentiable MPC for End-to-end Planning and Control</a></h2>
<p><strong>Authors:</strong> Brandon Amos, Ivan Dario Jimenez Rodriguez, Jacob Sacks, Byron Boots, J. Zico Kolter</p>
<p><strong>Summary:</strong> We present foundations for using Model Predictive Control (MPC) as a
differentiable policy class for reinforcement learning in continuous state and
action spaces. This provides one way of leveraging and combining the advantages
of model-free and model-based approaches. Specifically, we differentiate
through MPC by using the KKT conditions of the convex approximation at a fixed
point of the controller. Using this strategy, we are able to learn the cost and
dynamics of a controller via end-to-end ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.03177v2' target='_blank'>Structured agents for physical construction</a></h2>
<p><strong>Authors:</strong> Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L. Stachenfeld, Pushmeet Kohli, Peter W. Battaglia, Jessica B. Hamrick</p>
<p><strong>Summary:</strong> Physical construction---the ability to compose objects, subject to physical
dynamics, to serve some function---is fundamental to human intelligence. We
introduce a suite of challenging physical construction tasks inspired by how
children play with blocks, such as matching a target configuration, stacking
blocks to connect objects together, and creating shelter-like structures over
target objects. We examine how a range of deep reinforcement learning agents
fare on these challenges, and introduce...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.06471v1' target='_blank'>Synthesis of Provably Correct Autonomy Protocols for Shared Control</a></h2>
<p><strong>Authors:</strong> Murat Cubuktepe, Nils Jansen, Mohammed Alsiekh, Ufuk Topcu</p>
<p><strong>Summary:</strong> We synthesize shared control protocols subject to probabilistic temporal
logic specifications. More specifically, we develop a framework in which a
human and an autonomy protocol can issue commands to carry out a certain task.
We blend these commands into a joint input to a robot. We model the interaction
between the human and the robot as a Markov decision process (MDP) that
represents the shared control scenario. Using inverse reinforcement learning,
we obtain an abstraction of the human's beh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.07030v2' target='_blank'>Knowledge-Based Sequential Decision-Making Under Uncertainty</a></h2>
<p><strong>Authors:</strong> Daoming Lyu</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) algorithms have achieved great success on
sequential decision-making problems, yet is criticized for the lack of
data-efficiency and explainability. Especially, explainability of subtasks is
critical in hierarchical decision-making since it enhances the transparency of
black-box-style DRL methods and helps the RL practitioners to understand the
high-level behavior of the system better. To improve the data-efficiency and
explainability of DRL, declarative knowled...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.12941v2' target='_blank'>Learning Compositional Neural Programs with Recursive Tree Search and
  Planning</a></h2>
<p><strong>Authors:</strong> Thomas Pierrot, Guillaume Ligner, Scott Reed, Olivier Sigaud, Nicolas Perrin, Alexandre Laterre, David Kas, Karim Beguir, Nando de Freitas</p>
<p><strong>Summary:</strong> We propose a novel reinforcement learning algorithm, AlphaNPI, that
incorporates the strengths of Neural Programmer-Interpreters (NPI) and
AlphaZero. NPI contributes structural biases in the form of modularity,
hierarchy and recursion, which are helpful to reduce sample complexity, improve
generalization and increase interpretability. AlphaZero contributes powerful
neural network guided search algorithms, which we augment with recursion.
AlphaNPI only assumes a hierarchical program specification...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.03613v2' target='_blank'>Data Efficient Reinforcement Learning for Legged Robots</a></h2>
<p><strong>Authors:</strong> Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, Vikas Sindhwani</p>
<p><strong>Summary:</strong> We present a model-based framework for robot locomotion that achieves walking
based on only 4.5 minutes (45,000 control steps) of data collected on a
quadruped robot. To accurately model the robot's dynamics over a long horizon,
we introduce a loss function that tracks the model's prediction over multiple
timesteps. We adapt model predictive control to account for planning latency,
which allows the learned model to be used for real time control. Additionally,
to ensure safe exploration during mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.04484v1' target='_blank'>Co-training for Policy Learning</a></h2>
<p><strong>Authors:</strong> Jialin Song, Ravi Lanka, Yisong Yue, Masahiro Ono</p>
<p><strong>Summary:</strong> We study the problem of learning sequential decision-making policies in
settings with multiple state-action representations. Such settings naturally
arise in many domains, such as planning (e.g., multiple integer programming
formulations) and various combinatorial optimization problems (e.g., those with
both integer programming and graph-based formulations). Inspired by the
classical co-training framework for classification, we study the problem of
co-training for policy learning. We present suf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.11388v1' target='_blank'>Learning to Solve a Rubik's Cube with a Dexterous Hand</a></h2>
<p><strong>Authors:</strong> Tingguang Li, Weitao Xi, Meng Fang, Jia Xu, Max Qing-Hu Meng</p>
<p><strong>Summary:</strong> We present a learning-based approach to solving a Rubik's cube with a
multi-fingered dexterous hand. Despite the promising performance of dexterous
in-hand manipulation, solving complex tasks which involve multiple steps and
diverse internal object structure has remained an important, yet challenging
task. In this paper, we tackle this challenge with a hierarchical deep
reinforcement learning method, which separates planning and manipulation. A
model-based cube solver finds an optimal move seque...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.04300v3' target='_blank'>Learning Discrete State Abstractions With Deep Variational Inference</a></h2>
<p><strong>Authors:</strong> Ondrej Biza, Robert Platt, Jan-Willem van de Meent, Lawson L. S. Wong</p>
<p><strong>Summary:</strong> Abstraction is crucial for effective sequential decision making in domains
with large state spaces. In this work, we propose an information bottleneck
method for learning approximate bisimulations, a type of state abstraction. We
use a deep neural encoder to map states onto continuous embeddings. We map
these embeddings onto a discrete representation using an action-conditioned
hidden Markov model, which is trained end-to-end with the neural network. Our
method is suited for environments with hi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.06751v2' target='_blank'>Robot Playing Kendama with Model-Based and Model-Free Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Shidi Li</p>
<p><strong>Summary:</strong> Several model-based and model-free methods have been proposed for the robot
trajectory learning task. Both approaches have their benefits and drawbacks.
They can usually complement each other. Many research works are trying to
integrate some model-based and model-free methods into one algorithm and
perform well in simulators or quasi-static robot tasks. Difficulties still
exist when algorithms are used in particular trajectory learning tasks. In this
paper, we propose a robot trajectory learning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.04697v1' target='_blank'>Learning to Drive Off Road on Smooth Terrain in Unstructured
  Environments Using an On-Board Camera and Sparse Aerial Images</a></h2>
<p><strong>Authors:</strong> Travis Manderson, Stefan Wapnick, David Meger, Gregory Dudek</p>
<p><strong>Summary:</strong> We present a method for learning to drive on smooth terrain while
simultaneously avoiding collisions in challenging off-road and unstructured
outdoor environments using only visual inputs. Our approach applies a hybrid
model-based and model-free reinforcement learning method that is entirely
self-supervised in labeling terrain roughness and collisions using on-board
sensors. Notably, we provide both first-person and overhead aerial image inputs
to our model. We find that the fusion of these comp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.02598v3' target='_blank'>Offline Meta Learning of Exploration</a></h2>
<p><strong>Authors:</strong> Ron Dorfman, Idan Shenfeld, Aviv Tamar</p>
<p><strong>Summary:</strong> Consider the following instance of the Offline Meta Reinforcement Learning
(OMRL) problem: given the complete training logs of $N$ conventional RL agents,
trained on $N$ different tasks, design a meta-agent that can quickly maximize
reward in a new, unseen task from the same task distribution. In particular,
while each conventional RL agent explored and exploited its own different task,
the meta-agent must identify regularities in the data that lead to effective
exploration/exploitation in the u...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.08548v3' target='_blank'>A Survey of Knowledge-based Sequential Decision Making under Uncertainty</a></h2>
<p><strong>Authors:</strong> Shiqi Zhang, Mohan Sridharan</p>
<p><strong>Summary:</strong> Reasoning with declarative knowledge (RDK) and sequential decision-making
(SDM) are two key research areas in artificial intelligence. RDK methods reason
with declarative domain knowledge, including commonsense knowledge, that is
either provided a priori or acquired over time, while SDM methods
(probabilistic planning and reinforcement learning) seek to compute action
policies that maximize the expected cumulative utility over a time horizon;
both classes of methods reason in the presence of unc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.04298v1' target='_blank'>Vision-Based Autonomous Drone Control using Supervised Learning in
  Simulation</a></h2>
<p><strong>Authors:</strong> Max Christl</p>
<p><strong>Summary:</strong> Limited power and computational resources, absence of high-end sensor
equipment and GPS-denied environments are challenges faced by autonomous micro
areal vehicles (MAVs). We address these challenges in the context of autonomous
navigation and landing of MAVs in indoor environments and propose a
vision-based control approach using Supervised Learning. To achieve this, we
collected data samples in a simulation environment which were labelled
according to the optimal control command determined by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.10968v1' target='_blank'>Hierarchical Affordance Discovery using Intrinsic Motivation</a></h2>
<p><strong>Authors:</strong> Alexandre Manoury, Sao Mai Nguyen, C√©dric Buche</p>
<p><strong>Summary:</strong> To be capable of lifelong learning in a real-life environment, robots have to
tackle multiple challenges. Being able to relate physical properties they may
observe in their environment to possible interactions they may have is one of
them. This skill, named affordance learning, is strongly related to embodiment
and is mastered through each person's development: each individual learns
affordances differently through their own interactions with their surroundings.
Current methods for affordance le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.04391v1' target='_blank'>Reinforced Deep Markov Models With Applications in Automatic Trading</a></h2>
<p><strong>Authors:</strong> Tadeu A. Ferreira</p>
<p><strong>Summary:</strong> Inspired by the developments in deep generative models, we propose a
model-based RL approach, coined Reinforced Deep Markov Model (RDMM), designed
to integrate desirable properties of a reinforcement learning algorithm acting
as an automatic trading system. The network architecture allows for the
possibility that market dynamics are partially visible and are potentially
modified by the agent's actions. The RDMM filters incomplete and noisy data, to
create better-behaved input data for RL plannin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.12569v1' target='_blank'>Learning Certified Control using Contraction Metric</a></h2>
<p><strong>Authors:</strong> Dawei Sun, Susmit Jha, Chuchu Fan</p>
<p><strong>Summary:</strong> In this paper, we solve the problem of finding a certified control policy
that drives a robot from any given initial state and under any bounded
disturbance to the desired reference trajectory, with guarantees on the
convergence or bounds on the tracking error. Such a controller is crucial in
safe motion planning. We leverage the advanced theory in Control Contraction
Metric and design a learning framework based on neural networks to
co-synthesize the contraction metric and the controller for co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.01980v2' target='_blank'>A deep learning model for gas storage optimization</a></h2>
<p><strong>Authors:</strong> Nicolas Curin, Michael Kettler, Xi Kleisinger-Yu, Vlatka Komaric, Thomas Krabichler, Josef Teichmann, Hanna Wutte</p>
<p><strong>Summary:</strong> To the best of our knowledge, the application of deep learning in the field
of quantitative risk management is still a relatively recent phenomenon. In
this article, we utilize techniques inspired by reinforcement learning in order
to optimize the operation plans of underground natural gas storage facilities.
We provide a theoretical framework and assess the performance of the proposed
method numerically in comparison to a state-of-the-art least-squares
Monte-Carlo approach. Due to the inherent ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.08363v2' target='_blank'>COMBO: Conservative Offline Model-Based Policy Optimization</a></h2>
<p><strong>Authors:</strong> Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, Chelsea Finn</p>
<p><strong>Summary:</strong> Model-based algorithms, which learn a dynamics model from logged experience
and perform some sort of pessimistic planning under the learned model, have
emerged as a promising paradigm for offline reinforcement learning (offline
RL). However, practical variants of such model-based algorithms rely on
explicit uncertainty quantification for incorporating pessimism. Uncertainty
estimation with complex models, such as deep neural networks, can be difficult
and unreliable. We overcome this limitation ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.12571v1' target='_blank'>The Logical Options Framework</a></h2>
<p><strong>Authors:</strong> Brandon Araki, Xiao Li, Kiran Vodrahalli, Jonathan DeCastro, Micah J. Fry, Daniela Rus</p>
<p><strong>Summary:</strong> Learning composable policies for environments with complex rules and tasks is
a challenging problem. We introduce a hierarchical reinforcement learning
framework called the Logical Options Framework (LOF) that learns policies that
are satisfying, optimal, and composable. LOF efficiently learns policies that
satisfy tasks by representing the task as an automaton and integrating it into
learning and planning. We provide and prove conditions under which LOF will
learn satisfying, optimal policies. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.12924v2' target='_blank'>Visualizing MuZero Models</a></h2>
<p><strong>Authors:</strong> Joery A. de Vries, Ken S. Voskuil, Thomas M. Moerland, Aske Plaat</p>
<p><strong>Summary:</strong> MuZero, a model-based reinforcement learning algorithm that uses a value
equivalent dynamics model, achieved state-of-the-art performance in Chess,
Shogi and the game of Go. In contrast to standard forward dynamics models that
predict a full next state, value equivalent models are trained to predict a
future value, thereby emphasizing value relevant information in the
representations. While value equivalent models have shown strong empirical
success, there is no research yet that visualizes and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.13267v1' target='_blank'>CLAMGen: Closed-Loop Arm Motion Generation via Multi-view Vision-Based
  RL</a></h2>
<p><strong>Authors:</strong> Iretiayo Akinola, Zizhao Wang, Peter Allen</p>
<p><strong>Summary:</strong> We propose a vision-based reinforcement learning (RL) approach for
closed-loop trajectory generation in an arm reaching problem. Arm trajectory
generation is a fundamental robotics problem which entails finding
collision-free paths to move the robot's body (e.g. arm) in order to satisfy a
goal (e.g. place end-effector at a point).
  While classical methods typically require the model of the environment to
solve a planning, search or optimization problem, learning-based approaches
hold the promis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.14274v1' target='_blank'>Character Controllers Using Motion VAEs</a></h2>
<p><strong>Authors:</strong> Hung Yu Ling, Fabio Zinno, George Cheng, Michiel van de Panne</p>
<p><strong>Summary:</strong> A fundamental problem in computer animation is that of realizing purposeful
and realistic human movement given a sufficiently-rich set of motion capture
clips. We learn data-driven generative models of human movement using
autoregressive conditional variational autoencoders, or Motion VAEs. The latent
variables of the learned autoencoder define the action space for the movement
and thereby govern its evolution over time. Planning or control algorithms can
then use this action space to generate d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.14705v1' target='_blank'>Personalized Adaptive Cruise Control and Impacts on Mixed Traffic</a></h2>
<p><strong>Authors:</strong> Mehmet Ozkan, Yao Ma</p>
<p><strong>Summary:</strong> This paper presents a personalized adaptive cruise control (PACC) design that
can learn driver behavior and adaptively control the semi-autonomous vehicle
(SAV) in the car-following scenario, and investigates its impacts on mixed
traffic. In mixed traffic where the SAV and human-driven vehicles share the
road, the SAV's driver can choose a PACC tuning that better fits the driver's
preferred driving behaviors. The individual driver's preferences are learned
through the inverse reinforcement learn...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.16732v1' target='_blank'>Simultaneous Navigation and Construction Benchmarking Environments</a></h2>
<p><strong>Authors:</strong> Wenyu Han, Chen Feng, Haoran Wu, Alexander Gao, Armand Jordana, Dong Liu, Lerrel Pinto, Ludovic Righetti</p>
<p><strong>Summary:</strong> We need intelligent robots for mobile construction, the process of navigating
in an environment and modifying its structure according to a geometric design.
In this task, a major robot vision and learning challenge is how to exactly
achieve the design without GPS, due to the difficulty caused by the
bi-directional coupling of accurate robot localization and navigation together
with strategic environment manipulation. However, many existing robot vision
and learning tasks such as visual navigatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.01151v2' target='_blank'>Collaborative Visual Navigation</a></h2>
<p><strong>Authors:</strong> Haiyang Wang, Wenguan Wang, Xizhou Zhu, Jifeng Dai, Liwei Wang</p>
<p><strong>Summary:</strong> As a fundamental problem for Artificial Intelligence, multi-agent system
(MAS) is making rapid progress, mainly driven by multi-agent reinforcement
learning (MARL) techniques. However, previous MARL methods largely focused on
grid-world like or game environments; MAS in visually rich environments has
remained less explored. To narrow this gap and emphasize the crucial role of
perception in MAS, we propose a large-scale 3D dataset, CollaVN, for
multi-agent visual navigation (MAVN). In CollaVN, mu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.07046v2' target='_blank'>Backprop-Free Reinforcement Learning with Active Neural Generative
  Coding</a></h2>
<p><strong>Authors:</strong> Alexander Ororbia, Ankur Mali</p>
<p><strong>Summary:</strong> In humans, perceptual awareness facilitates the fast recognition and
extraction of information from sensory input. This awareness largely depends on
how the human agent interacts with the environment. In this work, we propose
active neural generative coding, a computational framework for learning
action-driven generative models without backpropagation of errors (backprop) in
dynamic environments. Specifically, we develop an intelligent agent that
operates even with sparse rewards, drawing inspir...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.02439v1' target='_blank'>Learning to Design and Construct Bridge without Blueprint</a></h2>
<p><strong>Authors:</strong> Yunfei Li, Tao Kong, Lei Li, Yifeng Li, Yi Wu</p>
<p><strong>Summary:</strong> Autonomous assembly has been a desired functionality of many intelligent
robot systems. We study a new challenging assembly task, designing and
constructing a bridge without a blueprint. In this task, the robot needs to
first design a feasible bridge architecture for arbitrarily wide cliffs and
then manipulate the blocks reliably to construct a stable bridge according to
the proposed design. In this paper, we propose a bi-level approach to tackle
this task. At the high level, the system learns a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.03866v1' target='_blank'>Mapless Humanoid Navigation Using Learned Latent Dynamics</a></h2>
<p><strong>Authors:</strong> Andre Brandenburger, Diego Rodriguez, Sven Behnke</p>
<p><strong>Summary:</strong> In this paper, we propose a novel Deep Reinforcement Learning approach to
address the mapless navigation problem, in which the locomotion actions of a
humanoid robot are taken online based on the knowledge encoded in learned
models. Planning happens by generating open-loop trajectories in a learned
latent space that captures the dynamics of the environment. Our planner
considers visual (RGB images) and non-visual observations (e.g., attitude
estimations). This confers the agent upon awareness no...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.06148v1' target='_blank'>Q-Mixing Network for Multi-Agent Pathfinding in Partially Observable
  Grid Environments</a></h2>
<p><strong>Authors:</strong> Vasilii Davydov, Alexey Skrynnik, Konstantin Yakovlev, Aleksandr I. Panov</p>
<p><strong>Summary:</strong> In this paper, we consider the problem of multi-agent navigation in partially
observable grid environments. This problem is challenging for centralized
planning approaches as they, typically, rely on the full knowledge of the
environment. We suggest utilizing the reinforcement learning approach when the
agents, first, learn the policies that map observations to actions and then
follow these policies to reach their goals. To tackle the challenge associated
with learning cooperative behavior, i.e....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.12054v1' target='_blank'>Deep Reinforcement Learning for Dynamic Band Switch in
  Cellular-Connected UAV</a></h2>
<p><strong>Authors:</strong> Gianluca Fontanesi, Anding Zhu, Hamed Ahmadi</p>
<p><strong>Summary:</strong> The choice of the transmitting frequency to provide cellular-connected
Unmanned Aerial Vehicle (UAV) reliable connectivity and mobility support
introduce several challenges. Conventional sub-6 GHz networks are optimized for
ground Users (UEs). Operating at the millimeter Wave (mmWave) band would
provide high-capacity but highly intermittent links. To reach the destination
while minimizing a weighted function of traveling time and number of radio
failures, we propose in this paper a UAV joint tra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.13184v2' target='_blank'>Path Planning for Cellular-Connected UAV: A DRL Solution with
  Quantum-Inspired Experience Replay</a></h2>
<p><strong>Authors:</strong> Yuanjian Li, A. Hamid Aghvami, Daoyi Dong</p>
<p><strong>Summary:</strong> In cellular-connected unmanned aerial vehicle (UAV) network, a minimization
problem on the weighted sum of time cost and expected outage duration is
considered. Taking advantage of UAV's adjustable mobility, an intelligent UAV
navigation approach is formulated to achieve the aforementioned optimization
goal. Specifically, after mapping the navigation task into a Markov decision
process (MDP), a deep reinforcement learning (DRL) solution with novel
quantum-inspired experience replay (QiER) framew...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.00541v3' target='_blank'>Validating Robotics Simulators on Real-World Impacts</a></h2>
<p><strong>Authors:</strong> Brian Acosta, William Yang, Michael Posa</p>
<p><strong>Summary:</strong> A realistic simulation environment is an essential tool in every roboticist's
toolkit, with uses ranging from planning and control to training policies with
reinforcement learning. Despite the centrality of simulation in modern
robotics, little work has been done to compare the performance of robotics
simulators against real-world data, especially for scenarios involving dynamic
motions with high speed impact events. Handling dynamic contact is the
computational bottleneck for most simulations, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.01747v1' target='_blank'>Mapless Navigation: Learning UAVs Motion forExploration of Unknown
  Environments</a></h2>
<p><strong>Authors:</strong> Sunggoo Jung, David Hyunchul Shim</p>
<p><strong>Summary:</strong> This study presents a new methodology for learning-based motion planning for
autonomous exploration using aerial robots. Through the reinforcement learning
method of learning through trial and error, the action policy is derived that
can guide autonomous exploration of underground and tunnel environments. A new
Markov decision process state is designed to learn the robot's action policy by
using simulation only, and the results are applied to the real-world
environment without further learning. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.09196v2' target='_blank'>Option Transfer and SMDP Abstraction with Successor Features</a></h2>
<p><strong>Authors:</strong> Dongge Han, Sebastian Tschiatschek</p>
<p><strong>Summary:</strong> Abstraction plays an important role in the generalisation of knowledge and
skills and is key to sample efficient learning. In this work, we study joint
temporal and state abstraction in reinforcement learning, where
temporally-extended actions in the form of options induce temporal
abstractions, while aggregation of similar states with respect to abstract
options induces state abstractions. Many existing abstraction schemes ignore
the interplay of state and temporal abstraction. Consequently, th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.12352v2' target='_blank'>DiffSRL: Learning Dynamical State Representation for Deformable Object
  Manipulation with Differentiable Simulator</a></h2>
<p><strong>Authors:</strong> Sirui Chen, Yunhao Liu, Jialong Li, Shang Wen Yao, Tingxiang Fan, Jia Pan</p>
<p><strong>Summary:</strong> Dynamic state representation learning is an important task in robot learning.
Latent space that can capture dynamics related information has wide application
in areas such as accelerating model free reinforcement learning, closing the
simulation to reality gap, as well as reducing the motion planning complexity.
However, current dynamic state representation learning methods scale poorly on
complex dynamic systems such as deformable objects, and cannot directly embed
well defined simulation funct...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.00262v1' target='_blank'>Learning Coordinated Terrain-Adaptive Locomotion by Imitating a
  Centroidal Dynamics Planner</a></h2>
<p><strong>Authors:</strong> Philemon Brakel, Steven Bohez, Leonard Hasenclever, Nicolas Heess, Konstantinos Bousmalis</p>
<p><strong>Summary:</strong> Dynamic quadruped locomotion over challenging terrains with precise foot
placements is a hard problem for both optimal control methods and Reinforcement
Learning (RL). Non-linear solvers can produce coordinated constraint satisfying
motions, but often take too long to converge for online application. RL methods
can learn dynamic reactive controllers but require carefully tuned shaping
rewards to produce good gaits and can have trouble discovering precise
coordinated movements. Imitation learning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.01364v1' target='_blank'>Learning to Explore by Reinforcement over High-Level Options</a></h2>
<p><strong>Authors:</strong> Liu Juncheng, McCane Brendan, Mills Steven</p>
<p><strong>Summary:</strong> Autonomous 3D environment exploration is a fundamental task for various
applications such as navigation. The goal of exploration is to investigate a
new environment and build its occupancy map efficiently. In this paper, we
propose a new method which grants an agent two intertwined options of
behaviors: "look-around" and "frontier navigation". This is implemented by an
option-critic architecture and trained by reinforcement learning algorithms. In
each timestep, an agent produces an option and a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.00940v1' target='_blank'>Reward-Free Attacks in Multi-Agent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ted Fujimoto, Timothy Doster, Adam Attarian, Jill Brandenberger, Nathan Hodas</p>
<p><strong>Summary:</strong> We investigate how effective an attacker can be when it only learns from its
victim's actions, without access to the victim's reward. In this work, we are
motivated by the scenario where the attacker wants to behave strategically when
the victim's motivations are unknown. We argue that one heuristic approach an
attacker can use is to maximize the entropy of the victim's policy. The policy
is generally not obfuscated, which implies it may be extracted simply by
passively observing the victim. We ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.03419v2' target='_blank'>Using Image Transformations to Learn Network Structure</a></h2>
<p><strong>Authors:</strong> Brayan Ortiz, Amitabh Sinha</p>
<p><strong>Summary:</strong> Many learning tasks require observing a sequence of images and making a
decision. In a transportation problem of designing and planning for shipping
boxes between nodes, we show how to treat the network of nodes and the flows
between them as images. These images have useful structural information that
can be statistically summarized. Using image compression techniques, we reduce
an image down to a set of numbers that contain interpretable geographic
information that we call geographic signatures...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.03577v1' target='_blank'>Pragmatic Implementation of Reinforcement Algorithms For Path Finding On
  Raspberry Pi</a></h2>
<p><strong>Authors:</strong> Serena Raju, Sherin Shibu, Riya Mol Raji, Joel Thomas</p>
<p><strong>Summary:</strong> In this paper, pragmatic implementation of an indoor autonomous delivery
system that exploits Reinforcement Learning algorithms for path planning and
collision avoidance is audited. The proposed system is a cost-efficient
approach that is implemented to facilitate a Raspberry Pi controlled
four-wheel-drive non-holonomic robot map a grid. This approach computes and
navigates the shortest path from a source key point to a destination key point
to carry out the desired delivery. Q learning and Deep...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.07406v1' target='_blank'>Branching Time Active Inference with Bayesian Filtering</a></h2>
<p><strong>Authors:</strong> Th√©ophile Champion, Marek Grze≈õ, Howard Bowman</p>
<p><strong>Summary:</strong> Branching Time Active Inference (Champion et al., 2021b,a) is a framework
proposing to look at planning as a form of Bayesian model expansion. Its root
can be found in Active Inference (Friston et al., 2016; Da Costa et al., 2020;
Champion et al., 2021c), a neuroscientific framework widely used for brain
modelling, as well as in Monte Carlo Tree Search (Browne et al., 2012), a
method broadly applied in the Reinforcement Learning literature. Up to now, the
inference of the latent variables was ca...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.09551v4' target='_blank'>Resilient Branching MPC for Multi-Vehicle Traffic Scenarios Using
  Adversarial Disturbance Sequences</a></h2>
<p><strong>Authors:</strong> Victor Fors, Bj√∂rn Olofsson, Erik Frisk</p>
<p><strong>Summary:</strong> An approach to resilient planning and control of autonomous vehicles in
multi-vehicle traffic scenarios is proposed. The proposed method is based on
model predictive control (MPC), where alternative predictions of the
surrounding traffic are determined automatically such that they are
intentionally adversarial to the ego vehicle. This provides robustness against
the inherent uncertainty in traffic predictions. To reduce conservatism, an
assumption that other agents are of no ill intent is formal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.00817v2' target='_blank'>Do Differentiable Simulators Give Better Policy Gradients?</a></h2>
<p><strong>Authors:</strong> H. J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake</p>
<p><strong>Summary:</strong> Differentiable simulators promise faster computation time for reinforcement
learning by replacing zeroth-order gradient estimates of a stochastic objective
with an estimate based on first-order gradients. However, it is yet unclear
what factors decide the performance of the two estimators on complex landscapes
that involve long-horizon planning and control on physical systems, despite the
crucial relevance of this question for the utility of differentiable
simulators. We show that characteristic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.09859v1' target='_blank'>Cooperative Artificial Intelligence</a></h2>
<p><strong>Authors:</strong> Tobias Baumann</p>
<p><strong>Summary:</strong> In the future, artificial learning agents are likely to become increasingly
widespread in our society. They will interact with both other learning agents
and humans in a variety of complex settings including social dilemmas. We argue
that there is a need for research on the intersection between game theory and
artificial intelligence, with the goal of achieving cooperative artificial
intelligence that can navigate social dilemmas well. We consider the problem of
how an external agent can promote...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.13680v1' target='_blank'>Hierarchical Policy Learning for Mechanical Search</a></h2>
<p><strong>Authors:</strong> Oussama Zenkri, Ngo Anh Vien, Gerhard Neumann</p>
<p><strong>Summary:</strong> Retrieving objects from clutters is a complex task, which requires multiple
interactions with the environment until the target object can be extracted.
These interactions involve executing action primitives like grasping or pushing
as well as setting priorities for the objects to manipulate and the actions to
execute. Mechanical Search (MS) is a framework for object retrieval, which uses
a heuristic algorithm for pushing and rule-based algorithms for high-level
planning. While rule-based policie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.02877v1' target='_blank'>MIRROR: Differentiable Deep Social Projection for Assistive Human-Robot
  Communication</a></h2>
<p><strong>Authors:</strong> Kaiqi Chen, Jeffrey Fong, Harold Soh</p>
<p><strong>Summary:</strong> Communication is a hallmark of intelligence. In this work, we present MIRROR,
an approach to (i) quickly learn human models from human demonstrations, and
(ii) use the models for subsequent communication planning in assistive
shared-control settings. MIRROR is inspired by social projection theory, which
hypothesizes that humans use self-models to understand others. Likewise, MIRROR
leverages self-models learned using reinforcement learning to bootstrap human
modeling. Experiments with simulated ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.04303v1' target='_blank'>Policy Regularization for Legible Behavior</a></h2>
<p><strong>Authors:</strong> Michele Persiani, Thomas Hellstr√∂m</p>
<p><strong>Summary:</strong> In Reinforcement Learning interpretability generally means to provide insight
into the agent's mechanisms such that its decisions are understandable by an
expert upon inspection. This definition, with the resulting methods from the
literature, may however fall short for online settings where the fluency of
interactions prohibits deep inspections of the decision-making algorithm. To
support interpretability in online settings it is useful to borrow from the
Explainable Planning literature methods...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.09637v1' target='_blank'>Investigating Compounding Prediction Errors in Learned Dynamics Models</a></h2>
<p><strong>Authors:</strong> Nathan Lambert, Kristofer Pister, Roberto Calandra</p>
<p><strong>Summary:</strong> Accurately predicting the consequences of agents' actions is a key
prerequisite for planning in robotic control. Model-based reinforcement
learning (MBRL) is one paradigm which relies on the iterative learning and
prediction of state-action transitions to solve a task. Deep MBRL has become a
popular candidate, using a neural network to learn a dynamics model that
predicts with each pass from high-dimensional states to actions. These
"one-step" predictions are known to become inaccurate over long...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.10518v1' target='_blank'>Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot
  Interactions</a></h2>
<p><strong>Authors:</strong> Francesco Del Duchetto, Marc Hanheide</p>
<p><strong>Summary:</strong> In this work, we propose a framework for allowing autonomous robots deployed
for extended periods of time in public spaces to adapt their own behaviour
online from user interactions. The robot behaviour planning is embedded in a
Reinforcement Learning (RL) framework, where the objective is maximising the
level of overall user engagement during the interactions. We use the
Upper-Confidence-Bound Value-Iteration (UCBVI) algorithm, which gives a helpful
way of managing the exploration-exploitation ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.13573v2' target='_blank'>Unsupervised Learning of Temporal Abstractions with Slot-based
  Transformers</a></h2>
<p><strong>Authors:</strong> Anand Gopalakrishnan, Kazuki Irie, J√ºrgen Schmidhuber, Sjoerd van Steenkiste</p>
<p><strong>Summary:</strong> The discovery of reusable sub-routines simplifies decision-making and
planning in complex reinforcement learning problems. Previous approaches
propose to learn such temporal abstractions in a purely unsupervised fashion
through observing state-action trajectories gathered from executing a policy.
However, a current limitation is that they process each trajectory in an
entirely sequential manner, which prevents them from revising earlier decisions
about sub-routine boundary points in light of new...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.16910v1' target='_blank'>End-to-End Trajectory Distribution Prediction Based on Occupancy Grid
  Maps</a></h2>
<p><strong>Authors:</strong> Ke Guo, Wenxi Liu, Jia Pan</p>
<p><strong>Summary:</strong> In this paper, we aim to forecast a future trajectory distribution of a
moving agent in the real world, given the social scene images and historical
trajectories. Yet, it is a challenging task because the ground-truth
distribution is unknown and unobservable, while only one of its samples can be
applied for supervising model learning, which is prone to bias. Most recent
works focus on predicting diverse trajectories in order to cover all modes of
the real distribution, but they may despise the p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.17106v2' target='_blank'>A Cooperative Optimal Control Framework for Connected and Automated
  Vehicles in Mixed Traffic Using Social Value Orientation</a></h2>
<p><strong>Authors:</strong> Viet-Anh Le, Andreas A. Malikopoulos</p>
<p><strong>Summary:</strong> In this paper, we develop a socially cooperative optimal control framework to
address the motion planning problem for connected and automated vehicles (CAVs)
in mixed traffic using social value orientation (SVO) and a potential game
approach. In the proposed framework, we formulate the interaction between a CAV
and a human-driven vehicle (HDV) as a simultaneous game where each vehicle
minimizes a weighted sum of its egoistic objective and a cooperative objective.
The SVO angles are used to quant...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.00619v2' target='_blank'>Maze Learning using a Hyperdimensional Predictive Processing Cognitive
  Architecture</a></h2>
<p><strong>Authors:</strong> Alexander Ororbia, M. Alex Kelly</p>
<p><strong>Summary:</strong> We present the COGnitive Neural GENerative system (CogNGen), a cognitive
architecture that combines two neurobiologically-plausible, computational
models: predictive processing and hyperdimensional/vector-symbolic models. We
draw inspiration from architectures such as ACT-R and Spaun/Nengo. CogNGen is
in broad agreement with these, providing a level of detail between ACT-R's
high-level symbolic description of human cognition and Spaun's low-level
neurobiological description, furthermore creating...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.13998v1' target='_blank'>Learning High-DOF Reaching-and-Grasping via Dynamic Representation of
  Gripper-Object Interaction</a></h2>
<p><strong>Authors:</strong> Qijin She, Ruizhen Hu, Juzhan Xu, Min Liu, Kai Xu, Hui Huang</p>
<p><strong>Summary:</strong> We approach the problem of high-DOF reaching-and-grasping via learning joint
planning of grasp and motion with deep reinforcement learning. To resolve the
sample efficiency issue in learning the high-dimensional and complex control of
dexterous grasping, we propose an effective representation of grasping state
characterizing the spatial interaction between the gripper and the target
object. To represent gripper-object interaction, we adopt Interaction Bisector
Surface (IBS) which is the Voronoi ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.01898v1' target='_blank'>Go Back in Time: Generating Flashbacks in Stories with Event Temporal
  Prompts</a></h2>
<p><strong>Authors:</strong> Rujun Han, Hong Chen, Yufei Tian, Nanyun Peng</p>
<p><strong>Summary:</strong> Stories or narratives are comprised of a sequence of events. To compose
interesting stories, professional writers often leverage a creative writing
technique called flashback that inserts past events into current storylines as
we commonly observe in novels and plays. However, it is challenging for
machines to generate flashback as it requires a solid understanding of event
temporal order (e.g. "feeling hungry" before "eat," not vice versa), and the
creativity to arrange storylines so that earlie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.00113v2' target='_blank'>BRExIt: On Opponent Modelling in Expert Iteration</a></h2>
<p><strong>Authors:</strong> Daniel Hernandez, Hendrik Baier, Michael Kaisers</p>
<p><strong>Summary:</strong> Finding a best response policy is a central objective in game theory and
multi-agent learning, with modern population-based training approaches
employing reinforcement learning algorithms as best-response oracles to improve
play against candidate opponents (typically previously learnt policies). We
propose Best Response Expert Iteration (BRExIt), which accelerates learning in
games by incorporating opponent models into the state-of-the-art learning
algorithm Expert Iteration (ExIt). BRExIt aims ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.02249v3' target='_blank'>Rapid Learning of Spatial Representations for Goal-Directed Navigation
  Based on a Novel Model of Hippocampal Place Fields</a></h2>
<p><strong>Authors:</strong> Adedapo Alabi, Dieter Vanderelst, Ali Minai</p>
<p><strong>Summary:</strong> The discovery of place cells and other spatially modulated neurons in the
hippocampal complex of rodents has been crucial to elucidating the neural basis
of spatial cognition. More recently, the replay of neural sequences encoding
previously experienced trajectories has been observed during consummatory
behavior potentially with implications for quick memory consolidation and
behavioral planning. Several promising models for robotic navigation and
reinforcement learning have been proposed based ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.03446v1' target='_blank'>Learning in Observable POMDPs, without Computationally Intractable
  Oracles</a></h2>
<p><strong>Authors:</strong> Noah Golowich, Ankur Moitra, Dhruv Rohatgi</p>
<p><strong>Summary:</strong> Much of reinforcement learning theory is built on top of oracles that are
computationally hard to implement. Specifically for learning near-optimal
policies in Partially Observable Markov Decision Processes (POMDPs), existing
algorithms either need to make strong assumptions about the model dynamics
(e.g. deterministic transitions) or assume access to an oracle for solving a
hard optimistic planning or estimation problem as a subroutine. In this work we
develop the first oracle-free learning alg...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.04199v3' target='_blank'>Deep Surrogate Assisted Generation of Environments</a></h2>
<p><strong>Authors:</strong> Varun Bhatt, Bryon Tjanaka, Matthew C. Fontaine, Stefanos Nikolaidis</p>
<p><strong>Summary:</strong> Recent progress in reinforcement learning (RL) has started producing
generally capable agents that can solve a distribution of complex environments.
These agents are typically tested on fixed, human-authored environments. On the
other hand, quality diversity (QD) optimization has been proven to be an
effective component of environment generation algorithms, which can generate
collections of high-quality environments that are diverse in the resulting
agent behaviors. However, these algorithms req...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.05096v3' target='_blank'>LTL-Transfer: Skill Transfer for Temporal Task Specification</a></h2>
<p><strong>Authors:</strong> Jason Xinyu Liu, Ankit Shah, Eric Rosen, Mingxi Jia, George Konidaris, Stefanie Tellex</p>
<p><strong>Summary:</strong> Deploying robots in real-world environments, such as households and
manufacturing lines, requires generalization across novel task specifications
without violating safety constraints. Linear temporal logic (LTL) is a widely
used task specification language with a compositional grammar that naturally
induces commonalities among tasks while preserving safety guarantees. However,
most prior work on reinforcement learning with LTL specifications treats every
new task independently, thus requiring la...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.08736v1' target='_blank'>Generalised Policy Improvement with Geometric Policy Composition</a></h2>
<p><strong>Authors:</strong> Shantanu Thakoor, Mark Rowland, Diana Borsa, Will Dabney, R√©mi Munos, Andr√© Barreto</p>
<p><strong>Summary:</strong> We introduce a method for policy improvement that interpolates between the
greedy approach of value-based reinforcement learning (RL) and the full
planning approach typical of model-based RL. The new method builds on the
concept of a geometric horizon model (GHM, also known as a gamma-model), which
models the discounted state-visitation distribution of a given policy. We show
that we can evaluate any non-Markov policy that switches between a set of base
Markov policies with fixed probability by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.13754v1' target='_blank'>DistSPECTRL: Distributing Specifications in Multi-Agent Reinforcement
  Learning Systems</a></h2>
<p><strong>Authors:</strong> Joe Eappen, Suresh Jagannathan</p>
<p><strong>Summary:</strong> While notable progress has been made in specifying and learning objectives
for general cyber-physical systems, applying these methods to distributed
multi-agent systems still pose significant challenges. Among these are the need
to (a) craft specification primitives that allow expression and interplay of
both local and global objectives, (b) tame explosion in the state and action
spaces to enable effective learning, and (c) minimize coordination frequency
and the set of engaged participants for ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.04987v1' target='_blank'>Vehicle Type Specific Waypoint Generation</a></h2>
<p><strong>Authors:</strong> Yunpeng Liu, Jonathan Wilder Lavington, Adam Scibior, Frank Wood</p>
<p><strong>Summary:</strong> We develop a generic mechanism for generating vehicle-type specific sequences
of waypoints from a probabilistic foundation model of driving behavior. Many
foundation behavior models are trained on data that does not include vehicle
information, which limits their utility in downstream applications such as
planning. Our novel methodology conditionally specializes such a behavior
predictive model to a vehicle-type by utilizing byproducts of the reinforcement
learning algorithms used to produce veh...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.03210v3' target='_blank'>Real-to-Sim: Predicting Residual Errors of Robotic Systems with Sparse
  Data using a Learning-based Unscented Kalman Filter</a></h2>
<p><strong>Authors:</strong> Alexander Schperberg, Yusuke Tanaka, Feng Xu, Marcel Menner, Dennis Hong</p>
<p><strong>Summary:</strong> Achieving highly accurate dynamic or simulator models that are close to the
real robot can facilitate model-based controls (e.g., model predictive control
or linear-quadradic regulators), model-based trajectory planning (e.g.,
trajectory optimization), and decrease the amount of learning time necessary
for reinforcement learning methods. Thus, the objective of this work is to
learn the residual errors between a dynamic and/or simulator model and the real
robot. This is achieved using a neural ne...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.09174v1' target='_blank'>Active Predicting Coding: Brain-Inspired Reinforcement Learning for
  Sparse Reward Robotic Control Problems</a></h2>
<p><strong>Authors:</strong> Alexander Ororbia, Ankur Mali</p>
<p><strong>Summary:</strong> In this article, we propose a backpropagation-free approach to robotic
control through the neuro-cognitive computational framework of neural
generative coding (NGC), designing an agent built completely from powerful
predictive coding/processing circuits that facilitate dynamic, online learning
from sparse rewards, embodying the principles of planning-as-inference.
Concretely, we craft an adaptive agent system, which we call active predictive
coding (ActPC), that balances an internally-generated ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.00121v1' target='_blank'>Visuo-Tactile Transformers for Manipulation</a></h2>
<p><strong>Authors:</strong> Yizhou Chen, Andrea Sipos, Mark Van der Merwe, Nima Fazeli</p>
<p><strong>Summary:</strong> Learning representations in the joint domain of vision and touch can improve
manipulation dexterity, robustness, and sample-complexity by exploiting mutual
information and complementary cues. Here, we present Visuo-Tactile Transformers
(VTTs), a novel multimodal representation learning approach suited for
model-based reinforcement learning and planning. Our approach extends the
Visual Transformer \cite{dosovitskiy2021image} to handle visuo-tactile
feedback. Specifically, VTT uses tactile feedbac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.01841v2' target='_blank'>Learning Perception-Aware Agile Flight in Cluttered Environments</a></h2>
<p><strong>Authors:</strong> Yunlong Song, Kexin Shi, Robert Penicka, Davide Scaramuzza</p>
<p><strong>Summary:</strong> Recently, neural control policies have outperformed existing model-based
planning-and-control methods for autonomously navigating quadrotors through
cluttered environments in minimum time. However, they are not perception aware,
a crucial requirement in vision-based navigation due to the camera's limited
field of view and the underactuated nature of a quadrotor. We propose a
learning-based system that achieves perception-aware, agile flight in cluttered
environments. Our method combines imitatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.03512v1' target='_blank'>Inferring Smooth Control: Monte Carlo Posterior Policy Iteration with
  Gaussian Processes</a></h2>
<p><strong>Authors:</strong> Joe Watson, Jan Peters</p>
<p><strong>Summary:</strong> Monte Carlo methods have become increasingly relevant for control of
non-differentiable systems, approximate dynamics models and learning from data.
These methods scale to high-dimensional spaces and are effective at the
non-convex optimizations often seen in robot learning. We look at sample-based
methods from the perspective of inference-based control, specifically posterior
policy iteration. From this perspective, we highlight how Gaussian noise priors
produce rough control actions that are u...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.06917v1' target='_blank'>A Direct Approximation of AIXI Using Logical State Abstractions</a></h2>
<p><strong>Authors:</strong> Samuel Yang-Zhao, Tianyu Wang, Kee Siong Ng</p>
<p><strong>Summary:</strong> We propose a practical integration of logical state abstraction with AIXI, a
Bayesian optimality notion for reinforcement learning agents, to significantly
expand the model class that AIXI agents can be approximated over to complex
history-dependent and structured environments. The state representation and
reasoning framework is based on higher-order logic, which can be used to define
and enumerate complex features on non-Markovian and structured environments. We
address the problem of selecting...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.09566v2' target='_blank'>Simple Emergent Action Representations from Multi-Task Policy Training</a></h2>
<p><strong>Authors:</strong> Pu Hua, Yubei Chen, Huazhe Xu</p>
<p><strong>Summary:</strong> The low-level sensory and motor signals in deep reinforcement learning, which
exist in high-dimensional spaces such as image observations or motor torques,
are inherently challenging to understand or utilize directly for downstream
tasks. While sensory representations have been extensively studied, the
representations of motor actions are still an area of active exploration. Our
work reveals that a space containing meaningful action representations emerges
when a multi-task policy network takes ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.12806v1' target='_blank'>Active Exploration for Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Tim Schneider, Boris Belousov, Georgia Chalvatzaki, Diego Romeres, Devesh K. Jha, Jan Peters</p>
<p><strong>Summary:</strong> Robotic manipulation stands as a largely unsolved problem despite significant
advances in robotics and machine learning in recent years. One of the key
challenges in manipulation is the exploration of the dynamics of the
environment when there is continuous contact between the objects being
manipulated. This paper proposes a model-based active exploration approach that
enables efficient learning in sparse-reward robotic manipulation tasks. The
proposed method estimates an information gain object...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.13066v2' target='_blank'>DaXBench: Benchmarking Deformable Object Manipulation with
  Differentiable Physics</a></h2>
<p><strong>Authors:</strong> Siwei Chen, Yiqing Xu, Cunjun Yu, Linfeng Li, Xiao Ma, Zhongwen Xu, David Hsu</p>
<p><strong>Summary:</strong> Deformable Object Manipulation (DOM) is of significant importance to both
daily and industrial applications. Recent successes in differentiable physics
simulators allow learning algorithms to train a policy with analytic gradients
through environment dynamics, which significantly facilitates the development
of DOM algorithms. However, existing DOM benchmarks are either
single-object-based or non-differentiable. This leaves the questions of 1) how
a task-specific algorithm performs on other tasks...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.00688v1' target='_blank'>Learning to Solve Voxel Building Embodied Tasks from Pixels and Natural
  Language Instructions</a></h2>
<p><strong>Authors:</strong> Alexey Skrynnik, Zoya Volovikova, Marc-Alexandre C√¥t√©, Anton Voronov, Artem Zholus, Negar Arabzadeh, Shrestha Mohanty, Milagro Teruel, Ahmed Awadallah, Aleksandr Panov, Mikhail Burtsev, Julia Kiseleva</p>
<p><strong>Summary:</strong> The adoption of pre-trained language models to generate action plans for
embodied agents is a promising research strategy. However, execution of
instructions in real or simulated environments requires verification of the
feasibility of actions as well as their relevance to the completion of a goal.
We propose a new method that combines a language model and reinforcement
learning for the task of building objects in a Minecraft-like environment
according to the natural language instructions. Our m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.04993v2' target='_blank'>RL-DWA Omnidirectional Motion Planning for Person Following in Domestic
  Assistance and Monitoring</a></h2>
<p><strong>Authors:</strong> Andrea Eirale, Mauro Martini, Marcello Chiaberge</p>
<p><strong>Summary:</strong> Robot assistants are emerging as high-tech solutions to support people in
everyday life. Following and assisting the user in the domestic environment
requires flexible mobility to safely move in cluttered spaces. We introduce a
new approach to person following for assistance and monitoring. Our methodology
exploits an omnidirectional robotic platform to detach the computation of
linear and angular velocities and navigate within the domestic environment
without losing track of the assisted person...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.06407v3' target='_blank'>Control Transformer: Robot Navigation in Unknown Environments through
  PRM-Guided Return-Conditioned Sequence Modeling</a></h2>
<p><strong>Authors:</strong> Daniel Lawson, Ahmed H. Qureshi</p>
<p><strong>Summary:</strong> Learning long-horizon tasks such as navigation has presented difficult
challenges for successfully applying reinforcement learning to robotics. From
another perspective, under known environments, sampling-based planning can
robustly find collision-free paths in environments without learning. In this
work, we propose Control Transformer that models return-conditioned sequences
from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM)
planner. We demonstrate that our framework...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.13937v1' target='_blank'>Operator Splitting Value Iteration</a></h2>
<p><strong>Authors:</strong> Amin Rakhsha, Andrew Wang, Mohammad Ghavamzadeh, Amir-massoud Farahmand</p>
<p><strong>Summary:</strong> We introduce new planning and reinforcement learning algorithms for
discounted MDPs that utilize an approximate model of the environment to
accelerate the convergence of the value function. Inspired by the splitting
approach in numerical linear algebra, we introduce Operator Splitting Value
Iteration (OS-VI) for both Policy Evaluation and Control problems. OS-VI
achieves a much faster convergence rate when the model is accurate enough. We
also introduce a sample-based version of the algorithm ca...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.02224v1' target='_blank'>Bi-Level Optimization Augmented with Conditional Variational Autoencoder
  for Autonomous Driving in Dense Traffic</a></h2>
<p><strong>Authors:</strong> Arun Kumar Singh, Jatan Shrestha, Nicola Albarella</p>
<p><strong>Summary:</strong> Autonomous driving has a natural bi-level structure. The goal of the upper
behavioural layer is to provide appropriate lane change, speeding up, and
braking decisions to optimize a given driving task. However, this layer can
only indirectly influence the driving efficiency through the lower-level
trajectory planner, which takes in the behavioural inputs to produce motion
commands. Existing sampling-based approaches do not fully exploit the strong
coupling between the behavioural and planning lay...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.07397v1' target='_blank'>Hierarchical Strategies for Cooperative Multi-Agent Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Majd Ibrahim, Ammar Fayad</p>
<p><strong>Summary:</strong> Adequate strategizing of agents behaviors is essential to solving cooperative
MARL problems. One intuitively beneficial yet uncommon method in this domain is
predicting agents future behaviors and planning accordingly. Leveraging this
point, we propose a two-level hierarchical architecture that combines a novel
information-theoretic objective with a trajectory prediction model to learn a
strategy. To this end, we introduce a latent policy that learns two types of
latent strategies: individual $z...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.03851v1' target='_blank'>ED-Batch: Efficient Automatic Batching of Dynamic Neural Networks via
  Learned Finite State Machines</a></h2>
<p><strong>Authors:</strong> Siyuan Chen, Pratik Fegade, Tianqi Chen, Phillip B. Gibbons, Todd C. Mowry</p>
<p><strong>Summary:</strong> Batching has a fundamental influence on the efficiency of deep neural network
(DNN) execution. However, for dynamic DNNs, efficient batching is particularly
challenging as the dataflow graph varies per input instance. As a result,
state-of-the-art frameworks use heuristics that result in suboptimal batching
decisions. Further, batching puts strict restrictions on memory adjacency and
can lead to high data movement costs. In this paper, we provide an approach for
batching dynamic DNNs based on fi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.03921v2' target='_blank'>Predictable MDP Abstraction for Unsupervised Model-Based RL</a></h2>
<p><strong>Authors:</strong> Seohong Park, Sergey Levine</p>
<p><strong>Summary:</strong> A key component of model-based reinforcement learning (RL) is a dynamics
model that predicts the outcomes of actions. Errors in this predictive model
can degrade the performance of model-based controllers, and complex Markov
decision processes (MDPs) can present exceptionally difficult prediction
problems. To mitigate this issue, we propose predictable MDP abstraction (PMA):
instead of training a predictive model on the original MDP, we train a model on
a transformed MDP with a learned action sp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.07515v2' target='_blank'>TiZero: Mastering Multi-Agent Football with Curriculum Learning and
  Self-Play</a></h2>
<p><strong>Authors:</strong> Fanqi Lin, Shiyu Huang, Tim Pearce, Wenze Chen, Wei-Wei Tu</p>
<p><strong>Summary:</strong> Multi-agent football poses an unsolved challenge in AI research. Existing
work has focused on tackling simplified scenarios of the game, or else
leveraging expert demonstrations. In this paper, we develop a multi-agent
system to play the full 11 vs. 11 game mode, without demonstrations. This game
mode contains aspects that present major challenges to modern reinforcement
learning algorithms; multi-agent coordination, long-term planning, and
non-transitivity. To address these challenges, we prese...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.11529v2' target='_blank'>Modular Deep Learning</a></h2>
<p><strong>Authors:</strong> Jonas Pfeiffer, Sebastian Ruder, Ivan Vuliƒá, Edoardo Maria Ponti</p>
<p><strong>Summary:</strong> Transfer learning has recently become the dominant paradigm of machine
learning. Pre-trained models fine-tuned for downstream tasks achieve better
performance with fewer labelled examples. Nonetheless, it remains unclear how
to develop models that specialise towards multiple tasks without incurring
negative interference and that generalise systematically to non-identically
distributed tasks. Modular deep learning has emerged as a promising solution to
these challenges. In this framework, units o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.00694v1' target='_blank'>The Virtues of Laziness in Model-based RL: A Unified Objective and
  Algorithms</a></h2>
<p><strong>Authors:</strong> Anirudh Vemula, Yuda Song, Aarti Singh, J. Andrew Bagnell, Sanjiban Choudhury</p>
<p><strong>Summary:</strong> We propose a novel approach to addressing two fundamental challenges in
Model-based Reinforcement Learning (MBRL): the computational expense of
repeatedly finding a good policy in the learned model, and the objective
mismatch between model fitting and policy computation. Our "lazy" method
leverages a novel unified objective, Performance Difference via Advantage in
Model, to capture the performance difference between the learned policy and
expert policy under the true dynamics. This objective dem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.03196v2' target='_blank'>Population-based Evaluation in Repeated Rock-Paper-Scissors as a
  Benchmark for Multiagent Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Marc Lanctot, John Schultz, Neil Burch, Max Olan Smith, Daniel Hennes, Thomas Anthony, Julien Perolat</p>
<p><strong>Summary:</strong> Progress in fields of machine learning and adversarial planning has benefited
significantly from benchmark domains, from checkers and the classic UCI data
sets to Go and Diplomacy. In sequential decision-making, agent evaluation has
largely been restricted to few interactions against experts, with the aim to
reach some desired level of performance (e.g. beating a human professional
player). We propose a benchmark for multiagent learning based on repeated play
of the simple game Rock, Paper, Scis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.17592v1' target='_blank'>Learning Human-to-Robot Handovers from Point Clouds</a></h2>
<p><strong>Authors:</strong> Sammy Christen, Wei Yang, Claudia P√©rez-D'Arpino, Otmar Hilliges, Dieter Fox, Yu-Wei Chao</p>
<p><strong>Summary:</strong> We propose the first framework to learn control policies for vision-based
human-to-robot handovers, a critical task for human-robot interaction. While
research in Embodied AI has made significant progress in training robot agents
in simulated environments, interacting with humans remains challenging due to
the difficulties of simulating humans. Fortunately, recent research has
developed realistic simulated environments for human-to-robot handovers.
Leveraging this result, we introduce a method t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.05332v1' target='_blank'>Emergent autonomous scientific research capabilities of large language
  models</a></h2>
<p><strong>Authors:</strong> Daniil A. Boiko, Robert MacKnight, Gabe Gomes</p>
<p><strong>Summary:</strong> Transformer-based large language models are rapidly advancing in the field of
machine learning research, with applications spanning natural language,
biology, chemistry, and computer programming. Extreme scaling and reinforcement
learning from human feedback have significantly improved the quality of
generated text, enabling these models to perform various tasks and reason about
their choices. In this paper, we present an Intelligent Agent system that
combines multiple large language models for ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.13567v1' target='_blank'>M-EMBER: Tackling Long-Horizon Mobile Manipulation via Factorized Domain
  Transfer</a></h2>
<p><strong>Authors:</strong> Bohan Wu, Roberto Martin-Martin, Li Fei-Fei</p>
<p><strong>Summary:</strong> In this paper, we propose a method to create visuomotor mobile manipulation
solutions for long-horizon activities. We propose to leverage the recent
advances in simulation to train visual solutions for mobile manipulation. While
previous works have shown success applying this procedure to autonomous visual
navigation and stationary manipulation, applying it to long-horizon visuomotor
mobile manipulation is still an open challenge that demands both perceptual and
compositional generalization of m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.18380v1' target='_blank'>Potential-based Credit Assignment for Cooperative RL-based Testing of
  Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Utku Ayvaz, Chih-Hong Cheng, Hao Shen</p>
<p><strong>Summary:</strong> While autonomous vehicles (AVs) may perform remarkably well in generic
real-life cases, their irrational action in some unforeseen cases leads to
critical safety concerns. This paper introduces the concept of collaborative
reinforcement learning (RL) to generate challenging test cases for AV planning
and decision-making module. One of the critical challenges for collaborative RL
is the credit assignment problem, where a proper assignment of rewards to
multiple agents interacting in the traffic s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.19923v1' target='_blank'>MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL</a></h2>
<p><strong>Authors:</strong> Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, Zhixuan Liang</p>
<p><strong>Summary:</strong> Recently, diffusion model shines as a promising backbone for the sequence
modeling paradigm in offline reinforcement learning(RL). However, these works
mostly lack the generalization ability across tasks with reward or dynamics
change. To tackle this challenge, in this paper we propose a task-oriented
conditioned diffusion planner for offline meta-RL(MetaDiffuser), which
considers the generalization problem as conditional trajectory generation task
with contextual representation. The key is to l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.09532v2' target='_blank'>Hierarchical Planning and Control for Box Loco-Manipulation</a></h2>
<p><strong>Authors:</strong> Zhaoming Xie, Jonathan Tseng, Sebastian Starke, Michiel van de Panne, C. Karen Liu</p>
<p><strong>Summary:</strong> Humans perform everyday tasks using a combination of locomotion and
manipulation skills. Building a system that can handle both skills is essential
to creating virtual humans. We present a physically-simulated human capable of
solving box rearrangement tasks, which requires a combination of both skills.
We propose a hierarchical control architecture, where each level solves the
task at a different level of abstraction, and the result is a physics-based
simulated virtual human capable of rearrang...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.15106v1' target='_blank'>Improvise, Adapt, Overcome: Dynamic Resiliency Against Unknown Attack
  Vectors in Microgrid Cybersecurity Games</a></h2>
<p><strong>Authors:</strong> Suman Rath, Tapadhir Das, Shamik Sengupta</p>
<p><strong>Summary:</strong> Cyber-physical microgrids are vulnerable to rootkit attacks that manipulate
system dynamics to create instabilities in the network. Rootkits tend to hide
their access level within microgrid system components to launch sudden attacks
that prey on the slow response time of defenders to manipulate system
trajectory. This problem can be formulated as a multi-stage, non-cooperative,
zero-sum game with the attacker and the defender modeled as opposing players.
To solve the game, this paper proposes a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.17484v2' target='_blank'>Landmark Guided Active Exploration with State-specific Balance
  Coefficient</a></h2>
<p><strong>Authors:</strong> Fei Cui, Jiaojiao Fang, Mengke Yang, Guizhong Liu</p>
<p><strong>Summary:</strong> Goal-conditioned hierarchical reinforcement learning (GCHRL) decomposes
long-horizon tasks into sub-tasks through a hierarchical framework and it has
demonstrated promising results across a variety of domains. However, the
high-level policy's action space is often excessively large, presenting a
significant challenge to effective exploration and resulting in potentially
inefficient training. In this paper, we design a measure of prospect for
sub-goals by planning in the goal space based on the g...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.03015v1' target='_blank'>Sequential Neural Barriers for Scalable Dynamic Obstacle Avoidance</a></h2>
<p><strong>Authors:</strong> Hongzhan Yu, Chiaki Hirayama, Chenning Yu, Sylvia Herbert, Sicun Gao</p>
<p><strong>Summary:</strong> There are two major challenges for scaling up robot navigation around dynamic
obstacles: the complex interaction dynamics of the obstacles can be hard to
model analytically, and the complexity of planning and control grows
exponentially in the number of obstacles. Data-driven and learning-based
methods are thus particularly valuable in this context. However, data-driven
methods are sensitive to distribution drift, making it hard to train and
generalize learned models across different obstacle de...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.03486v3' target='_blank'>Discovering Hierarchical Achievements in Reinforcement Learning via
  Contrastive Learning</a></h2>
<p><strong>Authors:</strong> Seungyong Moon, Junyoung Yeom, Bumsoo Park, Hyun Oh Song</p>
<p><strong>Summary:</strong> Discovering achievements with a hierarchical structure in procedurally
generated environments presents a significant challenge. This requires an agent
to possess a broad range of abilities, including generalization and long-term
reasoning. Many prior methods have been built upon model-based or hierarchical
approaches, with the belief that an explicit module for long-term planning
would be advantageous for learning hierarchical dependencies. However, these
methods demand an excessive number of en...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.09668v1' target='_blank'>Towards A Unified Agent with Foundation Models</a></h2>
<p><strong>Authors:</strong> Norman Di Palo, Arunkumar Byravan, Leonard Hasenclever, Markus Wulfmeier, Nicolas Heess, Martin Riedmiller</p>
<p><strong>Summary:</strong> Language Models and Vision Language Models have recently demonstrated
unprecedented capabilities in terms of understanding human intentions,
reasoning, scene understanding, and planning-like behaviour, in text form,
among many others. In this work, we investigate how to embed and leverage such
abilities in Reinforcement Learning (RL) agents. We design a framework that
uses language as the core reasoning tool, exploring how this enables an agent
to tackle a series of fundamental RL challenges, su...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.11922v1' target='_blank'>Selective Perception: Optimizing State Descriptions with Reinforcement
  Learning for Language Model Actors</a></h2>
<p><strong>Authors:</strong> Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</p>
<p><strong>Summary:</strong> Large language models (LLMs) are being applied as actors for sequential
decision making tasks in domains such as robotics and games, utilizing their
general world knowledge and planning abilities. However, previous work does
little to explore what environment state information is provided to LLM actors
via language. Exhaustively describing high-dimensional states can impair
performance and raise inference costs for LLM actors. Previous LLM actors avoid
the issue by relying on hand-engineered, ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.15798v1' target='_blank'>Coordination of Bounded Rational Drones through Informed Prior Policy</a></h2>
<p><strong>Authors:</strong> Durgakant Pushp, Junhong Xu, Lantao Liu</p>
<p><strong>Summary:</strong> Biological agents, such as humans and animals, are capable of making
decisions out of a very large number of choices in a limited time. They can do
so because they use their prior knowledge to find a solution that is not
necessarily optimal but good enough for the given task. In this work, we study
the motion coordination of multiple drones under the above-mentioned paradigm,
Bounded Rationality (BR), to achieve cooperative motion planning tasks.
Specifically, we design a prior policy that provi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.05075v1' target='_blank'>Bayesian Inverse Transition Learning for Offline Settings</a></h2>
<p><strong>Authors:</strong> Leo Benac, Sonali Parbhoo, Finale Doshi-Velez</p>
<p><strong>Summary:</strong> Offline Reinforcement learning is commonly used for sequential
decision-making in domains such as healthcare and education, where the rewards
are known and the transition dynamics $T$ must be estimated on the basis of
batch data. A key challenge for all tasks is how to learn a reliable estimate
of the transition dynamics $T$ that produce near-optimal policies that are safe
enough so that they never take actions that are far away from the best action
with respect to their value functions and info...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.10135v1' target='_blank'>A Review on Objective-Driven Artificial Intelligence</a></h2>
<p><strong>Authors:</strong> Apoorv Singh</p>
<p><strong>Summary:</strong> While advancing rapidly, Artificial Intelligence still falls short of human
intelligence in several key aspects due to inherent limitations in current AI
technologies and our understanding of cognition. Humans have an innate ability
to understand context, nuances, and subtle cues in communication, which allows
us to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret
such contextual information accurately. Humans possess a vast repository of
common-sense knowledge that helps...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.14295v1' target='_blank'>Traffic Light Control with Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Taoyu Pan</p>
<p><strong>Summary:</strong> Traffic light control is important for reducing congestion in urban mobility
systems. This paper proposes a real-time traffic light control method using
deep Q learning. Our approach incorporates a reward function considering queue
lengths, delays, travel time, and throughput. The model dynamically decides
phase changes based on current traffic conditions. The training of the deep Q
network involves an offline stage from pre-generated data with fixed schedules
and an online stage using real-time...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.15808v2' target='_blank'>Learning the References of Online Model Predictive Control for Urban
  Self-Driving</a></h2>
<p><strong>Authors:</strong> Yubin Wang, Zengqi Peng, Yusen Xie, Yulin Li, Hakim Ghazzai, Jun Ma</p>
<p><strong>Summary:</strong> In this work, we propose a novel learning-based model predictive control
(MPC) framework for motion planning and control of urban self-driving. In this
framework, instantaneous references and cost functions of online MPC are
learned from raw sensor data without relying on any oracle or predicted states
of traffic. Moreover, driving safety conditions are latently encoded via the
introduction of a learnable instantaneous reference vector. In particular, we
implement a deep reinforcement learning (...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.03708v2' target='_blank'>Chat Failures and Troubles: Reasons and Solutions</a></h2>
<p><strong>Authors:</strong> Manal Helal, Patrick Holthaus, Gabriella Lakatos, Farshid Amirabdollahian</p>
<p><strong>Summary:</strong> This paper examines some common problems in Human-Robot Interaction (HRI)
causing failures and troubles in Chat. A given use case's design decisions
start with the suitable robot, the suitable chatting model, identifying common
problems that cause failures, identifying potential solutions, and planning
continuous improvement. In conclusion, it is recommended to use a closed-loop
control algorithm that guides the use of trained Artificial Intelligence (AI)
pre-trained models and provides vocabula...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.08476v1' target='_blank'>A Spiking Binary Neuron -- Detector of Causal Links</a></h2>
<p><strong>Authors:</strong> Mikhail Kiselev, Denis Larionov, Andrey Urusov</p>
<p><strong>Summary:</strong> Causal relationship recognition is a fundamental operation in neural networks
aimed at learning behavior, action planning, and inferring external world
dynamics. This operation is particularly crucial for reinforcement learning
(RL). In the context of spiking neural networks (SNNs), events are represented
as spikes emitted by network neurons or input nodes. Detecting causal
relationships within these events is essential for effective RL implementation.
This research paper presents a novel approa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.15740v1' target='_blank'>Data-Driven Latent Space Representation for Robust Bipedal Locomotion
  Learning</a></h2>
<p><strong>Authors:</strong> Guillermo A. Castillo, Bowen Weng, Wei Zhang, Ayonga Hereid</p>
<p><strong>Summary:</strong> This paper presents a novel framework for learning robust bipedal walking by
combining a data-driven state representation with a Reinforcement Learning (RL)
based locomotion policy. The framework utilizes an autoencoder to learn a
low-dimensional latent space that captures the complex dynamics of bipedal
locomotion from existing locomotion data. This reduced dimensional state
representation is then used as states for training a robust RL-based gait
policy, eliminating the need for heuristic stat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.16291v1' target='_blank'>Efficiency Separation between RL Methods: Model-Free, Model-Based and
  Goal-Conditioned</a></h2>
<p><strong>Authors:</strong> Brieuc Pinon, Rapha√´l Jungers, Jean-Charles Delvenne</p>
<p><strong>Summary:</strong> We prove a fundamental limitation on the efficiency of a wide class of
Reinforcement Learning (RL) algorithms. This limitation applies to model-free
RL methods as well as a broad range of model-based methods, such as planning
with tree search.
  Under an abstract definition of this class, we provide a family of RL
problems for which these methods suffer a lower bound exponential in the
horizon for their interactions with the environment to find an optimal
behavior. However, there exists a method...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.16838v1' target='_blank'>Social Navigation in Crowded Environments with Model Predictive Control
  and Deep Learning-Based Human Trajectory Prediction</a></h2>
<p><strong>Authors:</strong> Viet-Anh Le, Behdad Chalaki, Vaishnav Tadiparthi, Hossein Nourkhiz Mahjoub, Jovin D'sa, Ehsan Moradi-Pari</p>
<p><strong>Summary:</strong> Crowd navigation has received increasing attention from researchers over the
last few decades, resulting in the emergence of numerous approaches aimed at
addressing this problem to date. Our proposed approach couples agent motion
prediction and planning to avoid the freezing robot problem while
simultaneously capturing multi-agent social interactions by utilizing a
state-of-the-art trajectory prediction model i.e., social long short-term
memory model (Social-LSTM). Leveraging the output of Socia...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.01824v2' target='_blank'>Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon
  Decision-Making in Embodied AI</a></h2>
<p><strong>Authors:</strong> Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, Roberto Mart√≠n-Mart√≠n</p>
<p><strong>Summary:</strong> We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges
agents to use reasoning and decision-making skills to solve complex activities
that resemble everyday human challenges. The Mini-BEHAVIOR environment is a
fast, realistic Gridworld environment that offers the benefits of rapid
prototyping and ease of use while preserving a symbolic level of physical
realism and complexity found in complex embodied AI benchmarks. We introduce
key features such as procedural generation, to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.03779v1' target='_blank'>HandMeThat: Human-Robot Communication in Physical and Social
  Environments</a></h2>
<p><strong>Authors:</strong> Yanming Wan, Jiayuan Mao, Joshua B. Tenenbaum</p>
<p><strong>Summary:</strong> We introduce HandMeThat, a benchmark for a holistic evaluation of instruction
understanding and following in physical and social environments. While previous
datasets primarily focused on language grounding and planning, HandMeThat
considers the resolution of human instructions with ambiguities based on the
physical (object states and relations) and social (human actions and goals)
information. HandMeThat contains 10,000 episodes of human-robot interactions.
In each episode, the robot first obse...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.06751v1' target='_blank'>EARL: Eye-on-Hand Reinforcement Learner for Dynamic Grasping with Active
  Pose Estimation</a></h2>
<p><strong>Authors:</strong> Baichuan Huang, Jingjin Yu, Siddarth Jain</p>
<p><strong>Summary:</strong> In this paper, we explore the dynamic grasping of moving objects through
active pose tracking and reinforcement learning for hand-eye coordination
systems. Most existing vision-based robotic grasping methods implicitly assume
target objects are stationary or moving predictably. Performing grasping of
unpredictably moving objects presents a unique set of challenges. For example,
a pre-computed robust grasp can become unreachable or unstable as the target
object moves, and motion planning must als...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.09714v2' target='_blank'>Enhancing Task Performance of Learned Simplified Models via
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hien Bui, Michael Posa</p>
<p><strong>Summary:</strong> In contact-rich tasks, the hybrid, multi-modal nature of contact dynamics
poses great challenges in model representation, planning, and control. Recent
efforts have attempted to address these challenges via data-driven methods,
learning dynamical models in combination with model predictive control. Those
methods, while effective, rely solely on minimizing forward prediction errors
to hope for better task performance with MPC controllers. This weak correlation
can result in data inefficiency as w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.14224v1' target='_blank'>Detrive: Imitation Learning with Transformer Detection for End-to-End
  Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Daoming Chen, Ning Wang, Feng Chen, Tony Pipe</p>
<p><strong>Summary:</strong> This Paper proposes a novel Transformer-based end-to-end autonomous driving
model named Detrive. This model solves the problem that the past end-to-end
models cannot detect the position and size of traffic participants. Detrive
uses an end-to-end transformer based detection model as its perception module;
a multi-layer perceptron as its feature fusion network; a recurrent neural
network with gate recurrent unit for path planning; and two controllers for the
vehicle's forward speed and turning an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.14503v1' target='_blank'>Diversify Question Generation with Retrieval-Augmented Style Transfer</a></h2>
<p><strong>Authors:</strong> Qi Gou, Zehua Xia, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, Nguyen Cam-Tu</p>
<p><strong>Summary:</strong> Given a textual passage and an answer, humans are able to ask questions with
various expressions, but this ability is still challenging for most question
generation (QG) systems. Existing solutions mainly focus on the internal
knowledge within the given passage or the semantic word space for diverse
content planning. These methods, however, have not considered the potential of
external knowledge for expression diversity. To bridge this gap, we propose
RAST, a framework for Retrieval-Augmented St...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.20266v2' target='_blank'>Beyond Average Return in Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Alexandre Marthe, Aur√©lien Garivier, Claire Vernade</p>
<p><strong>Summary:</strong> What are the functionals of the reward that can be computed and optimized
exactly in Markov Decision Processes?In the finite-horizon, undiscounted
setting, Dynamic Programming (DP) can only handle these operations efficiently
for certain classes of statistics. We summarize the characterization of these
classes for policy evaluation, and give a new answer for the planning problem.
Interestingly, we prove that only generalized means can be optimized exactly,
even in the more general framework of D...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.01728v1' target='_blank'>RDE: A Hybrid Policy Framework for Multi-Agent Path Finding Problem</a></h2>
<p><strong>Authors:</strong> Jianqi Gao, Yanjie Li, Xiaoqing Yang, Mingshan Tan</p>
<p><strong>Summary:</strong> Multi-agent path finding (MAPF) is an abstract model for the navigation of
multiple robots in warehouse automation, where multiple robots plan
collision-free paths from the start to goal positions. Reinforcement learning
(RL) has been employed to develop partially observable distributed MAPF
policies that can be scaled to any number of agents. However, RL-based MAPF
policies often get agents stuck in deadlock due to warehouse automation's dense
and structured obstacles. This paper proposes a nov...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.04107v1' target='_blank'>Interactive Semantic Map Representation for Skill-based Visual Object
  Navigation</a></h2>
<p><strong>Authors:</strong> Tatiana Zemskova, Aleksei Staroverov, Kirill Muravyev, Dmitry Yudin, Aleksandr Panov</p>
<p><strong>Summary:</strong> Visual object navigation using learning methods is one of the key tasks in
mobile robotics. This paper introduces a new representation of a scene semantic
map formed during the embodied agent interaction with the indoor environment.
It is based on a neural network method that adjusts the weights of the
segmentation model with backpropagation of the predicted fusion loss values
during inference on a regular (backward) or delayed (forward) image sequence.
We have implemented this representation in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.05511v3' target='_blank'>Anytime-Constrained Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jeremy McMahan, Xiaojin Zhu</p>
<p><strong>Summary:</strong> We introduce and study constrained Markov Decision Processes (cMDPs) with
anytime constraints. An anytime constraint requires the agent to never violate
its budget at any point in time, almost surely. Although Markovian policies are
no longer sufficient, we show that there exist optimal deterministic policies
augmented with cumulative costs. In fact, we present a fixed-parameter
tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our
reduction yields planning and learning a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.08244v2' target='_blank'>Language and Sketching: An LLM-driven Interactive Multimodal Multitask
  Robot Navigation Framework</a></h2>
<p><strong>Authors:</strong> Weiqin Zu, Wenbin Song, Ruiqing Chen, Ze Guo, Fanglei Sun, Zheng Tian, Wei Pan, Jun Wang</p>
<p><strong>Summary:</strong> The socially-aware navigation system has evolved to adeptly avoid various
obstacles while performing multiple tasks, such as point-to-point navigation,
human-following, and -guiding. However, a prominent gap persists: in
Human-Robot Interaction (HRI), the procedure of communicating commands to
robots demands intricate mathematical formulations. Furthermore, the transition
between tasks does not quite possess the intuitive control and user-centric
interactivity that one would desire. In this work...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.13443v2' target='_blank'>Guided Flows for Generative Modeling and Decision Making</a></h2>
<p><strong>Authors:</strong> Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, Ricky T. Q. Chen</p>
<p><strong>Summary:</strong> Classifier-free guidance is a key component for enhancing the performance of
conditional generative models across diverse tasks. While it has previously
demonstrated remarkable improvements for the sample quality, it has only been
exclusively employed for diffusion models. In this paper, we integrate
classifier-free guidance into Flow Matching (FM) models, an alternative
simulation-free approach that trains Continuous Normalizing Flows (CNFs) based
on regressing vector fields. We explore the usa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.11203v2' target='_blank'>Obstacle-Aware Navigation of Soft Growing Robots via Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Haitham El-Hussieny, Ibrahim Hameed</p>
<p><strong>Summary:</strong> Soft growing robots, are a type of robots that are designed to move and adapt
to their environment in a similar way to how plants grow and move with
potential applications where they could be used to navigate through tight
spaces, dangerous terrain, and hard-to-reach areas. This research explores the
application of deep reinforcement Q-learning algorithm for facilitating the
navigation of the soft growing robots in cluttered environments. The proposed
algorithm utilizes the flexibility of the so...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.12459v2' target='_blank'>Towards Socially and Morally Aware RL agent: Reward Design With LLM</a></h2>
<p><strong>Authors:</strong> Zhaoyue Wang</p>
<p><strong>Summary:</strong> When we design and deploy an Reinforcement Learning (RL) agent, reward
functions motivates agents to achieve an objective. An incorrect or incomplete
specification of the objective can result in behavior that does not align with
human values - failing to adhere with social and moral norms that are ambiguous
and context dependent, and cause undesired outcomes such as negative side
effects and exploration that is unsafe. Previous work have manually defined
reward functions to avoid negative side e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.13827v1' target='_blank'>Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink
  in Markovian IoT Models</a></h2>
<p><strong>Authors:</strong> Eslam Eldeeb, Mohammad Shehab, Hirley Alves</p>
<p><strong>Summary:</strong> The age of information (AoI) is used to measure the freshness of the data. In
IoT networks, the traditional resource management schemes rely on a message
exchange between the devices and the base station (BS) before communication
which causes high AoI, high energy consumption, and low reliability. Unmanned
aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the
AoI, energy-saving, and throughput improvement. In this paper, we present a
novel learning-based framework that esti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.07963v3' target='_blank'>SPO: Sequential Monte Carlo Policy Optimisation</a></h2>
<p><strong>Authors:</strong> Matthew V Macfarlane, Edan Toledo, Donal Byrne, Paul Duckworth, Alexandre Laterre</p>
<p><strong>Summary:</strong> Leveraging planning during learning and decision-making is central to the
long-term development of intelligent agents. Recent works have successfully
combined tree-based search methods and self-play learning mechanisms to this
end. However, these methods typically face scaling challenges due to the
sequential nature of their search. While practical engineering solutions can
partly overcome this, they often result in a negative impact on performance. In
this paper, we introduce SPO: Sequential Mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.15283v1' target='_blank'>When in Doubt, Think Slow: Iterative Reasoning with Latent Imagination</a></h2>
<p><strong>Authors:</strong> Martin Benfeghoul, Umais Zahid, Qinghai Guo, Zafeirios Fountas</p>
<p><strong>Summary:</strong> In an unfamiliar setting, a model-based reinforcement learning agent can be
limited by the accuracy of its world model. In this work, we present a novel,
training-free approach to improving the performance of such agents separately
from planning and learning. We do so by applying iterative inference at
decision-time, to fine-tune the inferred agent states based on the coherence of
future state representations. Our approach achieves a consistent improvement in
both reconstruction accuracy and tas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.18866v2' target='_blank'>Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming</a></h2>
<p><strong>Authors:</strong> Hany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, Sungjin Ahn</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) has been a primary approach to
ameliorating the sample efficiency issue as well as to make a generalist agent.
However, there has not been much effort toward enhancing the strategy of
dreaming itself. Therefore, it is a question whether and how an agent can
"dream better" in a more structured and strategic way. In this paper, inspired
by the observation from cognitive science suggesting that humans use a spatial
divide-and-conquer strategy in planning, w...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.04232v1' target='_blank'>Generalizing Cooperative Eco-driving via Multi-residual Task Learning</a></h2>
<p><strong>Authors:</strong> Vindula Jayawardana, Sirui Li, Cathy Wu, Yashar Farid, Kentaro Oguchi</p>
<p><strong>Summary:</strong> Conventional control, such as model-based control, is commonly utilized in
autonomous driving due to its efficiency and reliability. However, real-world
autonomous driving contends with a multitude of diverse traffic scenarios that
are challenging for these planning algorithms. Model-free Deep Reinforcement
Learning (DRL) presents a promising avenue in this direction, but learning DRL
control policies that generalize to multiple traffic scenarios is still a
challenge. To address this, we introdu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.04586v2' target='_blank'>Learning Speed Adaptation for Flight in Clutter</a></h2>
<p><strong>Authors:</strong> Guangyu Zhao, Tianyue Wu, Yeke Chen, Fei Gao</p>
<p><strong>Summary:</strong> Animals learn to adapt speed of their movements to their capabilities and the
environment they observe. Mobile robots should also demonstrate this ability to
trade-off aggressiveness and safety for efficiently accomplishing tasks. The
aim of this work is to endow flight vehicles with the ability of speed
adaptation in prior unknown and partially observable cluttered environments. We
propose a hierarchical learning and planning framework where we utilize both
well-established methods of model-bas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.05468v1' target='_blank'>Will GPT-4 Run DOOM?</a></h2>
<p><strong>Authors:</strong> Adrian de Wynter</p>
<p><strong>Summary:</strong> We show that GPT-4's reasoning and planning capabilities extend to the 1993
first-person shooter Doom. This large language model (LLM) is able to run and
play the game with only a few instructions, plus a textual
description--generated by the model itself from screenshots--about the state of
the game being observed. We find that GPT-4 can play the game to a passable
degree: it is able to manipulate doors, combat enemies, and perform pathing.
More complex prompting strategies involving multiple m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.10738v1' target='_blank'>Horizon-Free Regret for Linear Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Jason D. Lee, Yuxin Chen, Simon S. Du</p>
<p><strong>Summary:</strong> A recent line of works showed regret bounds in reinforcement learning (RL)
can be (nearly) independent of planning horizon, a.k.a.~the horizon-free
bounds. However, these regret bounds only apply to settings where a polynomial
dependency on the size of transition model is allowed, such as tabular Markov
Decision Process (MDP) and linear mixture MDP. We give the first horizon-free
bound for the popular linear MDP setting where the size of the transition model
can be exponentially large or even un...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.05074v1' target='_blank'>On the Uniqueness of Solution for the Bellman Equation of LTL Objectives</a></h2>
<p><strong>Authors:</strong> Zetong Xuan, Alper Kamil Bozkurt, Miroslav Pajic, Yu Wang</p>
<p><strong>Summary:</strong> Surrogate rewards for linear temporal logic (LTL) objectives are commonly
utilized in planning problems for LTL objectives. In a widely-adopted surrogate
reward approach, two discount factors are used to ensure that the expected
return approximates the satisfaction probability of the LTL objective. The
expected return then can be estimated by methods using the Bellman updates such
as reinforcement learning. However, the uniqueness of the solution to the
Bellman equation with two discount factors...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01394v1' target='_blank'>Analysis of a Modular Autonomous Driving Architecture: The Top
  Submission to CARLA Leaderboard 2.0 Challenge</a></h2>
<p><strong>Authors:</strong> Weize Zhang, Mohammed Elmahgiubi, Kasra Rezaee, Behzad Khamidehi, Hamidreza Mirkhani, Fazel Arasteh, Chunlin Li, Muhammad Ahsan Kaleem, Eduardo R. Corral-Soto, Dhruv Sharma, Tongtong Cao</p>
<p><strong>Summary:</strong> In this paper we present the architecture of the Kyber-E2E submission to the
map track of CARLA Leaderboard 2.0 Autonomous Driving (AD) challenge 2023,
which achieved first place. We employed a modular architecture for our solution
consists of five main components: sensing, localization, perception,
tracking/prediction, and planning/control. Our solution leverages
state-of-the-art language-assisted perception models to help our planner
perform more reliably in highly challenging traffic scenario...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01175v4' target='_blank'>NeoRL: Efficient Exploration for Nonepisodic RL</a></h2>
<p><strong>Authors:</strong> Bhavya Sukhija, Lenart Treven, Florian D√∂rfler, Stelian Coros, Andreas Krause</p>
<p><strong>Summary:</strong> We study the problem of nonepisodic reinforcement learning (RL) for nonlinear
dynamical systems, where the system dynamics are unknown and the RL agent has
to learn from a single trajectory, i.e., without resets. We propose Nonepisodic
Optimistic RL (NeoRL), an approach based on the principle of optimism in the
face of uncertainty. NeoRL uses well-calibrated probabilistic models and plans
optimistically w.r.t. the epistemic uncertainty about the unknown dynamics.
Under continuity and bounded ene...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01361v1' target='_blank'>Learning to Play Atari in a World of Tokens</a></h2>
<p><strong>Authors:</strong> Pranav Agarwal, Sheldon Andrews, Samira Ebrahimi Kahou</p>
<p><strong>Summary:</strong> Model-based reinforcement learning agents utilizing transformers have shown
improved sample efficiency due to their ability to model extended context,
resulting in more accurate world models. However, for complex reasoning and
planning tasks, these methods primarily rely on continuous representations.
This complicates modeling of discrete properties of the real world such as
disjoint object classes between which interpolation is not plausible. In this
work, we introduce discrete abstract represe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.01377v1' target='_blank'>Multi-Agent Transfer Learning via Temporal Contrastive Learning</a></h2>
<p><strong>Authors:</strong> Weihao Zeng, Joseph Campbell, Simon Stepputtis, Katia Sycara</p>
<p><strong>Summary:</strong> This paper introduces a novel transfer learning framework for deep
multi-agent reinforcement learning. The approach automatically combines
goal-conditioned policies with temporal contrastive learning to discover
meaningful sub-goals. The approach involves pre-training a goal-conditioned
agent, finetuning it on the target domain, and using contrastive learning to
construct a planning graph that guides the agent via sub-goals. Experiments on
multi-agent coordination Overcooked tasks demonstrate im...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.07875v2' target='_blank'>Carbon Market Simulation with Adaptive Mechanism Design</a></h2>
<p><strong>Authors:</strong> Han Wang, Wenhao Li, Hongyuan Zha, Baoxiang Wang</p>
<p><strong>Summary:</strong> A carbon market is a market-based tool that incentivizes economic agents to
align individual profits with the global utility, i.e., reducing carbon
emissions to tackle climate change. Cap and trade stands as a critical
principle based on allocating and trading carbon allowances (carbon emission
credit), enabling economic agents to follow planned emissions and penalizing
excess emissions. A central authority is responsible for introducing and
allocating those allowances in cap and trade. However,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.11978v1' target='_blank'>Dialogue Action Tokens: Steering Language Models in Goal-Directed
  Dialogue with a Multi-Turn Planner</a></h2>
<p><strong>Authors:</strong> Kenneth Li, Yiming Wang, Fernanda Vi√©gas, Martin Wattenberg</p>
<p><strong>Summary:</strong> We present an approach called Dialogue Action Tokens (DAT) that adapts
language model agents to plan goal-directed dialogues. The core idea is to
treat each utterance as an action, thereby converting dialogues into games
where existing approaches such as reinforcement learning can be applied.
Specifically, we freeze a pretrained language model and train a small planner
model that predicts a continuous action vector, used for controlled generation
in each round. This design avoids the problem of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.17098v1' target='_blank'>Learning Temporal Distances: Contrastive Successor Features Can Provide
  a Metric Structure for Decision-Making</a></h2>
<p><strong>Authors:</strong> Vivek Myers, Chongyi Zheng, Anca Dragan, Sergey Levine, Benjamin Eysenbach</p>
<p><strong>Summary:</strong> Temporal distances lie at the heart of many algorithms for planning, control,
and reinforcement learning that involve reaching goals, allowing one to
estimate the transit time between two states. However, prior attempts to define
such temporal distances in stochastic settings have been stymied by an
important limitation: these prior approaches do not satisfy the triangle
inequality. This is not merely a definitional concern, but translates to an
inability to generalize and find shortest paths. I...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.17840v2' target='_blank'>Human-Object Interaction from Human-Level Instructions</a></h2>
<p><strong>Authors:</strong> Zhen Wu, Jiaman Li, Pei Xu, C. Karen Liu</p>
<p><strong>Summary:</strong> Intelligent agents must autonomously interact with the environments to
perform daily tasks based on human-level instructions. They need a foundational
understanding of the world to accurately interpret these instructions, along
with precise low-level movement and interaction skills to execute the derived
actions. In this work, we propose the first complete system for synthesizing
physically plausible, long-horizon human-object interactions for object
manipulation in contextual environments, driv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02466v2' target='_blank'>PWM: Policy Learning with Large World Models</a></h2>
<p><strong>Authors:</strong> Ignat Georgiev, Varun Giridhar, Nicklas Hansen, Animesh Garg</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) has achieved impressive results on complex tasks
but struggles in multi-task settings with different embodiments. World models
offer scalability by learning a simulation of the environment, yet they often
rely on inefficient gradient-free optimization methods. We introduce Policy
learning with large World Models (PWM), a novel model-based RL algorithm that
learns continuous control policies from large multi-task world models. By
pre-training the world model on offline...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.04064v2' target='_blank'>Collision Avoidance for Multiple UAVs in Unknown Scenarios with Causal
  Representation Disentanglement</a></h2>
<p><strong>Authors:</strong> Jiafan Zhuang, Zihao Xia, Gaofei Han, Boxi Wang, Wenji Li, Dongliang Wang, Zhifeng Hao, Ruichu Cai, Zhun Fan</p>
<p><strong>Summary:</strong> Deep reinforcement learning (DRL) has achieved remarkable progress in online
path planning tasks for multi-UAV systems. However, existing DRL-based methods
often suffer from performance degradation when tackling unseen scenarios, since
the non-causal factors in visual representations adversely affect policy
learning. To address this issue, we propose a novel representation learning
approach, \ie, causal representation disentanglement, which can identify the
causal and non-causal factors in repre...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.06584v1' target='_blank'>HiLMa-Res: A General Hierarchical Framework via Residual RL for
  Combining Quadrupedal Locomotion and Manipulation</a></h2>
<p><strong>Authors:</strong> Xiaoyu Huang, Qiayuan Liao, Yiming Ni, Zhongyu Li, Laura Smith, Sergey Levine, Xue Bin Peng, Koushil Sreenath</p>
<p><strong>Summary:</strong> This work presents HiLMa-Res, a hierarchical framework leveraging
reinforcement learning to tackle manipulation tasks while performing continuous
locomotion using quadrupedal robots. Unlike most previous efforts that focus on
solving a specific task, HiLMa-Res is designed to be general for various
loco-manipulation tasks that require quadrupedal robots to maintain sustained
mobility. The novel design of this framework tackles the challenges of
integrating continuous locomotion control and manipu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.11070v2' target='_blank'>Optimal Defender Strategies for CAGE-2 using Causal Modeling and Tree
  Search</a></h2>
<p><strong>Authors:</strong> Kim Hammar, Neil Dhir, Rolf Stadler</p>
<p><strong>Summary:</strong> The CAGE-2 challenge is considered a standard benchmark to compare methods
for autonomous cyber defense. Current state-of-the-art methods evaluated
against this benchmark are based on model-free (offline) reinforcement
learning, which does not provide provably optimal defender strategies. We
address this limitation and present a formal (causal) model of CAGE-2 together
with a method that produces a provably optimal defender strategy, which we call
Causal Partially Observable Monte-Carlo Planning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.16254v1' target='_blank'>Negotiating Control: Neurosymbolic Variable Autonomy</a></h2>
<p><strong>Authors:</strong> Georgios Bakirtzis, Manolis Chiou, Andreas Theodorou</p>
<p><strong>Summary:</strong> Variable autonomy equips a system, such as a robot, with mixed initiatives
such that it can adjust its independence level based on the task's complexity
and the surrounding environment. Variable autonomy solves two main problems in
robotic planning: the first is the problem of humans being unable to keep focus
in monitoring and intervening during robotic tasks without appropriate human
factor indicators, and the second is achieving mission success in unforeseen
and uncertain environments in the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.16404v1' target='_blank'>Evaluating Uncertainties in Electricity Markets via Machine Learning and
  Quantum Computing</a></h2>
<p><strong>Authors:</strong> Shuyang Zhu, Ziqing Zhu, Linghua Zhu, Yujian Ye, Siqi Bu, Sasa Z. Djokic</p>
<p><strong>Summary:</strong> The analysis of decision-making process in electricity markets is crucial for
understanding and resolving issues related to market manipulation and reduced
social welfare. Traditional Multi-Agent Reinforcement Learning (MARL) method
can model decision-making of generation companies (GENCOs), but faces
challenges due to uncertainties in policy functions, reward functions, and
inter-agent interactions. Quantum computing offers a promising solution to
resolve these uncertainties, and this paper int...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.03394v1' target='_blank'>Faster Model Predictive Control via Self-Supervised Initialization
  Learning</a></h2>
<p><strong>Authors:</strong> Zhaoxin Li, Letian Chen, Rohan Paleja, Subramanya Nageshrao, Matthew Gombolay</p>
<p><strong>Summary:</strong> Optimization for robot control tasks, spanning various methodologies,
includes Model Predictive Control (MPC). However, the complexity of the system,
such as non-convex and non-differentiable cost functions and prolonged planning
horizons often drastically increases the computation time, limiting MPC's
real-world applicability. Prior works in speeding up the optimization have
limitations on solving convex problem and generalizing to hold out domains. To
overcome this challenge, we develop a nove...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.10635v2' target='_blank'>Strategist: Learning Strategic Skills by LLMs via Bi-Level Tree Search</a></h2>
<p><strong>Authors:</strong> Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, Ziniu Hu</p>
<p><strong>Summary:</strong> In this paper, we propose a new method STRATEGIST that utilizes LLMs to
acquire new skills for playing multi-agent games through a self-improvement
process. Our method gathers quality feedback through self-play simulations with
Monte Carlo tree search and LLM-based reflection, which can then be used to
learn high-level strategic skills such as how to evaluate states that guide the
low-level execution. We showcase how our method can be used in both action
planning and dialogue generation in the c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.12775v2' target='_blank'>Intelligent OPC Engineer Assistant for Semiconductor Manufacturing</a></h2>
<p><strong>Authors:</strong> Guojin Chen, Haoyu Yang, Bei Yu, Haoxing Ren</p>
<p><strong>Summary:</strong> Advancements in chip design and manufacturing have enabled the processing of
complex tasks such as deep learning and natural language processing, paving the
way for the development of artificial general intelligence (AGI). AI, on the
other hand, can be leveraged to innovate and streamline semiconductor
technology from planning and implementation to manufacturing. In this paper, we
present \textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered
methodology designed to solve the core manufa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.15950v2' target='_blank'>Atari-GPT: Benchmarking Multimodal Large Language Models as Low-Level
  Policies in Atari Games</a></h2>
<p><strong>Authors:</strong> Nicholas R. Waytowich, Devin White, MD Sunbeam, Vinicius G. Goecks</p>
<p><strong>Summary:</strong> Recent advancements in large language models (LLMs) have expanded their
capabilities beyond traditional text-based tasks to multimodal domains,
integrating visual, auditory, and textual data. While multimodal LLMs have been
extensively explored for high-level planning in domains like robotics and
games, their potential as low-level controllers remains largely untapped. In
this paper, we introduce a novel benchmark aimed at testing the emergent
capabilities of multimodal LLMs as low-level policie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.16125v1' target='_blank'>DECAF: a Discrete-Event based Collaborative Human-Robot Framework for
  Furniture Assembly</a></h2>
<p><strong>Authors:</strong> Giulio Giacomuzzo, Matteo Terreran, Siddarth Jain, Diego Romeres</p>
<p><strong>Summary:</strong> This paper proposes a task planning framework for collaborative Human-Robot
scenarios, specifically focused on assembling complex systems such as
furniture. The human is characterized as an uncontrollable agent, implying for
example that the agent is not bound by a pre-established sequence of actions
and instead acts according to its own preferences. Meanwhile, the task planner
computes reactively the optimal actions for the collaborative robot to
efficiently complete the entire assembly task in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.00134v4' target='_blank'>MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale</a></h2>
<p><strong>Authors:</strong> Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik</p>
<p><strong>Summary:</strong> Multi-agent pathfinding (MAPF) is a problem that generally requires finding
collision-free paths for multiple agents in a shared environment. Solving MAPF
optimally, even under restrictive assumptions, is NP-hard, yet efficient
solutions for this problem are critical for numerous applications, such as
automated warehouses and transportation systems. Recently, learning-based
approaches to MAPF have gained attention, particularly those leveraging deep
reinforcement learning. Typically, such learni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.02444v3' target='_blank'>USV-AUV Collaboration Framework for Underwater Tasks under Extreme Sea
  Conditions</a></h2>
<p><strong>Authors:</strong> Jingzehua Xu, Guanwen Xie, Xinqi Wang, Yimian Ding, Shuai Zhang</p>
<p><strong>Summary:</strong> Autonomous underwater vehicles (AUVs) are valuable for ocean exploration due
to their flexibility and ability to carry communication and detection units.
Nevertheless, AUVs alone often face challenges in harsh and extreme sea
conditions. This study introduces a unmanned surface vehicle (USV)-AUV
collaboration framework, which includes high-precision multi-AUV positioning
using USV path planning via Fisher information matrix optimization and
reinforcement learning for multi-AUV cooperative tasks....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.02747v1' target='_blank'>Tractable Offline Learning of Regular Decision Processes</a></h2>
<p><strong>Authors:</strong> Ahana Deb, Roberto Cipollone, Anders Jonsson, Alessandro Ronca, Mohammad Sadegh Talebi</p>
<p><strong>Summary:</strong> This work studies offline Reinforcement Learning (RL) in a class of
non-Markovian environments called Regular Decision Processes (RDPs). In RDPs,
the unknown dependency of future observations and rewards from the past
interactions can be captured by some hidden finite-state automaton. For this
reason, many RDP algorithms first reconstruct this unknown dependency using
automata learning techniques. In this paper, we show that it is possible to
overcome two strong limitations of previous offline R...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.03881v1' target='_blank'>Multi-agent Path Finding for Mixed Autonomy Traffic Coordination</a></h2>
<p><strong>Authors:</strong> Han Zheng, Zhongxia Yan, Cathy Wu</p>
<p><strong>Summary:</strong> In the evolving landscape of urban mobility, the prospective integration of
Connected and Automated Vehicles (CAVs) with Human-Driven Vehicles (HDVs)
presents a complex array of challenges and opportunities for autonomous driving
systems. While recent advancements in robotics have yielded Multi-Agent Path
Finding (MAPF) algorithms tailored for agent coordination task characterized by
simplified kinematics and complete control over agent behaviors, these
solutions are inapplicable in mixed-traffi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.13573v1' target='_blank'>Human-Robot Cooperative Distribution Coupling for
  Hamiltonian-Constrained Social Navigation</a></h2>
<p><strong>Authors:</strong> Weizheng Wang, Chao Yu, Yu Wang, Byung-Cheol Min</p>
<p><strong>Summary:</strong> Navigating in human-filled public spaces is a critical challenge for
deploying autonomous robots in real-world environments. This paper introduces
NaviDIFF, a novel Hamiltonian-constrained socially-aware navigation framework
designed to address the complexities of human-robot interaction and
socially-aware path planning. NaviDIFF integrates a port-Hamiltonian framework
to model dynamic physical interactions and a diffusion model to manage
uncertainty in human-robot cooperation. The framework lev...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.15717v2' target='_blank'>Autonomous Wheel Loader Navigation Using Goal-Conditioned Actor-Critic
  MPC</a></h2>
<p><strong>Authors:</strong> Aleksi M√§ki-Penttil√§, Naeim Ebrahimi Toulkani, Reza Ghabcheloo</p>
<p><strong>Summary:</strong> This paper proposes a novel control method for an autonomous wheel loader,
enabling time-efficient navigation to an arbitrary goal pose. Unlike prior
works that combine high-level trajectory planners with Model Predictive Control
(MPC), we directly enhance the planning capabilities of MPC by integrating a
cost function derived from Actor-Critic Reinforcement Learning (RL).
Specifically, we train an RL agent to solve the pose reaching task in
simulation, then incorporate the trained neural networ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.17138v1' target='_blank'>Landscape of Policy Optimization for Finite Horizon MDPs with General
  State and Action</a></h2>
<p><strong>Authors:</strong> Xin Chen, Yifan Hu, Minda Zhao</p>
<p><strong>Summary:</strong> Policy gradient methods are widely used in reinforcement learning. Yet, the
nonconvexity of policy optimization imposes significant challenges in
understanding the global convergence of policy gradient methods. For a class of
finite-horizon Markov Decision Processes (MDPs) with general state and action
spaces, we develop a framework that provides a set of easily verifiable
assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy
optimization. Leveraging the KL condition, policy...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.17469v2' target='_blank'>VertiSelector: Automatic Curriculum Learning for Wheeled Mobility on
  Vertically Challenging Terrain</a></h2>
<p><strong>Authors:</strong> Tong Xu, Chenhui Pan, Xuesu Xiao</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) has the potential to enable extreme off-road
mobility by circumventing complex kinodynamic modeling, planning, and control
by simulated end-to-end trial-and-error learning experiences. However, most RL
methods are sample-inefficient when training in a large amount of manually
designed simulation environments and struggle at generalizing to the real
world. To address these issues, we introduce VertiSelector (VS), an automatic
curriculum learning framework designed to e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.18343v1' target='_blank'>Improving Agent Behaviors with RL Fine-tuning for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Zhenghao Peng, Wenjie Luo, Yiren Lu, Tianyi Shen, Cole Gulino, Ari Seff, Justin Fu</p>
<p><strong>Summary:</strong> A major challenge in autonomous vehicle research is modeling agent behaviors,
which has critical applications including constructing realistic and reliable
simulations for off-board evaluation and forecasting traffic agents motion for
onboard planning. While supervised learning has shown success in modeling
agents across various domains, these models can suffer from distribution shift
when deployed at test-time. In this work, we improve the reliability of agent
behaviors by closed-loop fine-tuni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.03066v1' target='_blank'>Hybrid Classical/RL Local Planner for Ground Robot Navigation</a></h2>
<p><strong>Authors:</strong> Vishnu D. Sharma, Jeongran Lee, Matthew Andrews, Ilija Had≈æiƒá</p>
<p><strong>Summary:</strong> Local planning is an optimization process within a mobile robot navigation
stack that searches for the best velocity vector, given the robot and
environment state. Depending on how the optimization criteria and constraints
are defined, some planners may be better than others in specific situations. We
consider two conceptually different planners. The first planner explores the
velocity space in real-time and has superior path-tracking and motion
smoothness performance. The second planner was tra...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.05656v1' target='_blank'>On the Modeling Capabilities of Large Language Models for Sequential
  Decision Making</a></h2>
<p><strong>Authors:</strong> Martin Klissarov, Devon Hjelm, Alexander Toshev, Bogdan Mazoure</p>
<p><strong>Summary:</strong> Large pretrained models are showing increasingly better performance in
reasoning and planning tasks across different modalities, opening the
possibility to leverage them for complex sequential decision making problems.
In this paper, we investigate the capabilities of Large Language Models (LLMs)
for reinforcement learning (RL) across a diversity of interactive domains. We
evaluate their ability to produce decision-making policies, either directly, by
generating actions, or indirectly, by first ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.06627v1' target='_blank'>Variations in Multi-Agent Actor-Critic Frameworks for Joint
  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and
  Directions</a></h2>
<p><strong>Authors:</strong> Muhammad Morshed Alam, Muhammad Yeasir Aarafat, Tamim Hossain</p>
<p><strong>Summary:</strong> Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can
effectively execute surveillance, connectivity, and computing services to
ground users (GUs). These missions require trajectory planning, UAV-GUs
association, task offloading, next-hop selection, and resources such as
transmit power, bandwidth, caching, and computing allocation to improve network
performances. Owing to the highly dynamic topology, limited resources, and
non-availability of global knowledge, optimizing network p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.08377v1' target='_blank'>Optimizing Vital Sign Monitoring in Resource-Constrained Maternal Care:
  An RL-Based Restless Bandit Approach</a></h2>
<p><strong>Authors:</strong> Niclas Boehmer, Yunfan Zhao, Guojun Xiong, Paula Rodriguez-Diaz, Paola Del Cueto Cibrian, Joseph Ngonzi, Adeline Boatin, Milind Tambe</p>
<p><strong>Summary:</strong> Maternal mortality remains a significant global public health challenge. One
promising approach to reducing maternal deaths occurring during facility-based
childbirth is through early warning systems, which require the consistent
monitoring of mothers' vital signs after giving birth. Wireless vital sign
monitoring devices offer a labor-efficient solution for continuous monitoring,
but their scarcity raises the critical question of how to allocate them most
effectively. We devise an allocation al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.14081v1' target='_blank'>Reward-free World Models for Online Imitation Learning</a></h2>
<p><strong>Authors:</strong> Shangzhe Li, Zhiao Huang, Hao Su</p>
<p><strong>Summary:</strong> Imitation learning (IL) enables agents to acquire skills directly from expert
demonstrations, providing a compelling alternative to reinforcement learning.
However, prior online IL approaches struggle with complex tasks characterized
by high-dimensional inputs and complex dynamics. In this work, we propose a
novel approach to online imitation learning that leverages reward-free world
models. Our method learns environmental dynamics entirely in latent spaces
without reconstruction, enabling effic...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.17744v2' target='_blank'>Learning Versatile Skills with Curriculum Masking</a></h2>
<p><strong>Authors:</strong> Yao Tang, Zhihui Xie, Zichuan Lin, Deheng Ye, Shuai Li</p>
<p><strong>Summary:</strong> Masked prediction has emerged as a promising pretraining paradigm in offline
reinforcement learning (RL) due to its versatile masking schemes, enabling
flexible inference across various downstream tasks with a unified model.
Despite the versatility of masked prediction, it remains unclear how to balance
the learning of skills at different levels of complexity. To address this, we
propose CurrMask, a curriculum masking pretraining paradigm for sequential
decision making. Motivated by how humans l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.01521v2' target='_blank'>Diversity Progress for Goal Selection in Discriminability-Motivated RL</a></h2>
<p><strong>Authors:</strong> Erik M. Lintunen, Nadia M. Ady, Christian Guckelsberger</p>
<p><strong>Summary:</strong> Non-uniform goal selection has the potential to improve the reinforcement
learning (RL) of skills over uniform-random selection. In this paper, we
introduce a method for learning a goal-selection policy in
intrinsically-motivated goal-conditioned RL: "Diversity Progress" (DP). The
learner forms a curriculum based on observed improvement in discriminability
over its set of goals. Our proposed method is applicable to the class of
discriminability-motivated agents, where the intrinsic reward is com...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.02446v1' target='_blank'>Learning World Models for Unconstrained Goal Navigation</a></h2>
<p><strong>Authors:</strong> Yuanlin Duan, Wensen Mao, He Zhu</p>
<p><strong>Summary:</strong> Learning world models offers a promising avenue for goal-conditioned
reinforcement learning with sparse rewards. By allowing agents to plan actions
or exploratory goals without direct interaction with the environment, world
models enhance exploration efficiency. The quality of a world model hinges on
the richness of data stored in the agent's replay buffer, with expectations of
reasonable generalization across the state space surrounding recorded
trajectories. However, challenges arise in genera...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.10336v2' target='_blank'>BMP: Bridging the Gap between B-Spline and Movement Primitives</a></h2>
<p><strong>Authors:</strong> Weiran Liao, Ge Li, Hongyi Zhou, Rudolf Lioutikov, Gerhard Neumann</p>
<p><strong>Summary:</strong> This work introduces B-spline Movement Primitives (BMPs), a new Movement
Primitive (MP) variant that leverages B-splines for motion representation.
B-splines are a well-known concept in motion planning due to their ability to
generate complex, smooth trajectories with only a few control points while
satisfying boundary conditions, i.e., passing through a specified desired
position with desired velocity. However, current usages of B-splines tend to
ignore the higher-order statistics in trajectory...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.18272v1' target='_blank'>NeoHebbian Synapses to Accelerate Online Training of Neuromorphic
  Hardware</a></h2>
<p><strong>Authors:</strong> Shubham Pande, Sai Sukruth Bezugam, Tinish Bhattacharya, Ewelina Wlazlak, Anjan Chakaravorty, Bhaswar Chakrabarti, Dmitri Strukov</p>
<p><strong>Summary:</strong> Neuromorphic systems that employ advanced synaptic learning rules, such as
the three-factor learning rule, require synaptic devices of increased
complexity. Herein, a novel neoHebbian artificial synapse utilizing ReRAM
devices has been proposed and experimentally validated to meet this demand.
This synapse features two distinct state variables: a neuron coupling weight
and an "eligibility trace" that dictates synaptic weight updates. The coupling
weight is encoded in the ReRAM conductance, while...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.19497v1' target='_blank'>SANGO: Socially Aware Navigation through Grouped Obstacles</a></h2>
<p><strong>Authors:</strong> Rahath Malladi, Amol Harsh, Arshia Sangwan, Sunita Chauhan, Sandeep Manjanna</p>
<p><strong>Summary:</strong> This paper introduces SANGO (Socially Aware Navigation through Grouped
Obstacles), a novel method that ensures socially appropriate behavior by
dynamically grouping obstacles and adhering to social norms. Using deep
reinforcement learning, SANGO trains agents to navigate complex environments
leveraging the DBSCAN algorithm for obstacle clustering and Proximal Policy
Optimization (PPO) for path planning. The proposed approach improves safety and
social compliance by maintaining appropriate distan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.00555v1' target='_blank'>Learning Dynamic Weight Adjustment for Spatial-Temporal Trajectory
  Planning in Crowd Navigation</a></h2>
<p><strong>Authors:</strong> Muqing Cao, Xinhang Xu, Yizhuo Yang, Jianping Li, Tongxing Jin, Pengfei Wang, Tzu-Yi Hung, Guosheng Lin, Lihua Xie</p>
<p><strong>Summary:</strong> Robot navigation in dense human crowds poses a significant challenge due to
the complexity of human behavior in dynamic and obstacle-rich environments. In
this work, we propose a dynamic weight adjustment scheme using a neural network
to predict the optimal weights of objectives in an optimization-based motion
planner. We adopt a spatial-temporal trajectory planner and incorporate diverse
objectives to achieve a balance among safety, efficiency, and goal achievement
in complex and dynamic enviro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.01036v1' target='_blank'>Generating Freeform Endoskeletal Robots</a></h2>
<p><strong>Authors:</strong> Muhan Li, Lingji Kong, Sam Kriegman</p>
<p><strong>Summary:</strong> The automatic design of embodied agents (e.g. robots) has existed for 31
years and is experiencing a renaissance of interest in the literature. To date
however, the field has remained narrowly focused on two kinds of anatomically
simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless
bodies. Here we bridge these two extremes with the open ended creation of
terrestrial endoskeletal robots: deformable soft bodies that leverage jointed
internal skeletons to move efficiently a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.04369v1' target='_blank'>Intersection-Aware Assessment of EMS Accessibility in NYC: A Data-Driven
  Approach</a></h2>
<p><strong>Authors:</strong> Haoran Su, Joseph Y. J. Chow</p>
<p><strong>Summary:</strong> Emergency response times are critical in densely populated urban environments
like New York City (NYC), where traffic congestion significantly impedes
emergency vehicle (EMV) mobility. This study introduces an intersection-aware
emergency medical service (EMS) accessibility model to evaluate and improve EMV
travel times across NYC. Integrating intersection density metrics, road network
characteristics, and demographic data, the model identifies vulnerable regions
with inadequate EMS coverage. Th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.09117v1' target='_blank'>Reconfigurable Intelligent Surface for Internet of Robotic Things</a></h2>
<p><strong>Authors:</strong> Wanli Ni, Ruyu Luo, Xinran Zhang, Peng Wang, Wen Wang, Hui Tian</p>
<p><strong>Summary:</strong> With the rapid development of artificial intelligence, robotics, and Internet
of Things, multi-robot systems are progressively acquiring human-like
environmental perception and understanding capabilities, empowering them to
complete complex tasks through autonomous decision-making and interaction.
However, the Internet of Robotic Things (IoRT) faces significant challenges in
terms of spectrum resources, sensing accuracy, communication latency, and
energy supply. To address these issues, a reconf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.02811v1' target='_blank'>First-place Solution for Streetscape Shop Sign Recognition Competition</a></h2>
<p><strong>Authors:</strong> Bin Wang, Li Jing</p>
<p><strong>Summary:</strong> Text recognition technology applied to street-view storefront signs is
increasingly utilized across various practical domains, including map
navigation, smart city planning analysis, and business value assessments in
commercial districts. This technology holds significant research and commercial
potential. Nevertheless, it faces numerous challenges. Street view images often
contain signboards with complex designs and diverse text styles, complicating
the text recognition process. A notable advan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.09770v1' target='_blank'>EVAL: EigenVector-based Average-reward Learning</a></h2>
<p><strong>Authors:</strong> Jacob Adamczyk, Volodymyr Makarenko, Stas Tiomkin, Rahul V. Kulkarni</p>
<p><strong>Summary:</strong> In reinforcement learning, two objective functions have been developed
extensively in the literature: discounted and averaged rewards. The
generalization to an entropy-regularized setting has led to improved robustness
and exploration for both of these objectives. Recently, the entropy-regularized
average-reward problem was addressed using tools from large deviation theory in
the tabular setting. This method has the advantage of linearity, providing
access to both the optimal policy and average ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.13011v1' target='_blank'>MONA: Myopic Optimization with Non-myopic Approval Can Mitigate
  Multi-step Reward Hacking</a></h2>
<p><strong>Authors:</strong> Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah</p>
<p><strong>Summary:</strong> Future advanced AI systems may learn sophisticated strategies through
reinforcement learning (RL) that humans cannot understand well enough to safely
evaluate. We propose a training method which avoids agents learning undesired
multi-step plans that receive high reward (multi-step "reward hacks") even if
humans are not able to detect that the behaviour is undesired. The method,
Myopic Optimization with Non-myopic Approval (MONA), works by combining
short-sighted optimization with far-sighted rew...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.17529v1' target='_blank'>Accelerated DC loadflow solver for topology optimization</a></h2>
<p><strong>Authors:</strong> Nico Westerbeck, Joost van Dijk, Jan Viebahn, Christian Merz, Dirk Witthaut</p>
<p><strong>Summary:</strong> We present a massively parallel solver that accelerates DC loadflow
computations for power grid topology optimization tasks. Our approach leverages
low-rank updates of the Power Transfer Distribution Factors (PTDFs) to
represent substation splits, line outages, and reconfigurations without ever
refactorizing the system. Furthermore, we implement the core routines on
Graphics Processing Units (GPUs), thereby exploiting their high-throughput
architecture for linear algebra. A two-level decompositi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.03550v1' target='_blank'>TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy
  Constraint</a></h2>
<p><strong>Authors:</strong> Haotian Lin, Pengcheng Wang, Jeff Schneider, Guanya Shi</p>
<p><strong>Summary:</strong> Model-based reinforcement learning algorithms that combine model-based
planning and learned value/policy prior have gained significant recognition for
their high data efficiency and superior performance in continuous control.
However, we discover that existing methods that rely on standard SAC-style
policy iteration for value learning, directly using data generated by the
planner, often result in \emph{persistent value overestimation}. Through
theoretical analysis and experiments, we argue that ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.05454v2' target='_blank'>Temporal Representation Alignment: Successor Features Enable Emergent
  Compositionality in Robot Instruction Following</a></h2>
<p><strong>Authors:</strong> Vivek Myers, Bill Chunyuan Zheng, Anca Dragan, Kuan Fang, Sergey Levine</p>
<p><strong>Summary:</strong> Effective task representations should facilitate compositionality, such that
after learning a variety of basic tasks, an agent can perform compound tasks
consisting of multiple steps simply by composing the representations of the
constituent steps together. While this is conceptually simple and appealing, it
is not clear how to automatically learn representations that enable this sort
of compositionality. We show that learning to associate the representations of
current and future states with a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.06772v1' target='_blank'>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</a></h2>
<p><strong>Authors:</strong> Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang</p>
<p><strong>Summary:</strong> We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.10862v1' target='_blank'>Accelerated co-design of robots through morphological pretraining</a></h2>
<p><strong>Authors:</strong> Luke Strgar, Sam Kriegman</p>
<p><strong>Summary:</strong> The co-design of robot morphology and neural control typically requires using
reinforcement learning to approximate a unique control policy gradient for each
body plan, demanding massive amounts of training data to measure the
performance of each design. Here we show that a universal, morphology-agnostic
controller can be rapidly and directly obtained by gradient-based optimization
through differentiable simulation. This process of morphological pretraining
allows the designer to explore non-dif...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.11733v1' target='_blank'>Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking
  Practical Reasoning and Situation Modelling in a Text-Simulated Situated
  Environment</a></h2>
<p><strong>Authors:</strong> Jonathan Jordan, Sherzod Hakimov, David Schlangen</p>
<p><strong>Summary:</strong> Large language models (LLMs) have risen to prominence as 'chatbots' for users
to interact via natural language. However, their abilities to capture
common-sense knowledge make them seem promising as language-based planners of
situated or embodied action as well. We have implemented a simple text-based
environment -- similar to others that have before been used for
reinforcement-learning of agents -- that simulates, very abstractly, a
household setting. We use this environment and the detailed er...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.12198v1' target='_blank'>Maximize Your Diffusion: A Study into Reward Maximization and Alignment
  for Diffusion-based Control</a></h2>
<p><strong>Authors:</strong> Dom Huh, Prasant Mohapatra</p>
<p><strong>Summary:</strong> Diffusion-based planning, learning, and control methods present a promising
branch of powerful and expressive decision-making solutions. Given the growing
interest, such methods have undergone numerous refinements over the past years.
However, despite these advancements, existing methods are limited in their
investigations regarding general methods for reward maximization within the
decision-making process. In this work, we study extensions of fine-tuning
approaches for control applications. Spe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.13092v1' target='_blank'>Text2World: Benchmarking Large Language Models for Symbolic World Model
  Generation</a></h2>
<p><strong>Authors:</strong> Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Hongyuan Zhang, Wenqi Shao, Ping Luo</p>
<p><strong>Summary:</strong> Recently, there has been growing interest in leveraging large language models
(LLMs) to generate symbolic world models from textual descriptions. Although
LLMs have been extensively explored in the context of world modeling, prior
studies encountered several challenges, including evaluation randomness,
dependence on indirect metrics, and a limited domain scope. To address these
limitations, we introduce a novel benchmark, Text2World, based on planning
domain definition language (PDDL), featuring...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.13443v1' target='_blank'>Physics-Aware Robotic Palletization with Online Masking Inference</a></h2>
<p><strong>Authors:</strong> Tianqi Zhang, Zheng Wu, Yuxin Chen, Yixiao Wang, Boyuan Liang, Scott Moura, Masayoshi Tomizuka, Mingyu Ding, Wei Zhan</p>
<p><strong>Summary:</strong> The efficient planning of stacking boxes, especially in the online setting
where the sequence of item arrivals is unpredictable, remains a critical
challenge in modern warehouse and logistics management. Existing solutions
often address box size variations, but overlook their intrinsic and physical
properties, such as density and rigidity, which are crucial for real-world
applications. We use reinforcement learning (RL) to solve this problem by
employing action space masking to direct the RL pol...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1506.03624v1' target='_blank'>Bootstrapping Skills</a></h2>
<p><strong>Authors:</strong> Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor</p>
<p><strong>Summary:</strong> The monolithic approach to policy representation in Markov Decision Processes
(MDPs) looks for a single policy that can be represented as a function from
states to actions. For the monolithic approach to succeed (and this is not
always possible), a complex feature representation is often necessary since the
policy is a complex object that has to prescribe what actions to take all over
the state space. This is especially true in large domains with complicated
dynamics. It is also computationally ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1611.03531v2' target='_blank'>Estimating Dynamic Treatment Regimes in Mobile Health Using V-learning</a></h2>
<p><strong>Authors:</strong> Daniel J. Luckett, Eric B. Laber, Anna R. Kahkoska, David M. Maahs, Elizabeth Mayer-Davis, Michael R. Kosorok</p>
<p><strong>Summary:</strong> The vision for precision medicine is to use individual patient
characteristics to inform a personalized treatment plan that leads to the best
healthcare possible for each patient. Mobile technologies have an important
role to play in this vision as they offer a means to monitor a patient's health
status in real-time and subsequently to deliver interventions if, when, and in
the dose that they are needed. Dynamic treatment regimes formalize
individualized treatment plans as sequences of decision ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1801.06176v3' target='_blank'>Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy
  Learning</a></h2>
<p><strong>Authors:</strong> Baolin Peng, Xiujun Li, Jianfeng Gao, Jingjing Liu, Kam-Fai Wong, Shang-Yu Su</p>
<p><strong>Summary:</strong> Training a task-completion dialogue agent via reinforcement learning (RL) is
costly because it requires many interactions with real users. One common
alternative is to use a user simulator. However, a user simulator usually lacks
the language complexity of human interlocutors and the biases in its design may
tend to degrade the agent. To address these issues, we present Deep Dyna-Q,
which to our knowledge is the first deep RL framework that integrates planning
for task-completion dialogue policy...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.02501v1' target='_blank'>Simplifying Reward Design through Divide-and-Conquer</a></h2>
<p><strong>Authors:</strong> Ellis Ratner, Dylan Hadfield-Menell, Anca D. Dragan</p>
<p><strong>Summary:</strong> Designing a good reward function is essential to robot planning and
reinforcement learning, but it can also be challenging and frustrating. The
reward needs to work across multiple different environments, and that often
requires many iterations of tuning. We introduce a novel divide-and-conquer
approach that enables the designer to specify a reward separately for each
environment. By treating these separate reward functions as observations about
the underlying true reward, we derive an approach ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.01292v1' target='_blank'>The StreetLearn Environment and Dataset</a></h2>
<p><strong>Authors:</strong> Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Denis Teplyashin, Karl Moritz Hermann, Mateusz Malinowski, Matthew Koichi Grimes, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, Raia Hadsell</p>
<p><strong>Summary:</strong> Navigation is a rich and well-grounded problem domain that drives progress in
many different areas of research: perception, planning, memory, exploration,
and optimisation in particular. Historically these challenges have been
separately considered and solutions built that rely on stationary datasets -
for example, recorded trajectories through an environment. These datasets
cannot be used for decision-making and reinforcement learning, however, and in
general the perspective of navigation as an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.09624v1' target='_blank'>On the Feasibility of Learning, Rather than Assuming, Human Biases for
  Reward Inference</a></h2>
<p><strong>Authors:</strong> Rohin Shah, Noah Gundotra, Pieter Abbeel, Anca D. Dragan</p>
<p><strong>Summary:</strong> Our goal is for agents to optimize the right reward function, despite how
difficult it is for us to specify what that is. Inverse Reinforcement Learning
(IRL) enables us to infer reward functions from demonstrations, but it usually
assumes that the expert is noisily optimal. Real people, on the other hand,
often have systematic biases: risk-aversion, myopia, etc. One option is to try
to characterize these biases and account for them explicitly during learning.
But in the era of deep learning, a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.10971v1' target='_blank'>NeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory
  Learning for Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Sorin Grigorescu, Bogdan Trasnea, Liviu Marina, Andrei Vasilcoi, Tiberiu Cocias</p>
<p><strong>Summary:</strong> Autonomous vehicles are controlled today either based on sequences of
decoupled perception-planning-action operations, either based on End2End or
Deep Reinforcement Learning (DRL) systems. Current deep learning solutions for
autonomous driving are subject to several limitations (e.g. they estimate
driving actions through a direct mapping of sensors to actuators, or require
complex reward shaping methods). Although the cost function used for training
can aggregate multiple weighted objectives, th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.07738v2' target='_blank'>A Survey of Deep Learning Techniques for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, Gigel Macesanu</p>
<p><strong>Summary:</strong> The last decade witnessed increasingly rapid progress in self-driving vehicle
technology, mainly backed up by advances in the area of deep learning and
artificial intelligence. The objective of this paper is to survey the current
state-of-the-art on deep learning technologies used in autonomous driving. We
start by presenting AI-based self-driving architectures, convolutional and
recurrent neural networks, as well as the deep reinforcement learning paradigm.
These methodologies form a base for t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.00071v3' target='_blank'>Safety Guarantees for Planning Based on Iterative Gaussian Processes</a></h2>
<p><strong>Authors:</strong> Kyriakos Polymenakos, Luca Laurenti, Andrea Patane, Jan-Peter Calliess, Luca Cardelli, Marta Kwiatkowska, Alessandro Abate, Stephen Roberts</p>
<p><strong>Summary:</strong> Gaussian Processes (GPs) are widely employed in control and learning because
of their principled treatment of uncertainty. However, tracking uncertainty for
iterative, multi-step predictions in general leads to an analytically
intractable problem. While approximation methods exist, they do not come with
guarantees, making it difficult to estimate their reliability and to trust
their predictions. In this work, we derive formal probability error bounds for
iterative prediction and planning with GP...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.05896v1' target='_blank'>Learning Abstract Models for Strategic Exploration and Fast Reward
  Transfer</a></h2>
<p><strong>Authors:</strong> Evan Zheran Liu, Ramtin Keramati, Sudarshan Seshadri, Kelvin Guu, Panupong Pasupat, Emma Brunskill, Percy Liang</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL) is appealing because (i) it enables
planning and thus more strategic exploration, and (ii) by decoupling dynamics
from rewards, it enables fast transfer to new reward functions. However,
learning an accurate Markov Decision Process (MDP) over high-dimensional states
(e.g., raw pixels) is extremely challenging because it requires function
approximation, which leads to compounding errors. Instead, to avoid compounding
errors, we propose learning an abstract ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.01931v1' target='_blank'>Offline Learning for Planning: A Summary</a></h2>
<p><strong>Authors:</strong> Giorgio Angelotti, Nicolas Drougard, Caroline Ponzoni Carvalho Chanel</p>
<p><strong>Summary:</strong> The training of autonomous agents often requires expensive and unsafe
trial-and-error interactions with the environment. Nowadays several data sets
containing recorded experiences of intelligent agents performing various tasks,
spanning from the control of unmanned vehicles to human-robot interaction and
medical applications are accessible on the internet. With the intention of
limiting the costs of the learning procedure it is convenient to exploit the
information that is already available rath...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.08843v2' target='_blank'>Approximate information state for approximate planning and reinforcement
  learning in partially observed systems</a></h2>
<p><strong>Authors:</strong> Jayakumar Subramanian, Amit Sinha, Raihan Seraj, Aditya Mahajan</p>
<p><strong>Summary:</strong> We propose a theoretical framework for approximate planning and learning in
partially observed systems. Our framework is based on the fundamental notion of
information state. We provide two equivalent definitions of information state
-- i) a function of history which is sufficient to compute the expected reward
and predict its next value; ii) equivalently, a function of the history which
can be recursively updated and is sufficient to compute the expected reward and
predict the next observation....</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.09539v1' target='_blank'>Online Shielding for Stochastic Systems</a></h2>
<p><strong>Authors:</strong> Bettina K√∂nighofer, Julian Rudolf, Alexander Palmisano, Martin Tappler, Roderick Bloem</p>
<p><strong>Summary:</strong> In this paper, we propose a method to develop trustworthy reinforcement
learning systems. To ensure safety especially during exploration, we
automatically synthesize a correct-by-construction runtime enforcer, called a
shield, that blocks all actions that are unsafe with respect to a temporal
logic specification from the agent. Our main contribution is a new synthesis
algorithm for computing the shield online. Existing offline shielding
approaches compute exhaustively the safety of all states-ac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.10284v5' target='_blank'>Reinforcement Learning Based Temporal Logic Control with Soft
  Constraints Using Limit-deterministic Generalized Buchi Automata</a></h2>
<p><strong>Authors:</strong> Mingyu Cai, Shaoping Xiao, Zhijun Li, Zhen Kan</p>
<p><strong>Summary:</strong> This paper studies the control synthesis of motion planning subject to
uncertainties. The uncertainties are considered in robot motions and
environment properties, giving rise to the probabilistic labeled Markov
decision process (PL-MDP). A Model-Free Reinforcement The learning (RL) method
is developed to generate a finite-memory control policy to satisfy high-level
tasks expressed in linear temporal logic (LTL) formulas. Due to uncertainties
and potentially conflicting tasks, this work focuses ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.03311v1' target='_blank'>PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable
  Physics</a></h2>
<p><strong>Authors:</strong> Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, Chuang Gan</p>
<p><strong>Summary:</strong> Simulated virtual environments serve as one of the main driving forces behind
developing and evaluating skill learning algorithms. However, existing
environments typically only simulate rigid body physics. Additionally, the
simulation process usually does not provide gradients that might be useful for
planning and control optimizations. We introduce a new differentiable physics
benchmark called PasticineLab, which includes a diverse collection of soft body
manipulation tasks. In each task, the a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.04768v1' target='_blank'>Selection-Expansion: A Unifying Framework for Motion-Planning and
  Diversity Search Algorithms</a></h2>
<p><strong>Authors:</strong> Alexandre Chenu, Nicolas Perrin-Gilbert, St√©phane Doncieux, Olivier Sigaud</p>
<p><strong>Summary:</strong> Reinforcement learning agents need a reward signal to learn successful
policies. When this signal is sparse or the corresponding gradient is
deceptive, such agents need a dedicated mechanism to efficiently explore their
search space without relying on the reward. Looking for a large diversity of
behaviors or using Motion Planning (MP) algorithms are two options in this
context. In this paper, we build on the common roots between these two options
to investigate the properties of two diversity se...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.13415v1' target='_blank'>Building Intelligent Autonomous Navigation Agents</a></h2>
<p><strong>Authors:</strong> Devendra Singh Chaplot</p>
<p><strong>Summary:</strong> Breakthroughs in machine learning in the last decade have led to `digital
intelligence', i.e. machine learning models capable of learning from vast
amounts of labeled data to perform several digital tasks such as speech
recognition, face recognition, machine translation and so on. The goal of this
thesis is to make progress towards designing algorithms capable of `physical
intelligence', i.e. building intelligent autonomous navigation agents capable
of learning to perform complex navigation task...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.15200v1' target='_blank'>Action Set Based Policy Optimization for Safe Power Grid Management</a></h2>
<p><strong>Authors:</strong> Bo Zhou, Hongsheng Zeng, Yuecheng Liu, Kejiao Li, Fan Wang, Hao Tian</p>
<p><strong>Summary:</strong> Maintaining the stability of the modern power grid is becoming increasingly
difficult due to fluctuating power consumption, unstable power supply coming
from renewable energies, and unpredictable accidents such as man-made and
natural disasters. As the operation on the power grid must consider its impact
on future stability, reinforcement learning (RL) has been employed to provide
sequential decision-making in power grid management. However, existing methods
have not considered the environmental...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.07195v2' target='_blank'>Target Languages (vs. Inductive Biases) for Learning to Act and Plan</a></h2>
<p><strong>Authors:</strong> Hector Geffner</p>
<p><strong>Summary:</strong> Recent breakthroughs in AI have shown the remarkable power of deep learning
and deep reinforcement learning. These developments, however, have been tied to
specific tasks, and progress in out-of-distribution generalization has been
limited. While it is assumed that these limitations can be overcome by
incorporating suitable inductive biases, the notion of inductive biases itself
is often left vague and does not provide meaningful guidance. In the paper, I
articulate a different learning approach...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.11094v2' target='_blank'>PredictionNet: Real-Time Joint Probabilistic Traffic Prediction for
  Planning, Control, and Simulation</a></h2>
<p><strong>Authors:</strong> Alexey Kamenev, Lirui Wang, Ollin Boer Bohan, Ishwar Kulkarni, Bilal Kartal, Artem Molchanov, Stan Birchfield, David Nist√©r, Nikolai Smolyanskiy</p>
<p><strong>Summary:</strong> Predicting the future motion of traffic agents is crucial for safe and
efficient autonomous driving. To this end, we present PredictionNet, a deep
neural network (DNN) that predicts the motion of all surrounding traffic agents
together with the ego-vehicle's motion. All predictions are probabilistic and
are represented in a simple top-down rasterization that allows an arbitrary
number of agents. Conditioned on a multi-layer map with lane information, the
network outputs future positions, velocit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.11808v4' target='_blank'>A Dynamic Programming Algorithm for Finding an Optimal Sequence of
  Informative Measurements</a></h2>
<p><strong>Authors:</strong> Peter N. Loxley, Ka-Wai Cheung</p>
<p><strong>Summary:</strong> An informative measurement is the most efficient way to gain information
about an unknown state. We present a first-principles derivation of a
general-purpose dynamic programming algorithm that returns an optimal sequence
of informative measurements by sequentially maximizing the entropy of possible
measurement outcomes. This algorithm can be used by an autonomous agent or
robot to decide where best to measure next, planning a path corresponding to an
optimal sequence of informative measurements...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.12204v4' target='_blank'>Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on
  a Random Graph and Provable Auction-Fitted Q-learning</a></h2>
<p><strong>Authors:</strong> Hyunwook Kang, Taehwan Kwon, Jinkyoo Park, James R. Morrison</p>
<p><strong>Summary:</strong> This paper explores the possibility of near-optimally solving multi-agent,
multi-task NP-hard planning problems with time-dependent rewards using a
learning-based algorithm. In particular, we consider a class of robot/machine
scheduling problems called the multi-robot reward collection problem (MRRC).
Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety
of related problems. In representing the MRRC problem as a sequential
decision-making problem, we observe that each st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.02604v2' target='_blank'>BARK: Open Behavior Benchmarking in Multi-Agent Environments</a></h2>
<p><strong>Authors:</strong> Julian Bernhard, Klemens Esterle, Patrick Hart, Tobias Kessler</p>
<p><strong>Summary:</strong> Predicting and planning interactive behaviors in complex traffic situations
presents a challenging task. Especially in scenarios involving multiple traffic
participants that interact densely, autonomous vehicles still struggle to
interpret situations and to eventually achieve their own mission goal. As
driving tests are costly and challenging scenarios are hard to find and
reproduce, simulation is widely used to develop, test, and benchmark behavior
models. However, most simulations rely on data...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.12850v3' target='_blank'>Learning Neural-Symbolic Descriptive Planning Models via Cube-Space
  Priors: The Voyage Home (to STRIPS)</a></h2>
<p><strong>Authors:</strong> Masataro Asai, Christian Muise</p>
<p><strong>Summary:</strong> We achieved a new milestone in the difficult task of enabling agents to learn
about their environment autonomously. Our neuro-symbolic architecture is
trained end-to-end to produce a succinct and effective discrete state
transition model from images alone. Our target representation (the Planning
Domain Definition Language) is already in a form that off-the-shelf solvers can
consume, and opens the door to the rich array of modern heuristic search
capabilities. We demonstrate how the sophisticated...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.07081v2' target='_blank'>MIDAS: Multi-agent Interaction-aware Decision-making with Adaptive
  Strategies for Urban Autonomous Navigation</a></h2>
<p><strong>Authors:</strong> Xiaoyi Chen, Pratik Chaudhari</p>
<p><strong>Summary:</strong> Autonomous navigation in crowded, complex urban environments requires
interacting with other agents on the road. A common solution to this problem is
to use a prediction model to guess the likely future actions of other agents.
While this is reasonable, it leads to overly conservative plans because it does
not explicitly model the mutual influence of the actions of interacting agents.
This paper builds a reinforcement learning-based method named MIDAS where an
ego-agent learns to affect the cont...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.09439v2' target='_blank'>Latent Representation Prediction Networks</a></h2>
<p><strong>Authors:</strong> Hlynur Dav√≠√∞ Hlynsson, Merlin Sch√ºler, Robin Schiewer, Tobias Glasmachers, Laurenz Wiskott</p>
<p><strong>Summary:</strong> Deeply-learned planning methods are often based on learning representations
that are optimized for unrelated tasks. For example, they might be trained on
reconstructing the environment. These representations are then combined with
predictor functions for simulating rollouts to navigate the environment. We
find this principle of learning representations unsatisfying and propose to
learn them such that they are directly optimized for the task at hand: to be
maximally predictable for the predictor ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.00155v2' target='_blank'>Deep Reactive Planning in Dynamic Environments</a></h2>
<p><strong>Authors:</strong> Kei Ota, Devesh K. Jha, Tadashi Onishi, Asako Kanezaki, Yusuke Yoshiyasu, Yoko Sasaki, Toshisada Mariyama, Daniel Nikovski</p>
<p><strong>Summary:</strong> The main novelty of the proposed approach is that it allows a robot to learn
an end-to-end policy which can adapt to changes in the environment during
execution. While goal conditioning of policies has been studied in the RL
literature, such approaches are not easily extended to cases where the robot's
goal can change during execution. This is something that humans are naturally
able to do. However, it is difficult for robots to learn such reflexes (i.e.,
to naturally respond to dynamic environm...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.12363v3' target='_blank'>C-Learning: Horizon-Aware Cumulative Accessibility Estimation</a></h2>
<p><strong>Authors:</strong> Panteha Naderian, Gabriel Loaiza-Ganem, Harry J. Braviner, Anthony L. Caterini, Jesse C. Cresswell, Tong Li, Animesh Garg</p>
<p><strong>Summary:</strong> Multi-goal reaching is an important problem in reinforcement learning needed
to achieve algorithmic generalization. Despite recent advances in this field,
current algorithms suffer from three major challenges: high sample complexity,
learning only a single way of reaching the goals, and difficulties in solving
complex motion planning tasks. In order to address these limitations, we
introduce the concept of cumulative accessibility functions, which measure the
reachability of a goal from a given ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.03546v1' target='_blank'>Scalable, Decentralized Multi-Agent Reinforcement Learning Methods
  Inspired by Stigmergy and Ant Colonies</a></h2>
<p><strong>Authors:</strong> Austin Anhkhoi Nguyen</p>
<p><strong>Summary:</strong> Bolstering multi-agent learning algorithms to tackle complex coordination and
control tasks has been a long-standing challenge of on-going research. Numerous
methods have been proposed to help reduce the effects of non-stationarity and
unscalability. In this work, we investigate a novel approach to decentralized
multi-agent learning and planning that attempts to address these two
challenges. In particular, this method is inspired by the cohesion,
coordination, and behavior of ant colonies. As a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.04538v1' target='_blank'>Learning Interaction-aware Guidance Policies for Motion Planning in
  Dense Traffic Scenarios</a></h2>
<p><strong>Authors:</strong> Bruno Brito, Achin Agarwal, Javier Alonso-Mora</p>
<p><strong>Summary:</strong> Autonomous navigation in dense traffic scenarios remains challenging for
autonomous vehicles (AVs) because the intentions of other drivers are not
directly observable and AVs have to deal with a wide range of driving
behaviors. To maneuver through dense traffic, AVs must be able to reason how
their actions affect others (interaction model) and exploit this reasoning to
navigate through dense traffic safely. This paper presents a novel framework
for interaction-aware motion planning in dense traf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.03244v4' target='_blank'>Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with
  Plug-in Solver</a></h2>
<p><strong>Authors:</strong> Xiaoyu Chen, Jiachen Hu, Lin F. Yang, Liwei Wang</p>
<p><strong>Summary:</strong> Although model-based reinforcement learning (RL) approaches are considered
more sample efficient, existing algorithms are usually relying on sophisticated
planning algorithm to couple tightly with the model-learning procedure. Hence
the learned models may lack the ability of being re-used with more specialized
planners. In this paper we address this issue and provide approaches to learn
an RL model efficiently without the guidance of a reward signal. In particular,
we take a plug-in solver appro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.09771v2' target='_blank'>On Reward-Free RL with Kernel and Neural Function Approximations:
  Single-Agent MDP and Markov Game</a></h2>
<p><strong>Authors:</strong> Shuang Qiu, Jieping Ye, Zhaoran Wang, Zhuoran Yang</p>
<p><strong>Summary:</strong> To achieve sample efficiency in reinforcement learning (RL), it necessitates
efficiently exploring the underlying environment. Under the offline setting,
addressing the exploration challenge lies in collecting an offline dataset with
sufficient coverage. Motivated by such a challenge, we study the reward-free RL
problem, where an agent aims to thoroughly explore the environment without any
pre-specified reward function. Then, given any extrinsic reward, the agent
computes the policy via a planni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.01587v1' target='_blank'>Procedural Generalization by Planning with Self-Supervised World Models</a></h2>
<p><strong>Authors:</strong> Ankesh Anand, Jacob Walker, Yazhe Li, Eszter V√©rtes, Julian Schrittwieser, Sherjil Ozair, Th√©ophane Weber, Jessica B. Hamrick</p>
<p><strong>Summary:</strong> One of the key promises of model-based reinforcement learning is the ability
to generalize using an internal model of the world to make predictions in novel
environments and tasks. However, the generalization ability of model-based
agents is not well understood because existing work has focused on model-free
agents when benchmarking generalization. Here, we explicitly measure the
generalization ability of model-based agents in comparison to their model-free
counterparts. We focus our analysis on...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.05434v1' target='_blank'>A Reinforcement Learning-based Adaptive Control Model for Future Street
  Planning, An Algorithm and A Case Study</a></h2>
<p><strong>Authors:</strong> Qiming Ye, Yuxiang Feng, Jing Han, Marc Stettler, Panagiotis Angeloudis</p>
<p><strong>Summary:</strong> With the emerging technologies in Intelligent Transportation System (ITS),
the adaptive operation of road space is likely to be realised within decades.
An intelligent street can learn and improve its decision-making on the
right-of-way (ROW) for road users, liberating more active pedestrian space
while maintaining traffic safety and efficiency. However, there is a lack of
effective controlling techniques for these adaptive street infrastructures. To
fill this gap in existing studies, we formula...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.06545v3' target='_blank'>Provably Efficient Causal Model-Based Reinforcement Learning for
  Systematic Generalization</a></h2>
<p><strong>Authors:</strong> Mirco Mutti, Riccardo De Santi, Emanuele Rossi, Juan Felipe Calderon, Michael Bronstein, Marcello Restelli</p>
<p><strong>Summary:</strong> In the sequential decision making setting, an agent aims to achieve
systematic generalization over a large, possibly infinite, set of environments.
Such environments are modeled as discrete Markov decision processes with both
states and actions represented through a feature vector. The underlying
structure of the environments allows the transition dynamics to be factored
into two components: one that is environment-specific and another that is
shared. Consider a set of environments that share th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.07071v1' target='_blank'>A Unified Perspective on Value Backup and Exploration in Monte-Carlo
  Tree Search</a></h2>
<p><strong>Authors:</strong> Tuan Dam, Carlo D'Eramo, Jan Peters, Joni Pajarinen</p>
<p><strong>Summary:</strong> Monte-Carlo Tree Search (MCTS) is a class of methods for solving complex
decision-making problems through the synergy of Monte-Carlo planning and
Reinforcement Learning (RL). The highly combinatorial nature of the problems
commonly addressed by MCTS requires the use of efficient exploration strategies
for navigating the planning tree and quickly convergent value backup methods.
These crucial problems are particularly evident in recent advances that combine
MCTS with deep neural networks for func...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.09773v1' target='_blank'>Learning to Help Emergency Vehicles Arrive Faster: A Cooperative
  Vehicle-Road Scheduling Approach</a></h2>
<p><strong>Authors:</strong> Lige Ding, Dong Zhao, Zhaofeng Wang, Guang Wang, Chang Tan, Lei Fan, Huadong Ma</p>
<p><strong>Summary:</strong> The ever-increasing heavy traffic congestion potentially impedes the
accessibility of emergency vehicles (EVs), resulting in detrimental impacts on
critical services and even safety of people's lives. Hence, it is significant
to propose an efficient scheduling approach to help EVs arrive faster. Existing
vehicle-centric scheduling approaches aim to recommend the optimal paths for
EVs based on the current traffic status while the road-centric scheduling
approaches aim to improve the traffic condi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2203.01298v3' target='_blank'>Pareto Frontier Approximation Network (PA-Net) to Solve Bi-objective TSP</a></h2>
<p><strong>Authors:</strong> Ishaan Mehta, Sharareh Taghipour, Sajad Saeedi</p>
<p><strong>Summary:</strong> The travelling salesperson problem (TSP) is a classic resource allocation
problem used to find an optimal order of doing a set of tasks while minimizing
(or maximizing) an associated objective function. It is widely used in robotics
for applications such as planning and scheduling. In this work, we solve TSP
for two objectives using reinforcement learning (RL). Often in multi-objective
optimization problems, the associated objective functions can be conflicting in
nature. In such cases, the opti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.03943v1' target='_blank'>Learning to Brachiate via Simplified Model Imitation</a></h2>
<p><strong>Authors:</strong> Daniele Reda, Hung Yu Ling, Michiel van de Panne</p>
<p><strong>Summary:</strong> Brachiation is the primary form of locomotion for gibbons and siamangs, in
which these primates swing from tree limb to tree limb using only their arms.
It is challenging to control because of the limited control authority, the
required advance planning, and the precision of the required grasps. We present
a novel approach to this problem using reinforcement learning, and as
demonstrated on a finger-less 14-link planar model that learns to brachiate
across challenging handhold sequences. Key to ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.13743v5' target='_blank'>Personalized Algorithmic Recourse with Preference Elicitation</a></h2>
<p><strong>Authors:</strong> Giovanni De Toni, Paolo Viappiani, Stefano Teso, Bruno Lepri, Andrea Passerini</p>
<p><strong>Summary:</strong> Algorithmic Recourse (AR) is the problem of computing a sequence of actions
that -- once performed by a user -- overturns an undesirable machine decision.
It is paramount that the sequence of actions does not require too much effort
for users to implement. Yet, most approaches to AR assume that actions cost the
same for all users, and thus may recommend unfairly expensive recourse plans to
certain users. Prompted by this observation, we introduce PEAR, the first
human-in-the-loop approach capabl...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.04114v1' target='_blank'>Deep Hierarchical Planning from Pixels</a></h2>
<p><strong>Authors:</strong> Danijar Hafner, Kuang-Huei Lee, Ian Fischer, Pieter Abbeel</p>
<p><strong>Summary:</strong> Intelligent agents need to select long sequences of actions to solve complex
tasks. While humans easily break down tasks into subgoals and reach them
through millions of muscle commands, current artificial intelligence is limited
to tasks with horizons of a few hundred decisions, despite large compute
budgets. Research on hierarchical reinforcement learning aims to overcome this
limitation but has proven to be challenging, current methods rely on manually
specified goal spaces or subtasks, and n...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.11403v2' target='_blank'>Curious Exploration via Structured World Models Yields Zero-Shot Object
  Manipulation</a></h2>
<p><strong>Authors:</strong> Cansu Sancaktar, Sebastian Blaes, Georg Martius</p>
<p><strong>Summary:</strong> It has been a long-standing dream to design artificial agents that explore
their environment efficiently via intrinsic motivation, similar to how children
perform curious free play. Despite recent advances in intrinsically motivated
reinforcement learning (RL), sample-efficient exploration in object
manipulation scenarios remains a significant challenge as most of the relevant
information lies in the sparse agent-object and object-object interactions. In
this paper, we propose to use structured ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.14057v3' target='_blank'>Safe Exploration Incurs Nearly No Additional Sample Complexity for
  Reward-free RL</a></h2>
<p><strong>Authors:</strong> Ruiquan Huang, Jing Yang, Yingbin Liang</p>
<p><strong>Summary:</strong> Reward-free reinforcement learning (RF-RL), a recently introduced RL
paradigm, relies on random action-taking to explore the unknown environment
without any reward feedback information. While the primary goal of the
exploration phase in RF-RL is to reduce the uncertainty in the estimated model
with minimum number of trajectories, in practice, the agent often needs to
abide by certain safety constraint at the same time. It remains unclear how
such safe exploration requirement would affect the cor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.00254v2' target='_blank'>A Survey on Active Simultaneous Localization and Mapping: State of the
  Art and New Frontiers</a></h2>
<p><strong>Authors:</strong> Julio A. Placed, Jared Strader, Henry Carrillo, Nikolay Atanasov, Vadim Indelman, Luca Carlone, Jos√© A. Castellanos</p>
<p><strong>Summary:</strong> Active Simultaneous Localization and Mapping (SLAM) is the problem of
planning and controlling the motion of a robot to build the most accurate and
complete model of the surrounding environment. Since the first foundational
work in active perception appeared, more than three decades ago, this field has
received increasing attention across different scientific communities. This has
brought about many different approaches and formulations, and makes a review of
the current trends necessary and ext...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.07088v1' target='_blank'>Inverse Resource Rational Based Stochastic Driver Behavior Model</a></h2>
<p><strong>Authors:</strong> Mehmet Ozkan, Yao Ma</p>
<p><strong>Summary:</strong> Human drivers have limited and time-varying cognitive resources when making
decisions in real-world traffic scenarios, which often leads to unique and
stochastic behaviors that can not be explained by perfect rationality
assumption, a widely accepted premise in modeling driving behaviors that
presume drivers rationally make decisions to maximize their own rewards under
all circumstances. To explicitly address this disadvantage, this study presents
a novel driver behavior model that aims to captu...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.11040v1' target='_blank'>Strategic Decision-Making in the Presence of Information Asymmetry:
  Provably Efficient RL with Algorithmic Instruments</a></h2>
<p><strong>Authors:</strong> Mengxin Yu, Zhuoran Yang, Jianqing Fan</p>
<p><strong>Summary:</strong> We study offline reinforcement learning under a novel model called strategic
MDP, which characterizes the strategic interactions between a principal and a
sequence of myopic agents with private types. Due to the bilevel structure and
private types, strategic MDP involves information asymmetry between the
principal and the agents. We focus on the offline RL problem, where the goal is
to learn the optimal policy of the principal concerning a target population of
agents based on a pre-collected dat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.09206v1' target='_blank'>A Learning-Based Trajectory Planning of Multiple UAVs for AoI
  Minimization in IoT Networks</a></h2>
<p><strong>Authors:</strong> Eslam Eldeeb, Dian Echevarr√≠a P√©rez, Jean Michel de Souza Sant'Ana, Mohammad Shehab, Nurul Huda Mahmood, Hirley Alves, Matti Latva-aho</p>
<p><strong>Summary:</strong> Many emerging Internet of Things (IoT) applications rely on information
collected by sensor nodes where the freshness of information is an important
criterion. \textit{Age of Information} (AoI) is a metric that quantifies
information timeliness, i.e., the freshness of the received information or
status update. This work considers a setup of deployed sensors in an IoT
network, where multiple unmanned aerial vehicles (UAVs) serve as mobile relay
nodes between the sensors and the base station. We f...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.14935v2' target='_blank'>Does Zero-Shot Reinforcement Learning Exist?</a></h2>
<p><strong>Authors:</strong> Ahmed Touati, J√©r√©my Rapin, Yann Ollivier</p>
<p><strong>Summary:</strong> A zero-shot RL agent is an agent that can solve any RL task in a given
environment, instantly with no additional planning or learning, after an
initial reward-free learning phase. This marks a shift from the reward-centric
RL paradigm towards "controllable" agents that can follow arbitrary
instructions in an environment. Current RL agents can solve families of related
tasks at best, or require planning anew for each task. Strategies for
approximate zero-shot RL ave been suggested using successor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.03629v3' target='_blank'>ReAct: Synergizing Reasoning and Acting in Language Models</a></h2>
<p><strong>Authors:</strong> Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao</p>
<p><strong>Summary:</strong> While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to generate both reasoning traces and
task-specific actions in an interleaved manner, allowing for greater synergy
between the two: reasoning traces help th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.05180v1' target='_blank'>Neurosymbolic Motion and Task Planning for Linear Temporal Logic Tasks</a></h2>
<p><strong>Authors:</strong> Xiaowu Sun, Yasser Shoukry</p>
<p><strong>Summary:</strong> This paper presents a neurosymbolic framework to solve motion planning
problems for mobile robots involving temporal goals. The temporal goals are
described using temporal logic formulas such as Linear Temporal Logic (LTL) to
capture complex tasks. The proposed framework trains Neural Network (NN)-based
planners that enjoy strong correctness guarantees when applying to unseen
tasks, i.e., the exact task (including workspace, LTL formula, and dynamic
constraints of a robot) is unknown during the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.06601v2' target='_blank'>Generalization with Lossy Affordances: Leveraging Broad Offline Data for
  Learning Visuomotor Tasks</a></h2>
<p><strong>Authors:</strong> Kuan Fang, Patrick Yin, Ashvin Nair, Homer Walke, Gengchen Yan, Sergey Levine</p>
<p><strong>Summary:</strong> The utilization of broad datasets has proven to be crucial for generalization
for a wide range of fields. However, how to effectively make use of diverse
multi-task data for novel downstream tasks still remains a grand challenge in
robotics. To tackle this challenge, we introduce a framework that acquires
goal-conditioned policies for unseen temporally extended tasks via offline
reinforcement learning on broad data, in combination with online fine-tuning
guided by subgoals in learned lossy repre...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.09598v1' target='_blank'>Planning for Sample Efficient Imitation Learning</a></h2>
<p><strong>Authors:</strong> Zhao-Heng Yin, Weirui Ye, Qifeng Chen, Yang Gao</p>
<p><strong>Summary:</strong> Imitation learning is a class of promising policy learning algorithms that is
free from many practical issues with reinforcement learning, such as the reward
design issue and the exploration hardness. However, the current imitation
algorithm struggles to achieve both high performance and high in-environment
sample efficiency simultaneously. Behavioral Cloning (BC) does not need
in-environment interactions, but it suffers from the covariate shift problem
which harms its performance. Adversarial I...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.14673v1' target='_blank'>Evaluation Beyond Task Performance: Analyzing Concepts in AlphaZero in
  Hex</a></h2>
<p><strong>Authors:</strong> Charles Lovering, Jessica Zosa Forde, George Konidaris, Ellie Pavlick, Michael L. Littman</p>
<p><strong>Summary:</strong> AlphaZero, an approach to reinforcement learning that couples neural networks
and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies
for traditional board games like chess, Go, shogi, and Hex. While researchers
and game commentators have suggested that AlphaZero uses concepts that humans
consider important, it is unclear how these concepts are captured in the
network. We investigate AlphaZero's internal representations in the game of Hex
using two evaluation techniques from...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.08235v1' target='_blank'>A Simple Decentralized Cross-Entropy Method</a></h2>
<p><strong>Authors:</strong> Zichen Zhang, Jun Jin, Martin Jagersand, Jun Luo, Dale Schuurmans</p>
<p><strong>Summary:</strong> Cross-Entropy Method (CEM) is commonly used for planning in model-based
reinforcement learning (MBRL) where a centralized approach is typically
utilized to update the sampling distribution based on only the top-$k$
operation's results on samples. In this paper, we show that such a centralized
approach makes CEM vulnerable to local optima, thus impairing its sample
efficiency. To tackle this issue, we propose Decentralized CEM (DecentCEM), a
simple but effective improvement over classical CEM, by...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.10528v2' target='_blank'>An Incremental Inverse Reinforcement Learning Approach for Motion
  Planning with Separated Path and Velocity Preferences</a></h2>
<p><strong>Authors:</strong> Armin Avaei, Linda van der Spaa, Luka Peternel, Jens Kober</p>
<p><strong>Summary:</strong> Humans often demonstrate diverse behaviors due to their personal preferences,
for instance, related to their individual execution style or personal margin
for safety. In this paper, we consider the problem of integrating both path and
velocity preferences into trajectory planning for robotic manipulators. We
first learn reward functions that represent the user path and velocity
preferences from kinesthetic demonstration. We then optimize the trajectory in
two steps: first the path and then the v...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.01877v2' target='_blank'>AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners</a></h2>
<p><strong>Authors:</strong> Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, Ping Luo</p>
<p><strong>Summary:</strong> Diffusion models have demonstrated their powerful generative capability in
many tasks, with great potential to serve as a paradigm for offline
reinforcement learning. However, the quality of the diffusion model is limited
by the insufficient diversity of training data, which hinders the performance
of planning and the generalizability to new tasks. This paper introduces
AdaptDiffuser, an evolutionary planning method with diffusion that can
self-evolve to improve the diffusion model hence a bette...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.04321v2' target='_blank'>Shared Information-Based Safe And Efficient Behavior Planning For
  Connected Autonomous Vehicles</a></h2>
<p><strong>Authors:</strong> Songyang Han, Shanglin Zhou, Lynn Pepin, Jiangwei Wang, Caiwen Ding, Fei Miao</p>
<p><strong>Summary:</strong> The recent advancements in wireless technology enable connected autonomous
vehicles (CAVs) to gather data via vehicle-to-vehicle (V2V) communication, such
as processed LIDAR and camera data from other vehicles. In this work, we design
an integrated information sharing and safe multi-agent reinforcement learning
(MARL) framework for CAVs, to take advantage of the extra information when
making decisions to improve traffic efficiency and safety. We first use weight
pruned convolutional neural netwo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.07654v1' target='_blank'>Reinforcement Learning Based Power Grid Day-Ahead Planning and
  AI-Assisted Control</a></h2>
<p><strong>Authors:</strong> Anton R. Fuxj√§ger, Kristian Kozak, Matthias Dorfer, Patrick M. Blies, Marcel Wasserer</p>
<p><strong>Summary:</strong> The ongoing transition to renewable energy is increasing the share of
fluctuating power sources like wind and solar, raising power grid volatility
and making grid operation increasingly complex and costly. In our prior work,
we have introduced a congestion management approach consisting of a
redispatching optimizer combined with a machine learning-based topology
optimization agent. Compared to a typical redispatching-only agent, it was able
to keep a simulated grid in operation longer while at t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.12274v3' target='_blank'>A Hierarchical Hybrid Learning Framework for Multi-agent Trajectory
  Prediction</a></h2>
<p><strong>Authors:</strong> Yujun Jiao, Mingze Miao, Zhishuai Yin, Chunyuan Lei, Xu Zhu, Linzhen Nie, Bo Tao</p>
<p><strong>Summary:</strong> Accurate and robust trajectory prediction of neighboring agents is critical
for autonomous vehicles traversing in complex scenes. Most methods proposed in
recent years are deep learning-based due to their strength in encoding complex
interactions. However, unplausible predictions are often generated since they
rely heavily on past observations and cannot effectively capture the transient
and contingency interactions from sparse samples. In this paper, we propose a
hierarchical hybrid framework o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.03223v1' target='_blank'>DexDeform: Dexterous Deformable Object Manipulation with Human
  Demonstrations and Differentiable Physics</a></h2>
<p><strong>Authors:</strong> Sizhe Li, Zhiao Huang, Tao Chen, Tao Du, Hao Su, Joshua B. Tenenbaum, Chuang Gan</p>
<p><strong>Summary:</strong> In this work, we aim to learn dexterous manipulation of deformable objects
using multi-fingered hands. Reinforcement learning approaches for dexterous
rigid object manipulation would struggle in this setting due to the complexity
of physics interaction with deformable objects. At the same time, previous
trajectory optimization approaches with differentiable physics for deformable
manipulation would suffer from local optima caused by the explosion of contact
modes from hand-object interactions. T...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.14644v1' target='_blank'>KARNet: Kalman Filter Augmented Recurrent Neural Network for Learning
  World Models in Autonomous Driving Tasks</a></h2>
<p><strong>Authors:</strong> Hemanth Manjunatha, Andrey Pak, Dimitar Filev, Panagiotis Tsiotras</p>
<p><strong>Summary:</strong> Autonomous driving has received a great deal of attention in the automotive
industry and is often seen as the future of transportation. The development of
autonomous driving technology has been greatly accelerated by the growth of
end-to-end machine learning techniques that have been successfully used for
perception, planning, and control tasks. An important aspect of autonomous
driving planning is knowing how the environment evolves in the immediate future
and taking appropriate actions. An aut...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.00323v3' target='_blank'>Thought Cloning: Learning to Think while Acting by Imitating Human
  Thinking</a></h2>
<p><strong>Authors:</strong> Shengran Hu, Jeff Clune</p>
<p><strong>Summary:</strong> Language is often considered a key aspect of human thinking, providing us
with exceptional abilities to generalize, explore, plan, replan, and adapt to
new situations. However, Reinforcement Learning (RL) agents are far from
human-level performance in any of these abilities. We hypothesize one reason
for such cognitive deficiencies is that they lack the benefits of thinking in
language and that we can improve AI agents by training them to think like
humans do. We introduce a novel Imitation Lear...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.08815v1' target='_blank'>Decentralized Social Navigation with Non-Cooperative Robots via Bi-Level
  Optimization</a></h2>
<p><strong>Authors:</strong> Rohan Chandra, Rahul Menon, Zayne Sprague, Arya Anantula, Joydeep Biswas</p>
<p><strong>Summary:</strong> This paper presents a fully decentralized approach for realtime
non-cooperative multi-robot navigation in social mini-games, such as navigating
through a narrow doorway or negotiating right of way at a corridor
intersection. Our contribution is a new realtime bi-level optimization
algorithm, in which the top-level optimization consists of computing a fair and
collision-free ordering followed by the bottom-level optimization which plans
optimal trajectories conditioned on the ordering. We show th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.08843v1' target='_blank'>Real-Time Network-Level Traffic Signal Control: An Explicit Multiagent
  Coordination Method</a></h2>
<p><strong>Authors:</strong> Wanyuan Wang, Tianchi Qiao, Jinming Ma, Jiahui Jin, Zhibin Li, Weiwei Wu, Yichuan Jian</p>
<p><strong>Summary:</strong> Efficient traffic signal control (TSC) has been one of the most useful ways
for reducing urban road congestion. Key to the challenge of TSC includes 1) the
essential of real-time signal decision, 2) the complexity in traffic dynamics,
and 3) the network-level coordination. Recent efforts that applied
reinforcement learning (RL) methods can query policies by mapping the traffic
state to the signal decision in real-time, however, is inadequate for
unexpected traffic flows. By observing real traffi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.12729v2' target='_blank'>MP3: Movement Primitive-Based (Re-)Planning Policy</a></h2>
<p><strong>Authors:</strong> Fabian Otto, Hongyi Zhou, Onur Celik, Ge Li, Rudolf Lioutikov, Gerhard Neumann</p>
<p><strong>Summary:</strong> We introduce a novel deep reinforcement learning (RL) approach called
Movement Primitive-based Planning Policy (MP3). By integrating movement
primitives (MPs) into the deep RL framework, MP3 enables the generation of
smooth trajectories throughout the whole learning process while effectively
learning from sparse and non-Markovian rewards. Additionally, MP3 maintains the
capability to adapt to changes in the environment during execution. Although
many early successes in robot RL have been achieve...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.15950v3' target='_blank'>Human-Like Implicit Intention Expression for Autonomous Driving Motion
  Planning: A Method Based on Learning Human Intention Priors</a></h2>
<p><strong>Authors:</strong> Jiaqi Liu, Xiao Qi, Ying Ni, Jian Sun, Peng Hang</p>
<p><strong>Summary:</strong> One of the key factors determining whether autonomous vehicles (AVs) can be
seamlessly integrated into existing traffic systems is their ability to
interact smoothly and efficiently with human drivers and communicate their
intentions. While many studies have focused on enhancing AVs' human-like
interaction and communication capabilities at the behavioral decision-making
level, a significant gap remains between the actual motion trajectories of AVs
and the psychological expectations of human driv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.12843v2' target='_blank'>Actuator Trajectory Planning for UAVs with Overhead Manipulator using
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Hazim Alzorgan, Abolfazl Razi, Ata Jahangir Moshayedi</p>
<p><strong>Summary:</strong> In this paper, we investigate the operation of an aerial manipulator system,
namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with
two degrees of freedom to carry out actuation tasks on the fly. Our solution is
based on employing a Q-learning method to control the trajectory of the tip of
the arm, also called end-effector. More specifically, we develop a motion
planning model based on Time To Collision (TTC), which enables a quadrotor UAV
to navigate around obstacles whil...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.04976v1' target='_blank'>AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs</a></h2>
<p><strong>Authors:</strong> Jiaying Guo, Michael R. Jones, Soufiene Djahel, Shen Wang</p>
<p><strong>Summary:</strong> Reducing unexpected urban traffic congestion caused by en-route events (e.g.,
road closures, car crashes, etc.) often requires fast and accurate reactions to
choose the best-fit traffic signals. Traditional traffic light control systems,
such as SCATS and SCOOT, are not efficient as their traffic data provided by
induction loops has a low update frequency (i.e., longer than 1 minute).
Moreover, the traffic light signal plans used by these systems are selected
from a limited set of candidate plan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.15462v2' target='_blank'>DTC: Deep Tracking Control</a></h2>
<p><strong>Authors:</strong> Fabian Jenelten, Junzhe He, Farbod Farshidian, Marco Hutter</p>
<p><strong>Summary:</strong> Legged locomotion is a complex control problem that requires both accuracy
and robustness to cope with real-world challenges. Legged systems have
traditionally been controlled using trajectory optimization with inverse
dynamics. Such hierarchical model-based methods are appealing due to intuitive
cost function tuning, accurate planning, generalization, and most importantly,
the insightful understanding gained from more than one decade of extensive
research. However, model mismatch and violation ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.00760v1' target='_blank'>Uncertainty-aware hybrid paradigm of nonlinear MPC and model-based RL
  for offroad navigation: Exploration of transformers in the predictive model</a></h2>
<p><strong>Authors:</strong> Faraz Lotfi, Khalil Virji, Farnoosh Faraji, Lucas Berry, Andrew Holliday, David Meger, Gregory Dudek</p>
<p><strong>Summary:</strong> In this paper, we investigate a hybrid scheme that combines nonlinear model
predictive control (MPC) and model-based reinforcement learning (RL) for
navigation planning of an autonomous model car across offroad, unstructured
terrains without relying on predefined maps. Our innovative approach takes
inspiration from BADGR, an LSTM-based network that primarily concentrates on
environment modeling, but distinguishes itself by substituting LSTM modules
with transformers to greatly elevate the perfor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.04590v1' target='_blank'>Deep Model Predictive Optimization</a></h2>
<p><strong>Authors:</strong> Jacob Sacks, Rwik Rana, Kevin Huang, Alex Spitzer, Guanya Shi, Byron Boots</p>
<p><strong>Summary:</strong> A major challenge in robotics is to design robust policies which enable
complex and agile behaviors in the real world. On one end of the spectrum, we
have model-free reinforcement learning (MFRL), which is incredibly flexible and
general but often results in brittle policies. In contrast, model predictive
control (MPC) continually re-plans at each time step to remain robust to
perturbations and model inaccuracies. However, despite its real-world
successes, MPC often under-performs the optimal st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.08710v1' target='_blank'>Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous
  Driving Research</a></h2>
<p><strong>Authors:</strong> Cole Gulino, Justin Fu, Wenjie Luo, George Tucker, Eli Bronstein, Yiren Lu, Jean Harb, Xinlei Pan, Yan Wang, Xiangyu Chen, John D. Co-Reyes, Rishabh Agarwal, Rebecca Roelofs, Yao Lu, Nico Montali, Paul Mougin, Zoey Yang, Brandyn White, Aleksandra Faust, Rowan McAllister, Dragomir Anguelov, Benjamin Sapp</p>
<p><strong>Summary:</strong> Simulation is an essential tool to develop and benchmark autonomous vehicle
planning software in a safe and cost-effective manner. However, realistic
simulation requires accurate modeling of nuanced and complex multi-agent
interactive behaviors. To address these challenges, we introduce Waymax, a new
data-driven simulator for autonomous driving in multi-agent scenes, designed
for large-scale simulation and testing. Waymax uses publicly-released,
real-world driving data (e.g., the Waymo Open Moti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.08922v1' target='_blank'>LLaMA Rider: Spurring Large Language Models to Explore the Open World</a></h2>
<p><strong>Authors:</strong> Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, Zongqing Lu</p>
<p><strong>Summary:</strong> Recently, various studies have leveraged Large Language Models (LLMs) to help
decision-making and planning in environments, and try to align the LLMs'
knowledge with the world conditions. Nonetheless, the capacity of LLMs to
continuously acquire environmental knowledge and adapt in an open world remains
uncertain. In this paper, we propose an approach to spur LLMs to explore the
open world, gather experiences, and learn to improve their task-solving
capabilities. In this approach, a multi-round ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.01090v2' target='_blank'>Self Generated Wargame AI: Double Layer Agent Task Planning Based on
  Large Language Model</a></h2>
<p><strong>Authors:</strong> Y. Sun, J. Zhao, C. Yu, W. Wang, X. Zhou</p>
<p><strong>Summary:</strong> The large language models represented by ChatGPT have a disruptive impact on
the field of artificial intelligence. But it mainly focuses on natural language
processing, speech recognition, machine learning and natural language
understanding. This paper innovatively applies the large language model to the
field of intelligent decision-making, places the large language model in the
decision-making center, and constructs an agent architecture with the large
language model as the core. Based on this...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.04327v1' target='_blank'>Learning to sample in Cartesian MRI</a></h2>
<p><strong>Authors:</strong> Thomas Sanchez</p>
<p><strong>Summary:</strong> Despite its exceptional soft tissue contrast, Magnetic Resonance Imaging
(MRI) faces the challenge of long scanning times compared to other modalities
like X-ray radiography. Shortening scanning times is crucial in clinical
settings, as it increases patient comfort, decreases examination costs and
improves throughput. Recent advances in compressed sensing (CS) and deep
learning allow accelerated MRI acquisition by reconstructing high-quality
images from undersampled data. While reconstruction al...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.11896v1' target='_blank'>Stable Relay Learning Optimization Approach for Fast Power System
  Production Cost Minimization Simulation</a></h2>
<p><strong>Authors:</strong> Zishan Guo, Qinran Hu, Tao Qian, Xin Fang, Renjie Hu, Zaijun Wu</p>
<p><strong>Summary:</strong> Production cost minimization (PCM) simulation is commonly employed for
assessing the operational efficiency, economic viability, and reliability,
providing valuable insights for power system planning and operations. However,
solving a PCM problem is time-consuming, consisting of numerous binary
variables for simulation horizon extending over months and years. This hinders
rapid assessment of modern energy systems with diverse planning requirements.
Existing methods for accelerating PCM tend to s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.04851v1' target='_blank'>Graph Learning-based Fleet Scheduling for Urban Air Mobility under
  Operational Constraints, Varying Demand & Uncertainties</a></h2>
<p><strong>Authors:</strong> Steve Paul, Jhoel Witter, Souma Chowdhury</p>
<p><strong>Summary:</strong> This paper develops a graph reinforcement learning approach to online
planning of the schedule and destinations of electric aircraft that comprise an
urban air mobility (UAM) fleet operating across multiple vertiports. This fleet
scheduling problem is formulated to consider time-varying demand, constraints
related to vertiport capacity, aircraft capacity and airspace safety
guidelines, uncertainties related to take-off delay, weather-induced route
closures, and unanticipated aircraft downtime. C...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.08634v1' target='_blank'>Resilient Path Planning for UAVs in Data Collection under Adversarial
  Attacks</a></h2>
<p><strong>Authors:</strong> Xueyuan Wang, M. Cenk Gursoy</p>
<p><strong>Summary:</strong> In this paper, we investigate jamming-resilient UAV path planning strategies
for data collection in Internet of Things (IoT) networks, in which the typical
UAV can learn the optimal trajectory to elude such jamming attacks.
Specifically, the typical UAV is required to collect data from multiple
distributed IoT nodes under collision avoidance, mission completion deadline,
and kinematic constraints in the presence of jamming attacks. We first design a
fixed ground jammer with continuous jamming at...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.01786v2' target='_blank'>COA-GPT: Generative Pre-trained Transformers for Accelerated Course of
  Action Development in Military Operations</a></h2>
<p><strong>Authors:</strong> Vinicius G. Goecks, Nicholas Waytowich</p>
<p><strong>Summary:</strong> The development of Courses of Action (COAs) in military operations is
traditionally a time-consuming and intricate process. Addressing this
challenge, this study introduces COA-GPT, a novel algorithm employing Large
Language Models (LLMs) for rapid and efficient generation of valid COAs.
COA-GPT incorporates military doctrine and domain expertise to LLMs through
in-context learning, allowing commanders to input mission information - in both
text and image formats - and receive strategically alig...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.18558v2' target='_blank'>Unifying F1TENTH Autonomous Racing: Survey, Methods and Benchmarks</a></h2>
<p><strong>Authors:</strong> Benjamin David Evans, Raphael Trumpp, Marco Caccamo, Felix Jahncke, Johannes Betz, Hendrik Willem Jordaan, Herman Arnold Engelbrecht</p>
<p><strong>Summary:</strong> The F1TENTH autonomous driving platform, consisting of 1:10-scale
remote-controlled cars, has evolved into a well-established education and
research platform. The many publications and real-world competitions span many
domains, from classical path planning to novel learning-based algorithms.
Consequently, the field is wide and disjointed, hindering direct comparison of
developed methods and making it difficult to assess the state-of-the-art.
Therefore, we aim to unify the field by surveying curr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.17684v1' target='_blank'>Generalize by Touching: Tactile Ensemble Skill Transfer for Robotic
  Furniture Assembly</a></h2>
<p><strong>Authors:</strong> Haohong Lin, Radu Corcodel, Ding Zhao</p>
<p><strong>Summary:</strong> Furniture assembly remains an unsolved problem in robotic manipulation due to
its long task horizon and nongeneralizable operations plan. This paper presents
the Tactile Ensemble Skill Transfer (TEST) framework, a pioneering offline
reinforcement learning (RL) approach that incorporates tactile feedback in the
control loop. TEST's core design is to learn a skill transition model for
high-level planning, along with a set of adaptive intra-skill goal-reaching
policies. Such design aims to solve th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01314v2' target='_blank'>Non-iterative Optimization of Trajectory and Radio Resource for Aerial
  Network</a></h2>
<p><strong>Authors:</strong> Hyeonsu Lyu, Jonggyu Jang, Harim Lee, Hyun Jong Yang</p>
<p><strong>Summary:</strong> We address a joint trajectory planning, user association, resource
allocation, and power control problem to maximize proportional fairness in the
aerial IoT network, considering practical end-to-end quality-of-service (QoS)
and communication schedules. Though the problem is rather ancient, apart from
the fact that the previous approaches have never considered user- and
time-specific QoS, we point out a prevalent mistake in coordinate optimization
approaches adopted by the majority of the literat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01792v1' target='_blank'>Learning Robust Autonomous Navigation and Locomotion for Wheeled-Legged
  Robots</a></h2>
<p><strong>Authors:</strong> Joonho Lee, Marko Bjelonic, Alexander Reske, Lorenz Wellhausen, Takahiro Miki, Marco Hutter</p>
<p><strong>Summary:</strong> Autonomous wheeled-legged robots have the potential to transform logistics
systems, improving operational efficiency and adaptability in urban
environments. Navigating urban environments, however, poses unique challenges
for robots, necessitating innovative solutions for locomotion and navigation.
These challenges include the need for adaptive locomotion across varied
terrains and the ability to navigate efficiently around complex dynamic
obstacles. This work introduces a fully integrated system...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.04082v3' target='_blank'>Logic-Skill Programming: An Optimization-based Approach to Sequential
  Skill Planning</a></h2>
<p><strong>Authors:</strong> Teng Xue, Amirreza Razmjoo, Suhan Shetty, Sylvain Calinon</p>
<p><strong>Summary:</strong> Recent advances in robot skill learning have unlocked the potential to
construct task-agnostic skill libraries, facilitating the seamless sequencing
of multiple simple manipulation primitives (aka. skills) to tackle
significantly more complex tasks. Nevertheless, determining the optimal
sequence for independently learned skills remains an open problem, particularly
when the objective is given solely in terms of the final geometric
configuration rather than a symbolic goal. To address this challe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.10087v1' target='_blank'>Continuous Transfer Learning for UAV Communication-aware Trajectory
  Design</a></h2>
<p><strong>Authors:</strong> Chenrui Sun, Gianluca Fontanesi, Swarna Bindu Chetty, Xuanyu Liang, Berk Canberk, Hamed Ahmadi</p>
<p><strong>Summary:</strong> Deep Reinforcement Learning (DRL) emerges as a prime solution for Unmanned
Aerial Vehicle (UAV) trajectory planning, offering proficiency in navigating
high-dimensional spaces, adaptability to dynamic environments, and making
sequential decisions based on real-time feedback. Despite these advantages, the
use of DRL for UAV trajectory planning requires significant retraining when the
UAV is confronted with a new environment, resulting in wasted resources and
time. Therefore, it is essential to de...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.04935v1' target='_blank'>SLOPE: Search with Learned Optimal Pruning-based Expansion</a></h2>
<p><strong>Authors:</strong> Davor Bokan, Zlatan Ajanovic, Bakir Lacevic</p>
<p><strong>Summary:</strong> Heuristic search is often used for motion planning and pathfinding problems,
for finding the shortest path in a graph while also promising completeness and
optimal efficiency. The drawback is it's space complexity, specifically storing
all expanded child nodes in memory and sorting large lists of active nodes,
which can be a problem in real-time scenarios with limited on-board
computation. To combat this, we present the Search with Learned Optimal
Pruning-based Expansion (SLOPE), which, learns t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.00898v4' target='_blank'>Residual-MPPI: Online Policy Customization for Continuous Control</a></h2>
<p><strong>Authors:</strong> Pengcheng Wang, Chenran Li, Catherine Weaver, Kenta Kawamoto, Masayoshi Tomizuka, Chen Tang, Wei Zhan</p>
<p><strong>Summary:</strong> Policies learned through Reinforcement Learning (RL) and Imitation Learning
(IL) have demonstrated significant potential in achieving advanced performance
in continuous control tasks. However, in real-world environments, it is often
necessary to further customize a trained policy when there are additional
requirements that were unforeseen during the original training phase. It is
possible to fine-tune the policy to meet the new requirements, but this often
requires collecting new data with the a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.17980v1' target='_blank'>Personalized and Context-aware Route Planning for Edge-assisted Vehicles</a></h2>
<p><strong>Authors:</strong> Dinesh Cyril Selvaraj, Falko Dressler, Carla Fabiana Chiasserini</p>
<p><strong>Summary:</strong> Conventional route planning services typically offer the same routes to all
drivers, focusing primarily on a few standardized factors such as travel
distance or time, overlooking individual driver preferences. With the inception
of autonomous vehicles expected in the coming years, where vehicles will rely
on routes decided by such planners, there arises a need to incorporate the
specific preferences of each driver, ensuring personalized navigation
experiences. In this work, we propose a novel ap...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.21359v1' target='_blank'>ProSpec RL: Plan Ahead, then Execute</a></h2>
<p><strong>Authors:</strong> Liangliang Liu, Yi Guan, BoRan Wang, Rujia Shen, Yi Lin, Chaoran Kong, Lian Yan, Jingchi Jiang</p>
<p><strong>Summary:</strong> Imagining potential outcomes of actions before execution helps agents make
more informed decisions, a prospective thinking ability fundamental to human
cognition. However, mainstream model-free Reinforcement Learning (RL) methods
lack the ability to proactively envision future scenarios, plan, and guide
strategies. These methods typically rely on trial and error to adjust policy
functions, aiming to maximize cumulative rewards or long-term value, even if
such high-reward decisions place the envi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.07806v2' target='_blank'>From Decision to Action in Surgical Autonomy: Multi-Modal Large Language
  Models for Robot-Assisted Blood Suction</a></h2>
<p><strong>Authors:</strong> Sadra Zargarzadeh, Maryam Mirzaei, Yafei Ou, Mahdi Tavakoli</p>
<p><strong>Summary:</strong> The rise of Large Language Models (LLMs) has impacted research in robotics
and automation. While progress has been made in integrating LLMs into general
robotics tasks, a noticeable void persists in their adoption in more specific
domains such as surgery, where critical factors such as reasoning,
explainability, and safety are paramount. Achieving autonomy in robotic
surgery, which entails the ability to reason and adapt to changes in the
environment, remains a significant challenge. In this wor...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.01060v1' target='_blank'>Multiagent Reinforcement Learning Enhanced Decision-making of Crew
  Agents During Floor Construction Process</a></h2>
<p><strong>Authors:</strong> Bin Yang, Boda Liu, Yilong Han, Xin Meng, Yifan Wang, Hansi Yang, Jianzhuang Xia</p>
<p><strong>Summary:</strong> Fine-grained simulation of floor construction processes is essential for
supporting lean management and the integration of information technology.
However, existing research does not adequately address the on-site
decision-making of constructors in selecting tasks and determining their
sequence within the entire construction process. Moreover, decision-making
frameworks from computer science and robotics are not directly applicable to
construction scenarios. To facilitate intelligent simulation ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.09686v2' target='_blank'>Generalization of Compositional Tasks with Logical Specification via
  Implicit Planning</a></h2>
<p><strong>Authors:</strong> Duo Xu, Faramarz Fekri</p>
<p><strong>Summary:</strong> In this study, we address the challenge of learning generalizable policies
for compositional tasks defined by logical specifications. These tasks consist
of multiple temporally extended sub-tasks. Due to the sub-task
inter-dependencies and sparse reward issue in long-horizon tasks, existing
reinforcement learning (RL) approaches, such as task-conditioned and
goal-conditioned policies, continue to struggle with slow convergence and
sub-optimal performance in generalizing to compositional tasks. T...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.15184v1' target='_blank'>Action abstractions for amortized sampling</a></h2>
<p><strong>Authors:</strong> Oussama Boussif, L√©na N√©hale Ezzine, Joseph D Viviano, Micha≈Ç Koziarski, Moksh Jain, Nikolay Malkin, Emmanuel Bengio, Rim Assouel, Yoshua Bengio</p>
<p><strong>Summary:</strong> As trajectories sampled by policies used by reinforcement learning (RL) and
generative flow networks (GFlowNets) grow longer, credit assignment and
exploration become more challenging, and the long planning horizon hinders mode
discovery and generalization. The challenge is particularly pronounced in
entropy-seeking RL methods, such as generative flow networks, where the agent
must learn to sample from a structured distribution and discover multiple
high-reward states, each of which take many st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.11717v1' target='_blank'>Learning UAV-based path planning for efficient localization of objects
  using prior knowledge</a></h2>
<p><strong>Authors:</strong> Rick van Essen, Eldert van Henten, Gert Kootstra</p>
<p><strong>Summary:</strong> UAV's are becoming popular for various object search applications in
agriculture, however they usually use time-consuming row-by-row flight paths.
This paper presents a deep-reinforcement-learning method for path planning to
efficiently localize objects of interest using UAVs with a minimal flight-path
length. The method uses some global prior knowledge with uncertain object
locations and limited resolution in combination with a local object map created
using the output of an object detection ne...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.11974v2' target='_blank'>Emma-X: An Embodied Multimodal Action Model with Grounded Chain of
  Thought and Look-ahead Spatial Reasoning</a></h2>
<p><strong>Authors:</strong> Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, U-Xuan Tan, Deepanway Ghosal, Soujanya Poria</p>
<p><strong>Summary:</strong> Traditional reinforcement learning-based robotic control methods are often
task-specific and fail to generalize across diverse environments or unseen
objects and instructions. Visual Language Models (VLMs) demonstrate strong
scene understanding and planning capabilities but lack the ability to generate
actionable policies tailored to specific robotic embodiments. To address this,
Visual-Language-Action (VLA) models have emerged, yet they face challenges in
long-horizon spatial reasoning and grou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.20484v1' target='_blank'>Exploiting NOMA Transmissions in Multi-UAV-assisted Wireless Networks:
  From Aerial-RIS to Mode-switching UAVs</a></h2>
<p><strong>Authors:</strong> Songhan Zhao, Shimin Gong, Bo Gu, Lanhua Li, Bin Lyu, Dinh Thai Hoang, Changyan Yi</p>
<p><strong>Summary:</strong> In this paper, we consider an aerial reconfigurable intelligent surface
(ARIS)-assisted wireless network, where multiple unmanned aerial vehicles
(UAVs) collect data from ground users (GUs) by using the non-orthogonal
multiple access (NOMA) method. The ARIS provides enhanced channel
controllability to improve the NOMA transmissions and reduce the co-channel
interference among UAVs. We also propose a novel dual-mode switching scheme,
where each UAV equipped with both an ARIS and a radio frequency...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.02116v1' target='_blank'>Humanoid Locomotion and Manipulation: Current Progress and Challenges in
  Control, Planning, and Learning</a></h2>
<p><strong>Authors:</strong> Zhaoyuan Gu, Junheng Li, Wenlan Shen, Wenhao Yu, Zhaoming Xie, Stephen McCrory, Xianyi Cheng, Abdulaziz Shamsah, Robert Griffin, C. Karen Liu, Abderrahmane Kheddar, Xue Bin Peng, Yuke Zhu, Guanya Shi, Quan Nguyen, Gordon Cheng, Huijun Gao, Ye Zhao</p>
<p><strong>Summary:</strong> Humanoid robots have great potential to perform various human-level skills.
These skills involve locomotion, manipulation, and cognitive capabilities.
Driven by advances in machine learning and the strength of existing model-based
approaches, these capabilities have progressed rapidly, but often separately.
Therefore, a timely overview of current progress and future trends in this
fast-evolving field is essential. This survey first summarizes the model-based
planning and control that have been t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.02709v1' target='_blank'>Horizon Generalization in Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Vivek Myers, Catherine Ji, Benjamin Eysenbach</p>
<p><strong>Summary:</strong> We study goal-conditioned RL through the lens of generalization, but not in
the traditional sense of random augmentations and domain randomization. Rather,
we aim to learn goal-directed policies that generalize with respect to the
horizon: after training to reach nearby goals (which are easy to learn), these
policies should succeed in reaching distant goals (which are quite challenging
to learn). In the same way that invariance is closely linked with
generalization is other areas of machine lear...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.06605v3' target='_blank'>RoboHorizon: An LLM-Assisted Multi-View World Model for Long-Horizon
  Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Zixuan Chen, Jing Huo, Yangtao Chen, Yang Gao</p>
<p><strong>Summary:</strong> Efficient control in long-horizon robotic manipulation is challenging due to
complex representation and policy learning requirements. Model-based visual
reinforcement learning (RL) has shown great potential in addressing these
challenges but still faces notable limitations, particularly in handling sparse
rewards and complex visual features in long-horizon environments. To address
these limitations, we propose the Recognize-Sense-Plan-Act (RSPA) pipeline for
long-horizon tasks and further introd...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.11260v2' target='_blank'>A Survey of World Models for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Tuo Feng, Wenguan Wang, Yi Yang</p>
<p><strong>Summary:</strong> Recent breakthroughs in autonomous driving have been propelled by advances in
robust world modeling, fundamentally transforming how vehicles interpret
dynamic scenes and execute safe decision-making. In particular, world models
have emerged as a linchpin technology, offering high-fidelity representations
of the driving environment that integrate multi-sensor data, semantic cues, and
temporal dynamics. This paper systematically reviews recent advances in world
models for autonomous driving, propo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.05187v1' target='_blank'>An Adaptable Budget Planner for Enhancing Budget-Constrained
  Auto-Bidding in Online Advertising</a></h2>
<p><strong>Authors:</strong> Zhijian Duan, Yusen Huo, Tianyu Wang, Zhilin Zhang, Yeshu Li, Chuan Yu, Jian Xu, Bo Zheng, Xiaotie Deng</p>
<p><strong>Summary:</strong> In online advertising, advertisers commonly utilize auto-bidding services to
bid for impression opportunities. A typical objective of the auto-bidder is to
optimize the advertiser's cumulative value of winning impressions within
specified budget constraints. However, such a problem is challenging due to the
complex bidding environment faced by diverse advertisers. To address this
challenge, we introduce ABPlanner, a few-shot adaptable budget planner designed
to improve budget-constrained auto-bi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1808.05032v1' target='_blank'>Deep RTS: A Game Environment for Deep Reinforcement Learning in
  Real-Time Strategy Games</a></h2>
<p><strong>Authors:</strong> Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is an area of research that has blossomed
tremendously in recent years and has shown remarkable potential for artificial
intelligence based opponents in computer games. This success is primarily due
to the vast capabilities of convolutional neural networks, that can extract
useful features from noisy and complex data. Games are excellent tools to test
and push the boundaries of novel RL algorithms because they give valuable
insight into how well an algorithm can perfo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.09957v3' target='_blank'>Reinforcement Learning-based Application Autoscaling in the Cloud: A
  Survey</a></h2>
<p><strong>Authors:</strong> Yisel Gar√≠, David A. Monge, Elina Pacini, Cristian Mateos, Carlos Garc√≠a Garino</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) has demonstrated a great potential for
automatically solving decision-making problems in complex uncertain
environments. RL proposes a computational approach that allows learning through
interaction in an environment with stochastic behavior, where agents take
actions to maximize some cumulative short-term and long-term rewards. Some of
the most impressive results have been shown in Game Theory where agents
exhibited superhuman performance in games like Go or Starcraf...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.04523v3' target='_blank'>Objective Mismatch in Model-based Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Nathan Lambert, Brandon Amos, Omry Yadan, Roberto Calandra</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (MBRL) has been shown to be a powerful
framework for data-efficiently learning control of continuous tasks. Recent
work in MBRL has mostly focused on using more advanced function approximators
and planning schemes, with little development of the general framework. In this
paper, we identify a fundamental issue of the standard MBRL framework -- what
we call the objective mismatch issue. Objective mismatch arises when one
objective is optimized in the hope that a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.12900v8' target='_blank'>Breaking the Sample Size Barrier in Model-Based Reinforcement Learning
  with a Generative Model</a></h2>
<p><strong>Authors:</strong> Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen</p>
<p><strong>Summary:</strong> This paper is concerned with the sample efficiency of reinforcement learning,
assuming access to a generative model (or simulator). We first consider
$\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with
state space $\mathcal{S}$ and action space $\mathcal{A}$. Despite a number of
prior works tackling this problem, a complete picture of the trade-offs between
sample complexity and statistical accuracy is yet to be determined. In
particular, all prior results suffer from a se...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.01610v1' target='_blank'>Combining Reinforcement Learning and Constraint Programming for
  Combinatorial Optimization</a></h2>
<p><strong>Authors:</strong> Quentin Cappart, Thierry Moisan, Louis-Martin Rousseau, Isabeau Pr√©mont-Schwarz, Andre Cire</p>
<p><strong>Summary:</strong> Combinatorial optimization has found applications in numerous fields, from
aerospace to transportation planning and economics. The goal is to find an
optimal solution among a finite set of possibilities. The well-known challenge
one faces with combinatorial optimization is the state-space explosion problem:
the number of possibilities grows exponentially with the problem size, which
makes solving intractable for large problems. In the last years, deep
reinforcement learning (DRL) has shown its p...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.09812v2' target='_blank'>ViNG: Learning Open-World Navigation with Visual Goals</a></h2>
<p><strong>Authors:</strong> Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, Sergey Levine</p>
<p><strong>Summary:</strong> We propose a learning-based navigation system for reaching visually indicated
goals and demonstrate this system on a real mobile robot platform. Learning
provides an appealing alternative to conventional methods for robotic
navigation: instead of reasoning about environments in terms of geometry and
maps, learning can enable a robot to learn about navigational affordances,
understand what types of obstacles are traversable (e.g., tall grass) or not
(e.g., walls), and generalize over patterns in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.02766v3' target='_blank'>Active Screening for Recurrent Diseases: A Reinforcement Learning
  Approach</a></h2>
<p><strong>Authors:</strong> Han-Ching Ou, Haipeng Chen, Shahin Jabbari, Milind Tambe</p>
<p><strong>Summary:</strong> Active screening is a common approach in controlling the spread of recurring
infectious diseases such as tuberculosis and influenza. In this approach,
health workers periodically select a subset of population for screening.
However, given the limited number of health workers, only a small subset of the
population can be visited in any given time period. Given the recurrent nature
of the disease and rapid spreading, the goal is to minimize the number of
infections over a long time horizon. Active...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.11071v1' target='_blank'>The MineRL 2020 Competition on Sample Efficient Reinforcement Learning
  using Human Priors</a></h2>
<p><strong>Authors:</strong> William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov, John Schulman, Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, Oriol Vinyals</p>
<p><strong>Summary:</strong> Although deep reinforcement learning has led to breakthroughs in many
difficult domains, these successes have required an ever-increasing number of
samples, affording only a shrinking segment of the AI community access to their
development. Resolution of these limitations requires new, sample-efficient
methods. To facilitate research in this direction, we propose this second
iteration of the MineRL Competition. The primary goal of the competition is to
foster the development of algorithms which ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.00203v2' target='_blank'>AdaPool: A Diurnal-Adaptive Fleet Management Framework using Model-Free
  Deep Reinforcement Learning and Change Point Detection</a></h2>
<p><strong>Authors:</strong> Marina Haliem, Vaneet Aggarwal, Bharat Bhargava</p>
<p><strong>Summary:</strong> This paper introduces an adaptive model-free deep reinforcement approach that
can recognize and adapt to the diurnal patterns in the ride-sharing environment
with car-pooling. Deep Reinforcement Learning (RL) suffers from catastrophic
forgetting due to being agnostic to the timescale of changes in the
distribution of experiences. Although RL algorithms are guaranteed to converge
to optimal policies in Markov decision processes (MDPs), this only holds in the
presence of static environments. Howev...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.05864v3' target='_blank'>Verifiable and Compositional Reinforcement Learning Systems</a></h2>
<p><strong>Authors:</strong> Cyrus Neary, Christos Verginis, Murat Cubuktepe, Ufuk Topcu</p>
<p><strong>Summary:</strong> We propose a framework for verifiable and compositional reinforcement
learning (RL) in which a collection of RL subsystems, each of which learns to
accomplish a separate subtask, are composed to achieve an overall task. The
framework consists of a high-level model, represented as a parametric Markov
decision process (pMDP) which is used to plan and to analyze compositions of
subsystems, and of the collection of low-level subsystems themselves. By
defining interfaces between the subsystems, the f...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.10312v2' target='_blank'>Example-Driven Model-Based Reinforcement Learning for Solving
  Long-Horizon Visuomotor Tasks</a></h2>
<p><strong>Authors:</strong> Bohan Wu, Suraj Nair, Li Fei-Fei, Chelsea Finn</p>
<p><strong>Summary:</strong> In this paper, we study the problem of learning a repertoire of low-level
skills from raw images that can be sequenced to complete long-horizon
visuomotor tasks. Reinforcement learning (RL) is a promising approach for
acquiring short-horizon skills autonomously. However, the focus of RL
algorithms has largely been on the success of those individual skills, more so
than learning and grounding a large repertoire of skills that can be sequenced
to complete extended multi-stage tasks. The latter dem...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.02691v3' target='_blank'>Adaptive Coordination Offsets for Signalized Arterial Intersections
  using Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Keith Anshilo Diaz, Damian Dailisan, Umang Sharaf, Carissa Santos, Qijian Gan, Francis Aldrine Uy, May T. Lim, Alexandre M. Bayen</p>
<p><strong>Summary:</strong> Coordinating intersections in arterial networks is critical to the
performance of urban transportation systems. Deep reinforcement learning (RL)
has gained traction in traffic control research along with data-driven
approaches for traffic control systems. To date, proposed deep RL-based traffic
schemes control phase activation or duration. Yet, such approaches may bypass
low volume links for several cycles in order to optimize the network-level
traffic flow. Here, we propose a deep RL framework ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.11634v2' target='_blank'>Assessment of Reward Functions for Reinforcement Learning Traffic Signal
  Control under Real-World Limitations</a></h2>
<p><strong>Authors:</strong> Alvaro Cabrejas-Egea, Shaun Howell, Maksis Knutins, Colm Connaughton</p>
<p><strong>Summary:</strong> Adaptive traffic signal control is one key avenue for mitigating the growing
consequences of traffic congestion. Incumbent solutions such as SCOOT and SCATS
require regular and time-consuming calibration, can't optimise well for
multiple road use modalities, and require the manual curation of many
implementation plans. A recent alternative to these approaches are deep
reinforcement learning algorithms, in which an agent learns how to take the
most appropriate action for a given state of the syst...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.14665v1' target='_blank'>Facilitating Connected Autonomous Vehicle Operations Using
  Space-weighted Information Fusion and Deep Reinforcement Learning Based
  Control</a></h2>
<p><strong>Authors:</strong> Jiqian Dong, Sikai Chen, Yujie Li, Runjia Du, Aaron Steinfeld, Samuel Labi</p>
<p><strong>Summary:</strong> The connectivity aspect of connected autonomous vehicles (CAV) is beneficial
because it facilitates dissemination of traffic-related information to vehicles
through Vehicle-to-External (V2X) communication. Onboard sensing equipment
including LiDAR and camera can reasonably characterize the traffic environment
in the immediate locality of the CAV. However, their performance is limited by
their sensor range (SR). On the other hand, longer-range information is helpful
for characterizing imminent co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.10915v2' target='_blank'>Multi-Agent Reinforcement Learning for Markov Routing Games: A New
  Modeling Paradigm For Dynamic Traffic Assignment</a></h2>
<p><strong>Authors:</strong> Zhenyu Shou, Xu Chen, Yongjie Fu, Xuan Di</p>
<p><strong>Summary:</strong> This paper aims to develop a paradigm that models the learning behavior of
intelligent agents (including but not limited to autonomous vehicles, connected
and automated vehicles, or human-driven vehicles with intelligent navigation
systems where human drivers follow the navigation instructions completely) with
a utility-optimizing goal and the system's equilibrating processes in a routing
game among atomic selfish agents. Such a paradigm can assist policymakers in
devising optimal operational an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.05556v1' target='_blank'>Addressing crash-imminent situations caused by human driven vehicle
  errors in a mixed traffic stream: a model-based reinforcement learning
  approach for CAV</a></h2>
<p><strong>Authors:</strong> Jiqian Dong, Sikai Chen, Samuel Labi</p>
<p><strong>Summary:</strong> It is anticipated that the era of fully autonomous vehicle operations will be
preceded by a lengthy "Transition Period" where the traffic stream will be
mixed, that is, consisting of connected autonomous vehicles (CAVs),
human-driven vehicles (HDVs) and connected human-driven vehicles (CHDVs). In
recognition of the fact that public acceptance of CAVs will hinge on safety
performance of automated driving systems, and that there will likely be safety
challenges in the early part of the transition ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.05740v3' target='_blank'>Temporal Abstraction in Reinforcement Learning with the Successor
  Representation</a></h2>
<p><strong>Authors:</strong> Marlos C. Machado, Andre Barreto, Doina Precup, Michael Bowling</p>
<p><strong>Summary:</strong> Reasoning at multiple levels of temporal abstraction is one of the key
attributes of intelligence. In reinforcement learning, this is often modeled
through temporally extended courses of actions called options. Options allow
agents to make predictions and to operate at different levels of abstraction
within an environment. Nevertheless, approaches based on the options framework
often start with the assumption that a reasonable set of options is known
beforehand. When this is not the case, there ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.05294v1' target='_blank'>Universal Learning Waveform Selection Strategies for Adaptive Target
  Tracking</a></h2>
<p><strong>Authors:</strong> Charles E. Thornton, R. Michael Buehrer, Harpreet S. Dhillon, Anthony F. Martone</p>
<p><strong>Summary:</strong> Online selection of optimal waveforms for target tracking with active sensors
has long been a problem of interest. Many conventional solutions utilize an
estimation-theoretic interpretation, in which a waveform-specific
Cram\'{e}r-Rao lower bound on measurement error is used to select the optimal
waveform for each tracking step. However, this approach is only valid in the
high SNR regime, and requires a rather restrictive set of assumptions regarding
the target motion and measurement models. Fur...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.10678v1' target='_blank'>Sequential Information Design: Markov Persuasion Process and Its
  Efficient Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I. Jordan, Haifeng Xu</p>
<p><strong>Summary:</strong> In today's economy, it becomes important for Internet platforms to consider
the sequential information design problem to align its long term interest with
incentives of the gig service providers. This paper proposes a novel model of
sequential information design, namely the Markov persuasion processes (MPPs),
where a sender, with informational advantage, seeks to persuade a stream of
myopic receivers to take actions that maximizes the sender's cumulative
utilities in a finite horizon Markovian e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2208.05265v1' target='_blank'>Fairness Based Energy-Efficient 3D Path Planning of a Portable Access
  Point: A Deep Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Nithin Babu, Igor Donevski, Alvaro Valcarce, Petar Popovski, Jimmy Jessen Nielsen, Constantinos B. Papadias</p>
<p><strong>Summary:</strong> In this work, we optimize the 3D trajectory of an unmanned aerial vehicle
(UAV)-based portable access point (PAP) that provides wireless services to a
set of ground nodes (GNs). Moreover, as per the Peukert effect, we consider
pragmatic non-linear battery discharge for the battery of the UAV. Thus, we
formulate the problem in a novel manner that represents the maximization of a
fairness-based energy efficiency metric and is named fair energy efficiency
(FEE). The FEE metric defines a system that...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.01092v1' target='_blank'>Inference and dynamic decision-making for deteriorating systems with
  probabilistic dependencies through Bayesian networks and deep reinforcement
  learning</a></h2>
<p><strong>Authors:</strong> Pablo G. Morato, Charalampos P. Andriotis, Konstantinos G. Papakonstantinou, Philippe Rigo</p>
<p><strong>Summary:</strong> In the context of modern environmental and societal concerns, there is an
increasing demand for methods able to identify management strategies for civil
engineering systems, minimizing structural failure risks while optimally
planning inspection and maintenance (I&M) processes. Most available methods
simplify the I&M decision problem to the component level due to the
computational complexity associated with global optimization methodologies
under joint system-level state descriptions. In this pa...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.10064v1' target='_blank'>AdverSAR: Adversarial Search and Rescue via Multi-Agent Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Aowabin Rahman, Arnab Bhattacharya, Thiagarajan Ramachandran, Sayak Mukherjee, Himanshu Sharma, Ted Fujimoto, Samrat Chatterjee</p>
<p><strong>Summary:</strong> Search and Rescue (SAR) missions in remote environments often employ
autonomous multi-robot systems that learn, plan, and execute a combination of
local single-robot control actions, group primitives, and global
mission-oriented coordination and collaboration. Often, SAR coordination
strategies are manually designed by human experts who can remotely control the
multi-robot system and enable semi-autonomous operations. However, in remote
environments where connectivity is limited and human interv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.14870v1' target='_blank'>Bi-Manual Block Assembly via Sim-to-Real Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Satoshi Kataoka, Youngseog Chung, Seyed Kamyar Seyed Ghasemipour, Pannag Sanketi, Shixiang Shane Gu, Igor Mordatch</p>
<p><strong>Summary:</strong> Most successes in robotic manipulation have been restricted to single-arm
gripper robots, whose low dexterity limits the range of solvable tasks to
pick-and-place, inser-tion, and object rearrangement. More complex tasks such
as assembly require dual and multi-arm platforms, but entail a suite of unique
challenges such as bi-arm coordination and collision avoidance, robust
grasping, and long-horizon planning. In this work we investigate the
feasibility of training deep reinforcement learning (RL...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.13586v3' target='_blank'>Settling the Sample Complexity of Online Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du</p>
<p><strong>Summary:</strong> A central issue lying at the heart of online reinforcement learning (RL) is
data efficiency. While a number of recent works achieved asymptotically minimal
regret in online RL, the optimality of these results is only guaranteed in a
``large-sample'' regime, imposing enormous burn-in cost in order for their
algorithms to operate optimally. How to achieve minimax-optimal regret without
incurring any burn-in cost has been an open problem in RL theory.
  We settle this problem for the context of fin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.16280v1' target='_blank'>A reinforcement learning based construction material supply strategy
  using robotic crane and computer vision for building reconstruction after an
  earthquake</a></h2>
<p><strong>Authors:</strong> Yifei Xiao, T. Y. Yang, Xiao Pan, Fan Xie, Zhongwei Chen</p>
<p><strong>Summary:</strong> After an earthquake, it is particularly important to provide the necessary
resources on site because a large number of infrastructures need to be repaired
or newly constructed. Due to the complex construction environment after the
disaster, there are potential safety hazards for human labors working in this
environment. With the advancement of robotic technology and artificial
intelligent (AI) algorithms, smart robotic technology is the potential solution
to provide construction resources after ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.13672v7' target='_blank'>RL-I2IT: Image-to-Image Translation with Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu, Hongtu Zhu, Xin Li, Siwei Lyu</p>
<p><strong>Summary:</strong> Most existing Image-to-Image Translation (I2IT) methods generate images in a
single run of a deep learning (DL) model. However, designing such a single-step
model is always challenging, requiring a huge number of parameters and easily
falling into bad global minimums and overfitting. In this work, we reformulate
I2IT as a step-wise decision-making problem via deep reinforcement learning
(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The
key feature in the RL-I2IT fram...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.11727v2' target='_blank'>Highway Graph to Accelerate Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zidu Yin, Zhen Zhang, Dong Gong, Stefano V. Albrecht, Javen Q. Shi</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) algorithms often struggle with low training
efficiency. A common approach to address this challenge is integrating
model-based planning algorithms, such as Monte Carlo Tree Search (MCTS) or
Value Iteration (VI), into the environmental model. However, VI requires
iterating over a large tensor which updates the value of the preceding state
based on the succeeding state through value propagation, resulting in
computationally intensive operations. To enhance the RL traini...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.09760v1' target='_blank'>Combining RL and IL using a dynamic, performance-based modulation over
  learning signals and its application to local planning</a></h2>
<p><strong>Authors:</strong> Francisco Leiva, Javier Ruiz-del-Solar</p>
<p><strong>Summary:</strong> This paper proposes a method to combine reinforcement learning (RL) and
imitation learning (IL) using a dynamic, performance-based modulation over
learning signals. The proposed method combines RL and behavioral cloning (IL),
or corrective feedback in the action space (interactive IL/IIL), by dynamically
weighting the losses to be optimized, taking into account the backpropagated
gradients used to update the policy and the agent's estimated performance. In
this manner, RL and IL/IIL losses are c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.10591v1' target='_blank'>MINT: a Multi-modal Image and Narrative Text Dubbing Dataset for Foley
  Audio Content Planning and Generation</a></h2>
<p><strong>Authors:</strong> Ruibo Fu, Shuchen Shi, Hongming Guo, Tao Wang, Chunyu Qiang, Zhengqi Wen, Jianhua Tao, Xin Qi, Yi Lu, Xiaopeng Wang, Zhiyong Wang, Yukun Liu, Xuefei Liu, Shuai Zhang, Guanjun Li</p>
<p><strong>Summary:</strong> Foley audio, critical for enhancing the immersive experience in multimedia
content, faces significant challenges in the AI-generated content (AIGC)
landscape. Despite advancements in AIGC technologies for text and image
generation, the foley audio dubbing remains rudimentary due to difficulties in
cross-modal scene matching and content correlation. Current text-to-audio
technology, which relies on detailed and acoustically relevant textual
descriptions, falls short in practical video dubbing app...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.11756v1' target='_blank'>Synthesizing Evolving Symbolic Representations for Autonomous Systems</a></h2>
<p><strong>Authors:</strong> Gabriele Sartor, Angelo Oddi, Riccardo Rasconi, Vieri Giuliano Santucci, Rosa Meo</p>
<p><strong>Summary:</strong> Recently, AI systems have made remarkable progress in various tasks. Deep
Reinforcement Learning(DRL) is an effective tool for agents to learn policies
in low-level state spaces to solve highly complex tasks. Researchers have
introduced Intrinsic Motivation(IM) to the RL mechanism, which simulates the
agent's curiosity, encouraging agents to explore interesting areas of the
environment. This new feature has proved vital in enabling agents to learn
policies without being given specific goals. How...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.05023v1' target='_blank'>Towards Big data processing in IoT: Path Planning and Resource
  Management of UAV Base Stations in Mobile-Edge Computing System</a></h2>
<p><strong>Authors:</strong> Shuo Wan, Jiaxun Lu, Pingyi Fan, Khaled B. Letaief</p>
<p><strong>Summary:</strong> Heavy data load and wide cover range have always been crucial problems for
online data processing in internet of things (IoT). Recently, mobile-edge
computing (MEC) and unmanned aerial vehicle base stations (UAV-BSs) have
emerged as promising techniques in IoT. In this paper, we propose a three-layer
online data processing network based on MEC technique. On the bottom layer, raw
data are generated by widely distributed sensors, which reflects local
information. Upon them, unmanned aerial vehicle...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.08815v1' target='_blank'>The impact of analytical outage modeling on expansion planning problems
  in the area of power systems</a></h2>
<p><strong>Authors:</strong> S. Tsianikas, N. Yousefi, J. Zhou, D. W. Coit</p>
<p><strong>Summary:</strong> Expansion planning problems refer to the monetary and unit investment needed
for energy production or storage. An inherent element in these problems is the
element of stochasticity in various aspects, such as the generation output of
the units, climate change or frequency and duration of grid outages. Especially
for the latter one, outage modeling is crucial to be carefully considered when
designing systems with distributed generation at their core, such as
microgrids. In most studies so far, a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.10316v2' target='_blank'>Proper Value Equivalence</a></h2>
<p><strong>Authors:</strong> Christopher Grimm, Andr√© Barreto, Gregory Farquhar, David Silver, Satinder Singh</p>
<p><strong>Summary:</strong> One of the main challenges in model-based reinforcement learning (RL) is to
decide which aspects of the environment should be modeled. The
value-equivalence (VE) principle proposes a simple answer to this question: a
model should capture the aspects of the environment that are relevant for
value-based planning. Technically, VE distinguishes models based on a set of
policies and a set of functions: a model is said to be VE to the environment if
the Bellman operators it induces for the policies yi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.05713v2' target='_blank'>Towards real-world navigation with deep differentiable planners</a></h2>
<p><strong>Authors:</strong> Shu Ishida, Jo√£o F. Henriques</p>
<p><strong>Summary:</strong> We train embodied neural networks to plan and navigate unseen complex 3D
environments, emphasising real-world deployment. Rather than requiring prior
knowledge of the agent or environment, the planner learns to model the state
transitions and rewards. To avoid the potentially hazardous trial-and-error of
reinforcement learning, we focus on differentiable planners such as Value
Iteration Networks (VIN), which are trained offline from safe expert
demonstrations. Although they work well in small si...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.05723v3' target='_blank'>Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey</p>
<p><strong>Summary:</strong> Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm well matched to a real-world RL
deployment process. In this scenario, we aim to find the best-performing policy
within a limited budget of online interactions. Previous work in the OtO
setting has focused on correcting for bias introduced by the policy-constraint
mechanisms of offline RL algorithms. Such constraints keep the learned policy
close to the behavior policy that collected...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.08588v2' target='_blank'>Octopus: Embodied Vision-Language Programmer from Environmental Feedback</a></h2>
<p><strong>Authors:</strong> Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu</p>
<p><strong>Summary:</strong> Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. When integrated into an embodied agent,
existing embodied VLM works either output detailed action sequences at the
manipulation level or only provide plans at an abstract level, leaving a gap
between high-level planning and real-world manipulation. To bridge this gap, we
introduce Octopus, an embodied vision-language programmer that uses executable
code generation as a medium to connect...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.08994v3' target='_blank'>Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order
  Bounds</a></h2>
<p><strong>Authors:</strong> Zhiyong Wang, Dongruo Zhou, John C. S. Lui, Wen Sun</p>
<p><strong>Summary:</strong> Learning a transition model via Maximum Likelihood Estimation (MLE) followed
by planning inside the learned model is perhaps the most standard and simplest
Model-based Reinforcement Learning (RL) framework. In this work, we show that
such a simple Model-based RL scheme, when equipped with optimistic and
pessimistic planning procedures, achieves strong regret and sample complexity
bounds in online and offline RL settings. Particularly, we demonstrate that
under the conditions where the trajectory...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1412.2818v1' target='_blank'>The hippocampal-striatal circuit for goal-directed and habitual choice</a></h2>
<p><strong>Authors:</strong> Fabian Chersi</p>
<p><strong>Summary:</strong> It is now widely accepted that one of the roles of the hippocampus is to
maintain episodic spatial representations, while parallel striatal pathways
contribute to both declarative and procedural value computations by encoding
different input-specific outcome predictions. In this paper we investigate the
use of these brain mechanisms for action selection, linking them to model-based
and model-free controllers for decision making. To this aim we propose a
biologically inspired computational model ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1502.02860v2' target='_blank'>Gaussian Processes for Data-Efficient Learning in Robotics and Control</a></h2>
<p><strong>Authors:</strong> Marc Peter Deisenroth, Dieter Fox, Carl Edward Rasmussen</p>
<p><strong>Summary:</strong> Autonomous learning has been a promising direction in control and robotics
for more than a decade since data-driven learning allows to reduce the amount
of engineering knowledge, which is otherwise required. However, autonomous
reinforcement learning (RL) approaches typically require many interactions with
the system to learn controllers, which is a practical limitation in real
systems, such as robots, where many interactions can be impractical and time
consuming. To address this problem, curren...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1706.04038v1' target='_blank'>Meta learning Framework for Automated Driving</a></h2>
<p><strong>Authors:</strong> Ahmad El Sallab, Mahmoud Saeed, Omar Abdel Tawab, Mohammed Abdou</p>
<p><strong>Summary:</strong> The success of automated driving deployment is highly depending on the
ability to develop an efficient and safe driving policy. The problem is well
formulated under the framework of optimal control as a cost optimization
problem. Model based solutions using traditional planning are efficient, but
require the knowledge of the environment model. On the other hand, model free
solutions suffer sample inefficiency and require too many interactions with the
environment, which is infeasible in practice...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1707.06354v2' target='_blank'>Pragmatic-Pedagogic Value Alignment</a></h2>
<p><strong>Authors:</strong> Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry, Thomas L. Griffiths, Anca D. Dragan</p>
<p><strong>Summary:</strong> As intelligent systems gain autonomy and capability, it becomes vital to
ensure that their objectives match those of their human users; this is known as
the value-alignment problem. In robotics, value alignment is key to the design
of collaborative robots that can integrate into human workflows, successfully
inferring and adapting to their users' objectives as they go. We argue that a
meaningful solution to value alignment must combine multi-agent decision theory
with rich mathematical models of...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1709.10489v3' target='_blank'>Self-supervised Deep Reinforcement Learning with Generalized Computation
  Graphs for Robot Navigation</a></h2>
<p><strong>Authors:</strong> Gregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, Sergey Levine</p>
<p><strong>Summary:</strong> Enabling robots to autonomously navigate complex environments is essential
for real-world deployment. Prior methods approach this problem by having the
robot maintain an internal map of the world, and then use a localization and
planning method to navigate through the internal map. However, these approaches
often include a variety of assumptions, are computationally intensive, and do
not learn from failures. In contrast, learning-based methods improve as the
robot acts in the environment, but ar...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1712.03316v3' target='_blank'>IQA: Visual Question Answering in Interactive Environments</a></h2>
<p><strong>Authors:</strong> Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi</p>
<p><strong>Summary:</strong> We introduce Interactive Question Answering (IQA), the task of answering
questions that require an autonomous agent to interact with a dynamic visual
environment. IQA presents the agent with a scene and a question, like: "Are
there any apples in the fridge?" The agent must navigate around the scene,
acquire visual understanding of scene elements, interact with objects (e.g.
open refrigerators) and plan for a series of actions conditioned on the
question. Popular reinforcement learning approaches...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1802.07834v1' target='_blank'>Learning to Gather without Communication</a></h2>
<p><strong>Authors:</strong> El Mahdi El Mhamdi, Rachid Guerraoui, Alexandre Maurer, Vladislav Tempez</p>
<p><strong>Summary:</strong> A standard belief on emerging collective behavior is that it emerges from
simple individual rules. Most of the mathematical research on such collective
behavior starts from imperative individual rules, like always go to the center.
But how could an (optimal) individual rule emerge during a short period within
the group lifetime, especially if communication is not available. We argue that
such rules can actually emerge in a group in a short span of time via
collective (multi-agent) reinforcement ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.04686v3' target='_blank'>Task Transfer by Preference-Based Cost Learning</a></h2>
<p><strong>Authors:</strong> Mingxuan Jing, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu</p>
<p><strong>Summary:</strong> The goal of task transfer in reinforcement learning is migrating the action
policy of an agent to the target task from the source task. Given their
successes on robotic action planning, current methods mostly rely on two
requirements: exactly-relevant expert demonstrations or the explicitly-coded
cost function on target task, both of which, however, are inconvenient to
obtain in practice. In this paper, we relax these two strong conditions by
developing a novel task transfer framework where the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.11548v3' target='_blank'>The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in
  Medical Decision Making</a></h2>
<p><strong>Authors:</strong> Luchen Li, Matthieu Komorowski, Aldo A. Faisal</p>
<p><strong>Summary:</strong> Off-policy reinforcement learning enables near-optimal policy from suboptimal
experience, thereby provisions opportunity for artificial intelligence
applications in healthcare. Previous works have mainly framed patient-clinician
interactions as Markov decision processes, while true physiological states are
not necessarily fully observable from clinical data. We capture this situation
with partially observable Markov decision process, in which an agent optimises
its actions in a belief represente...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1805.11593v1' target='_blank'>Observe and Look Further: Achieving Consistent Performance on Atari</a></h2>
<p><strong>Authors:</strong> Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad Gheshlaghi Azar, Dan Horgan, David Budden, Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel Veƒçer√≠k, Matteo Hessel, R√©mi Munos, Olivier Pietquin</p>
<p><strong>Summary:</strong> Despite significant advances in the field of deep Reinforcement Learning
(RL), today's algorithms still fail to learn human-level policies consistently
over a set of diverse tasks such as Atari 2600 games. We identify three key
challenges that any algorithm needs to master in order to perform well on all
games: processing diverse reward distributions, reasoning over long time
horizons, and exploring efficiently. In this paper, we propose an algorithm
that addresses each of these challenges and i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1806.05780v4' target='_blank'>Surprising Negative Results for Generative Adversarial Tree Search</a></h2>
<p><strong>Authors:</strong> Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Zachary C Lipton, Animashree Anandkumar</p>
<p><strong>Summary:</strong> While many recent advances in deep reinforcement learning (RL) rely on
model-free methods, model-based approaches remain an alluring prospect for
their potential to exploit unsupervised data to learn environment model. In
this work, we provide an extensive study on the design of deep generative
models for RL environments and propose a sample efficient and robust method to
learn the model of Atari environments. We deploy this model and propose
generative adversarial tree search (GATS) a deep RL a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1901.06085v1' target='_blank'>Theory of Minds: Understanding Behavior in Groups Through Inverse
  Planning</a></h2>
<p><strong>Authors:</strong> Michael Shum, Max Kleiman-Weiner, Michael L. Littman, Joshua B. Tenenbaum</p>
<p><strong>Summary:</strong> Human social behavior is structured by relationships. We form teams, groups,
tribes, and alliances at all scales of human life. These structures guide
multi-agent cooperation and competition, but when we observe others these
underlying relationships are typically unobservable and hence must be inferred.
Humans make these inferences intuitively and flexibly, often making rapid
generalizations about the latent relationships that underlie behavior from just
sparse and noisy observations. Rapid and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.01883v3' target='_blank'>Separating value functions across time-scales</a></h2>
<p><strong>Authors:</strong> Joshua Romoff, Peter Henderson, Ahmed Touati, Emma Brunskill, Joelle Pineau, Yann Ollivier</p>
<p><strong>Summary:</strong> In many finite horizon episodic reinforcement learning (RL) settings, it is
desirable to optimize for the undiscounted return - in settings like Atari, for
instance, the goal is to collect the most points while staying alive in the
long run. Yet, it may be difficult (or even intractable) mathematically to
learn with this target. As such, temporal discounting is often applied to
optimize over a shorter effective planning horizon. This comes at the risk of
potentially biasing the optimization targ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1902.05703v1' target='_blank'>Network Offloading Policies for Cloud Robotics: a Learning-based
  Approach</a></h2>
<p><strong>Authors:</strong> Sandeep Chinchali, Apoorva Sharma, James Harrison, Amine Elhafsi, Daniel Kang, Evgenya Pergament, Eyal Cidon, Sachin Katti, Marco Pavone</p>
<p><strong>Summary:</strong> Today's robotic systems are increasingly turning to computationally expensive
models such as deep neural networks (DNNs) for tasks like localization,
perception, planning, and object detection. However, resource-constrained
robots, like low-power drones, often have insufficient on-board compute
resources or power reserves to scalably run the most accurate, state-of-the art
neural network compute models. Cloud robotics allows mobile robots the benefit
of offloading compute to centralized servers ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.00401v2' target='_blank'>Learning To Follow Directions in Street View</a></h2>
<p><strong>Authors:</strong> Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, Raia Hadsell</p>
<p><strong>Summary:</strong> Navigating and understanding the real world remains a key challenge in
machine learning and inspires a great variety of research in areas such as
language grounding, planning, navigation and computer vision. We propose an
instruction-following task that requires all of the above, and which combines
the practicality of simulated environments with the challenges of ambiguous,
noisy real world data. StreetNav is built on top of Google Street View and
provides visually accurate environments represen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.08643v4' target='_blank'>Online Gaussian Process State-Space Model: Learning and Planning for
  Partially Observable Dynamical Systems</a></h2>
<p><strong>Authors:</strong> Soon-Seo Park, Young-Jin Park, Youngjae Min, Han-Lim Choi</p>
<p><strong>Summary:</strong> This paper proposes an online learning method of Gaussian process state-space
model (GP-SSM). GP-SSM is a probabilistic representation learning scheme that
represents unknown state transition and/or measurement models as Gaussian
processes (GPs). While the majority of prior literature on learning of GP-SSM
are focused on processing a given set of time series data, data may arrive and
accumulate sequentially over time in most dynamical systems. Storing all such
sequential data and updating the mo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1906.05093v2' target='_blank'>Optimizing city-scale traffic through modeling observations of vehicle
  movements</a></h2>
<p><strong>Authors:</strong> Fan Yang, Alina Vereshchaka, Bruno Lepri, Wen Dong</p>
<p><strong>Summary:</strong> The capability of traffic-information systems to sense the movement of
millions of users and offer trip plans through mobile phones has enabled a new
way of optimizing city traffic dynamics, turning transportation big data into
insights and actions in a closed-loop and evaluating this approach in the real
world. Existing research has applied dynamic Bayesian networks and deep neural
networks to make traffic predictions from floating car data, utilized dynamic
programming and simulation approache...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.06962v2' target='_blank'>Dynamic Pricing and Fleet Management for Electric Autonomous Mobility on
  Demand Systems</a></h2>
<p><strong>Authors:</strong> Berkay Turan, Ramtin Pedarsani, Mahnoosh Alizadeh</p>
<p><strong>Summary:</strong> The proliferation of ride sharing systems is a major drive in the advancement
of autonomous and electric vehicle technologies. This paper considers the joint
routing, battery charging, and pricing problem faced by a profit-maximizing
transportation service provider that operates a fleet of autonomous electric
vehicles. We first establish the static planning problem by considering
time-invariant system parameters and determine the optimal static policy. While
the static policy provides stability ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1909.12271v1' target='_blank'>RLBench: The Robot Learning Benchmark & Learning Environment</a></h2>
<p><strong>Authors:</strong> Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J. Davison</p>
<p><strong>Summary:</strong> We present a challenging new benchmark and learning-environment for robot
learning: RLBench. The benchmark features 100 completely unique, hand-designed
tasks ranging in difficulty, from simple target reaching and door opening, to
longer multi-stage tasks, such as opening an oven and placing a tray in it. We
provide an array of both proprioceptive observations and visual observations,
which include rgb, depth, and segmentation masks from an over-the-shoulder
stereo camera and an eye-in-hand mono...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.05664v1' target='_blank'>Extracting Incentives from Black-Box Decisions</a></h2>
<p><strong>Authors:</strong> Yonadav Shavit, William S. Moses</p>
<p><strong>Summary:</strong> An algorithmic decision-maker incentivizes people to act in certain ways to
receive better decisions. These incentives can dramatically influence subjects'
behaviors and lives, and it is important that both decision-makers and
decision-recipients have clarity on which actions are incentivized by the
chosen model. While for linear functions, the changes a subject is incentivized
to make may be clear, we prove that for many non-linear functions (e.g. neural
networks, random forests), classical met...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.13249v1' target='_blank'>Navigation Agents for the Visually Impaired: A Sidewalk Simulator and
  Experiments</a></h2>
<p><strong>Authors:</strong> Martin Weiss, Simon Chamorro, Roger Girgis, Margaux Luck, Samira E. Kahou, Joseph P. Cohen, Derek Nowrouzezahrai, Doina Precup, Florian Golemo, Chris Pal</p>
<p><strong>Summary:</strong> Millions of blind and visually-impaired (BVI) people navigate urban
environments every day, using smartphones for high-level path-planning and
white canes or guide dogs for local information. However, many BVI people still
struggle to travel to new places. In our endeavor to create a navigation
assistant for the BVI, we found that existing Reinforcement Learning (RL)
environments were unsuitable for the task. This work introduces SEVN, a
sidewalk simulation environment and a neural network-based...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.02368v3' target='_blank'>Inter-Level Cooperation in Hierarchical Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Abdul Rahman Kreidieh, Glen Berseth, Brandon Trabucco, Samyak Parajuli, Sergey Levine, Alexandre M. Bayen</p>
<p><strong>Summary:</strong> Hierarchies of temporally decoupled policies present a promising approach for
enabling structured exploration in complex long-term planning problems. To
fully achieve this approach an end-to-end training paradigm is needed. However,
training these multi-level policies has had limited success due to challenges
arising from interactions between the goal-assigning and goal-achieving levels
within a hierarchy. In this article, we consider the policy optimization
process as a multi-agent process. Thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.12970v5' target='_blank'>Pontryagin Differentiable Programming: An End-to-End Learning and
  Control Framework</a></h2>
<p><strong>Authors:</strong> Wanxin Jin, Zhaoran Wang, Zhuoran Yang, Shaoshuai Mou</p>
<p><strong>Summary:</strong> This paper develops a Pontryagin Differentiable Programming (PDP)
methodology, which establishes a unified framework to solve a broad class of
learning and control tasks. The PDP distinguishes from existing methods by two
novel techniques: first, we differentiate through Pontryagin's Maximum
Principle, and this allows to obtain the analytical derivative of a trajectory
with respect to tunable parameters within an optimal control system, enabling
end-to-end learning of dynamics, policies, or/and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.07937v1' target='_blank'>Machine Learning assisted Handover and Resource Management for Cellular
  Connected Drones</a></h2>
<p><strong>Authors:</strong> Amin Azari, Fayezeh Ghavimi, Mustafa Ozger, Riku Jantti, Cicek Cavdar</p>
<p><strong>Summary:</strong> Enabling cellular connectivity for drones introduces a wide set of challenges
and opportunities. Communication of cellular-connected drones is influenced by
3-dimensional mobility and line-of-sight channel characteristics which results
in higher number of handovers with increasing altitude. Our cell planning
simulations in coexistence of aerial and terrestrial users indicate that the
severe interference from drones to base stations is a major challenge for
uplink communications of terrestrial us...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2001.10208v1' target='_blank'>Towards Learning Multi-agent Negotiations via Self-Play</a></h2>
<p><strong>Authors:</strong> Yichuan Charlie Tang</p>
<p><strong>Summary:</strong> Making sophisticated, robust, and safe sequential decisions is at the heart
of intelligent systems. This is especially critical for planning in complex
multi-agent environments, where agents need to anticipate other agents'
intentions and possible future actions. Traditional methods formulate the
problem as a Markov Decision Process, but the solutions often rely on various
assumptions and become brittle when presented with corner cases. In contrast,
deep reinforcement learning (Deep RL) has been...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.01587v2' target='_blank'>Deep Learning Tubes for Tube MPC</a></h2>
<p><strong>Authors:</strong> David D. Fan, Ali-akbar Agha-mohammadi, Evangelos A. Theodorou</p>
<p><strong>Summary:</strong> Learning-based control aims to construct models of a system to use for
planning or trajectory optimization, e.g. in model-based reinforcement
learning. In order to obtain guarantees of safety in this context, uncertainty
must be accurately quantified. This uncertainty may come from errors in
learning (due to a lack of data, for example), or may be inherent to the
system. Propagating uncertainty forward in learned dynamics models is a
difficult problem. In this work we use deep learning to obtain...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.05822v1' target='_blank'>Frequency-based Search-control in Dyna</a></h2>
<p><strong>Authors:</strong> Yangchen Pan, Jincheng Mei, Amir-massoud Farahmand</p>
<p><strong>Summary:</strong> Model-based reinforcement learning has been empirically demonstrated as a
successful strategy to improve sample efficiency. In particular, Dyna is an
elegant model-based architecture integrating learning and planning that
provides huge flexibility of using a model. One of the most important
components in Dyna is called search-control, which refers to the process of
generating state or state-action pairs from which we query the model to acquire
simulated experiences. Search-control is critical in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2002.09869v1' target='_blank'>Near-optimal Regret Bounds for Stochastic Shortest Path</a></h2>
<p><strong>Authors:</strong> Alon Cohen, Haim Kaplan, Yishay Mansour, Aviv Rosenberg</p>
<p><strong>Summary:</strong> Stochastic shortest path (SSP) is a well-known problem in planning and
control, in which an agent has to reach a goal state in minimum total expected
cost. In the learning formulation of the problem, the agent is unaware of the
environment dynamics (i.e., the transition function) and has to repeatedly play
for a given number of episodes while reasoning about the problem's optimal
solution. Unlike other well-studied models in reinforcement learning (RL), the
length of an episode is not predetermi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2005.02979v3' target='_blank'>A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical
  Systems</a></h2>
<p><strong>Authors:</strong> Anthony Corso, Robert J. Moss, Mark Koren, Ritchie Lee, Mykel J. Kochenderfer</p>
<p><strong>Summary:</strong> Autonomous cyber-physical systems (CPS) can improve safety and efficiency for
safety-critical applications, but require rigorous testing before deployment.
The complexity of these systems often precludes the use of formal verification
and real-world testing can be too dangerous during development. Therefore,
simulation-based techniques have been developed that treat the system under
test as a black box operating in a simulated environment. Safety validation
tasks include finding disturbances in ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2006.13760v2' target='_blank'>The NetHack Learning Environment</a></h2>
<p><strong>Authors:</strong> Heinrich K√ºttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, Tim Rockt√§schel</p>
<p><strong>Summary:</strong> Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the
development of challenging environments that test the limits of current
methods. While existing RL environments are either sufficiently complex or
based on fast simulation, they are rarely both. Here, we present the NetHack
Learning Environment (NLE), a scalable, procedurally generated, stochastic,
rich, and challenging environment for RL research based on the popular
single-player terminal-based roguelike game, NetHac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.00791v2' target='_blank'>Learning a Distributed Control Scheme for Demand Flexibility in
  Thermostatically Controlled Loads</a></h2>
<p><strong>Authors:</strong> Bingqing Chen, Weiran Yao, Jonathan Francis, Mario Berg√©s</p>
<p><strong>Summary:</strong> Demand flexibility is increasingly important for power grids, in light of
growing penetration of renewable generation. Careful coordination of
thermostatically controlled loads (TCLs) can potentially modulate energy
demand, decrease operating costs, and increase grid resiliency. However, it is
challenging to control a heterogeneous population of TCLs: the control problem
has a large state action space; each TCL has unique and complex dynamics; and
multiple system-level objectives need to be opti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.04047v1' target='_blank'>Design, Control, and Applications of a Soft Robotic Arm</a></h2>
<p><strong>Authors:</strong> Hao Jiang, Zhanchi Wang, Yusong Jin, Xiaotong Chen, Peijin Li, Yinghao Gan, Sen Lin, Xiaoping Chen</p>
<p><strong>Summary:</strong> This paper presents the design, control, and applications of a multi-segment
soft robotic arm. In order to design a soft arm with large load capacity,
several design principles are proposed by analyzing two kinds of buckling
issues, under which we present a novel structure named Honeycomb Pneumatic
Networks (HPN). Parameter optimization method, based on finite element method
(FEM), is proposed to optimize HPN Arm design parameters. Through a quick
fabrication process, several prototypes with dif...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.03289v2' target='_blank'>QarSUMO: A Parallel, Congestion-optimized Traffic Simulator</a></h2>
<p><strong>Authors:</strong> Hao Chen, Ke Yang, Stefano Giovanni Rizzo, Giovanna Vantini, Phillip Taylor, Xiaosong Ma, Sanjay Chawla</p>
<p><strong>Summary:</strong> Traffic simulators are important tools for tasks such as urban planning and
transportation management. Microscopic simulators allow per-vehicle movement
simulation, but require longer simulation time. The simulation overhead is
exacerbated when there is traffic congestion and most vehicles move slowly.
This in particular hurts the productivity of emerging urban computing studies
based on reinforcement learning, where traffic simulations are heavily and
repeatedly used for designing policies to o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.05437v1' target='_blank'>A DRL-based Multiagent Cooperative Control Framework for CAV Networks: a
  Graphic Convolution Q Network</a></h2>
<p><strong>Authors:</strong> Jiqian Dong, Sikai Chen, Paul Young Joun Ha, Yujie Li, Samuel Labi</p>
<p><strong>Summary:</strong> Connected Autonomous Vehicle (CAV) Network can be defined as a collection of
CAVs operating at different locations on a multilane corridor, which provides a
platform to facilitate the dissemination of operational information as well as
control instructions. Cooperation is crucial in CAV operating systems since it
can greatly enhance operation in terms of safety and mobility, and high-level
cooperation between CAVs can be expected by jointly plan and control within CAV
network. However, due to th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.11234v2' target='_blank'>Learning Spring Mass Locomotion: Guiding Policies with a Reduced-Order
  Model</a></h2>
<p><strong>Authors:</strong> Kevin Green, Yesh Godse, Jeremy Dao, Ross L. Hatton, Alan Fern, Jonathan Hurst</p>
<p><strong>Summary:</strong> In this paper, we describe an approach to achieve dynamic legged locomotion
on physical robots which combines existing methods for control with
reinforcement learning. Specifically, our goal is a control hierarchy in which
highest-level behaviors are planned through reduced-order models, which
describe the fundamental physics of legged locomotion, and lower level
controllers utilize a learned policy that can bridge the gap between the
idealized, simple model and the complex, full order robot. Th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.01913v1' target='_blank'>Transfer Learning as an Enabler of the Intelligent Digital Twin</a></h2>
<p><strong>Authors:</strong> Benjamin Maschler, Dominik Braun, Nasser Jazdi, Michael Weyrich</p>
<p><strong>Summary:</strong> Digital Twins have been described as beneficial in many areas, such as
virtual commissioning, fault prediction or reconfiguration planning. Equipping
Digital Twins with artificial intelligence functionalities can greatly expand
those beneficial applications or open up altogether new areas of application,
among them cross-phase industrial transfer learning. In the context of machine
learning, transfer learning represents a set of approaches that enhance
learning new tasks based upon previously ac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.10773v6' target='_blank'>Forming Real-World Human-Robot Cooperation for Tasks With General Goal</a></h2>
<p><strong>Authors:</strong> Lingfeng Tao, Michael Bowman, Jiucai Zhang, Xiaoli Zhang</p>
<p><strong>Summary:</strong> In human-robot cooperation, the robot cooperates with humans to accomplish
the task together. Existing approaches assume the human has a specific goal
during the cooperation, and the robot infers and acts toward it. However, in
real-world environments, a human usually only has a general goal (e.g., general
direction or area in motion planning) at the beginning of the cooperation,
which needs to be clarified to a specific goal (i.e., an exact position) during
cooperation. The specification proces...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2012.15436v2' target='_blank'>Robotic Grasping of Fully-Occluded Objects using RF Perception</a></h2>
<p><strong>Authors:</strong> Tara Boroushaki, Junshan Leng, Ian Clester, Alberto Rodriguez, Fadel Adib</p>
<p><strong>Summary:</strong> We present the design, implementation, and evaluation of RF-Grasp, a robotic
system that can grasp fully-occluded objects in unknown and unstructured
environments. Unlike prior systems that are constrained by the line-of-sight
perception of vision and infrared sensors, RF-Grasp employs RF (Radio
Frequency) perception to identify and locate target objects through occlusions,
and perform efficient exploration and complex manipulation tasks in
non-line-of-sight settings.
  RF-Grasp relies on an eye...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.00494v1' target='_blank'>A Provably Efficient Algorithm for Linear Markov Decision Process with
  Low Switching Cost</a></h2>
<p><strong>Authors:</strong> Minbo Gao, Tianle Xie, Simon S. Du, Lin F. Yang</p>
<p><strong>Summary:</strong> Many real-world applications, such as those in medical domains,
recommendation systems, etc, can be formulated as large state space
reinforcement learning problems with only a small budget of the number of
policy changes, i.e., low switching cost. This paper focuses on the linear
Markov Decision Process (MDP) recently studied in [Yang et al 2019, Jin et al
2020] where the linear function approximation is used for generalization on the
large state space. We present the first algorithm for linear ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.12745v4' target='_blank'>Improved Variance-Aware Confidence Sets for Linear Bandits and Linear
  Mixture MDP</a></h2>
<p><strong>Authors:</strong> Zihan Zhang, Jiaqi Yang, Xiangyang Ji, Simon S. Du</p>
<p><strong>Summary:</strong> This paper presents new \emph{variance-aware} confidence sets for linear
bandits and linear mixture Markov Decision Processes (MDPs). With the new
confidence sets, we obtain the follow regret bounds: For linear bandits, we
obtain an $\tilde{O}(poly(d)\sqrt{1 + \sum_{k=1}^{K}\sigma_k^2})$
data-dependent regret bound, where $d$ is the feature dimension, $K$ is the
number of rounds, and $\sigma_k^2$ is the \emph{unknown} variance of the reward
at the $k$-th round. This is the first regret bound tha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.01646v3' target='_blank'>SOLO: Search Online, Learn Offline for Combinatorial Optimization
  Problems</a></h2>
<p><strong>Authors:</strong> Joel Oren, Chana Ross, Maksym Lefarov, Felix Richter, Ayal Taitler, Zohar Feldman, Christian Daniel, Dotan Di Castro</p>
<p><strong>Summary:</strong> We study combinatorial problems with real world applications such as machine
scheduling, routing, and assignment. We propose a method that combines
Reinforcement Learning (RL) and planning. This method can equally be applied to
both the offline, as well as online, variants of the combinatorial problem, in
which the problem components (e.g., jobs in scheduling problems) are not known
in advance, but rather arrive during the decision-making process. Our solution
is quite generic, scalable, and lev...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.02844v1' target='_blank'>GEM: Group Enhanced Model for Learning Dynamical Control Systems</a></h2>
<p><strong>Authors:</strong> Philippe Hansen-Estruch, Wenling Shang, Lerrel Pinto, Pieter Abbeel, Stas Tiomkin</p>
<p><strong>Summary:</strong> Learning the dynamics of a physical system wherein an autonomous agent
operates is an important task. Often these systems present apparent geometric
structures. For instance, the trajectories of a robotic manipulator can be
broken down into a collection of its transitional and rotational motions, fully
characterized by the corresponding Lie groups and Lie algebras. In this work,
we take advantage of these structures to build effective dynamical models that
are amenable to sample-based learning. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.02863v1' target='_blank'>The Value of Planning for Infinite-Horizon Model Predictive Control</a></h2>
<p><strong>Authors:</strong> Nathan Hatch, Byron Boots</p>
<p><strong>Summary:</strong> Model Predictive Control (MPC) is a classic tool for optimal control of
complex, real-world systems. Although it has been successfully applied to a
wide range of challenging tasks in robotics, it is fundamentally limited by the
prediction horizon, which, if too short, will result in myopic decisions.
Recently, several papers have suggested using a learned value function as the
terminal cost for MPC. If the value function is accurate, it effectively allows
MPC to reason over an infinite horizon. ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2104.04477v2' target='_blank'>Jamming-Resilient Path Planning for Multiple UAVs via Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Xueyuan Wang, M. Cenk Gursoy, Tugba Erpek, Yalin E. Sagduyu</p>
<p><strong>Summary:</strong> Unmanned aerial vehicles (UAVs) are expected to be an integral part of
wireless networks. In this paper, we aim to find collision-free paths for
multiple cellular-connected UAVs, while satisfying requirements of connectivity
with ground base stations (GBSs) in the presence of a dynamic jammer. We first
formulate the problem as a sequential decision making problem in discrete
domain, with connectivity, collision avoidance, and kinematic constraints. We,
then, propose an offline temporal differenc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.02322v1' target='_blank'>UAV Swarm Path Planning with Reinforcement Learning for Field
  prospecting</a></h2>
<p><strong>Authors:</strong> Alejandro Puente-Castro, Daniel Rivero, Alejandro Pazos, Enrique Fernandez-Blanco</p>
<p><strong>Summary:</strong> Unmanned Aerial Vehicle (UAV) swarms adoption shows a steady growth among
operators due to the benefits in time and cost arisen from their use. However,
this kind of system faces an important problem which is the calculation of many
optimal paths for each UAV. Solving this problem would allow a to control many
UAVs without human intervention at the same time while saving battery between
recharges and performing several tasks simultaneously. The main aim is to
develop a system capable of calculat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.03665v1' target='_blank'>Hierarchical Robot Navigation in Novel Environments using Rough 2-D Maps</a></h2>
<p><strong>Authors:</strong> Chengguang Xu, Christopher Amato, Lawson L. S. Wong</p>
<p><strong>Summary:</strong> In robot navigation, generalizing quickly to unseen environments is
essential. Hierarchical methods inspired by human navigation have been
proposed, typically consisting of a high-level landmark proposer and a
low-level controller. However, these methods either require precise high-level
information to be given in advance or need to construct such guidance from
extensive interaction with the environment. In this work, we propose an
approach that leverages a rough 2-D map of the environment to na...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.14405v2' target='_blank'>Habitat 2.0: Training Home Assistants to Rearrange their Habitat</a></h2>
<p><strong>Authors:</strong> Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra</p>
<p><strong>Summary:</strong> We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual
robots in interactive 3D environments and complex physics-enabled scenarios. We
make comprehensive contributions to all levels of the embodied AI stack - data,
simulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an
artist-authored, annotated, reconfigurable 3D dataset of apartments (matching
real spaces) with articulated objects (e.g. cabinets and drawers that can
open/close); (ii) H2.0: a high-per...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.15653v1' target='_blank'>Survivable Robotic Control through Guided Bayesian Policy Search with
  Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Sayyed Jaffar Ali Raza, Apan Dastider, Mingjie Lin</p>
<p><strong>Summary:</strong> Many robot manipulation skills can be represented with deterministic
characteristics and there exist efficient techniques for learning parameterized
motor plans for those skills. However, one of the active research challenge
still remains to sustain manipulation capabilities in situation of a mechanical
failure. Ideally, like biological creatures, a robotic agent should be able to
reconfigure its control policy by adapting to dynamic adversaries. In this
paper, we propose a method that allows an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.02066v2' target='_blank'>Hierarchical Object-to-Zone Graph for Object Navigation</a></h2>
<p><strong>Authors:</strong> Sixian Zhang, Xinhang Song, Yubing Bai, Weijie Li, Yakui Chu, Shuqiang Jiang</p>
<p><strong>Summary:</strong> The goal of object navigation is to reach the expected objects according to
visual information in the unseen environments. Previous works usually implement
deep models to train an agent to predict actions in real-time. However, in the
unseen environment, when the target object is not in egocentric view, the agent
may not be able to make wise decisions due to the lack of guidance. In this
paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent
in a coarse-to-fine manner, an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.04205v2' target='_blank'>DAN: Decentralized Attention-based Neural Network for the MinMax
  Multiple Traveling Salesman Problem</a></h2>
<p><strong>Authors:</strong> Yuhong Cao, Zhanhong Sun, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> The multiple traveling salesman problem (mTSP) is a well-known NP-hard
problem with numerous real-world applications. In particular, this work
addresses MinMax mTSP, where the objective is to minimize the max tour length
among all agents. Many robotic deployments require recomputing potentially
large mTSP instances frequently, making the natural trade-off between computing
time and solution quality of great importance. However, exact and heuristic
algorithms become inefficient as the number of c...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2109.13863v3' target='_blank'>A First-Occupancy Representation for Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Ted Moskovitz, Spencer R. Wilson, Maneesh Sahani</p>
<p><strong>Summary:</strong> Both animals and artificial agents benefit from state representations that
support rapid transfer of learning across tasks and which enable them to
efficiently traverse their environments to reach rewarding states. The
successor representation (SR), which measures the expected cumulative,
discounted state occupancy under a fixed policy, enables efficient transfer to
different reward structures in an otherwise constant Markovian environment and
has been hypothesized to underlie aspects of biologi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1610.03518v1' target='_blank'>Transfer from Simulation to Real World through Learning Deep Inverse
  Dynamics Model</a></h2>
<p><strong>Authors:</strong> Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter Abbeel, Wojciech Zaremba</p>
<p><strong>Summary:</strong> Developing control policies in simulation is often more practical and safer
than directly running experiments in the real world. This applies to policies
obtained from planning and optimization, and even more so to policies obtained
from reinforcement learning, which is often very data demanding. However, a
policy that succeeds in simulation often doesn't work when deployed on a real
robot. Nevertheless, often the overall gist of what the policy does in
simulation remains valid in the real world...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1810.00510v1' target='_blank'>Interactive Agent Modeling by Learning to Probe</a></h2>
<p><strong>Authors:</strong> Tianmin Shu, Caiming Xiong, Ying Nian Wu, Song-Chun Zhu</p>
<p><strong>Summary:</strong> The ability of modeling the other agents, such as understanding their
intentions and skills, is essential to an agent's interactions with other
agents. Conventional agent modeling relies on passive observation from
demonstrations. In this work, we propose an interactive agent modeling scheme
enabled by encouraging an agent to learn to probe. In particular, the probing
agent (i.e. a learner) learns to interact with the environment and with a
target agent (i.e., a demonstrator) to maximize the cha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1812.00600v1' target='_blank'>Resource Constrained Deep Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Abhinav Bhatia, Pradeep Varakantham, Akshat Kumar</p>
<p><strong>Summary:</strong> In urban environments, supply resources have to be constantly matched to the
"right" locations (where customer demand is present) so as to improve quality
of life. For instance, ambulances have to be matched to base stations regularly
so as to reduce response time for emergency incidents in EMS (Emergency
Management Systems); vehicles (cars, bikes, scooters etc.) have to be matched
to docking stations so as to reduce lost demand in shared mobility systems.
Such problem domains are challenging ow...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1904.02579v2' target='_blank'>Can a Robot Become a Movie Director? Learning Artistic Principles for
  Aerial Cinematography</a></h2>
<p><strong>Authors:</strong> Mirko Gschwindt, Efe Camci, Rogerio Bonatti, Wenshan Wang, Erdal Kayacan, Sebastian Scherer</p>
<p><strong>Summary:</strong> Aerial filming is constantly gaining importance due to the recent advances in
drone technology. It invites many intriguing, unsolved problems at the
intersection of aesthetical and scientific challenges. In this work, we propose
a deep reinforcement learning agent which supervises motion planning of a
filming drone by making desirable shot mode selections based on aesthetical
values of video shots. Unlike most of the current state-of-the-art approaches
that require explicit guidance by a human e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1907.03046v1' target='_blank'>Learning a Behavioral Repertoire from Demonstrations</a></h2>
<p><strong>Authors:</strong> Niels Justesen, Miguel Gonzalez Duque, Daniel Cabarcas Jaramillo, Jean-Baptiste Mouret, Sebastian Risi</p>
<p><strong>Summary:</strong> Imitation Learning (IL) is a machine learning approach to learn a policy from
a dataset of demonstrations. IL can be useful to kick-start learning before
applying reinforcement learning (RL) but it can also be useful on its own, e.g.
to learn to imitate human players in video games. However, a major limitation
of current IL approaches is that they learn only a single "average" policy
based on a dataset that possibly contains demonstrations of numerous different
types of behaviors. In this paper,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.07088v1' target='_blank'>Value Variance Minimization for Learning Approximate Equilibrium in
  Aggregation Systems</a></h2>
<p><strong>Authors:</strong> Tanvi Verma, Pradeep Varakantham</p>
<p><strong>Summary:</strong> For effective matching of resources (e.g., taxis, food, bikes, shopping
items) to customer demand, aggregation systems have been extremely successful.
In aggregation systems, a central entity (e.g., Uber, Food Panda, Ofo)
aggregates supply (e.g., drivers, delivery personnel) and matches demand to
supply on a continuous basis (sequential decisions). Due to the objective of
the central entity to maximize its profits, individual suppliers get sacrificed
thereby creating incentive for individuals to...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2003.13949v1' target='_blank'>Enhanced Rolling Horizon Evolution Algorithm with Opponent Model
  Learning: Results for the Fighting Game AI Competition</a></h2>
<p><strong>Authors:</strong> Zhentao Tang, Yuanheng Zhu, Dongbin Zhao, Simon M. Lucas</p>
<p><strong>Summary:</strong> The Fighting Game AI Competition (FTGAIC) provides a challenging benchmark
for 2-player video game AI. The challenge arises from the large action space,
diverse styles of characters and abilities, and the real-time nature of the
game. In this paper, we propose a novel algorithm that combines Rolling Horizon
Evolution Algorithm (RHEA) with opponent model learning. The approach is
readily applicable to any 2-player video game. In contrast to conventional
RHEA, an opponent model is proposed and is ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.02616v2' target='_blank'>The Emergence of Adversarial Communication in Multi-Agent Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Jan Blumenkamp, Amanda Prorok</p>
<p><strong>Summary:</strong> Many real-world problems require the coordination of multiple autonomous
agents. Recent work has shown the promise of Graph Neural Networks (GNNs) to
learn explicit communication strategies that enable complex multi-agent
coordination. These works use models of cooperative multi-agent systems whereby
agents strive to achieve a shared global goal. When considering agents with
self-interested local objectives, the standard design choice is to model these
as separate learning systems (albeit sharin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.04567v1' target='_blank'>Woodpecker-DL: Accelerating Deep Neural Networks via Hardware-Aware
  Multifaceted Optimizations</a></h2>
<p><strong>Authors:</strong> Yongchao Liu, Yue Jin, Yong Chen, Teng Teng, Hang Ou, Rui Zhao, Yao Zhang</p>
<p><strong>Summary:</strong> Accelerating deep model training and inference is crucial in practice.
Existing deep learning frameworks usually concentrate on optimizing training
speed and pay fewer attentions to inference-specific optimizations. Actually,
model inference differs from training in terms of computation, e.g. parameters
are refreshed each gradient update step during training, but kept invariant
during inference. These special characteristics of model inference open new
opportunities for its optimization. In this...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.06073v1' target='_blank'>Visuomotor Mechanical Search: Learning to Retrieve Target Objects in
  Clutter</a></h2>
<p><strong>Authors:</strong> Andrey Kurenkov, Joseph Taglic, Rohun Kulkarni, Marcus Dominguez-Kuhne, Animesh Garg, Roberto Mart√≠n-Mart√≠n, Silvio Savarese</p>
<p><strong>Summary:</strong> When searching for objects in cluttered environments, it is often necessary
to perform complex interactions in order to move occluding objects out of the
way and fully reveal the object of interest and make it graspable. Due to the
complexity of the physics involved and the lack of accurate models of the
clutter, planning and controlling precise predefined interactions with accurate
outcome is extremely hard, when not impossible. In problems where accurate
(forward) models are lacking, Deep Rein...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2008.08157v2' target='_blank'>Heteroscedastic Uncertainty for Robust Generative Latent Dynamics</a></h2>
<p><strong>Authors:</strong> Oliver Limoyo, Bryan Chan, Filip Mariƒá, Brandon Wagstaff, Rupam Mahmood, Jonathan Kelly</p>
<p><strong>Summary:</strong> Learning or identifying dynamics from a sequence of high-dimensional
observations is a difficult challenge in many domains, including reinforcement
learning and control. The problem has recently been studied from a generative
perspective through latent dynamics: high-dimensional observations are embedded
into a lower-dimensional space in which the dynamics can be learned. Despite
some successes, latent dynamics models have not yet been applied to real-world
robotic systems where learned represen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2009.00433v2' target='_blank'>Solving the single-track train scheduling problem via Deep Reinforcement
  Learning</a></h2>
<p><strong>Authors:</strong> Valerio Agasucci, Giorgio Grani, Leonardo Lamorgese</p>
<p><strong>Summary:</strong> Every day, railways experience disturbances and disruptions, both on the
network and the fleet side, that affect the stability of rail traffic. Induced
delays propagate through the network, which leads to a mismatch in demand and
offer for goods and passengers, and, in turn, to a loss in service quality. In
these cases, it is the duty of human traffic controllers, the so-called
dispatchers, to do their best to minimize the impact on traffic. However,
dispatchers inevitably have a limited depth o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.11722v1' target='_blank'>From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion</a></h2>
<p><strong>Authors:</strong> Deepali Jain, Atil Iscen, Ken Caluwaerts</p>
<p><strong>Summary:</strong> Legged robots navigating crowded scenes and complex terrains in the real
world are required to execute dynamic leg movements while processing visual
input for obstacle avoidance and path planning. We show that a quadruped robot
can acquire both of these skills by means of hierarchical reinforcement
learning (HRL). By virtue of their hierarchical structure, our policies learn
to implicitly break down this joint problem by concurrently learning High Level
(HL) and Low Level (LL) neural network pol...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2011.14391v2' target='_blank'>Reinforcement Learning in Nonzero-sum Linear Quadratic Deep Structured
  Games: Global Convergence of Policy Optimization</a></h2>
<p><strong>Authors:</strong> Masoud Roudneshin, Jalal Arabneydi, Amir G. Aghdam</p>
<p><strong>Summary:</strong> We study model-based and model-free policy optimization in a class of
nonzero-sum stochastic dynamic games called linear quadratic (LQ) deep
structured games. In such games, players interact with each other through a set
of weighted averages (linear regressions) of the states and actions. In this
paper, we focus our attention to homogeneous weights; however, for the special
case of infinite population, the obtained results extend to asymptotically
vanishing weights wherein the players learn the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.05449v3' target='_blank'>Adaptive Processor Frequency Adjustment for Mobile Edge Computing with
  Intermittent Energy Supply</a></h2>
<p><strong>Authors:</strong> Tiansheng Huang, Weiwei Lin, Xiaobin Hong, Xiumin Wang, Qingbo Wu, Rui Li, Ching-Hsien Hsu, Albert Y. Zomaya</p>
<p><strong>Summary:</strong> With astonishing speed, bandwidth, and scale, Mobile Edge Computing (MEC) has
played an increasingly important role in the next generation of connectivity
and service delivery. Yet, along with the massive deployment of MEC servers,
the ensuing energy issue is now on an increasingly urgent agenda. In the
current context, the large scale deployment of renewable-energy-supplied MEC
servers is perhaps the most promising solution for the incoming energy issue.
Nonetheless, as a result of the intermit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2102.09703v5' target='_blank'>Near-Optimal Randomized Exploration for Tabular Markov Decision
  Processes</a></h2>
<p><strong>Authors:</strong> Zhihan Xiong, Ruoqi Shen, Qiwen Cui, Maryam Fazel, Simon S. Du</p>
<p><strong>Summary:</strong> We study algorithms using randomized value functions for exploration in
reinforcement learning. This type of algorithms enjoys appealing empirical
performance. We show that when we use 1) a single random seed in each episode,
and 2) a Bernstein-type magnitude of noise, we obtain a worst-case
$\widetilde{O}\left(H\sqrt{SAT}\right)$ regret bound for episodic
time-inhomogeneous Markov Decision Process where $S$ is the size of state
space, $A$ is the size of action space, $H$ is the planning horizon...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.04706v1' target='_blank'>A Taxonomy of Similarity Metrics for Markov Decision Processes</a></h2>
<p><strong>Authors:</strong> √Ålvaro Vis√∫s, Javier Garc√≠a, Fernando Fern√°ndez</p>
<p><strong>Summary:</strong> Although the notion of task similarity is potentially interesting in a wide
range of areas such as curriculum learning or automated planning, it has mostly
been tied to transfer learning. Transfer is based on the idea of reusing the
knowledge acquired in the learning of a set of source tasks to a new learning
process in a target task, assuming that the target and source tasks are close
enough. In recent years, transfer learning has succeeded in making
Reinforcement Learning (RL) algorithms more ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.05225v3' target='_blank'>A Scavenger Hunt for Service Robots</a></h2>
<p><strong>Authors:</strong> Harel Yedidsion, Jennifer Suriadinata, Zifan Xu, Stefan Debruyn, Peter Stone</p>
<p><strong>Summary:</strong> Creating robots that can perform general-purpose service tasks in a
human-populated environment has been a longstanding grand challenge for AI and
Robotics research. One particularly valuable skill that is relevant to a wide
variety of tasks is the ability to locate and retrieve objects upon request.
This paper models this skill as a Scavenger Hunt (SH) game, which we formulate
as a variation of the NP-hard stochastic traveling purchaser problem. In this
problem, the goal is to find a set of obj...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.16817v1' target='_blank'>Learning Generalizable Robotic Reward Functions from "In-The-Wild" Human
  Videos</a></h2>
<p><strong>Authors:</strong> Annie S. Chen, Suraj Nair, Chelsea Finn</p>
<p><strong>Summary:</strong> We are motivated by the goal of generalist robots that can complete a wide
range of tasks across many environments. Critical to this is the robot's
ability to acquire some metric of task success or reward, which is necessary
for reinforcement learning, planning, or knowing when to ask for help. For a
general-purpose robot operating in the real world, this reward function must
also be able to generalize broadly across environments, tasks, and objects,
while depending only on on-board sensor obser...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2107.00200v4' target='_blank'>Social Coordination and Altruism in Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Behrad Toghi, Rodolfo Valiente, Dorsa Sadigh, Ramtin Pedarsani, Yaser P. Fallah</p>
<p><strong>Summary:</strong> Despite the advances in the autonomous driving domain, autonomous vehicles
(AVs) are still inefficient and limited in terms of cooperating with each other
or coordinating with vehicles operated by humans. A group of autonomous and
human-driven vehicles (HVs) which work together to optimize an altruistic
social utility -- as opposed to the egoistic individual utility -- can co-exist
seamlessly and assure safety and efficiency on the road. Achieving this mission
without explicit coordination among...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.05030v2' target='_blank'>DQ-GAT: Towards Safe and Efficient Autonomous Driving with Deep
  Q-Learning and Graph Attention Networks</a></h2>
<p><strong>Authors:</strong> Peide Cai, Hengli Wang, Yuxiang Sun, Ming Liu</p>
<p><strong>Summary:</strong> Autonomous driving in multi-agent dynamic traffic scenarios is challenging:
the behaviors of road users are uncertain and are hard to model explicitly, and
the ego-vehicle should apply complicated negotiation skills with them, such as
yielding, merging and taking turns, to achieve both safe and efficient driving
in various settings. Traditional planning methods are largely rule-based and
scale poorly in these complex dynamic scenarios, often leading to reactive or
even overly conservative behavi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2108.13680v3' target='_blank'>Learning Practically Feasible Policies for Online 3D Bin Packing</a></h2>
<p><strong>Authors:</strong> Hang Zhao, Chenyang Zhu, Xin Xu, Hui Huang, Kai Xu</p>
<p><strong>Summary:</strong> We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.02879v2' target='_blank'>Compositional Q-learning for electrolyte repletion with imbalanced
  patient sub-populations</a></h2>
<p><strong>Authors:</strong> Aishwarya Mandyam, Andrew Jones, Jiayu Yao, Krzysztof Laudanski, Barbara Engelhardt</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) is an effective framework for solving sequential
decision-making tasks. However, applying RL methods in medical care settings is
challenging in part due to heterogeneity in treatment response among patients.
Some patients can be treated with standard protocols whereas others, such as
those with chronic diseases, need personalized treatment planning. Traditional
RL methods often fail to account for this heterogeneity, because they assume
that all patients respond to th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.08847v2' target='_blank'>Provable RL with Exogenous Distractors via Multistep Inverse Dynamics</a></h2>
<p><strong>Authors:</strong> Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, John Langford</p>
<p><strong>Summary:</strong> Many real-world applications of reinforcement learning (RL) require the agent
to deal with high-dimensional observations such as those generated from a
megapixel camera. Prior work has addressed such problems with representation
learning, through which the agent can provably extract endogenous, latent state
information from raw observations and subsequently plan efficiently. However,
such approaches can fail in the presence of temporally correlated noise in the
observations, a phenomenon that is...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.15465v1' target='_blank'>A hierarchical behavior prediction framework at signalized intersections</a></h2>
<p><strong>Authors:</strong> Zhen Yang, Rusheng Zhang, Henry X. Liu</p>
<p><strong>Summary:</strong> Road user behavior prediction is one of the most critical components in
trajectory planning for autonomous driving, especially in urban scenarios
involving traffic signals. In this paper, a hierarchical framework is proposed
to predict vehicle behaviors at a signalized intersection, using the traffic
signal information of the intersection. The framework is composed of two
phases: a discrete intention prediction phase and a continuous trajectory
prediction phase. In the discrete intention predict...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2111.07775v1' target='_blank'>Learning Representations for Pixel-based Control: What Matters and Why?</a></h2>
<p><strong>Authors:</strong> Manan Tomar, Utkarsh A. Mishra, Amy Zhang, Matthew E. Taylor</p>
<p><strong>Summary:</strong> Learning representations for pixel-based control has garnered significant
attention recently in reinforcement learning. A wide range of methods have been
proposed to enable efficient learning, leading to sample complexities similar
to those in the full state setting. However, moving beyond carefully curated
pixel data sets (centered crop, appropriate lighting, clear background, etc.)
remains challenging. In this paper, we adopt a more difficult setting,
incorporating background distractors, as a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2112.04153v3' target='_blank'>Model-Value Inconsistency as a Signal for Epistemic Uncertainty</a></h2>
<p><strong>Authors:</strong> Angelos Filos, Eszter V√©rtes, Zita Marinho, Gregory Farquhar, Diana Borsa, Abram Friesen, Feryal Behbahani, Tom Schaul, Andr√© Barreto, Simon Osindero</p>
<p><strong>Summary:</strong> Using a model of the environment and a value function, an agent can construct
many estimates of a state's value, by unrolling the model for different lengths
and bootstrapping with its value function. Our key insight is that one can
treat this set of value estimates as a type of ensemble, which we call an
\emph{implicit value ensemble} (IVE). Consequently, the discrepancy between
these estimates can be used as a proxy for the agent's epistemic uncertainty;
we term this signal \emph{model-value i...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.01529v1' target='_blank'>Supervised Permutation Invariant Networks for Solving the CVRP with
  Bounded Fleet Size</a></h2>
<p><strong>Authors:</strong> Daniela Thyssens, Jonas Falkner, Lars Schmidt-Thieme</p>
<p><strong>Summary:</strong> Learning to solve combinatorial optimization problems, such as the vehicle
routing problem, offers great computational advantages over classical
operations research solvers and heuristics. The recently developed deep
reinforcement learning approaches either improve an initially given solution
iteratively or sequentially construct a set of individual tours. However, most
of the existing learning-based approaches are not able to work for a fixed
number of vehicles and thus bypass the complex assig...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.06216v1' target='_blank'>Learning to Reformulate for Linear Programming</a></h2>
<p><strong>Authors:</strong> Xijun Li, Qingyu Qu, Fangzhou Zhu, Jia Zeng, Mingxuan Yuan, Kun Mao, Jie Wang</p>
<p><strong>Summary:</strong> It has been verified that the linear programming (LP) is able to formulate
many real-life optimization problems, which can obtain the optimum by resorting
to corresponding solvers such as OptVerse, Gurobi and CPLEX. In the past
decades, a serial of traditional operation research algorithms have been
proposed to obtain the optimum of a given LP in a fewer solving time. Recently,
there is a trend of using machine learning (ML) techniques to improve the
performance of above solvers. However, almost...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.10041v2' target='_blank'>Migration of self-propelling agent in a turbulent environment with
  minimal energy consumption</a></h2>
<p><strong>Authors:</strong> Ao Xu, Hua-Lin Wu, Heng-Dong Xi</p>
<p><strong>Summary:</strong> We present a numerical study of training a self-propelling agent to migrate
in the unsteady flow environment. We control the agent to utilize the
background flow structure by adopting the reinforcement learning algorithm to
minimize energy consumption. We considered the agent migrating in two types of
flows: one is simple periodical double-gyre flow as a proof-of-concept example,
while the other is complex turbulent Rayleigh-B\'enard convection as a paradigm
for migrating in the convective atmos...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.01266v2' target='_blank'>CIRS: Bursting Filter Bubbles by Counterfactual Interactive Recommender
  System</a></h2>
<p><strong>Authors:</strong> Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, Peng Jiang</p>
<p><strong>Summary:</strong> While personalization increases the utility of recommender systems, it also
brings the issue of filter bubbles. E.g., if the system keeps exposing and
recommending the items that the user is interested in, it may also make the
user feel bored and less satisfied. Existing work studies filter bubbles in
static recommendation, where the effect of overexposure is hard to capture. In
contrast, we believe it is more meaningful to study the issue in interactive
recommendation and optimize long-term use...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.05509v1' target='_blank'>Learning Design and Construction with Varying-Sized Materials via
  Prioritized Memory Resets</a></h2>
<p><strong>Authors:</strong> Yunfei Li, Tao Kong, Lei Li, Yi Wu</p>
<p><strong>Summary:</strong> Can a robot autonomously learn to design and construct a bridge from
varying-sized blocks without a blueprint? It is a challenging task with long
horizon and sparse reward -- the robot has to figure out physically stable
design schemes and feasible actions to manipulate and transport blocks. Due to
diverse block sizes, the state space and action trajectories are vast to
explore. In this paper, we propose a hierarchical approach for this problem. It
consists of a reinforcement-learning designer t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.13070v3' target='_blank'>Hierarchical Control for Cooperative Teams in Competitive Autonomous
  Racing</a></h2>
<p><strong>Authors:</strong> Rishabh Saumil Thakkar, Aryaman Singh Samyal, David Fridovich-Keil, Zhe Xu, Ufuk Topcu</p>
<p><strong>Summary:</strong> We investigate the problem of autonomous racing among teams of cooperative
agents that are subject to realistic racing rules. Our work extends previous
research on hierarchical control in head-to-head autonomous racing by
considering a generalized version of the problem while maintaining the
two-level hierarchical control structure. A high-level tactical planner
constructs a discrete game that encodes the complex rules using simplified
dynamics to produce a sequence of target waypoints. The low-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.13307v3' target='_blank'>AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at
  Test Time</a></h2>
<p><strong>Authors:</strong> Johannes Scheiermann, Wolfgang Konen</p>
<p><strong>Summary:</strong> Recently, the seminal algorithms AlphaGo and AlphaZero have started a new era
in game learning and deep reinforcement learning. While the achievements of
AlphaGo and AlphaZero - playing Go and other complex games at super human level
- are truly impressive, these architectures have the drawback that they require
high computational resources. Many researchers are looking for methods that are
similar to AlphaZero, but have lower computational demands and are thus more
easily reproducible.
  In thi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.11291v2' target='_blank'>Cooperative Reinforcement Learning on Traffic Signal Control</a></h2>
<p><strong>Authors:</strong> Chi-Chun Chao, Jun-Wei Hsieh, Bor-Shiun Wang</p>
<p><strong>Summary:</strong> Traffic signal control is a challenging real-world problem aiming to minimize
overall travel time by coordinating vehicle movements at road intersections.
Existing traffic signal control systems in use still rely heavily on
oversimplified information and rule-based methods. Specifically, the
periodicity of green/red light alternations can be considered as a prior for
better planning of each agent in policy optimization. To better learn such
adaptive and predictive priors, traditional
  RL-based ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.14313v3' target='_blank'>Learning to Use Chopsticks in Diverse Gripping Styles</a></h2>
<p><strong>Authors:</strong> Zeshi Yang, KangKang Yin, Libin Liu</p>
<p><strong>Summary:</strong> Learning dexterous manipulation skills is a long-standing challenge in
computer graphics and robotics, especially when the task involves complex and
delicate interactions between the hands, tools and objects. In this paper, we
focus on chopsticks-based object relocation tasks, which are common yet
demanding. The key to successful chopsticks skills is steady gripping of the
sticks that also supports delicate maneuvers. We automatically discover
physically valid chopsticks holding poses by Bayesia...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.14665v1' target='_blank'>Multi-Domain Virtual Network Embedding Algorithm based on Horizontal
  Federated Learning</a></h2>
<p><strong>Authors:</strong> Peiying Zhang, Ning Chen, Shibao Li, Kim-Kwang Raymond Choo, Chunxiao Jiang</p>
<p><strong>Summary:</strong> Network Virtualization (NV) is an emerging network dynamic planning technique
to overcome network rigidity. As its necessary challenge, Virtual Network
Embedding (VNE) enhances the scalability and flexibility of the network by
decoupling the resources and services of the underlying physical network. For
the future multi-domain physical network modeling with the characteristics of
dynamics, heterogeneity, privacy, and real-time, the existing related works
perform satisfactorily. Federated learnin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.00059v1' target='_blank'>A Mixture-of-Expert Approach to RL-based Dialogue Management</a></h2>
<p><strong>Authors:</strong> Yinlam Chow, Aza Tulepbergenov, Ofir Nachum, MoonKyung Ryu, Mohammad Ghavamzadeh, Craig Boutilier</p>
<p><strong>Summary:</strong> Despite recent advancements in language models (LMs), their application to
dialogue management (DM) problems and ability to carry on rich conversations
remain a challenge. We use reinforcement learning (RL) to develop a dialogue
agent that avoids being short-sighted (outputting generic utterances) and
maximizes overall user satisfaction. Most existing RL approaches to DM train
the agent at the word-level, and thus, have to deal with a combinatorially
complex action space even for a medium-size v...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2206.10544v1' target='_blank'>Multi-UAV Planning for Cooperative Wildfire Coverage and Tracking with
  Quality-of-Service Guarantees</a></h2>
<p><strong>Authors:</strong> Esmaeil Seraj, Andrew Silva, Matthew Gombolay</p>
<p><strong>Summary:</strong> In recent years, teams of robot and Unmanned Aerial Vehicles (UAVs) have been
commissioned by researchers to enable accurate, online wildfire coverage and
tracking. While the majority of prior work focuses on the coordination and
control of such multi-robot systems, to date, these UAV teams have not been
given the ability to reason about a fire's track (i.e., location and
propagation dynamics) to provide performance guarantee over a time horizon.
Motivated by the problem of aerial wildfire monit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.01613v2' target='_blank'>Doubly-Asynchronous Value Iteration: Making Value Iteration Asynchronous
  in Actions</a></h2>
<p><strong>Authors:</strong> Tian Tian, Kenny Young, Richard S. Sutton</p>
<p><strong>Summary:</strong> Value iteration (VI) is a foundational dynamic programming method, important
for learning and planning in optimal control and reinforcement learning. VI
proceeds in batches, where the update to the value of each state must be
completed before the next batch of updates can begin. Completing a single batch
is prohibitively expensive if the state space is large, rendering VI
impractical for many applications. Asynchronous VI helps to address the large
state space problem by updating one state at a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.03483v1' target='_blank'>Finding Fallen Objects Via Asynchronous Audio-Visual Integration</a></h2>
<p><strong>Authors:</strong> Chuang Gan, Yi Gu, Siyuan Zhou, Jeremy Schwartz, Seth Alter, James Traer, Dan Gutfreund, Joshua B. Tenenbaum, Josh McDermott, Antonio Torralba</p>
<p><strong>Summary:</strong> The way an object looks and sounds provide complementary reflections of its
physical properties. In many settings cues from vision and audition arrive
asynchronously but must be integrated, as when we hear an object dropped on the
floor and then must find it. In this paper, we introduce a setting in which to
study multi-modal object localization in 3D virtual environments. An object is
dropped somewhere in a room. An embodied robot agent, equipped with a camera
and microphone, must determine wha...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.08655v1' target='_blank'>An Enhanced Graph Representation for Machine Learning Based Automatic
  Intersection Management</a></h2>
<p><strong>Authors:</strong> Marvin Klimke, Jasper Gerigk, Benjamin V√∂lz, Michael Buchholz</p>
<p><strong>Summary:</strong> The improvement of traffic efficiency at urban intersections receives strong
research interest in the field of automated intersection management. So far,
mostly non-learning algorithms like reservation or optimization-based ones were
proposed to solve the underlying multi-agent planning problem. At the same
time, automated driving functions for a single ego vehicle are increasingly
implemented using machine learning methods. In this work, we build upon a
previously presented graph-based scene re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2207.12267v1' target='_blank'>Continuous ErrP detections during multimodal human-robot interaction</a></h2>
<p><strong>Authors:</strong> Su Kyoung Kim, Michael Maurus, Mathias Trampler, Marc Tabie, Elsa Andrea Kirchner</p>
<p><strong>Summary:</strong> Human-in-the-loop approaches are of great importance for robot applications.
In the presented study, we implemented a multimodal human-robot interaction
(HRI) scenario, in which a simulated robot communicates with its human partner
through speech and gestures. The robot announces its intention verbally and
selects the appropriate action using pointing gestures. The human partner, in
turn, evaluates whether the robot's verbal announcement (intention) matches the
action (pointing gesture) chosen b...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.00579v1' target='_blank'>Neural Approaches to Co-Optimization in Robotics</a></h2>
<p><strong>Authors:</strong> Charles Schaff</p>
<p><strong>Summary:</strong> Robots and intelligent systems that sense or interact with the world are
increasingly being used to automate a wide array of tasks. The ability of these
systems to complete these tasks depends on a large range of technologies such
as the mechanical and electrical parts that make up the physical body of the
robot and its sensors, perception algorithms to perceive the environment, and
planning and control algorithms to produce meaningful actions. Therefore, it is
often necessary to consider the in...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.12827v1' target='_blank'>Advanced Skills by Learning Locomotion and Local Navigation End-to-End</a></h2>
<p><strong>Authors:</strong> Nikita Rudin, David Hoeller, Marko Bjelonic, Marco Hutter</p>
<p><strong>Summary:</strong> The common approach for local navigation on challenging environments with
legged robots requires path planning, path following and locomotion, which
usually requires a locomotion control policy that accurately tracks a commanded
velocity. However, by breaking down the navigation problem into these
sub-tasks, we limit the robot's capabilities since the individual tasks do not
consider the full solution space. In this work, we propose to solve the
complete problem by training an end-to-end policy ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.09026v1' target='_blank'>WILD-SCAV: Benchmarking FPS Gaming AI on Unity3D-based Environments</a></h2>
<p><strong>Authors:</strong> Xi Chen, Tianyu Shi, Qingpeng Zhao, Yuchen Sun, Yunfei Gao, Xiangjun Wang</p>
<p><strong>Summary:</strong> Recent advances in deep reinforcement learning (RL) have demonstrated complex
decision-making capabilities in simulation environments such as Arcade Learning
Environment, MuJoCo, and ViZDoom. However, they are hardly extensible to more
complicated problems, mainly due to the lack of complexity and variations in
the environments they are trained and tested on. Furthermore, they are not
extensible to an open-world environment to facilitate long-term exploration
research. To learn realistic task-so...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.05346v1' target='_blank'>RARE: Renewable Energy Aware Resource Management in Datacenters</a></h2>
<p><strong>Authors:</strong> Vanamala Venkataswamy, Jake Grigsby, Andrew Grimshaw, Yanjun Qi</p>
<p><strong>Summary:</strong> The exponential growth in demand for digital services drives massive
datacenter energy consumption and negative environmental impacts. Promoting
sustainable solutions to pressing energy and digital infrastructure challenges
is crucial. Several hyperscale cloud providers have announced plans to power
their datacenters using renewable energy. However, integrating renewables to
power the datacenters is challenging because the power generation is
intermittent, necessitating approaches to tackle powe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.07638v1' target='_blank'>Legged Locomotion in Challenging Terrains using Egocentric Vision</a></h2>
<p><strong>Authors:</strong> Ananye Agarwal, Ashish Kumar, Jitendra Malik, Deepak Pathak</p>
<p><strong>Summary:</strong> Animals are capable of precise and agile locomotion using vision. Replicating
this ability has been a long-standing goal in robotics. The traditional
approach has been to decompose this problem into elevation mapping and foothold
planning phases. The elevation mapping, however, is susceptible to failure and
large noise artifacts, requires specialized hardware, and is biologically
implausible. In this paper, we present the first end-to-end locomotion system
capable of traversing stairs, curbs, st...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.01691v2' target='_blank'>XTENTH-CAR: A Proportionally Scaled Experimental Vehicle Platform for
  Connected Autonomy and All-Terrain Research</a></h2>
<p><strong>Authors:</strong> Shathushan Sivashangaran, Azim Eskandarian</p>
<p><strong>Summary:</strong> Connected Autonomous Vehicles (CAVs) are key components of the Intelligent
Transportation System (ITS), and all-terrain Autonomous Ground Vehicles (AGVs)
are indispensable tools for a wide range of applications such as disaster
response, automated mining, agriculture, military operations, search and rescue
missions, and planetary exploration. Experimental validation is a requisite for
CAV and AGV research, but requires a large, safe experimental environment when
using full-size vehicles which is...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.04280v1' target='_blank'>Model-based trajectory stitching for improved behavioural cloning and
  its applications</a></h2>
<p><strong>Authors:</strong> Charles A. Hepburn, Giovanni Montana</p>
<p><strong>Summary:</strong> Behavioural cloning (BC) is a commonly used imitation learning method to
infer a sequential decision-making policy from expert demonstrations. However,
when the quality of the data is not optimal, the resulting behavioural policy
also performs sub-optimally once deployed. Recently, there has been a surge in
offline reinforcement learning methods that hold the promise to extract
high-quality policies from sub-optimal historical data. A common approach is to
perform regularisation during training,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2212.05241v5' target='_blank'>AutoDRIVE: A Comprehensive, Flexible and Integrated Digital Twin
  Ecosystem for Enhancing Autonomous Driving Research and Education</a></h2>
<p><strong>Authors:</strong> Tanmay Vilas Samak, Chinmay Vilas Samak, Sivanathan Kandhasamy, Venkat Krovi, Ming Xie</p>
<p><strong>Summary:</strong> Prototyping and validating hardware-software components, sub-systems and
systems within the intelligent transportation system-of-systems framework
requires a modular yet flexible and open-access ecosystem. This work presents
our attempt towards developing such a comprehensive research and education
ecosystem, called AutoDRIVE, for synergistically prototyping, simulating and
deploying cyber-physical solutions pertaining to autonomous driving as well as
smart city management. AutoDRIVE features bo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.03423v1' target='_blank'>Multi-UAV Path Learning for Age and Power Optimization in IoT with UAV
  Battery Recharge</a></h2>
<p><strong>Authors:</strong> Eslam Eldeeb, Jean Michel de Souza Sant'Ana, Dian Echevarr√≠a P√©rez, Mohammad Shehab, Nurul Huda Mahmood, Hirley Alves</p>
<p><strong>Summary:</strong> In many emerging Internet of Things (IoT) applications, the freshness of the
is an important design criterion. Age of Information (AoI) quantifies the
freshness of the received information or status update. This work considers a
setup of deployed IoT devices in an IoT network; multiple unmanned aerial
vehicles (UAVs) serve as mobile relay nodes between the sensors and the base
station. We formulate an optimization problem to jointly plan the UAVs'
trajectory, while minimizing the AoI of the rece...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.05028v1' target='_blank'>MotorFactory: A Blender Add-on for Large Dataset Generation of Small
  Electric Motors</a></h2>
<p><strong>Authors:</strong> Chengzhi Wu, Kanran Zhou, Jan-Philipp Kaiser, Norbert Mitschke, Jan-Felix Klein, Julius Pfrommer, J√ºrgen Beyerer, Gisela Lanza, Michael Heizmann, Kai Furmans</p>
<p><strong>Summary:</strong> To enable automatic disassembly of different product types with uncertain
conditions and degrees of wear in remanufacturing, agile production systems
that can adapt dynamically to changing requirements are needed. Machine
learning algorithms can be employed due to their generalization capabilities of
learning from various types and variants of products. However, in reality,
datasets with a diversity of samples that can be used to train models are
difficult to obtain in the initial period. This m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.11461v2' target='_blank'>Learning to Generate All Feasible Actions</a></h2>
<p><strong>Authors:</strong> Mirco Theile, Daniele Bernardini, Raphael Trumpp, Cristina Piazza, Marco Caccamo, Alberto L. Sangiovanni-Vincentelli</p>
<p><strong>Summary:</strong> Modern cyber-physical systems are becoming increasingly complex to model,
thus motivating data-driven techniques such as reinforcement learning (RL) to
find appropriate control agents. However, most systems are subject to hard
constraints such as safety or operational bounds. Typically, to learn to
satisfy these constraints, the agent must violate them systematically, which is
computationally prohibitive in most systems. Recent efforts aim to utilize
feasibility models that assess whether a prop...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.12050v2' target='_blank'>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making
  using Language Guided World Modelling</a></h2>
<p><strong>Authors:</strong> Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) agents typically learn tabula rasa, without prior
knowledge of the world. However, if initialized with knowledge of high-level
subgoals and transitions between subgoals, RL agents could utilize this
Abstract World Model (AWM) for planning and exploration. We propose using
few-shot large language models (LLMs) to hypothesize an AWM, that will be
verified through world experience, to improve sample efficiency of RL agents.
Our DECKARD agent applies LLM-guided exploratio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.08463v3' target='_blank'>Dynamic Grasping with a Learned Meta-Controller</a></h2>
<p><strong>Authors:</strong> Yinsen Jia, Jingxi Xu, Dinesh Jayaraman, Shuran Song</p>
<p><strong>Summary:</strong> Grasping moving objects is a challenging task that requires multiple
submodules such as object pose predictor, arm motion planner, etc. Each
submodule operates under its own set of meta-parameters. For example, how far
the pose predictor should look into the future (i.e., look-ahead time) and the
maximum amount of time the motion planner can spend planning a motion (i.e.,
time budget). Many previous works assign fixed values to these parameters;
however, at different moments within a single epis...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.11233v2' target='_blank'>Learning Agile Flights through Narrow Gaps with Varying Angles using
  Onboard Sensing</a></h2>
<p><strong>Authors:</strong> Yuhan Xie, Minghao Lu, Rui Peng, Peng Lu</p>
<p><strong>Summary:</strong> This paper addresses the problem of traversing through unknown, tilted, and
narrow gaps for quadrotors using Deep Reinforcement Learning (DRL). Previous
learning-based methods relied on accurate knowledge of the environment,
including the gap's pose and size. In contrast, we integrate onboard sensing
and detect the gap from a single onboard camera. The training problem is
challenging for two reasons: a precise and robust whole-body planning and
control policy is required for variable-tilted and ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.02267v1' target='_blank'>Towards Safety Assured End-to-End Vision-Based Control for Autonomous
  Racing</a></h2>
<p><strong>Authors:</strong> Dvij Kalaria, Qin Lin, John M. Dolan</p>
<p><strong>Summary:</strong> Autonomous car racing is a challenging task, as it requires precise
applications of control while the vehicle is operating at cornering speeds.
Traditional autonomous pipelines require accurate pre-mapping, localization,
and planning which make the task computationally expensive and
environment-dependent. Recent works propose use of imitation and reinforcement
learning to train end-to-end deep neural networks and have shown promising
results for high-speed racing. However, the end-to-end models ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.03365v1' target='_blank'>Efficient Skill Acquisition for Complex Manipulation Tasks in Obstructed
  Environments</a></h2>
<p><strong>Authors:</strong> Jun Yamada, Jack Collins, Ingmar Posner</p>
<p><strong>Summary:</strong> Data efficiency in robotic skill acquisition is crucial for operating robots
in varied small-batch assembly settings. To operate in such environments,
robots must have robust obstacle avoidance and versatile goal conditioning
acquired from only a few simple demonstrations. Existing approaches, however,
fall short of these requirements. Deep reinforcement learning (RL) enables a
robot to learn complex manipulation tasks but is often limited to small task
spaces in the real world due to sample ine...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.04116v2' target='_blank'>TrafficBots: Towards World Models for Autonomous Driving Simulation and
  Motion Prediction</a></h2>
<p><strong>Authors:</strong> Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, Luc Van Gool</p>
<p><strong>Summary:</strong> Data-driven simulation has become a favorable way to train and test
autonomous driving algorithms. The idea of replacing the actual environment
with a learned simulator has also been explored in model-based reinforcement
learning in the context of world models. In this work, we show data-driven
traffic simulation can be formulated as a world model. We present TrafficBots,
a multi-agent policy built upon motion prediction and end-to-end driving, and
based on TrafficBots we obtain a world model ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.04129v1' target='_blank'>Foundation Models for Decision Making: Problems, Methods, and
  Opportunities</a></h2>
<p><strong>Authors:</strong> Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, Dale Schuurmans</p>
<p><strong>Summary:</strong> Foundation models pretrained on diverse data at scale have demonstrated
extraordinary capabilities in a wide range of vision and language tasks. When
such models are deployed in real world environments, they inevitably interface
with other entities and agents. For example, language models are often used to
interact with human beings through dialogue, and visual perception models are
used to autonomously navigate neighborhood streets. In response to these
developments, new paradigms are emerging ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.04651v3' target='_blank'>MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder</a></h2>
<p><strong>Authors:</strong> Guoliang He, Zak Singh, Eiko Yoneki</p>
<p><strong>Summary:</strong> Rewrite systems [6, 10, 12] have been widely employing equality saturation
[9], which is an optimisation methodology that uses a saturated e-graph to
represent all possible sequences of rewrite simultaneously, and then extracts
the optimal one. As such, optimal results can be achieved by avoiding the
phase-ordering problem. However, we observe that when the e-graph is not
saturated, it cannot represent all possible rewrite opportunities and therefore
the phase-ordering problem is re-introduced d...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.06350v1' target='_blank'>Spatio-Temporal Attention Network for Persistent Monitoring of Multiple
  Mobile Targets</a></h2>
<p><strong>Authors:</strong> Yizhuo Wang, Yutong Wang, Yuhong Cao, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> This work focuses on the persistent monitoring problem, where a set of
targets moving based on an unknown model must be monitored by an autonomous
mobile robot with a limited sensing range. To keep each target's position
estimate as accurate as possible, the robot needs to adaptively plan its path
to (re-)visit all the targets and update its belief from measurements collected
along the way. In doing so, the main challenge is to strike a balance between
exploitation, i.e., re-visiting previously-...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.06531v2' target='_blank'>Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous
  Cleaning</a></h2>
<p><strong>Authors:</strong> Yabin Wang, Xiaopeng Hong, Zhiheng Ma, Tiedong Ma, Baoxing Qin, Zhou Su</p>
<p><strong>Summary:</strong> Task allocation plays a vital role in multi-robot autonomous cleaning
systems, where multiple robots work together to clean a large area. However,
most current studies mainly focus on deterministic, single-task allocation for
cleaning robots, without considering hybrid tasks in uncertain working
environments. Moreover, there is a lack of datasets and benchmarks for relevant
research. In this paper, to address these problems, we formulate multi-robot
hybrid-task allocation under the uncertain cle...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.08933v2' target='_blank'>Efficient Planning of Multi-Robot Collective Transport using Graph
  Reinforcement Learning with Higher Order Topological Abstraction</a></h2>
<p><strong>Authors:</strong> Steve Paul, Wenyuan Li, Brian Smyth, Yuzhou Chen, Yulia Gel, Souma Chowdhury</p>
<p><strong>Summary:</strong> Efficient multi-robot task allocation (MRTA) is fundamental to various
time-sensitive applications such as disaster response, warehouse operations,
and construction. This paper tackles a particular class of these problems that
we call MRTA-collective transport or MRTA-CT -- here tasks present varying
workloads and deadlines, and robots are subject to flight range, communication
range, and payload constraints. For large instances of these problems involving
100s-1000's of tasks and 10s-100s of ro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.00789v1' target='_blank'>Combinatorial Optimization enriched Machine Learning to solve the
  Dynamic Vehicle Routing Problem with Time Windows</a></h2>
<p><strong>Authors:</strong> L√©o Baty, Kai Jungel, Patrick S. Klein, Axel Parmentier, Maximilian Schiffer</p>
<p><strong>Summary:</strong> With the rise of e-commerce and increasing customer requirements, logistics
service providers face a new complexity in their daily planning, mainly due to
efficiently handling same day deliveries. Existing multi-stage stochastic
optimization approaches that allow to solve the underlying dynamic vehicle
routing problem are either computationally too expensive for an application in
online settings, or -- in the case of reinforcement learning -- struggle to
perform well on high-dimensional combinat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2304.05703v1' target='_blank'>Human-Robot Skill Transfer with Enhanced Compliance via Dynamic Movement
  Primitives</a></h2>
<p><strong>Authors:</strong> Jayden Hong, Zengjie Zhang, Amir M. Soufi Enayati, Homayoun Najjaran</p>
<p><strong>Summary:</strong> Finding an efficient way to adapt robot trajectory is a priority to improve
overall performance of robots. One approach for trajectory planning is through
transferring human-like skills to robots by Learning from Demonstrations (LfD).
The human demonstration is considered the target motion to mimic. However,
human motion is typically optimal for human embodiment but not for robots
because of the differences between human biomechanics and robot dynamics. The
Dynamic Movement Primitives (DMP) fram...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.00780v2' target='_blank'>AI-based Radio and Computing Resource Allocation and Path Planning in
  NOMA NTNs: AoI Minimization under CSI Uncertainty</a></h2>
<p><strong>Authors:</strong> Maryam Ansarifard, Nader Mokari, Mohammadreza Javan, Hamid Saeedi, Eduard A. Jorswieck</p>
<p><strong>Summary:</strong> In this paper, we develop a hierarchical aerial computing framework composed
of high altitude platform (HAP) and unmanned aerial vehicles (UAVs) to compute
the fully offloaded tasks of terrestrial mobile users which are connected
through an uplink non-orthogonal multiple access (UL-NOMA). To better assess
the freshness of information in computation-intensive applications the
criterion of age of information (AoI) is considered. In particular, the problem
is formulated to minimize the average AoI ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.06141v5' target='_blank'>Active Semantic Localization with Graph Neural Embedding</a></h2>
<p><strong>Authors:</strong> Mitsuki Yoshida, Kanji Tanaka, Ryogo Yamamoto, Daiki Iwata</p>
<p><strong>Summary:</strong> Semantic localization, i.e., robot self-localization with semantic image
modality, is critical in recently emerging embodied AI applications (e.g.,
point-goal navigation, object-goal navigation, vision language navigation) and
topological mapping applications (e.g., graph neural SLAM, ego-centric
topological map). However, most existing works on semantic localization focus
on passive vision tasks without viewpoint planning, or rely on additional rich
modalities (e.g., depth measurements). Thus, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.09901v2' target='_blank'>On the Difficulty of Intersection Checking with Polynomial Zonotopes</a></h2>
<p><strong>Authors:</strong> Yushen Huang, Ertai Luo, Stanley Bak, Yifan Sun</p>
<p><strong>Summary:</strong> Polynomial zonotopes, a non-convex set representation, have a wide range of
applications from real-time motion planning and control in robotics, to
reachability analysis of nonlinear systems and safety shielding in
reinforcement learning. Despite this widespread use, a frequently overlooked
difficulty associated with polynomial zonotopes is intersection checking.
Determining whether the reachable set, represented as a polynomial zonotope,
intersects an unsafe set is not straightforward. In fact,...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.11854v4' target='_blank'>Multimodal Web Navigation with Instruction-Finetuned Foundation Models</a></h2>
<p><strong>Authors:</strong> Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, Izzeddin Gur</p>
<p><strong>Summary:</strong> The progress of autonomous web navigation has been hindered by the dependence
on billions of exploratory interactions via online reinforcement learning, and
domain-specific model designs that make it difficult to leverage generalization
from rich out-of-domain data. In this work, we study data-driven offline
training for web agents with vision-language foundation models. We propose an
instruction-following multimodal agent, WebGUM, that observes both webpage
screenshots and HTML pages and output...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.12821v1' target='_blank'>FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon
  Complex Manipulation</a></h2>
<p><strong>Authors:</strong> Minho Heo, Youngwoon Lee, Doohyun Lee, Joseph J. Lim</p>
<p><strong>Summary:</strong> Reinforcement learning (RL), imitation learning (IL), and task and motion
planning (TAMP) have demonstrated impressive performance across various robotic
manipulation tasks. However, these approaches have been limited to learning
simple behaviors in current real-world manipulation benchmarks, such as pushing
or pick-and-place. To enable more complex, long-horizon behaviors of an
autonomous robot, we propose to focus on real-world furniture assembly, a
complex, long-horizon robot manipulation tas...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.17018v2' target='_blank'>Formal Modelling for Multi-Robot Systems Under Uncertainty</a></h2>
<p><strong>Authors:</strong> Charlie Street, Masoumeh Mansouri, Bruno Lacerda</p>
<p><strong>Summary:</strong> Purpose of Review: To effectively synthesise and analyse multi-robot
behaviour, we require formal task-level models which accurately capture
multi-robot execution. In this paper, we review modelling formalisms for
multi-robot systems under uncertainty, and discuss how they can be used for
planning, reinforcement learning, model checking, and simulation.
  Recent Findings: Recent work has investigated models which more accurately
capture multi-robot execution by considering different forms of unc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.05696v1' target='_blank'>Embodied Executable Policy Learning with Language-based Scene
  Summarization</a></h2>
<p><strong>Authors:</strong> Jielin Qiu, Mengdi Xu, William Han, Seungwhan Moon, Ding Zhao</p>
<p><strong>Summary:</strong> Large Language models (LLMs) have shown remarkable success in assisting robot
learning tasks, i.e., complex household planning. However, the performance of
pretrained LLMs heavily relies on domain-specific templated text data, which
may be infeasible in real-world robot learning tasks with image-based
observations. Moreover, existing LLMs with text inputs lack the capability to
evolve with non-expert interactions with environments. In this work, we
introduce a novel learning paradigm that genera...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.12371v2' target='_blank'>Optimistic Active Exploration of Dynamical Systems</a></h2>
<p><strong>Authors:</strong> Bhavya Sukhija, Lenart Treven, Cansu Sancaktar, Sebastian Blaes, Stelian Coros, Andreas Krause</p>
<p><strong>Summary:</strong> Reinforcement learning algorithms commonly seek to optimize policies for
solving one particular task. How should we explore an unknown dynamical system
such that the estimated model globally approximates the dynamics and allows us
to solve multiple downstream tasks in a zero-shot manner? In this paper, we
address this challenge, by developing an algorithm -- OPAX -- for active
exploration. OPAX uses well-calibrated probabilistic models to quantify the
epistemic uncertainty about the unknown dyna...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.13333v2' target='_blank'>Energy Optimization for HVAC Systems in Multi-VAV Open Offices: A Deep
  Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Hao Wang, Xiwen Chen, Natan Vital, Edward. Duffy, Abolfazl Razi</p>
<p><strong>Summary:</strong> With more than 32% of the global energy used by commercial and residential
buildings, there is an urgent need to revisit traditional approaches to
Building Energy Management (BEM). With HVAC systems accounting for about 40% of
the total energy cost in the commercial sector, we propose a low-complexity
DRL-based model with multi-input multi-output architecture for the HVAC energy
optimization of open-plan offices, which uses only a handful of controllable
and accessible factors. The efficacy of o...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.03061v1' target='_blank'>Learning Constrained Corner Node Trajectories of a Tether Net System for
  Space Debris Capture</a></h2>
<p><strong>Authors:</strong> Feng Liu, Achira Boonrath, Prajit KrisshnaKumar, Elenora M. Botta, Souma Chowdhury</p>
<p><strong>Summary:</strong> The earth's orbit is becoming increasingly crowded with debris that poses
significant safety risks to the operation of existing and new spacecraft and
satellites. The active tether-net system, which consists of a flexible net with
maneuverable corner nodes launched from a small autonomous spacecraft, is a
promising solution for capturing and disposing of such space debris. The
requirement of autonomous operation and the need to generalize over scenarios
with debris scenarios in different rotatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.08962v2' target='_blank'>REX: Rapid Exploration and eXploitation for AI Agents</a></h2>
<p><strong>Authors:</strong> Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese</p>
<p><strong>Summary:</strong> In this paper, we propose an enhanced approach for Rapid Exploration and
eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have
inherent limitations, such as a heavy reliance on precise descriptions for
decision-making, and the lack of a systematic approach to leverage try-and-fail
procedures akin to traditional Reinforcement Learning (RL). REX introduces an
additional layer of rewards and integrates concepts similar to Upper Confidence
Bound (UCB) scores, leading to more ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2308.12086v2' target='_blank'>Out of the Cage: How Stochastic Parrots Win in Cyber Security
  Environments</a></h2>
<p><strong>Authors:</strong> Maria Rigaki, Ond≈ôej Luk√°≈°, Carlos A. Catania, Sebastian Garcia</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) have gained widespread popularity across diverse
domains involving text generation, summarization, and various natural language
processing tasks. Despite their inherent limitations, LLM-based designs have
shown promising capabilities in planning and navigating open-world scenarios.
This paper introduces a novel application of pre-trained LLMs as agents within
cybersecurity network environments, focusing on their utility for sequential
decision-making processes.
  We ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.05131v1' target='_blank'>Signal Temporal Logic Neural Predictive Control</a></h2>
<p><strong>Authors:</strong> Yue Meng, Chuchu Fan</p>
<p><strong>Summary:</strong> Ensuring safety and meeting temporal specifications are critical challenges
for long-term robotic tasks. Signal temporal logic (STL) has been widely used
to systematically and rigorously specify these requirements. However,
traditional methods of finding the control policy under those STL requirements
are computationally complex and not scalable to high-dimensional or systems
with complex nonlinear dynamics. Reinforcement learning (RL) methods can learn
the policy to satisfy the STL specificatio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.08297v1' target='_blank'>Optimal Mobility and Communication Strategy to Maximize the Value of
  Information in IoT Networks</a></h2>
<p><strong>Authors:</strong> Zijing Wang, Mihai-Alin Badiu, Justin P. Coon</p>
<p><strong>Summary:</strong> The Internet of Things (IoT) is an emerging next-generation technology in the
fourth industrial revolution. In industrial IoT networks, sensing devices are
largely deployed to monitor various types of physical processes. They are
required to transmit the collected data in a timely manner to support real-time
monitoring, control and automation. The timeliness of information is very
important in such systems. Recently, an information-theoretic metric named the
"value of information" (VoI) has been...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.11359v3' target='_blank'>Prompt, Plan, Perform: LLM-based Humanoid Control via Quantized
  Imitation Learning</a></h2>
<p><strong>Authors:</strong> Jingkai Sun, Qiang Zhang, Yiqun Duan, Xiaoyang Jiang, Chong Cheng, Renjing Xu</p>
<p><strong>Summary:</strong> In recent years, reinforcement learning and imitation learning have shown
great potential for controlling humanoid robots' motion. However, these methods
typically create simulation environments and rewards for specific tasks,
resulting in the requirements of multiple policies and limited capabilities for
tackling complex and unknown tasks. To overcome these issues, we present a
novel approach that combines adversarial imitation learning with large language
models (LLMs). This innovative method ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.13171v2' target='_blank'>Robust Perception-Informed Navigation using PAC-NMPC with a Learned
  Value Function</a></h2>
<p><strong>Authors:</strong> Adam Polevoy, Mark Gonzales, Marin Kobilarov, Joseph Moore</p>
<p><strong>Summary:</strong> Nonlinear model predictive control (NMPC) is typically restricted to short,
finite horizons to limit the computational burden of online optimization. As a
result, global planning frameworks are frequently necessary to avoid local
minima when using NMPC for navigation in complex environments. By contrast,
reinforcement learning (RL) can generate policies that minimize the expected
cost over an infinite-horizon and can often avoid local minima, even when
operating only on current sensor measuremen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.02054v2' target='_blank'>AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable
  Diffusion Model</a></h2>
<p><strong>Authors:</strong> Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Tangjie Lv, Changjie Fan, Zhipeng Hu</p>
<p><strong>Summary:</strong> Aligning agent behaviors with diverse human preferences remains a challenging
problem in reinforcement learning (RL), owing to the inherent abstractness and
mutability of human preferences. To address these issues, we propose AlignDiff,
a novel framework that leverages RL from Human Feedback (RLHF) to quantify
human preferences, covering abstractness, and utilizes them to guide diffusion
planning for zero-shot behavior customizing, covering mutability. AlignDiff can
accurately match user-customi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.04675v2' target='_blank'>Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Haojie Shi, Qingxu Zhu, Lei Han, Wanchao Chi, Tingguang Li, Max Q. -H. Meng</p>
<p><strong>Summary:</strong> In nature, legged animals have developed the ability to adapt to challenging
terrains through perception, allowing them to plan safe body and foot
trajectories in advance, which leads to safe and energy-efficient locomotion.
Inspired by this observation, we present a novel approach to train a Deep
Neural Network (DNN) policy that integrates proprioceptive and exteroceptive
states with a parameterized trajectory generator for quadruped robots to
traverse rough terrains. Our key idea is to use a D...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.10330v1' target='_blank'>Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS
  Planning</a></h2>
<p><strong>Authors:</strong> Guillermo Encinas-Lago, Antonio Albanese, Vincenzo Sciancalepore, Marco Di Renzo, Xavier Costa-P√©rez</p>
<p><strong>Summary:</strong> The advent of reconfigurable intelligent surfaces(RISs) brings along
significant improvements for wireless technology on the verge of
beyond-fifth-generation networks (B5G).The proven flexibility in influencing
the propagation environment opens up the possibility of programmatically
altering the wireless channel to the advantage of network designers, enabling
the exploitation of higher-frequency bands for superior throughput overcoming
the challenging electromagnetic (EM) propagation properties ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.16029v1' target='_blank'>Finetuning Offline World Models in the Real World</a></h2>
<p><strong>Authors:</strong> Yunhai Feng, Nicklas Hansen, Ziyan Xiong, Chandramouli Rajagopalan, Xiaolong Wang</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) is notoriously data-inefficient, which makes
training on a real robot difficult. While model-based RL algorithms (world
models) improve data-efficiency to some extent, they still require hours or
days of interaction to learn skills. Recently, offline RL has been proposed as
a framework for training RL policies on pre-existing datasets without any
online interaction. However, constraining an algorithm to a fixed dataset
induces a state-action distribution shift between...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2310.17785v3' target='_blank'>Learning Extrinsic Dexterity with Parameterized Manipulation Primitives</a></h2>
<p><strong>Authors:</strong> Shih-Min Yang, Martin Magnusson, Johannes A. Stork, Todor Stoyanov</p>
<p><strong>Summary:</strong> Many practically relevant robot grasping problems feature a target object for
which all grasps are occluded, e.g., by the environment. Single-shot grasp
planning invariably fails in such scenarios. Instead, it is necessary to first
manipulate the object into a configuration that affords a grasp. We solve this
problem by learning a sequence of actions that utilize the environment to
change the object's pose. Concretely, we employ hierarchical reinforcement
learning to combine a sequence of learne...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.01455v3' target='_blank'>RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning
  via Generative Simulation</a></h2>
<p><strong>Authors:</strong> Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, Chuang Gan</p>
<p><strong>Summary:</strong> We present RoboGen, a generative robotic agent that automatically learns
diverse robotic skills at scale via generative simulation. RoboGen leverages
the latest advancements in foundation and generative models. Instead of
directly using or adapting these models to produce policies or low-level
actions, we advocate for a generative scheme, which uses these models to
automatically generate diversified tasks, scenes, and training supervisions,
thereby scaling up robotic skill learning with minimal ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.04390v2' target='_blank'>Force-Constrained Visual Policy: Safe Robot-Assisted Dressing via
  Multi-Modal Sensing</a></h2>
<p><strong>Authors:</strong> Zhanyi Sun, Yufei Wang, David Held, Zackory Erickson</p>
<p><strong>Summary:</strong> Robot-assisted dressing could profoundly enhance the quality of life of
adults with physical disabilities. To achieve this, a robot can benefit from
both visual and force sensing. The former enables the robot to ascertain human
body pose and garment deformations, while the latter helps maintain safety and
comfort during the dressing process. In this paper, we introduce a new
technique that leverages both vision and force modalities for this assistive
task. Our approach first trains a vision-base...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.11965v1' target='_blank'>Provably Efficient CVaR RL in Low-rank MDPs</a></h2>
<p><strong>Authors:</strong> Yulai Zhao, Wenhao Zhan, Xiaoyan Hu, Ho-fung Leung, Farzan Farnia, Wen Sun, Jason D. Lee</p>
<p><strong>Summary:</strong> We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize
the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Prior
theoretical work studying risk-sensitive RL focuses on the tabular Markov
Decision Processes (MDPs) setting. To extend CVaR RL to settings where state
space is large, function approximation must be deployed. We study CVaR RL in
low-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the
underlying transition kernel admits a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.12975v1' target='_blank'>Neural Approximate Dynamic Programming for the Ultra-fast Order
  Dispatching Problem</a></h2>
<p><strong>Authors:</strong> Arash Dehghan, Mucahit Cevik, Merve Bodur</p>
<p><strong>Summary:</strong> Same-Day Delivery (SDD) services aim to maximize the fulfillment of online
orders while minimizing delivery delays but are beset by operational
uncertainties such as those in order volumes and courier planning. Our work
aims to enhance the operational efficiency of SDD by focusing on the ultra-fast
Order Dispatching Problem (ODP), which involves matching and dispatching orders
to couriers within a centralized warehouse setting, and completing the delivery
within a strict timeline (e.g., within m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.15925v1' target='_blank'>Reinforcement Learning for Wildfire Mitigation in Simulated Disaster
  Environments</a></h2>
<p><strong>Authors:</strong> Alexander Tapley, Marissa Dotter, Michael Doyle, Aidan Fennelly, Dhanuj Gandikota, Savanna Smith, Michael Threet, Tim Welsh</p>
<p><strong>Summary:</strong> Climate change has resulted in a year over year increase in adverse weather
and weather conditions which contribute to increasingly severe fire seasons.
Without effective mitigation, these fires pose a threat to life, property,
ecology, cultural heritage, and critical infrastructure. To better prepare for
and react to the increasing threat of wildfires, more accurate fire modelers
and mitigation responses are necessary. In this paper, we introduce SimFire, a
versatile wildland fire projection si...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.04805v1' target='_blank'>Development and Assessment of Autonomous Vehicles in Both Fully
  Automated and Mixed Traffic Conditions</a></h2>
<p><strong>Authors:</strong> Ahmed Abdelrahman</p>
<p><strong>Summary:</strong> Autonomous Vehicle (AV) technology is advancing rapidly, promising a
significant shift in road transportation safety and potentially resolving
various complex transportation issues. With the increasing deployment of AVs by
various companies, questions emerge about how AVs interact with each other and
with human drivers, especially when AVs are prevalent on the roads. Ensuring
cooperative interaction between AVs and between AVs and human drivers is
critical, though there are concerns about possib...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.02652v1' target='_blank'>Adaptive Discounting of Training Time Attacks</a></h2>
<p><strong>Authors:</strong> Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich</p>
<p><strong>Summary:</strong> Among the most insidious attacks on Reinforcement Learning (RL) solutions are
training-time attacks (TTAs) that create loopholes and backdoors in the learned
behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are
now available, where the attacker forces a specific, target behaviour upon a
training RL agent (victim). However, even state-of-the-art C-TTAs focus on
target behaviours that could be naturally adopted by the victim if not for a
particular feature of the environme...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.03197v2' target='_blank'>Decision Making in Non-Stationary Environments with Policy-Augmented
  Search</a></h2>
<p><strong>Authors:</strong> Ava Pettet, Yunuo Zhang, Baiting Luo, Kyle Wray, Hendrik Baier, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay</p>
<p><strong>Summary:</strong> Sequential decision-making under uncertainty is present in many important
problems. Two popular approaches for tackling such problems are reinforcement
learning and online search (e.g., Monte Carlo tree search). While the former
learns a policy by interacting with the environment (typically done before
execution), the latter uses a generative model of the environment to sample
promising action trajectories at decision time. Decision-making is particularly
challenging in non-stationary environmen...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.09042v1' target='_blank'>LLMs for Relational Reasoning: How Far are We?</a></h2>
<p><strong>Authors:</strong> Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, Xu Liu, Yon Shin Teo, Shang-wei Lin, Yang Liu</p>
<p><strong>Summary:</strong> Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs posses...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.00823v2' target='_blank'>SLIM: Skill Learning with Multiple Critics</a></h2>
<p><strong>Authors:</strong> David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders</p>
<p><strong>Summary:</strong> Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been successful in this task but still
struggle in the context of robotic manipulation. As it requires impacting a
possibly large set of degrees of freedom composing the environment, mutual
information maximization fails alone in producing useful and safe manipulation
behaviors. Furthermore, tackling this ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.02551v3' target='_blank'>Integrating DeepRL with Robust Low-Level Control in Robotic Manipulators
  for Non-Repetitive Reaching Tasks</a></h2>
<p><strong>Authors:</strong> Mehdi Heydari Shahna, Seyed Adel Alizadeh Kolagar, Jouni Mattila</p>
<p><strong>Summary:</strong> In robotics, contemporary strategies are learning-based, characterized by a
complex black-box nature and a lack of interpretability, which may pose
challenges in ensuring stability and safety. To address these issues, we
propose integrating a collision-free trajectory planner based on deep
reinforcement learning (DRL) with a novel auto-tuning low-level control
strategy, all while actively engaging in the learning phase through
interactions with the environment. This approach circumvents the cont...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.02772v3' target='_blank'>Contrastive Diffuser: Planning Towards High Return States via
  Contrastive Learning</a></h2>
<p><strong>Authors:</strong> Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan Zhang, Liang Yin</p>
<p><strong>Summary:</strong> The performance of offline reinforcement learning (RL) is sensitive to the
proportion of high-return trajectories in the offline dataset. However, in many
simulation environments and real-world scenarios, there are large ratios of
low-return trajectories rather than high-return trajectories, which makes
learning an efficient policy challenging. In this paper, we propose a method
called Contrastive Diffuser (CDiffuser) to make full use of low-return
trajectories and improve the performance of off...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.09961v1' target='_blank'>Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through
  Dynamic Shift Extensions: A Deep Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Zead Saleh, Ahmad Al Hanbali, Ahmad Baubaid</p>
<p><strong>Summary:</strong> Crowdsourced delivery platforms face complex scheduling challenges to match
couriers and customer orders. We consider two types of crowdsourced couriers,
namely, committed and occasional couriers, each with different compensation
schemes. Crowdsourced delivery platforms usually schedule committed courier
shifts based on predicted demand. Therefore, platforms may devise an offline
schedule for committed couriers before the planning period. However, due to the
unpredictability of demand, there are...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.10810v1' target='_blank'>Double Duality: Variational Primal-Dual Policy Optimization for
  Constrained Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Zihao Li, Boyi Liu, Zhuoran Yang, Zhaoran Wang, Mengdi Wang</p>
<p><strong>Summary:</strong> We study the Constrained Convex Markov Decision Process (MDP), where the goal
is to minimize a convex functional of the visitation measure, subject to a
convex constraint. Designing algorithms for a constrained convex MDP faces
several challenges, including (1) handling the large state space, (2) managing
the exploration/exploitation tradeoff, and (3) solving the constrained
optimization where the objective and the constraint are both nonlinear
functions of the visitation measure. In this work, ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.16181v1' target='_blank'>How Can LLM Guide RL? A Value-Based Approach</a></h2>
<p><strong>Authors:</strong> Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) has become the de facto standard practice for
sequential decision-making problems by improving future acting policies with
feedback. However, RL algorithms may require extensive trial-and-error
interactions to collect useful feedback for improvement. On the other hand,
recent developments in large language models (LLMs) have showcased impressive
capabilities in language understanding and generation, yet they fall short in
exploration and self-improvement capabilities ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.17139v1' target='_blank'>Video as the New Language for Real-World Decision Making</a></h2>
<p><strong>Authors:</strong> Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, Dale Schuurmans</p>
<p><strong>Summary:</strong> Both text and video data are abundant on the internet and support large-scale
self-supervised learning through next token or frame prediction. However, they
have not been equally leveraged: language models have had significant
real-world impact, whereas video generation has remained largely limited to
media entertainment. Yet video data captures important information about the
physical world that is difficult to express in language. To address this gap,
we discuss an under-appreciated opportunit...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.18936v1' target='_blank'>Energy-Efficient UAV Swarm Assisted MEC with Dynamic Clustering and
  Scheduling</a></h2>
<p><strong>Authors:</strong> Jialiuyuan Li, Jiayuan Chen, Changyan Yi, Tong Zhang, Kun Zhu, Jun Cai</p>
<p><strong>Summary:</strong> In this paper, the energy-efficient unmanned aerial vehicle (UAV) swarm
assisted mobile edge computing (MEC) with dynamic clustering and scheduling is
studied. In the considered system model, UAVs are divided into multiple swarms,
with each swarm consisting of a leader UAV and several follower UAVs to provide
computing services to end-users. Unlike existing work, we allow UAVs to
dynamically cluster into different swarms, i.e., each follower UAV can change
its leader based on the time-varying sp...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.01450v3' target='_blank'>Collision-Free Robot Navigation in Crowded Environments using Learning
  based Convex Model Predictive Control</a></h2>
<p><strong>Authors:</strong> Zhuanglei Wen, Mingze Dong, Xiai Chen</p>
<p><strong>Summary:</strong> Navigating robots safely and efficiently in crowded and complex environments
remains a significant challenge. However, due to the dynamic and intricate
nature of these settings, planning efficient and collision-free paths for
robots to track is particularly difficult. In this paper, we uniquely bridge
the robot's perception, decision-making and control processes by utilizing the
convex obstacle-free region computed from 2D LiDAR data. The overall pipeline
is threefold: (1) We proposes a robot na...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.06328v4' target='_blank'>Distributional Successor Features Enable Zero-Shot Policy Optimization</a></h2>
<p><strong>Authors:</strong> Chuning Zhu, Xinqi Wang, Tyler Han, Simon S. Du, Abhishek Gupta</p>
<p><strong>Summary:</strong> Intelligent agents must be generalists, capable of quickly adapting to
various tasks. In reinforcement learning (RL), model-based RL learns a dynamics
model of the world, in principle enabling transfer to arbitrary reward
functions through planning. However, autoregressive model rollouts suffer from
compounding error, making model-based RL ineffective for long-horizon problems.
Successor features offer an alternative by modeling a policy's long-term state
occupancy, reducing policy evaluation un...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.10570v2' target='_blank'>Symbiotic Game and Foundation Models for Cyber Deception Operations in
  Strategic Cyber Warfare</a></h2>
<p><strong>Authors:</strong> Tao Li, Quanyan Zhu</p>
<p><strong>Summary:</strong> We are currently facing unprecedented cyber warfare with the rapid evolution
of tactics, increasing asymmetry of intelligence, and the growing accessibility
of hacking tools. In this landscape, cyber deception emerges as a critical
component of our defense strategy against increasingly sophisticated attacks.
This chapter aims to highlight the pivotal role of game-theoretic models and
foundation models (FMs) in analyzing, designing, and implementing cyber
deception tactics. Game models (GMs) serv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.10927v1' target='_blank'>Distributed Multi-Objective Dynamic Offloading Scheduling for Air-Ground
  Cooperative MEC</a></h2>
<p><strong>Authors:</strong> Yang Huang, Miaomiao Dong, Yijie Mao, Wenqiang Liu, Zhen Gao</p>
<p><strong>Summary:</strong> Utilizing unmanned aerial vehicles (UAVs) with edge server to assist
terrestrial mobile edge computing (MEC) has attracted tremendous attention.
Nevertheless, state-of-the-art schemes based on deterministic optimizations or
single-objective reinforcement learning (RL) cannot reduce the backlog of task
bits and simultaneously improve energy efficiency in highly dynamic network
environments, where the design problem amounts to a sequential decision-making
problem. In order to address the aforement...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.12884v2' target='_blank'>HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning</a></h2>
<p><strong>Authors:</strong> Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, Hamid Rezatofighi</p>
<p><strong>Summary:</strong> Recent advances in visual reasoning (VR), particularly with the aid of Large
Vision-Language Models (VLMs), show promise but require access to large-scale
datasets and face challenges such as high computational costs and limited
generalization capabilities. Compositional visual reasoning approaches have
emerged as effective strategies; however, they heavily rely on the commonsense
knowledge encoded in Large Language Models (LLMs) to perform planning,
reasoning, or both, without considering the e...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.04869v2' target='_blank'>Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving
  Imitation Learning with LLMs</a></h2>
<p><strong>Authors:</strong> Yiqun Duan, Qiang Zhang, Renjing Xu</p>
<p><strong>Summary:</strong> The utilization of Large Language Models (LLMs) within the realm of
reinforcement learning, particularly as planners, has garnered a significant
degree of attention in recent scholarly literature. However, a substantial
proportion of existing research predominantly focuses on planning models for
robotics that transmute the outputs derived from perception models into
linguistic forms, thus adopting a `pure-language' strategy. In this research,
we propose a hybrid End-to-End learning framework for...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2404.12308v2' target='_blank'>ASID: Active Exploration for System Identification in Robotic
  Manipulation</a></h2>
<p><strong>Authors:</strong> Marius Memmel, Andrew Wagenmaker, Chuning Zhu, Patrick Yin, Dieter Fox, Abhishek Gupta</p>
<p><strong>Summary:</strong> Model-free control strategies such as reinforcement learning have shown the
ability to learn control strategies without requiring an accurate model or
simulator of the world. While this is appealing due to the lack of modeling
requirements, such methods can be sample inefficient, making them impractical
in many real-world domains. On the other hand, model-based control techniques
leveraging accurate simulators can circumvent these challenges and use a large
amount of cheap simulation data to lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.12460v1' target='_blank'>Physics-based Scene Layout Generation from Human Motion</a></h2>
<p><strong>Authors:</strong> Jianan Li, Tao Huang, Qingxu Zhu, Tien-Tsin Wong</p>
<p><strong>Summary:</strong> Creating scenes for captured motions that achieve realistic human-scene
interaction is crucial for 3D animation in movies or video games. As character
motion is often captured in a blue-screened studio without real furniture or
objects in place, there may be a discrepancy between the planned motion and the
captured one. This gives rise to the need for automatic scene layout generation
to relieve the burdens of selecting and positioning furniture and objects.
Previous approaches cannot avoid arti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.13969v1' target='_blank'>Uncertainty-Aware DRL for Autonomous Vehicle Crowd Navigation in Shared
  Space</a></h2>
<p><strong>Authors:</strong> Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</p>
<p><strong>Summary:</strong> Safe, socially compliant, and efficient navigation of low-speed autonomous
vehicles (AVs) in pedestrian-rich environments necessitates considering
pedestrians' future positions and interactions with the vehicle and others.
Despite the inevitable uncertainties associated with pedestrians' predicted
trajectories due to their unobserved states (e.g., intent), existing deep
reinforcement learning (DRL) algorithms for crowd navigation often neglect
these uncertainties when using predicted trajectorie...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.14128v2' target='_blank'>Transformers for Image-Goal Navigation</a></h2>
<p><strong>Authors:</strong> Nikhilanj Pelluri</p>
<p><strong>Summary:</strong> Visual perception and navigation have emerged as major focus areas in the
field of embodied artificial intelligence. We consider the task of image-goal
navigation, where an agent is tasked to navigate to a goal specified by an
image, relying only on images from an onboard camera. This task is particularly
challenging since it demands robust scene understanding, goal-oriented planning
and long-horizon navigation. Most existing approaches typically learn
navigation policies reliant on recurrent ne...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.19883v2' target='_blank'>From Words to Actions: Unveiling the Theoretical Underpinnings of
  LLM-Driven Autonomous Systems</a></h2>
<p><strong>Authors:</strong> Jianliang He, Siyu Chen, Fengzhuo Zhang, Zhuoran Yang</p>
<p><strong>Summary:</strong> In this work, from a theoretical lens, we aim to understand why large
language model (LLM) empowered agents are able to solve decision-making
problems in the physical world. To this end, consider a hierarchical
reinforcement learning (RL) model where the LLM Planner and the Actor perform
high-level task planning and low-level execution, respectively. Under this
model, the LLM Planner navigates a partially observable Markov decision process
(POMDP) by iteratively generating language-based subgoal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.03816v3' target='_blank'>ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</a></h2>
<p><strong>Authors:</strong> Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, Jie Tang</p>
<p><strong>Summary:</strong> Recent methodologies in LLM self-training mostly rely on LLM generating
responses and filtering those with correct output answers as training data.
This approach often yields a low-quality fine-tuning training set (e.g.,
incorrect plans or intermediate reasoning). In this paper, we develop a
reinforced self-training approach, called ReST-MCTS*, based on integrating
process reward guidance with tree search MCTS* for collecting higher-quality
reasoning traces as well as per-step value to train pol...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.06005v2' target='_blank'>WoCoCo: Learning Whole-Body Humanoid Control with Sequential Contacts</a></h2>
<p><strong>Authors:</strong> Chong Zhang, Wenli Xiao, Tairan He, Guanya Shi</p>
<p><strong>Summary:</strong> Humanoid activities involving sequential contacts are crucial for complex
robotic interactions and operations in the real world and are traditionally
solved by model-based motion planning, which is time-consuming and often relies
on simplified dynamics models. Although model-free reinforcement learning (RL)
has become a powerful tool for versatile and robust whole-body humanoid
control, it still requires tedious task-specific tuning and state machine
design and suffers from long-horizon explorat...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.11563v3' target='_blank'>Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI</a></h2>
<p><strong>Authors:</strong> Andr√© Platzer</p>
<p><strong>Summary:</strong> This perspective piece calls for the study of the new field of Intersymbolic
AI, by which we mean the combination of symbolic AI, whose building blocks have
inherent significance/meaning, with subsymbolic AI, whose entirety creates
significance/effect despite the fact that individual building blocks escape
meaning. Canonical kinds of symbolic AI are logic, games and planning.
Canonical kinds of subsymbolic AI are (un)supervised machine and reinforcement
learning. Intersymbolic AI interlinks the ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.16934v1' target='_blank'>Multi-UAV Multi-RIS QoS-Aware Aerial Communication Systems using DRL and
  PSO</a></h2>
<p><strong>Authors:</strong> Marwan Dhuheir, Aiman Erbad, Ala Al-Fuqaha, Mohsen Guizani</p>
<p><strong>Summary:</strong> Recently, Unmanned Aerial Vehicles (UAVs) have attracted the attention of
researchers in academia and industry for providing wireless services to ground
users in diverse scenarios like festivals, large sporting events, natural and
man-made disasters due to their advantages in terms of versatility and
maneuverability. However, the limited resources of UAVs (e.g., energy budget
and different service requirements) can pose challenges for adopting UAVs for
such applications. Our system model conside...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2406.18529v3' target='_blank'>Confident Natural Policy Gradient for Local Planning in
  $q_œÄ$-realizable Constrained MDPs</a></h2>
<p><strong>Authors:</strong> Tian Tian, Lin F. Yang, Csaba Szepesv√°ri</p>
<p><strong>Summary:</strong> The constrained Markov decision process (CMDP) framework emerges as an
important reinforcement learning approach for imposing safety or other critical
objectives while maximizing cumulative reward. However, the current
understanding of how to learn efficiently in a CMDP environment with a
potentially infinite number of states remains under investigation, particularly
when function approximation is applied to the value functions. In this paper,
we address the learning problem given linear functio...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.01458v2' target='_blank'>Contractual Reinforcement Learning: Pulling Arms with Invisible Hands</a></h2>
<p><strong>Authors:</strong> Jibang Wu, Siyu Chen, Mengdi Wang, Huazheng Wang, Haifeng Xu</p>
<p><strong>Summary:</strong> The agency problem emerges in today's large scale machine learning tasks,
where the learners are unable to direct content creation or enforce data
collection. In this work, we propose a theoretical framework for aligning
economic interests of different stakeholders in the online learning problems
through contract design. The problem, termed \emph{contractual reinforcement
learning}, naturally arises from the classic model of Markov decision
processes, where a learning principal seeks to optimall...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.01573v1' target='_blank'>Model-Based Diffusion for Trajectory Optimization</a></h2>
<p><strong>Authors:</strong> Chaoyi Pan, Zeji Yi, Guanya Shi, Guannan Qu</p>
<p><strong>Summary:</strong> Recent advances in diffusion models have demonstrated their strong
capabilities in generating high-fidelity samples from complex distributions
through an iterative refinement process. Despite the empirical success of
diffusion models in motion planning and control, the model-free nature of these
methods does not leverage readily available model information and limits their
generalization to new scenarios beyond the training data (e.g., new robots with
different dynamics). In this work, we introd...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02521v1' target='_blank'>Performance Comparison of Deep RL Algorithms for Mixed Traffic
  Cooperative Lane-Changing</a></h2>
<p><strong>Authors:</strong> Xue Yao, Shengren Hou, Serge P. Hoogendoorn, Simeon C. Calvert</p>
<p><strong>Summary:</strong> Lane-changing (LC) is a challenging scenario for connected and automated
vehicles (CAVs) because of the complex dynamics and high uncertainty of the
traffic environment. This challenge can be handled by deep reinforcement
learning (DRL) approaches, leveraging their data-driven and model-free nature.
Our previous work proposed a cooperative lane-changing in mixed traffic (CLCMT)
mechanism based on TD3 to facilitate an optimal lane-changing strategy. This
study enhances the current CLCMT mechanism...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.02664v3' target='_blank'>The path towards contact-based physical human-robot interaction</a></h2>
<p><strong>Authors:</strong> Mohammad Farajtabar, Marie Charbonneau</p>
<p><strong>Summary:</strong> With the advancements in human-robot interaction (HRI), robots are now
capable of operating in close proximity and engaging in physical interactions
with humans (pHRI). Likewise, contact-based pHRI is becoming increasingly
common as robots are equipped with a range of sensors to perceive human
motions. Despite the presence of surveys exploring various aspects of HRI and
pHRI, there is presently a gap in comprehensive studies that collect, organize
and relate developments across all aspects of co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.08725v2' target='_blank'>MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility</a></h2>
<p><strong>Authors:</strong> Wayne Wu, Honglin He, Jack He, Yiran Wang, Chenda Duan, Zhizheng Liu, Quanyi Li, Bolei Zhou</p>
<p><strong>Summary:</strong> Public urban spaces like streetscapes and plazas serve residents and
accommodate social life in all its vibrant variations. Recent advances in
Robotics and Embodied AI make public urban spaces no longer exclusive to
humans. Food delivery bots and electric wheelchairs have started sharing
sidewalks with pedestrians, while robot dogs and humanoids have recently
emerged in the street. Micromobility enabled by AI for short-distance travel in
public urban spaces plays a crucial component in the futur...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.13107v1' target='_blank'>DITTO: A Visual Digital Twin for Interventions and Temporal Treatment
  Outcomes in Head and Neck Cancer</a></h2>
<p><strong>Authors:</strong> Andrew Wentzel, Serageldin Attia, Xinhua Zhang, Guadalupe Canahuate, Clifton David Fuller, G. Elisabeta Marai</p>
<p><strong>Summary:</strong> Digital twin models are of high interest to Head and Neck Cancer (HNC)
oncologists, who have to navigate a series of complex treatment decisions that
weigh the efficacy of tumor control against toxicity and mortality risks.
Evaluating individual risk profiles necessitates a deeper understanding of the
interplay between different factors such as patient health, spatial tumor
location and spread, and risk of subsequent toxicities that can not be
adequately captured through simple heuristics. To su...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2407.13622v1' target='_blank'>Misspecified $Q$-Learning with Sparse Linear Function Approximation:
  Tight Bounds on Approximation Error</a></h2>
<p><strong>Authors:</strong> Ally Yalei Du, Lin F. Yang, Ruosong Wang</p>
<p><strong>Summary:</strong> The recent work by Dong & Yang (2023) showed for misspecified sparse linear
bandits, one can obtain an $O\left(\epsilon\right)$-optimal policy using a
polynomial number of samples when the sparsity is a constant, where $\epsilon$
is the misspecification error. This result is in sharp contrast to misspecified
linear bandits without sparsity, which require an exponential number of samples
to get the same guarantee. In order to study whether the analog result is
possible in the reinforcement learni...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.02559v1' target='_blank'>Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan:
  A Multi-Player Cooperative Game under Imperfect Information</a></h2>
<p><strong>Authors:</strong> Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, Yangqiu Song</p>
<p><strong>Summary:</strong> Large language models (LLMs) have shown success in handling simple games with
imperfect information and enabling multi-agent coordination, but their ability
to facilitate practical collaboration against other agents in complex,
imperfect information environments, especially in a non-English environment,
still needs to be explored. This study investigates the applicability of
knowledge acquired by open-source and API-based LLMs to sophisticated
text-based games requiring agent collaboration under...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.04215v2' target='_blank'>Comp-LTL: Temporal Logic Planning via Zero-Shot Policy Composition</a></h2>
<p><strong>Authors:</strong> Taylor Bergeron, Zachary Serlin, Kevin Leahy</p>
<p><strong>Summary:</strong> This work develops a zero-shot mechanism, Comp-LTL, for an agent to satisfy a
Linear Temporal Logic (LTL) specification given existing task primitives
trained via reinforcement learning (RL). Autonomous robots often need to
satisfy spatial and temporal goals that are unknown until run time. Prior work
focuses on learning policies for executing a task specified using LTL, but they
incorporate the specification into the learning process. Any change to the
specification requires retraining the poli...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.06087v1' target='_blank'>Building Decision Making Models Through Language Model Regime</a></h2>
<p><strong>Authors:</strong> Yu Zhang, Haoxiang Liu, Feijun Jiang, Weihua Luo, Kaifu Zhang</p>
<p><strong>Summary:</strong> We propose a novel approach for decision making problems leveraging the
generalization capabilities of large language models (LLMs). Traditional
methods such as expert systems, planning algorithms, and reinforcement learning
often exhibit limited generalization, typically requiring the training of new
models for each unique task. In contrast, LLMs demonstrate remarkable success
in generalizing across varied language tasks, inspiring a new strategy for
training decision making models. Our approac...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.10517v5' target='_blank'>Integrating Multi-Modal Input Token Mixer Into Mamba-Based Decision
  Models: Decision MetaMamba</a></h2>
<p><strong>Authors:</strong> Wall Kim</p>
<p><strong>Summary:</strong> Sequence modeling with State Space models (SSMs) has demonstrated performance
surpassing that of Transformers in various tasks, raising expectations for
their potential to outperform the Decision Transformer and its enhanced
variants in offline reinforcement learning (RL). However, decision models based
on Mamba, a state-of-the-art SSM, failed to achieve superior performance
compared to these enhanced Decision Transformers. We hypothesize that this
limitation arises from information loss during ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.08750v1' target='_blank'>DexSim2Real$^{2}$: Building Explicit World Model for Precise Articulated
  Object Dexterous Manipulation</a></h2>
<p><strong>Authors:</strong> Taoran Jiang, Liqian Ma, Yixuan Guan, Jiaojiao Meng, Weihang Chen, Zecui Zeng, Lusong Li, Dan Wu, Jing Xu, Rui Chen</p>
<p><strong>Summary:</strong> Articulated object manipulation is ubiquitous in daily life. In this paper,
we present DexSim2Real$^{2}$, a novel robot learning framework for
goal-conditioned articulated object manipulation using both two-finger grippers
and multi-finger dexterous hands. The key of our framework is constructing an
explicit world model of unseen articulated objects through active one-step
interactions. This explicit world model enables sampling-based model predictive
control to plan trajectories achieving diffe...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.10923v2' target='_blank'>Agile Continuous Jumping in Discontinuous Terrains</a></h2>
<p><strong>Authors:</strong> Yuxiang Yang, Guanya Shi, Changyi Lin, Xiangyun Meng, Rosario Scalise, Mateo Guaman Castro, Wenhao Yu, Tingnan Zhang, Ding Zhao, Jie Tan, Byron Boots</p>
<p><strong>Summary:</strong> We focus on agile, continuous, and terrain-adaptive jumping of quadrupedal
robots in discontinuous terrains such as stairs and stepping stones. Unlike
single-step jumping, continuous jumping requires accurately executing highly
dynamic motions over long horizons, which is challenging for existing
approaches. To accomplish this task, we design a hierarchical learning and
control framework, which consists of a learned heightmap predictor for robust
terrain perception, a reinforcement-learning-base...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.11238v2' target='_blank'>Leveraging Symmetry to Accelerate Learning of Trajectory Tracking
  Controllers for Free-Flying Robotic Systems</a></h2>
<p><strong>Authors:</strong> Jake Welde, Nishanth Rao, Pratik Kunapuli, Dinesh Jayaraman, Vijay Kumar</p>
<p><strong>Summary:</strong> Tracking controllers enable robotic systems to accurately follow planned
reference trajectories. In particular, reinforcement learning (RL) has shown
promise in the synthesis of controllers for systems with complex dynamics and
modest online compute budgets. However, the poor sample efficiency of RL and
the challenges of reward design make training slow and sometimes unstable,
especially for high-dimensional systems. In this work, we leverage the inherent
Lie group symmetries of robotic systems ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.12153v1' target='_blank'>Robots that Learn to Safely Influence via Prediction-Informed
  Reach-Avoid Dynamic Games</a></h2>
<p><strong>Authors:</strong> Ravi Pandya, Changliu Liu, Andrea Bajcsy</p>
<p><strong>Summary:</strong> Robots can influence people to accomplish their tasks more efficiently:
autonomous cars can inch forward at an intersection to pass through, and
tabletop manipulators can go for an object on the table first. However, a
robot's ability to influence can also compromise the safety of nearby people if
naively executed. In this work, we pose and solve a novel robust reach-avoid
dynamic game which enables robots to be maximally influential, but only when a
safety backup control exists. On the human si...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.12889v2' target='_blank'>Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a
  Study Case</a></h2>
<p><strong>Authors:</strong> Peng Chen, Pi Bu, Jun Song, Yuan Gao, Bo Zheng</p>
<p><strong>Summary:</strong> Recently, large language model (LLM)-based agents have made significant
advances across various fields. One of the most popular research areas involves
applying these agents to video games. Traditionally, these methods have relied
on game APIs to access in-game environmental and action data. However, this
approach is limited by the availability of APIs and does not reflect how humans
play games. With the advent of vision language models (VLMs), agents now have
enhanced visual understanding capab...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.13244v2' target='_blank'>From Cognition to Precognition: A Future-Aware Framework for Social
  Navigation</a></h2>
<p><strong>Authors:</strong> Zeying Gong, Tianshuai Hu, Ronghe Qiu, Junwei Liang</p>
<p><strong>Summary:</strong> To navigate safely and efficiently in crowded spaces, robots should not only
perceive the current state of the environment but also anticipate future human
movements. In this paper, we propose a reinforcement learning architecture,
namely Falcon, to tackle socially-aware navigation by explicitly predicting
human trajectories and penalizing actions that block future human paths. To
facilitate realistic evaluation, we introduce a novel SocialNav benchmark
containing two new datasets, Social-HM3D a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.14491v1' target='_blank'>Work Smarter Not Harder: Simple Imitation Learning with CS-PIBT
  Outperforms Large Scale Imitation Learning for MAPF</a></h2>
<p><strong>Authors:</strong> Rishi Veerapaneni, Arthur Jakobsson, Kevin Ren, Samuel Kim, Jiaoyang Li, Maxim Likhachev</p>
<p><strong>Summary:</strong> Multi-Agent Path Finding (MAPF) is the problem of effectively finding
efficient collision-free paths for a group of agents in a shared workspace. The
MAPF community has largely focused on developing high-performance heuristic
search methods. Recently, several works have applied various machine learning
(ML) techniques to solve MAPF, usually involving sophisticated architectures,
reinforcement learning techniques, and set-ups, but none using large amounts of
high-quality supervised data. Our init...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.15634v1' target='_blank'>NavRL: Learning Safe Flight in Dynamic Environments</a></h2>
<p><strong>Authors:</strong> Zhefan Xu, Xinming Han, Haoyu Shen, Hanyu Jin, Kenji Shimada</p>
<p><strong>Summary:</strong> Safe flight in dynamic environments requires autonomous unmanned aerial
vehicles (UAVs) to make effective decisions when navigating cluttered spaces
with moving obstacles. Traditional approaches often decompose decision-making
into hierarchical modules for prediction and planning. Although these
handcrafted systems can perform well in specific settings, they might fail if
environmental conditions change and often require careful parameter tuning.
Additionally, their solutions could be suboptimal...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.16451v1' target='_blank'>Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic
  Assembly</a></h2>
<p><strong>Authors:</strong> Jiankai Sun, Aidan Curtis, Yang You, Yan Xu, Michael Koehle, Leonidas Guibas, Sachin Chitta, Mac Schwager, Hui Li</p>
<p><strong>Summary:</strong> Generalizable long-horizon robotic assembly requires reasoning at multiple
levels of abstraction. End-to-end imitation learning (IL) has been proven a
promising approach, but it requires a large amount of demonstration data for
training and often fails to meet the high-precision requirement of assembly
tasks. Reinforcement Learning (RL) approaches have succeeded in high-precision
assembly tasks, but suffer from sample inefficiency and hence, are less
competent at long-horizon tasks. To address t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.18382v1' target='_blank'>CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot
  Skills using Large Language Models</a></h2>
<p><strong>Authors:</strong> Kanghyun Ryu, Qiayuan Liao, Zhongyu Li, Koushil Sreenath, Negar Mehr</p>
<p><strong>Summary:</strong> Curriculum learning is a training mechanism in reinforcement learning (RL)
that facilitates the achievement of complex policies by progressively
increasing the task difficulty during training. However, designing effective
curricula for a specific task often requires extensive domain knowledge and
human intervention, which limits its applicability across various domains. Our
core idea is that large language models (LLMs), with their extensive training
on diverse language data and ability to encap...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.19949v2' target='_blank'>Task-agnostic Pre-training and Task-guided Fine-tuning for Versatile
  Diffusion Planner</a></h2>
<p><strong>Authors:</strong> Chenyou Fan, Chenjia Bai, Zhao Shan, Haoran He, Yang Zhang, Zhen Wang</p>
<p><strong>Summary:</strong> Diffusion models have demonstrated their capabilities in modeling
trajectories of multi-tasks. However, existing multi-task planners or policies
typically rely on task-specific demonstrations via multi-task imitation, or
require task-specific reward labels to facilitate policy optimization via
Reinforcement Learning (RL). They are costly due to the substantial human
efforts required to collect expert data or design reward functions. To address
these challenges, we aim to develop a versatile diff...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.00564v2' target='_blank'>Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model
  Pretraining</a></h2>
<p><strong>Authors:</strong> Jie Cheng, Ruixi Qiao, Gang Xiong, Qinghai Miao, Yingwei Ma, Binhua Li, Yongbin Li, Yisheng Lv</p>
<p><strong>Summary:</strong> A significant aspiration of offline reinforcement learning (RL) is to develop
a generalist agent with high capabilities from large and heterogeneous
datasets. However, prior approaches that scale offline RL either rely heavily
on expert trajectories or struggle to generalize to diverse unseen tasks.
Inspired by the excellent generalization of world model in conditional video
generation, we explore the potential of image observation-based world model for
scaling offline RL and enhancing generaliz...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.02141v3' target='_blank'>E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic
  Whole-Body Control Framework</a></h2>
<p><strong>Authors:</strong> Yiqun Duan, Qiang Zhang, Jinzhao Zhou, Jingkai Sun, Xiaowei Jiang, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin</p>
<p><strong>Summary:</strong> Recent advancements in humanoid robotics, including the integration of
hierarchical reinforcement learning-based control and the utilization of LLM
planning, have significantly enhanced the ability of robots to perform complex
tasks. In contrast to the highly developed humanoid robots, the human factors
involved remain relatively unexplored. Directly controlling humanoid robots
with the brain has already appeared in many science fiction novels, such as
Pacific Rim and Gundam. In this work, we pr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.03441v1' target='_blank'>CLoSD: Closing the Loop between Simulation and Diffusion for multi-task
  character control</a></h2>
<p><strong>Authors:</strong> Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit H. Bermano, Michiel van de Panne</p>
<p><strong>Summary:</strong> Motion diffusion models and Reinforcement Learning (RL) based control for
physics-based simulations have complementary strengths for human motion
generation. The former is capable of generating a wide variety of motions,
adhering to intuitive control such as text, while the latter offers physically
plausible motion and direct interaction with the environment. In this work, we
present a method that combines their respective strengths. CLoSD is a
text-driven RL physics-based controller, guided by ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.05093v1' target='_blank'>Reinforcement Learning Control for Autonomous Hydraulic Material
  Handling Machines with Underactuated Tools</a></h2>
<p><strong>Authors:</strong> Filippo A. Spinelli, Pascal Egli, Julian Nubert, Fang Nan, Thilo Bleumer, Patrick Goegler, Stephan Brockes, Ferdinand Hofmann, Marco Hutter</p>
<p><strong>Summary:</strong> The precise and safe control of heavy material handling machines presents
numerous challenges due to the hard-to-model hydraulically actuated joints and
the need for collision-free trajectory planning with a free-swinging
end-effector tool. In this work, we propose an RL-based controller that
commands the cabin joint and the arm simultaneously. It is trained in a
simulation combining data-driven modeling techniques with first-principles
modeling. On the one hand, we employ a neural network model...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.04348v1' target='_blank'>UEVAVD: A Dataset for Developing UAV's Eye View Active Object Detection</a></h2>
<p><strong>Authors:</strong> Xinhua Jiang, Tianpeng Liu, Li Liu, Zhen Liu, Yongxiang Liu</p>
<p><strong>Summary:</strong> Occlusion is a longstanding difficulty that challenges the UAV-based object
detection. Many works address this problem by adapting the detection model.
However, few of them exploit that the UAV could fundamentally improve detection
performance by changing its viewpoint. Active Object Detection (AOD) offers an
effective way to achieve this purpose. Through Deep Reinforcement Learning
(DRL), AOD endows the UAV with the ability of autonomous path planning to
search for the observation that is more ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.13543v1' target='_blank'>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</a></h2>
<p><strong>Authors:</strong> Davide Paglieri, Bart≈Çomiej Cupia≈Ç, Samuel Coward, Ulyana Piterbarg, Maciej Wolczyk, Akbir Khan, Eduardo Pignatelli, ≈Åukasz Kuci≈Ñski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, Tim Rockt√§schel</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) and Vision Language Models (VLMs) possess
extensive knowledge and exhibit promising reasoning abilities; however, they
still struggle to perform well in complex, dynamic environments. Real-world
tasks require handling intricate interactions, advanced spatial reasoning,
long-term planning, and continuous exploration of new strategies-areas in which
we lack effective methodologies for comprehensively evaluating these
capabilities. To address this gap, we introduce BALR...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.17075v5' target='_blank'>Don't Command, Cultivate: An Exploratory Study of System-2 Alignment</a></h2>
<p><strong>Authors:</strong> Yuhang Wang, Yuxiang Zhang, Yanxu Zhu, Xinyan Wen, Jitao Sang</p>
<p><strong>Summary:</strong> The o1 system card identifies the o1 models as the most robust within OpenAI,
with their defining characteristic being the progression from rapid, intuitive
thinking to slower, more deliberate reasoning. This observation motivated us to
investigate the influence of System-2 thinking patterns on model safety. In our
preliminary research, we conducted safety evaluations of the o1 model,
including complex jailbreak attack scenarios using adversarial natural language
prompts and mathematical encodin...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2412.03338v2' target='_blank'>AI-Driven Day-to-Day Route Choice</a></h2>
<p><strong>Authors:</strong> Leizhen Wang, Peibo Duan, Zhengbing He, Cheng Lyu, Xin Chen, Nan Zheng, Li Yao, Zhenliang Ma</p>
<p><strong>Summary:</strong> Understanding travelers' route choices can help policymakers devise optimal
operational and planning strategies for both normal and abnormal circumstances.
However, existing choice modeling methods often rely on predefined assumptions
and struggle to capture the dynamic and adaptive nature of travel behavior.
Recently, Large Language Models (LLMs) have emerged as a promising alternative,
demonstrating remarkable ability to replicate human-like behaviors across
various fields. Despite this potent...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.02652v2' target='_blank'>A View of the Certainty-Equivalence Method for PAC RL as an Application
  of the Trajectory Tree Method</a></h2>
<p><strong>Authors:</strong> Shivaram Kalyanakrishnan, Sheel Shah, Santhosh Kumar Guguloth</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) enables an agent interacting with an unknown MDP
$M$ to optimise its behaviour by observing transitions sampled from $M$. A
natural entity that emerges in the agent's reasoning is $\widehat{M}$, the
maximum likelihood estimate of $M$ based on the observed transitions. The
well-known \textit{certainty-equivalence} method (CEM) dictates that the agent
update its behaviour to $\widehat{\pi}$, which is an optimal policy for
$\widehat{M}$. Not only is CEM intuitive, it has...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2501.09781v1' target='_blank'>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</a></h2>
<p><strong>Authors:</strong> Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin</p>
<p><strong>Summary:</strong> This work explores whether a deep generative model can learn complex
knowledge solely from visual input, in contrast to the prevalent focus on
text-based models like large language models (LLMs). We develop VideoWorld, an
auto-regressive video generation model trained on unlabeled video data, and
test its knowledge acquisition abilities in video-based Go and robotic control
tasks. Our experiments reveal two key findings: (1) video-only training
provides sufficient information for learning knowle...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.06725v1' target='_blank'>AgilePilot: DRL-Based Drone Agent for Real-Time Motion Planning in
  Dynamic Environments by Leveraging Object Detection</a></h2>
<p><strong>Authors:</strong> Roohan Ahmed Khan, Valerii Serpiva, Demetros Aschalew, Aleksey Fedoseev, Dzmitry Tsetserukou</p>
<p><strong>Summary:</strong> Autonomous drone navigation in dynamic environments remains a critical
challenge, especially when dealing with unpredictable scenarios including
fast-moving objects with rapidly changing goal positions. While traditional
planners and classical optimisation methods have been extensively used to
address this dynamic problem, they often face real-time, unpredictable changes
that ultimately leads to sub-optimal performance in terms of adaptiveness and
real-time decision making. In this work, we prop...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.06996v1' target='_blank'>A view on learning robust goal-conditioned value functions: Interplay
  between RL and MPC</a></h2>
<p><strong>Authors:</strong> Nathan P. Lawrence, Philip D. Loewen, Michael G. Forbes, R. Bhushan Gopaluni, Ali Mesbah</p>
<p><strong>Summary:</strong> Reinforcement learning (RL) and model predictive control (MPC) offer a wealth
of distinct approaches for automatic decision-making. Given the impact both
fields have had independently across numerous domains, there is growing
interest in combining the general-purpose learning capability of RL with the
safety and robustness features of MPC. To this end, this paper presents a
tutorial-style treatment of RL and MPC, treating them as alternative approaches
to solving Markov decision processes. In ou...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.11298v1' target='_blank'>Integrating Language Models for Enhanced Network State Monitoring in
  DRL-Based SFC Provisioning</a></h2>
<p><strong>Authors:</strong> Parisa Fard Moshiri, Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Emil Janulewicz</p>
<p><strong>Summary:</strong> Efficient Service Function Chain (SFC) provisioning and Virtual Network
Function (VNF) placement are critical for enhancing network performance in
modern architectures such as Software-Defined Networking (SDN) and Network
Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids
decision-making in dynamic network environments, its reliance on structured
inputs and predefined rules limits adaptability in unforeseen scenarios.
Additionally, incorrect actions by a DRL agent may re...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.11312v1' target='_blank'>AI Generations: From AI 1.0 to AI 4.0</a></h2>
<p><strong>Authors:</strong> Jiahao Wu, Hengxu You, Jing Du</p>
<p><strong>Summary:</strong> This paper proposes that Artificial Intelligence (AI) progresses through
several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),
AI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of
these AI generations is driven by shifting priorities among algorithms,
computing power, and data. AI 1.0 ushered in breakthroughs in pattern
recognition and information processing, fueling advances in computer vision,
natural language processing, and recommendation system...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.14111v1' target='_blank'>Comprehensive Review on the Control of Heat Pumps for Energy Flexibility
  in Distribution Networks</a></h2>
<p><strong>Authors:</strong> Gustavo L. Aschidamini, Mina Pavlovic, Bradley A. Reinholz, Malcolm S. Metcalfe, Taco Niet, Mariana Resener</p>
<p><strong>Summary:</strong> Decarbonization plans promote the transition to heat pumps (HPs), creating
new opportunities for their energy flexibility in demand response programs,
solar photovoltaic integration and optimization of distribution networks. This
paper reviews scheduling-based and real-time optimization methods for
controlling HPs with a focus on energy flexibility in distribution networks.
Scheduling-based methods fall into two categories: rule-based controllers
(RBCs), which rely on predefined control rules wi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1903.08772v3' target='_blank'>ToyArchitecture: Unsupervised Learning of Interpretable Models of the
  World</a></h2>
<p><strong>Authors:</strong> Jaroslav V√≠tk≈Ø, Petr Dluho≈°, Joseph Davidson, Matƒõj Nikl, Simon Andersson, P≈ôemysl Pa≈°ka, Jan ≈†inkora, Petr Hlubuƒçek, Martin Str√°nsk√Ω, Martin Hyben, Martin Poliak, Jan Feyereisl, Marek Rosa</p>
<p><strong>Summary:</strong> Research in Artificial Intelligence (AI) has focused mostly on two extremes:
either on small improvements in narrow AI domains, or on universal theoretical
frameworks which are usually uncomputable, incompatible with theories of
biological intelligence, or lack practical implementations. The goal of this
work is to combine the main advantages of the two: to follow a big picture
view, while providing a particular theory and its implementation. In contrast
with purely theoretical approaches, the r...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1910.06070v2' target='_blank'>Review of Learning-based Longitudinal Motion Planning for Autonomous
  Vehicles: Research Gaps between Self-driving and Traffic Congestion</a></h2>
<p><strong>Authors:</strong> Hao Zhou, Jorge Laval, Anye Zhou, Yu Wang, Wenchao Wu, Zhu Qing, Srinivas Peeta</p>
<p><strong>Summary:</strong> Self-driving technology companies and the research community are accelerating
their pace to use machine learning longitudinal motion planning (mMP) for
autonomous vehicles (AVs). This paper reviews the current state of the art in
mMP, with an exclusive focus on its impact on traffic congestion. We identify
the availability of congestion scenarios in current datasets, and summarize the
required features for training mMP. For learning methods, we survey the major
methods in both imitation learning...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2007.07461v3' target='_blank'>Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal
  Sample Complexity</a></h2>
<p><strong>Authors:</strong> Kaiqing Zhang, Sham M. Kakade, Tamer Ba≈üar, Lin F. Yang</p>
<p><strong>Summary:</strong> Model-based reinforcement learning (RL), which finds an optimal policy using
an empirical model, has long been recognized as one of the corner stones of RL.
It is especially suitable for multi-agent RL (MARL), as it naturally decouples
the learning and the planning phases, and avoids the non-stationarity problem
when all agents are improving their policies simultaneously using samples.
Though intuitive and widely-used, the sample complexity of model-based MARL
algorithms has not been fully inves...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.08184v3' target='_blank'>PRIMAL2: Pathfinding via Reinforcement and Imitation Multi-Agent
  Learning -- Lifelong</a></h2>
<p><strong>Authors:</strong> Mehul Damani, Zhiyao Luo, Emerson Wenzel, Guillaume Sartoretti</p>
<p><strong>Summary:</strong> Multi-agent path finding (MAPF) is an indispensable component of large-scale
robot deployments in numerous domains ranging from airport management to
warehouse automation. In particular, this work addresses lifelong MAPF (LMAPF)
- an online variant of the problem where agents are immediately assigned a new
goal upon reaching their current one - in dense and highly structured
environments, typical of real-world warehouse operations. Effectively solving
LMAPF in such environments requires expensiv...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1905.07681v3' target='_blank'>Spatial Positioning Token (SPToken) for Smart Mobility</a></h2>
<p><strong>Authors:</strong> Roman Overko, Rodrigo H. Ordonez-Hurtado, Sergiy Zhuk, Pietro Ferraro, Andrew Cullen, Robert Shorten</p>
<p><strong>Summary:</strong> We introduce a permissioned distributed ledger technology (DLT) design for
crowdsourced smart mobility applications. This architecture is based on a
directed acyclic graph architecture (similar to the IOTA tangle) and uses both
Proof-of-Work and Proof-of-Position mechanisms to provide protection against
spam attacks and malevolent actors. In addition to enabling individuals to
retain ownership of their data and to monetize it, the architecture also is
suitable for distributed privacy-preserving ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2004.07822v2' target='_blank'>Order Matters: Generating Progressive Explanations for Planning Tasks in
  Human-Robot Teaming</a></h2>
<p><strong>Authors:</strong> Mehrdad Zakershahrak, Shashank Rao Marpally, Akshay Sharma, Ze Gong, Yu Zhang</p>
<p><strong>Summary:</strong> Prior work on generating explanations in a planning and decision-making
context has focused on providing the rationale behind an AI agent's decision
making. While these methods provide the right explanations from the explainer's
perspective, they fail to heed the cognitive requirement of understanding an
explanation from the explainee's (the human's) perspective. In this work, we
set out to address this issue by first considering the influence of information
order in an explanation, or the progr...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.00199v2' target='_blank'>RFUniverse: A Multiphysics Simulation Platform for Embodied AI</a></h2>
<p><strong>Authors:</strong> Haoyuan Fu, Wenqiang Xu, Ruolin Ye, Han Xue, Zhenjun Yu, Tutian Tang, Yutong Li, Wenxin Du, Jieyi Zhang, Cewu Lu</p>
<p><strong>Summary:</strong> Multiphysics phenomena, the coupling effects involving different aspects of
physics laws, are pervasive in the real world and can often be encountered when
performing everyday household tasks. Intelligent agents which seek to assist or
replace human laborers will need to learn to cope with such phenomena in
household task settings. To equip the agents with such kind of abilities, the
research community needs a simulation environment, which will have the
capability to serve as the testbed for the...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2202.12861v6' target='_blank'>Hierarchical Control for Head-to-Head Autonomous Racing</a></h2>
<p><strong>Authors:</strong> Rishabh Saumil Thakkar, Aryaman Singh Samyal, David Fridovich-Keil, Zhe Xu, Ufuk Topcu</p>
<p><strong>Summary:</strong> We develop a hierarchical controller for head-to-head autonomous racing. We
first introduce a formulation of a racing game with realistic safety and
fairness rules. A high-level planner approximates the original formulation as a
discrete game with simplified state, control, and dynamics to easily encode the
complex safety and fairness rules and calculates a series of target waypoints.
The low-level controller takes the resulting waypoints as a reference
trajectory and computes high-resolution co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2204.11411v4' target='_blank'>Road Traffic Law Adaptive Decision-making for Self-Driving Vehicles</a></h2>
<p><strong>Authors:</strong> Jiaxin Liu, Wenhui Zhou, Hong Wang, Zhong Cao, Wenhao Yu, Chengxiang Zhao, Ding Zhao, Diange Yang, Jun Li</p>
<p><strong>Summary:</strong> Self-driving vehicles have their own intelligence to drive on open roads.
However, vehicle managers, e.g., government or industrial companies, still need
a way to tell these self-driving vehicles what behaviors are encouraged or
forbidden. Unlike human drivers, current self-driving vehicles cannot
understand the traffic laws, thus rely on the programmers manually writing the
corresponding principles into the driving systems. It would be less efficient
and hard to adapt some temporary traffic law...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.09370v2' target='_blank'>TC-Driver: Trajectory Conditioned Driving for Robust Autonomous Racing
  -- A Reinforcement Learning Approach</a></h2>
<p><strong>Authors:</strong> Edoardo Ghignone, Nicolas Baumann, Mike Boss, Michele Magno</p>
<p><strong>Summary:</strong> Autonomous racing is becoming popular for academic and industry researchers
as a test for general autonomous driving by pushing perception, planning, and
control algorithms to their limits. While traditional control methods such as
MPC are capable of generating an optimal control sequence at the edge of the
vehicles physical controllability, these methods are sensitive to the accuracy
of the modeling parameters. This paper presents TC-Driver, a RL approach for
robust control in autonomous racing...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.04265v3' target='_blank'>Route Planning for Last-Mile Deliveries Using Mobile Parcel Lockers: A
  Hybrid Q-Learning Network Approach</a></h2>
<p><strong>Authors:</strong> Yubin Liu, Qiming Ye, Jose Escribano-Macias, Yuxiang Feng, Eduardo Candela, Panagiotis Angeloudis</p>
<p><strong>Summary:</strong> Mobile parcel lockers have been recently proposed by logistics operators as a
technology that could help reduce traffic congestion and operational costs in
urban freight distribution. Given their ability to relocate throughout their
area of deployment, they hold the potential to improve customer accessibility
and convenience. In this study, we formulate the Mobile Parcel Locker Problem
(MPLP) , a special case of the Location-Routing Problem (LRP) which determines
the optimal stopover location fo...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.05372v3' target='_blank'>Towards Minimax Optimality of Model-based Robust Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Pierre Clavier, Erwan Le Pennec, Matthieu Geist</p>
<p><strong>Summary:</strong> We study the sample complexity of obtaining an $\epsilon$-optimal policy in
\emph{Robust} discounted Markov Decision Processes (RMDPs), given only access
to a generative model of the nominal kernel. This problem is widely studied in
the non-robust case, and it is known that any planning approach applied to an
empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A
\mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which is
minimax optimal. Results in the rob...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2303.17655v2' target='_blank'>Q-Learning based system for path planning with unmanned aerial vehicles
  swarms in obstacle environments</a></h2>
<p><strong>Authors:</strong> Alejandro Puente-Castro, Daniel Rivero, Eurico Pedrosa, Artur Pereira, Nuno Lau, Enrique Fernandez-Blanco</p>
<p><strong>Summary:</strong> Path Planning methods for autonomous control of Unmanned Aerial Vehicle (UAV)
swarms are on the rise because of all the advantages they bring. There are more
and more scenarios where autonomous control of multiple UAVs is required. Most
of these scenarios present a large number of obstacles, such as power lines or
trees. If all UAVs can be operated autonomously, personnel expenses can be
decreased. In addition, if their flight paths are optimal, energy consumption
is reduced. This ensures that m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.01392v1' target='_blank'>LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous
  Space Exploration</a></h2>
<p><strong>Authors:</strong> David Maranto</p>
<p><strong>Summary:</strong> As spacecraft journey further from Earth with more complex missions, systems
of greater autonomy and onboard intelligence are called for. Reducing reliance
on human-based mission control becomes increasingly critical if we are to
increase our rate of solar-system-wide exploration. Recent work has explored
AI-based goal-oriented systems to increase the level of autonomy in mission
execution. These systems make use of symbolic reasoning managers to make
inferences from the state of a spacecraft an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2409.00735v1' target='_blank'>AgGym: An agricultural biotic stress simulation environment for
  ultra-precision management planning</a></h2>
<p><strong>Authors:</strong> Mahsa Khosravi, Matthew Carroll, Kai Liang Tan, Liza Van der Laan, Joscif Raigne, Daren S. Mueller, Arti Singh, Aditya Balu, Baskar Ganapathysubramanian, Asheesh Kumar Singh, Soumik Sarkar</p>
<p><strong>Summary:</strong> Agricultural production requires careful management of inputs such as
fungicides, insecticides, and herbicides to ensure a successful crop that is
high-yielding, profitable, and of superior seed quality. Current
state-of-the-art field crop management relies on coarse-scale crop management
strategies, where entire fields are sprayed with pest and disease-controlling
chemicals, leading to increased cost and sub-optimal soil and crop management.
To overcome these challenges and optimize crop produc...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.00371v1' target='_blank'>AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures
  in Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Jiafei Duan, Wilbert Pumacay, Nishanth Kumar, Yi Ru Wang, Shulin Tian, Wentao Yuan, Ranjay Krishna, Dieter Fox, Ajay Mandlekar, Yijie Guo</p>
<p><strong>Summary:</strong> Robotic manipulation in open-world settings requires not only task execution
but also the ability to detect and learn from failures. While recent advances
in vision-language models (VLMs) and large language models (LLMs) have improved
robots' spatial reasoning and problem-solving abilities, they still struggle
with failure recognition, limiting their real-world applicability. We introduce
AHA, an open-source VLM designed to detect and reason about failures in robotic
manipulation using natural l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.06113v1' target='_blank'>Towards Bio-inspired Heuristically Accelerated Reinforcement Learning
  for Adaptive Underwater Multi-Agents Behaviour</a></h2>
<p><strong>Authors:</strong> Antoine Vivien, Thomas Chaffre, Matthew Stephenson, Eva Artusi, Paulo Santos, Benoit Clement, Karl Sammut</p>
<p><strong>Summary:</strong> This paper describes the problem of coordination of an autonomous Multi-Agent
System which aims to solve the coverage planning problem in a complex
environment. The considered applications are the detection and identification
of objects of interest while covering an area. These tasks, which are highly
relevant for space applications, are also of interest among various domains
including the underwater context, which is the focus of this study. In this
context, coverage planning is traditionally m...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1206.6842v1' target='_blank'>Chi-square Tests Driven Method for Learning the Structure of Factored
  MDPs</a></h2>
<p><strong>Authors:</strong> Thomas Degris, Olivier Sigaud, Pierre-Henri Wuillemin</p>
<p><strong>Summary:</strong> SDYNA is a general framework designed to address large stochastic
reinforcement learning problems. Unlike previous model based methods in FMDPs,
it incrementally learns the structure and the parameters of a RL problem using
supervised learning techniques. Then, it integrates decision-theoric planning
algorithms based on FMDPs to compute its policy. SPITI is an instanciation of
SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the
reward function and the Dynamic Bayesian N...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1611.04201v4' target='_blank'>CAD2RL: Real Single-Image Flight without a Single Real Image</a></h2>
<p><strong>Authors:</strong> Fereshteh Sadeghi, Sergey Levine</p>
<p><strong>Summary:</strong> Deep reinforcement learning has emerged as a promising and powerful technique
for automatically acquiring control policies that can process raw sensory
inputs, such as images, and perform complex behaviors. However, extending deep
RL to real-world robotic tasks has proven challenging, particularly in
safety-critical domains such as autonomous flight, where a trial-and-error
learning process is often impractical. In this paper, we explore the following
question: can we train vision-based navigati...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1801.08093v3' target='_blank'>Learning Symmetric and Low-energy Locomotion</a></h2>
<p><strong>Authors:</strong> Wenhao Yu, Greg Turk, C. Karen Liu</p>
<p><strong>Summary:</strong> Learning locomotion skills is a challenging problem. To generate realistic
and smooth locomotion, existing methods use motion capture, finite state
machines or morphology-specific knowledge to guide the motion generation
algorithms. Deep reinforcement learning (DRL) is a promising approach for the
automatic creation of locomotion control. Indeed, a standard benchmark for DRL
is to automatically create a running controller for a biped character from a
simple reward function. Although several diff...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1809.03478v1' target='_blank'>Towards a Fatality-Aware Benchmark of Probabilistic Reaction Prediction
  in Highly Interactive Driving Scenarios</a></h2>
<p><strong>Authors:</strong> Wei Zhan, Liting Sun, Yeping Hu, Jiachen Li, Masayoshi Tomizuka</p>
<p><strong>Summary:</strong> Autonomous vehicles should be able to generate accurate probabilistic
predictions for uncertain behavior of other road users. Moreover, reactive
predictions are necessary in highly interactive driving scenarios to answer
"what if I take this action in the future" for autonomous vehicles. There is no
existing unified framework to homogenize the problem formulation,
representation simplification, and evaluation metric for various prediction
methods, such as probabilistic graphical models (PGM), ne...</p>
<hr>
<h2><a href='http://arxiv.org/abs/1912.03963v7' target='_blank'>Data Collection versus Data Estimation: A Fundamental Trade-off in
  Dynamic Networks</a></h2>
<p><strong>Authors:</strong> Jalal Arabneydi, Amir G. Aghdam</p>
<p><strong>Summary:</strong> An important question that often arises in the operation of networked systems
is whether to collect the real-time data or to estimate them based on the
previously collected data. Various factors should be taken into account such as
how informative the data are at each time instant for state estimation, how
costly and credible the collected data are, and how rapidly the data vary with
time. The above question can be formulated as a dynamic decision making problem
with imperfect information struct...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2010.04296v2' target='_blank'>CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and
  Transfer Learning</a></h2>
<p><strong>Authors:</strong> Ossama Ahmed, Frederik Tr√§uble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Sch√∂lkopf, Manuel W√ºthrich, Stefan Bauer</p>
<p><strong>Summary:</strong> Despite recent successes of reinforcement learning (RL), it remains a
challenge for agents to transfer learned skills to related environments. To
facilitate research addressing this problem, we propose CausalWorld, a
benchmark for causal structure and transfer learning in a robotic manipulation
environment. The environment is a simulation of an open-source robotic
platform, hence offering the possibility of sim-to-real transfer. Tasks consist
of constructing 3D shapes from a given set of blocks ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2101.03525v1' target='_blank'>Cross-Modal Contrastive Learning of Representations for Navigation using
  Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental
  Conditions</a></h2>
<p><strong>Authors:</strong> Jui-Te Huang, Chen-Lung Lu, Po-Kai Chang, Ching-I Huang, Chao-Chun Hsu, Zu Lin Ewe, Po-Jui Huang, Hsueh-Cheng Wang</p>
<p><strong>Summary:</strong> Deep reinforcement learning (RL), where the agent learns from mistakes, has
been successfully applied to a variety of tasks. With the aim of learning
collision-free policies for unmanned vehicles, deep RL has been used for
training with various types of data, such as colored images, depth images, and
LiDAR point clouds, without the use of classic map--localize--plan approaches.
However, existing methods are limited by their reliance on cameras and LiDAR
devices, which have degraded sensing under...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.00241v1' target='_blank'>Reinforced Iterative Knowledge Distillation for Cross-Lingual Named
  Entity Recognition</a></h2>
<p><strong>Authors:</strong> Shining Liang, Ming Gong, Jian Pei, Linjun Shou, Wanli Zuo, Xianglin Zuo, Daxin Jiang</p>
<p><strong>Summary:</strong> Named entity recognition (NER) is a fundamental component in many
applications, such as Web Search and Voice Assistants. Although deep neural
networks greatly improve the performance of NER, due to the requirement of
large amounts of training data, deep neural networks can hardly scale out to
many languages in an industry setting. To tackle this challenge, cross-lingual
NER transfers knowledge from a rich-resource language to languages with low
resources through pre-trained multilingual language...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2106.08053v1' target='_blank'>On the Power of Multitask Representation Learning in Linear MDP</a></h2>
<p><strong>Authors:</strong> Rui Lu, Gao Huang, Simon S. Du</p>
<p><strong>Summary:</strong> While multitask representation learning has become a popular approach in
reinforcement learning (RL), theoretical understanding of why and when it works
remains limited. This paper presents analyses for the statistical benefit of
multitask representation learning in linear Markov Decision Process (MDP) under
a generative model. In this paper, we consider an agent to learn a
representation function $\phi$ out of a function class $\Phi$ from $T$ source
tasks with $N$ data per task, and then use th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.04112v2' target='_blank'>Applying Machine Learning in Self-Adaptive Systems: A Systematic
  Literature Review</a></h2>
<p><strong>Authors:</strong> Omid Gheibi, Danny Weyns, Federico Quin</p>
<p><strong>Summary:</strong> Recently, we witness a rapid increase in the use of machine learning in
self-adaptive systems. Machine learning has been used for a variety of reasons,
ranging from learning a model of the environment of a system during operation
to filtering large sets of possible configurations before analysing them. While
a body of work on the use of machine learning in self-adaptive systems exists,
there is currently no systematic overview of this area. Such overview is
important for researchers to understan...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2103.17162v2' target='_blank'>RIS-Assisted UAV for Timely Data Collection in IoT Networks</a></h2>
<p><strong>Authors:</strong> Ahmed Al-Hilo, Moataz Samir, Mohamed Elhattab, Chadi Assi, Sanaa Sharafeddine</p>
<p><strong>Summary:</strong> Intelligent Transportation Systems are thriving thanks to a wide range of
technological advances, namely 5G communications, Internet of Things,
artificial intelligence and edge computing. Central to this is the wide
deployment of smart sensing devices and accordingly the large amount of
harvested information to be processed for timely decision making. Robust
network access is, hence, essential for offloading the collected data before a
set deadline, beyond which the data loses its value. In envi...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2105.12196v1' target='_blank'>From Motor Control to Team Play in Simulated Humanoid Football</a></h2>
<p><strong>Authors:</strong> Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, S. M. Ali Eslami, Daniel Hennes, Wojciech M. Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y. Siegel, Leonard Hasenclever, Luke Marris, Saran Tunyasuvunakool, H. Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D. Tracey, Karl Tuyls, Thore Graepel, Nicolas Heess</p>
<p><strong>Summary:</strong> Intelligent behaviour in the physical world exhibits structure at multiple
spatial and temporal scales. Although movements are ultimately executed at the
level of instantaneous muscle tensions or joint torques, they must be selected
to serve goals defined on much longer timescales, and in terms of relations
that extend far beyond the body itself, ultimately involving coordination with
other agents. Recent research in artificial intelligence has shown the promise
of learning-based approaches to t...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2110.02758v2' target='_blank'>Mismatched No More: Joint Model-Policy Optimization for Model-Based RL</a></h2>
<p><strong>Authors:</strong> Benjamin Eysenbach, Alexander Khazatsky, Sergey Levine, Ruslan Salakhutdinov</p>
<p><strong>Summary:</strong> Many model-based reinforcement learning (RL) methods follow a similar
template: fit a model to previously observed data, and then use data from that
model for RL or planning. However, models that achieve better training
performance (e.g., lower MSE) are not necessarily better for control: an RL
agent may seek out the small fraction of states where an accurate model makes
mistakes, or it might act in ways that do not expose the errors of an
inaccurate model. As noted in prior work, there is an ob...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2201.12518v4' target='_blank'>Zeroth-Order Actor-Critic: An Evolutionary Framework for Sequential
  Decision Problems</a></h2>
<p><strong>Authors:</strong> Yuheng Lei, Yao Lyu, Guojian Zhan, Tao Zhang, Jiangtao Li, Jianyu Chen, Shengbo Eben Li, Sifa Zheng</p>
<p><strong>Summary:</strong> Evolutionary algorithms (EAs) have shown promise in solving sequential
decision problems (SDPs) by simplifying them to static optimization problems
and searching for the optimal policy parameters in a zeroth-order way. While
these methods are highly versatile, they often suffer from high sample
complexity due to their ignorance of the underlying temporal structures. In
contrast, reinforcement learning (RL) methods typically formulate SDPs as
Markov Decision Process (MDP). Although more sample ef...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2205.13578v2' target='_blank'>Dynamic Network Reconfiguration for Entropy Maximization using Deep
  Reinforcement Learning</a></h2>
<p><strong>Authors:</strong> Christoffel Doorman, Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi</p>
<p><strong>Summary:</strong> A key problem in network theory is how to reconfigure a graph in order to
optimize a quantifiable objective. Given the ubiquity of networked systems,
such work has broad practical applications in a variety of situations, ranging
from drug and material design to telecommunications. The large decision space
of possible reconfigurations, however, makes this problem computationally
intensive. In this paper, we cast the problem of network rewiring for
optimizing a specified structural property as a M...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2209.04100v2' target='_blank'>Task-Agnostic Learning to Accomplish New Tasks</a></h2>
<p><strong>Authors:</strong> Xianqi Zhang, Xingtao Wang, Xu Liu, Wenrui Wang, Xiaopeng Fan, Debin Zhao</p>
<p><strong>Summary:</strong> Reinforcement Learning (RL) and Imitation Learning (IL) have made great
progress in robotic control in recent years. However, these methods show
obvious deterioration for new tasks that need to be completed through new
combinations of actions. RL methods heavily rely on reward functions that
cannot generalize well for new tasks, while IL methods are limited by expert
demonstrations which do not cover new tasks. In contrast, humans can easily
complete these tasks with the fragmented knowledge lea...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2210.01162v5' target='_blank'>Learning Minimally-Violating Continuous Control for Infeasible Linear
  Temporal Logic Specifications</a></h2>
<p><strong>Authors:</strong> Mingyu Cai, Makai Mann, Zachary Serlin, Kevin Leahy, Cristian-Ioan Vasile</p>
<p><strong>Summary:</strong> This paper explores continuous-time control synthesis for target-driven
navigation to satisfy complex high-level tasks expressed as linear temporal
logic (LTL). We propose a model-free framework using deep reinforcement
learning (DRL) where the underlying dynamic system is unknown (an opaque box).
Unlike prior work, this paper considers scenarios where the given LTL
specification might be infeasible and therefore cannot be accomplished
globally. Instead of modifying the given LTL formula, we pro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2211.10851v4' target='_blank'>Reward is not Necessary: How to Create a Modular & Compositional
  Self-Preserving Agent for Life-Long Learning</a></h2>
<p><strong>Authors:</strong> Thomas J. Ringstrom</p>
<p><strong>Summary:</strong> Reinforcement Learning views the maximization of rewards and avoidance of
punishments as central to explaining goal-directed behavior. However, over a
life, organisms will need to learn about many different aspects of the world's
structure: the states of the world and state-vector transition dynamics. The
number of combinations of states grows exponentially as an agent incorporates
new knowledge, and there is no obvious weighted combination of pre-existing
rewards or costs defined for a given co...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2301.04810v2' target='_blank'>Long-distance migration with minimal energy consumption in a thermal
  turbulent environment</a></h2>
<p><strong>Authors:</strong> Ao Xu, Hua-Lin Wu, Heng-Dong Xi</p>
<p><strong>Summary:</strong> We adopt the reinforcement learning algorithm to train the self-propelling
agent migrating long-distance in a thermal turbulent environment. We choose the
Rayleigh-B\'enard turbulent convection cell with an aspect ratio ($\Gamma$,
which is defined as the ratio between cell length and cell height) of 2 as the
training environment. Our results showed that, compared to a naive agent that
moves straight from the origin to the destination, the smart agent can learn to
utilize the carrier flow current...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2302.13378v1' target='_blank'>Puppeteer and Marionette: Learning Anticipatory Quadrupedal Locomotion
  Based on Interactions of a Central Pattern Generator and Supraspinal Drive</a></h2>
<p><strong>Authors:</strong> Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert</p>
<p><strong>Summary:</strong> Quadruped animal locomotion emerges from the interactions between the spinal
central pattern generator (CPG), sensory feedback, and supraspinal drive
signals from the brain. Computational models of CPGs have been widely used for
investigating the spinal cord contribution to animal locomotion control in
computational neuroscience and in bio-inspired robotics. However, the
contribution of supraspinal drive to anticipatory behavior, i.e. motor behavior
that involves planning ahead of time (e.g. of ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2305.17144v2' target='_blank'>Ghost in the Minecraft: Generally Capable Agents for Open-World
  Environments via Large Language Models with Text-based Knowledge and Memory</a></h2>
<p><strong>Authors:</strong> Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai</p>
<p><strong>Summary:</strong> The captivating realm of Minecraft has attracted substantial research
interest in recent years, serving as a rich platform for developing intelligent
agents capable of functioning in open-world environments. However, the current
research landscape predominantly focuses on specific objectives, such as the
popular "ObtainDiamond" task, and has not yet shown effective generalization to
a broader spectrum of tasks. Furthermore, the current leading success rate for
the "ObtainDiamond" task stands at ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.08044v3' target='_blank'>Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning
  Approach to Critical Care</a></h2>
<p><strong>Authors:</strong> Ali Shirali, Alexander Schubert, Ahmed Alaa</p>
<p><strong>Summary:</strong> Medical treatments often involve a sequence of decisions, each informed by
previous outcomes. This process closely aligns with reinforcement learning
(RL), a framework for optimizing sequential decisions to maximize cumulative
rewards under unknown dynamics. While RL shows promise for creating data-driven
treatment plans, its application in medical contexts is challenging due to the
frequent need to use sparse rewards, primarily defined based on mortality
outcomes. This sparsity can reduce the s...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2306.14898v3' target='_blank'>InterCode: Standardizing and Benchmarking Interactive Coding with
  Execution Feedback</a></h2>
<p><strong>Authors:</strong> John Yang, Akshara Prabhakar, Karthik Narasimhan, Shunyu Yao</p>
<p><strong>Summary:</strong> Humans write code in a fundamentally interactive manner and rely on constant
execution feedback to correct errors, resolve ambiguities, and decompose tasks.
While LLMs have recently exhibited promising coding capabilities, current
coding benchmarks mostly consider a static instruction-to-code sequence
transduction process, which has the potential for error propagation and a
disconnect between the generated code and its final execution environment. To
address this gap, we introduce InterCode, a l...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.02691v1' target='_blank'>SACHA: Soft Actor-Critic with Heuristic-Based Attention for Partially
  Observable Multi-Agent Path Finding</a></h2>
<p><strong>Authors:</strong> Qiushi Lin, Hang Ma</p>
<p><strong>Summary:</strong> Multi-Agent Path Finding (MAPF) is a crucial component for many large-scale
robotic systems, where agents must plan their collision-free paths to their
given goal positions. Recently, multi-agent reinforcement learning has been
introduced to solve the partially observable variant of MAPF by learning a
decentralized single-agent policy in a centralized fashion based on each
agent's partial observation. However, existing learning-based methods are
ineffective in achieving complex multi-agent coope...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2307.09213v3' target='_blank'>Machine-directed gravitational-wave counterpart discovery</a></h2>
<p><strong>Authors:</strong> Niharika Sravan, Matthew J. Graham, Michael W. Coughlin, Tomas Ahumada, Shreya Anand</p>
<p><strong>Summary:</strong> Joint observations in electromagnetic and gravitational waves shed light on
the physics of objects and surrounding environments with extreme gravity that
are otherwise unreachable via siloed observations in each messenger. However,
such detections remain challenging due to the rapid and faint nature of
counterparts. Protocols for discovery and inference still rely on human experts
manually inspecting survey alert streams and intuiting optimal usage of limited
follow-up resources. Strategizing an...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2309.16609v1' target='_blank'>Qwen Technical Report</a></h2>
<p><strong>Authors:</strong> Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu</p>
<p><strong>Summary:</strong> Large language models (LLMs) have revolutionized the field of artificial
intelligence, enabling natural language processing tasks that were previously
thought to be exclusive to humans. In this work, we introduce Qwen, the first
installment of our large language model series. Qwen is a comprehensive
language model series that encompasses distinct models with varying parameter
counts. It includes Qwen, the base pretrained language models, and Qwen-Chat,
the chat models finetuned with human alignm...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2311.02847v4' target='_blank'>Kinematic-aware Prompting for Generalizable Articulated Object
  Manipulation with LLMs</a></h2>
<p><strong>Authors:</strong> Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li</p>
<p><strong>Summary:</strong> Generalizable articulated object manipulation is essential for home-assistant
robots. Recent efforts focus on imitation learning from demonstrations or
reinforcement learning in simulation, however, due to the prohibitive costs of
real-world data collection and precise object simulation, it still remains
challenging for these works to achieve broad adaptability across diverse
articulated objects. Recently, many works have tried to utilize the strong
in-context learning ability of Large Language ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2312.10490v1' target='_blank'>Spatial Deep Learning for Site-Specific Movement Optimization of Aerial
  Base Stations</a></h2>
<p><strong>Authors:</strong> Jiangbin Lyu, Xu Chen, Jiefeng Zhang, Liqun Fu</p>
<p><strong>Summary:</strong> Unmanned aerial vehicles (UAVs) can be utilized as aerial base stations
(ABSs) to provide wireless connectivity for ground users (GUs) in various
emergency scenarios. However, it is a NP-hard problem with exponential
complexity in $M$ and $N$, in order to maximize the coverage rate of $M$ GUs by
jointly placing $N$ ABSs with limited coverage range. The problem is further
complicated when the coverage range becomes irregular due to site-specific
blockages (e.g., buildings) on the air-ground chann...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.02456v1' target='_blank'>A comprehensive survey of research towards AI-enabled unmanned aerial
  systems in pre-, active-, and post-wildfire management</a></h2>
<p><strong>Authors:</strong> Sayed Pedram Haeri Boroujeni, Abolfazl Razi, Sahand Khoshdel, Fatemeh Afghah, Janice L. Coen, Leo ONeill, Peter Z. Fule, Adam Watts, Nick-Marios T. Kokolakis, Kyriakos G. Vamvoudakis</p>
<p><strong>Summary:</strong> Wildfires have emerged as one of the most destructive natural disasters
worldwide, causing catastrophic losses in both human lives and forest wildlife.
Recently, the use of Artificial Intelligence (AI) in wildfires, propelled by
the integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,
has created an unprecedented momentum to implement and develop more effective
wildfire management. Although some of the existing survey papers have explored
various learning-based approaches, a ...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2401.12275v2' target='_blank'>Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation</a></h2>
<p><strong>Authors:</strong> Jiachen Li, Chuanbo Hua, Jianpeng Yao, Hengbo Ma, Jinkyoo Park, Victoria Dax, Mykel J. Kochenderfer</p>
<p><strong>Summary:</strong> Social robot navigation can be helpful in various contexts of daily life but
requires safe human-robot interactions and efficient trajectory planning. While
modeling pairwise relations has been widely studied in multi-agent interacting
systems, the ability to capture larger-scale group-wise activities is limited.
In this paper, we propose a systematic relational reasoning approach with
explicit inference of the underlying dynamically evolving relational
structures, and we demonstrate its effecti...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2402.16720v2' target='_blank'>Think2Drive: Efficient Reinforcement Learning by Thinking in Latent
  World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)</a></h2>
<p><strong>Authors:</strong> Qifeng Li, Xiaosong Jia, Shaobo Wang, Junchi Yan</p>
<p><strong>Summary:</strong> Real-world autonomous driving (AD) especially urban driving involves many
corner cases. The lately released AD simulator CARLA v2 adds 39 common events
in the driving scene, and provide more quasi-realistic testbed compared to
CARLA v1. It poses new challenge to the community and so far no literature has
reported any success on the new scenarios in V2 as existing works mostly have
to rely on specific rules for planning yet they cannot cover the more complex
cases in CARLA v2. In this work, we ta...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2403.07125v1' target='_blank'>Learning-Aided Control of Robotic Tether-Net with Maneuverable Nodes to
  Capture Large Space Debris</a></h2>
<p><strong>Authors:</strong> Achira Boonrath, Feng Liu, Elenora M. Botta, Souma Chowdhury</p>
<p><strong>Summary:</strong> Maneuverable tether-net systems launched from an unmanned spacecraft offer a
promising solution for the active removal of large space debris. Guaranteeing
the successful capture of such space debris is dependent on the ability to
reliably maneuver the tether-net system -- a flexible, many-DoF (thus complex)
system -- for a wide range of launch scenarios. Here, scenarios are defined by
the relative location of the debris with respect to the chaser spacecraft. This
paper represents and solves this...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2405.14251v2' target='_blank'>Efficient Navigation of a Robotic Fish Swimming Across the Vortical Flow
  Field</a></h2>
<p><strong>Authors:</strong> Haodong Feng, Dehan Yuan, Jiale Miao, Jie You, Yue Wang, Yi Zhu, Dixia Fan</p>
<p><strong>Summary:</strong> Navigating efficiently across vortical flow fields presents a significant
challenge in various robotic applications. The dynamic and unsteady nature of
vortical flows often disturbs the control of underwater robots, complicating
their operation in hydrodynamic environments. Conventional control methods,
which depend on accurate modeling, fail in these settings due to the complexity
of fluid-structure interactions (FSI) caused by unsteady hydrodynamics. This
study proposes a deep reinforcement le...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2408.05609v1' target='_blank'>Mitigating Metropolitan Carbon Emissions with Dynamic Eco-driving at
  Scale</a></h2>
<p><strong>Authors:</strong> Vindula Jayawardana, Baptiste Freydt, Ao Qu, Cameron Hickert, Edgar Sanchez, Catherine Tang, Mark Taylor, Blaine Leonard, Cathy Wu</p>
<p><strong>Summary:</strong> The sheer scale and diversity of transportation make it a formidable sector
to decarbonize. Here, we consider an emerging opportunity to reduce carbon
emissions: the growing adoption of semi-autonomous vehicles, which can be
programmed to mitigate stop-and-go traffic through intelligent speed commands
and, thus, reduce emissions. But would such dynamic eco-driving move the needle
on climate change? A comprehensive impact analysis has been out of reach due to
the vast array of traffic scenarios a...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.04612v1' target='_blank'>Regressing the Relative Future: Efficient Policy Optimization for
  Multi-turn RLHF</a></h2>
<p><strong>Authors:</strong> Zhaolin Gao, Wenhao Zhan, Jonathan D. Chang, Gokul Swamy, Kiant√© Brantley, Jason D. Lee, Wen Sun</p>
<p><strong>Summary:</strong> Large Language Models (LLMs) have achieved remarkable success at tasks like
summarization that involve a single turn of interaction. However, they can
still struggle with multi-turn tasks like dialogue that require long-term
planning. Previous works on multi-turn dialogue extend single-turn
reinforcement learning from human feedback (RLHF) methods to the multi-turn
setting by treating all prior dialogue turns as a long context. Such approaches
suffer from covariate shift: the conversations in th...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.10621v2' target='_blank'>Traversability-Aware Legged Navigation by Learning from Real-World
  Visual Data</a></h2>
<p><strong>Authors:</strong> Hongbo Zhang, Zhongyu Li, Xuanqi Zeng, Laura Smith, Kyle Stachowicz, Dhruv Shah, Linzhu Yue, Zhitao Song, Weipeng Xia, Sergey Levine, Koushil Sreenath, Yun-hui Liu</p>
<p><strong>Summary:</strong> The enhanced mobility brought by legged locomotion empowers quadrupedal
robots to navigate through complex and unstructured environments. However,
optimizing agile locomotion while accounting for the varying energy costs of
traversing different terrains remains an open challenge. Most previous work
focuses on planning trajectories with traversability cost estimation based on
human-labeled environmental features. However, this human-centric approach is
insufficient because it does not account for...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2410.15178v3' target='_blank'>GUIDEd Agents: Enhancing Navigation Policies through Task-Specific
  Uncertainty Abstraction in Localization-Limited Environments</a></h2>
<p><strong>Authors:</strong> Gokul Puthumanaillam, Paulo Padrao, Jose Fuentes, Leonardo Bobadilla, Melkior Ornik</p>
<p><strong>Summary:</strong> Autonomous vehicles performing navigation tasks in complex environments face
significant challenges due to uncertainty in state estimation. In many
scenarios, such as stealth operations or resource-constrained settings,
accessing high-precision localization comes at a significant cost, forcing
robots to rely primarily on less precise state estimates. Our key observation
is that different tasks require varying levels of precision in different
regions: a robot navigating a crowded space might need...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2411.00820v1' target='_blank'>AutoGLM: Autonomous Foundation Agents for GUIs</a></h2>
<p><strong>Authors:</strong> Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, Junjie Gao, Junjun Shan, Kangning Liu, Shudan Zhang, Shuntian Yao, Siyi Cheng, Wentao Yao, Wenyi Zhao, Xinghan Liu, Xinyi Liu, Xinying Chen, Xinyue Yang, Yang Yang, Yifan Xu, Yu Yang, Yujia Wang, Yulin Xu, Zehan Qi, Yuxiao Dong, Jie Tang</p>
<p><strong>Summary:</strong> We present AutoGLM, a new series in the ChatGLM family, designed to serve as
foundation agents for autonomous control of digital devices through Graphical
User Interfaces (GUIs). While foundation models excel at acquiring human
knowledge, they often struggle with decision-making in dynamic real-world
environments, limiting their progress toward artificial general intelligence.
This limitation underscores the importance of developing foundation agents
capable of learning through autonomous enviro...</p>
<hr>
<h2><a href='http://arxiv.org/abs/2502.04399v1' target='_blank'>Online Location Planning for AI-Defined Vehicles: Optimizing Joint Tasks
  of Order Serving and Spatio-Temporal Heterogeneous Model Fine-Tuning</a></h2>
<p><strong>Authors:</strong> Bokeng Zheng, Bo Rao, Tianxiang Zhu, Chee Wei Tan, Jingpu Duan, Zhi Zhou, Xu Chen, Xiaoxi Zhang</p>
<p><strong>Summary:</strong> Advances in artificial intelligence (AI) including foundation models (FMs),
are increasingly transforming human society, with smart city driving the
evolution of urban living.Meanwhile, vehicle crowdsensing (VCS) has emerged as
a key enabler, leveraging vehicles' mobility and sensor-equipped capabilities.
In particular, ride-hailing vehicles can effectively facilitate flexible data
collection and contribute towards urban intelligence, despite resource
limitations. Therefore, this work explores a...</p>
<hr>
</body>
</html>