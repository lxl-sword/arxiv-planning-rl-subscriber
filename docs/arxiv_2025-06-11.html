<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-06-11</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-06-11</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.09049v1' target='_blank'>VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-10 17:59:44</h6>
<p class='card-text'>Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08639v1' target='_blank'>Deep Reinforcement Learning-Based Motion Planning and PDE Control for
  Flexible Manipulators</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Amir Hossein Barjini, Seyed Adel Alizadeh Kolagar, Sadeq Yaqubi, Jouni Mattila</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-10 09:53:57</h6>
<p class='card-text'>This article presents a motion planning and control framework for flexible
robotic manipulators, integrating deep reinforcement learning (DRL) with a
nonlinear partial differential equation (PDE) controller. Unlike conventional
approaches that focus solely on control, we demonstrate that the desired
trajectory significantly influences endpoint vibrations. To address this, a DRL
motion planner, trained using the soft actor-critic (SAC) algorithm, generates
optimized trajectories that inherently minimize vibrations. The PDE nonlinear
controller then computes the required torques to track the planned trajectory
while ensuring closed-loop stability using Lyapunov analysis. The proposed
methodology is validated through both simulations and real-world experiments,
demonstrating superior vibration suppression and tracking accuracy compared to
traditional methods. The results underscore the potential of combining
learning-based motion planning with model-based control for enhancing the
precision and stability of flexible robotic manipulators.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08532v1' target='_blank'>Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A
  Hybrid DRL-LLM Approach with Compliance Awareness</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yanwei Gong, Xiaolin Chang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-10 07:51:29</h6>
<p class='card-text'>The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08434v1' target='_blank'>Attention-based Learning for 3D Informative Path Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Rui Zhao, Xingjian Zhang, Yuhong Cao, Yizhuo Wang, Guillaume Sartoretti</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-10 04:16:10</h6>
<p class='card-text'>In this work, we propose an attention-based deep reinforcement learning
approach to address the adaptive informative path planning (IPP) problem in 3D
space, where an aerial robot equipped with a downward-facing sensor must
dynamically adjust its 3D position to balance sensing footprint and accuracy,
and finally obtain a high-quality belief of an underlying field of interest
over a given domain (e.g., presence of specific plants, hazardous gas,
geological structures, etc.). In adaptive IPP tasks, the agent is tasked with
maximizing information collected under time/distance constraints, continuously
adapting its path based on newly acquired sensor data. To this end, we leverage
attention mechanisms for their strong ability to capture global spatial
dependencies across large action spaces, allowing the agent to learn an
implicit estimation of environmental transitions. Our model builds a contextual
belief representation over the entire domain, guiding sequential movement
decisions that optimize both short- and long-term search objectives.
Comparative evaluations against state-of-the-art planners demonstrate that our
approach significantly reduces environmental uncertainty within constrained
budgets, thus allowing the agent to effectively balance exploration and
exploitation. We further show our model generalizes well to environments of
varying sizes, highlighting its potential for many real-world applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08416v1' target='_blank'>Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel
  Gait Planner for Humanoid Robots</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bolin Li, Linwei Sun, Xuecong Huang, Yuzhi Jiang, Lijun Zhu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-10 03:42:04</h6>
<p class='card-text'>This paper presents a periodic bipedal gait learning method using reward
composition, integrated with a real-time gait planner for humanoid robots.
First, we introduce a novel gait planner that incorporates dynamics to design
the desired joint trajectory. In the gait design process, the 3D robot model is
decoupled into two 2D models, which are then approximated as hybrid inverted
pendulums (H-LIP) for trajectory planning. The gait planner operates in
parallel in real time within the robot's learning environment. Second, based on
this gait planner, we design three effective reward functions within a
reinforcement learning framework, forming a reward composition to achieve
periodic bipedal gait. This reward composition reduces the robot's learning
time and enhances locomotion performance. Finally, a gait design example and
performance comparison are presented to demonstrate the effectiveness of the
proposed method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08344v1' target='_blank'>Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep
  Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Neşet Ünver Akmandor, Sarvesh Prajapati, Mark Zolotas, Taşkın Padır</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-10 01:58:32</h6>
<p class='card-text'>Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08292v1' target='_blank'>From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via
  Bayesian Nash Equilibrium</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 23:49:14</h6>
<p class='card-text'>Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08149v1' target='_blank'>Ego-centric Learning of Communicative World Models for Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hang Wang, Dechen Gao, Junshan Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 18:56:40</h6>
<p class='card-text'>We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07980v1' target='_blank'>Realistic Urban Traffic Generator using Decentralized Federated Learning
  for the SUMO simulator</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alberto Bazán-Guillén, Carlos Beis-Penedo, Diego Cajaraville-Aboy, Pablo Barbecho-Bautista, Rebeca P. Díaz-Redondo, Luis J. de la Cruz Llopis, Ana Fernández-Vilas, Mónica Aguilar Igartua, Manuel Fernández-Veiga</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 17:51:45</h6>
<p class='card-text'>Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07976v2' target='_blank'>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, Aviral Kumar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 17:50:02</h6>
<p class='card-text'>The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07822v1' target='_blank'>Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency
  Trajectory Distillation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xintong Duan, Yutong He, Fahim Tajwar, Ruslan Salakhutdinov, J. Zico Kolter, Jeff Schneider</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 14:48:19</h6>
<p class='card-text'>Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07527v1' target='_blank'>Learning What Reinforcement Learning Can't: Interleaved Online
  Fine-Tuning for Hardest Questions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu Shen, Runming He, Bin Cui, Wentao Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 08:11:20</h6>
<p class='card-text'>Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07468v1' target='_blank'>Chasing Moving Targets with Online Self-Play Reinforcement Learning for
  Safer Language Models</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Mickel Liu, Liwei Jiang, Yancheng Liang, Simon Shaolei Du, Yejin Choi, Tim Althoff, Natasha Jaques</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 06:35:12</h6>
<p class='card-text'>Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.08052v1' target='_blank'>ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous
  Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, Xinggang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-09 03:14:04</h6>
<p class='card-text'>Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.07232v1' target='_blank'>Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in
  Embodied Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xinran Li, Chenjia Bai, Zijian Li, Jiakun Zheng, Ting Xiao, Jun Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-08 17:32:03</h6>
<p class='card-text'>Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06981v1' target='_blank'>Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by
  Model-Free Agents in Open-Ended Environments</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Riley Simmons-Edler, Ryan P. Badman, Felix Baastad Berg, Raymond Chua, John J. Vastola, Joshua Lunger, William Qian, Kanaka Rajan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-08 03:43:48</h6>
<p class='card-text'>Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06261v1' target='_blank'>Reflect-then-Plan: Offline Model-Based Planning through a Doubly
  Bayesian Lens</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jihwan Jeong, Xiaoyu Wang, Jingmin Wang, Scott Sanner, Pascal Poupart</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 17:40:12</h6>
<p class='card-text'>Offline reinforcement learning (RL) is crucial when online exploration is
costly or unsafe but often struggles with high epistemic uncertainty due to
limited data. Existing methods rely on fixed conservative policies, restricting
adaptivity and generalization. To address this, we propose Reflect-then-Plan
(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.
RefPlan unifies uncertainty modeling and MB planning by recasting planning as
Bayesian posterior estimation. At deployment, it updates a belief over
environment dynamics using real-time observations, incorporating uncertainty
into MB planning via marginalization. Empirical results on standard benchmarks
show that RefPlan significantly improves the performance of conservative
offline RL policies. In particular, RefPlan maintains robust performance under
high epistemic uncertainty and limited data, while demonstrating resilience to
changing environment dynamics, improving the flexibility, generalizability, and
robustness of offline-learned policies.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.06094v1' target='_blank'>On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Elim Kwan, Rehman Qureshi, Liam Fletcher, Colin Laganier, Victoria Nockles, Richard Walters</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 13:54:19</h6>
<p class='card-text'>Cooperative autonomous robotic systems have significant potential for
executing complex multi-task missions across space, air, ground, and maritime
domains. But they commonly operate in remote, dynamic and hazardous
environments, requiring rapid in-mission adaptation without reliance on fragile
or slow communication links to centralised compute. Fast, on-board replanning
algorithms are therefore needed to enhance resilience. Reinforcement Learning
shows strong promise for efficiently solving mission planning tasks when
formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)
are unsuitable for replanning, where agents do not start at a single location;
2) do not allow cooperation between agents; 3) are unable to model tasks with
variable durations; or 4) lack practical considerations for on-board
deployment. Here we define the Cooperative Mission Replanning Problem as a
novel variant of multiple TSP with adaptations to overcome these issues, and
develop a new encoder/decoder-based model using Graph Attention Networks and
Attention Models to solve it effectively and efficiently. Using a simple
example of cooperative drones, we show our replanner consistently (90% of the
time) maintains performance within 10% of the state-of-the-art LKH3 heuristic
solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves
the way for increased resilience in autonomous multi-agent systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05997v1' target='_blank'>Improving Long-Range Navigation with Spatially-Enhanced Recurrent Memory
  via End-to-End Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Fan Yang, Per Frivik, David Hoeller, Chen Wang, Cesar Cadena, Marco Hutter</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-06 11:35:48</h6>
<p class='card-text'>Recent advancements in robot navigation, especially with end-to-end learning
approaches like reinforcement learning (RL), have shown remarkable efficiency
and effectiveness. Yet, successful navigation still relies on two key
capabilities: mapping and planning, whether explicit or implicit. Classical
approaches use explicit mapping pipelines to register ego-centric observations
into a coherent map frame for the planner. In contrast, end-to-end learning
achieves this implicitly, often through recurrent neural networks (RNNs) that
fuse current and past observations into a latent space for planning. While
architectures such as LSTM and GRU capture temporal dependencies, our findings
reveal a key limitation: their inability to perform effective spatial
memorization. This skill is essential for transforming and integrating
sequential observations from varying perspectives to build spatial
representations that support downstream planning. To address this, we propose
Spatially-Enhanced Recurrent Units (SRUs), a simple yet effective modification
to existing RNNs, designed to enhance spatial memorization capabilities. We
introduce an attention-based architecture with SRUs, enabling long-range
navigation using a single forward-facing stereo camera. Regularization
techniques are employed to ensure robust end-to-end recurrent training via RL.
Experimental results show our approach improves long-range navigation by 23.5%
compared to existing RNNs. Furthermore, with SRU memory, our method outperforms
the RL baseline with explicit mapping and memory modules, achieving a 29.6%
improvement in diverse environments requiring long-horizon mapping and
memorization. Finally, we address the sim-to-real gap by leveraging large-scale
pretraining on synthetic depth data, enabling zero-shot transfer to diverse and
complex real-world environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2506.05168v1' target='_blank'>Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated
  Planning and Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yunsheng Tian, Joshua Jacob, Yijiang Huang, Jialiang Zhao, Edward Gu, Pingchuan Ma, Annan Zhang, Farhad Javid, Branden Romero, Sachin Chitta, Shinjiro Sueda, Hui Li, Wojciech Matusik</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-06-05 15:43:52</h6>
<p class='card-text'>Multi-part assembly poses significant challenges for robots to execute
long-horizon, contact-rich manipulation with generalization across complex
geometries. We present Fabrica, a dual-arm robotic system capable of end-to-end
planning and control for autonomous assembly of general multi-part objects. For
planning over long horizons, we develop hierarchies of precedence, sequence,
grasp, and motion planning with automated fixture generation, enabling general
multi-step assembly on any dual-arm robots. The planner is made efficient
through a parallelizable design and is optimized for downstream control
stability. For contact-rich assembly steps, we propose a lightweight
reinforcement learning framework that trains generalist policies across object
geometries, assembly directions, and grasp poses, guided by equivariance and
residual actions obtained from the plan. These policies transfer zero-shot to
the real world and achieve 80% successful steps. For systematic evaluation, we
propose a benchmark suite of multi-part assemblies resembling industrial and
daily objects across diverse categories and geometries. By integrating
efficient global planning and robust local control, we showcase the first
system to achieve complete and generalizable real-world multi-part assembly
without domain knowledge or human demonstrations. Project website:
http://fabrica.csail.mit.edu/</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>