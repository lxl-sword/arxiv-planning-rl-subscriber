<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-05-01</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-05-01</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.21318v1' target='_blank'>Phi-4-reasoning Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-30 05:05:09</h6>
<p class='card-text'>We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.21111v1' target='_blank'>How to Coordinate UAVs and UGVs for Efficient Mission Planning?
  Optimizing Energy-Constrained Cooperative Routing with a DRL Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Safwan Mondal, Subramanian Ramasamy, Luca Russo, James D. Humann, James M. Dotterweich, Pranav Bhounsule</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 18:43:59</h6>
<p class='card-text'>Efficient mission planning for cooperative systems involving Unmanned Aerial
Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) requires addressing energy
constraints, scalability, and coordination challenges between agents. UAVs
excel in rapidly covering large areas but are constrained by limited battery
life, while UGVs, with their extended operational range and capability to serve
as mobile recharging stations, are hindered by slower speeds. This
heterogeneity makes coordination between UAVs and UGVs critical for achieving
optimal mission outcomes. In this work, we propose a scalable deep
reinforcement learning (DRL) framework to address the energy-constrained
cooperative routing problem for multi-agent UAV-UGV teams, aiming to visit a
set of task points in minimal time with UAVs relying on UGVs for recharging
during the mission. The framework incorporates sortie-wise agent switching to
efficiently manage multiple agents, by allocating task points and coordinating
actions. Using an encoder-decoder transformer architecture, it optimizes routes
and recharging rendezvous for the UAV-UGV team in the task scenario. Extensive
computational experiments demonstrate the framework's superior performance over
heuristic methods and a DRL baseline, delivering significant improvements in
solution quality and runtime efficiency across diverse scenarios.
Generalization studies validate its robustness, while dynamic scenario
highlights its adaptability to real-time changes with a case study. This work
advances UAV-UGV cooperative routing by providing a scalable, efficient, and
robust solution for multi-agent mission planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20969v1' target='_blank'>XPG-RL: Reinforcement Learning with Explainable Priority Guidance for
  Efficiency-Boosted Mechanical Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiting Zhang, Shichen Li, Elena Shrestha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 17:37:45</h6>
<p class='card-text'>Mechanical search (MS) in cluttered environments remains a significant
challenge for autonomous manipulators, requiring long-horizon planning and
robust state estimation under occlusions and partial observability. In this
work, we introduce XPG-RL, a reinforcement learning framework that enables
agents to efficiently perform MS tasks through explainable, priority-guided
decision-making based on raw sensory inputs. XPG-RL integrates a task-driven
action prioritization mechanism with a learned context-aware switching strategy
that dynamically selects from a discrete set of action primitives such as
target grasping, occlusion removal, and viewpoint adjustment. Within this
strategy, a policy is optimized to output adaptive threshold values that govern
the discrete selection among action primitives. The perception module fuses
RGB-D inputs with semantic and geometric features to produce a structured scene
representation for downstream decision-making. Extensive experiments in both
simulation and real-world settings demonstrate that XPG-RL consistently
outperforms baseline methods in task success rates and motion efficiency,
achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These
results underscore the benefits of integrating domain knowledge with learnable
decision-making policies for robust and efficient robotic manipulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20782v1' target='_blank'>Integrating Human Feedback into a Reinforcement Learning-Based Framework
  for Adaptive User Interfaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Gaspar-Figueiredo, Marta Fernández-Diego, Silvia Abrahão, Emilio Insfran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 14:00:22</h6>
<p class='card-text'>Adaptive User Interfaces (AUI) play a crucial role in modern software
applications by dynamically adjusting interface elements to accommodate users'
diverse and evolving needs. However, existing adaptation strategies often lack
real-time responsiveness. Reinforcement Learning (RL) has emerged as a
promising approach for addressing complex, sequential adaptation challenges,
enabling adaptive systems to learn optimal policies based on previous
adaptation experiences. Although RL has been applied to AUIs,integrating RL
agents effectively within user interactions remains a challenge.
  In this paper, we enhance a RL-based Adaptive User Interface adaption
framework by incorporating personalized human feedback directly into the
leaning process. Unlike prior approaches that rely on a single pre-trained RL
model, our approach trains a unique RL agent for each user, allowing
individuals to actively shape their personal RL agent's policy, potentially
leading to more personalized and responsive UI adaptations. To evaluate this
approach, we conducted an empirical study to assess the impact of integrating
human feedback into the RL-based Adaptive User Interface adaption framework and
its effect on User Experience (UX). The study involved 33 participants
interacting with AUIs incorporating human feedback and non-adaptive user
interfaces in two domains: an e-learning platform and a trip-planning
application. The results suggest that incorporating human feedback into
RL-driven adaptations significantly enhances UX, offering promising directions
for advancing adaptive capabilities and user-centered design in AUIs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20660v1' target='_blank'>Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic
  Path Planning in Autonomous Systems</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Sahil Tomar, Shamshe Alam, Sandeep Kumar, Amit Mathur</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 11:36:08</h6>
<p class='card-text'>In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20464v1' target='_blank'>A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jiahao Li, Kaer Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 06:55:15</h6>
<p class='card-text'>Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured summary of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.19838v1' target='_blank'>LLM-Powered GUI Agents in Phone Automation: Surveying Progress and
  Prospects</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han, Shuai Ren, Hao Wang, Xiaoyu Liang, Wenhao Wang, Tianze Wu, Linghao Li, Hao Wang, Guanjing Xiong, Yong Liu, Hongsheng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-28 14:39:25</h6>
<p class='card-text'>With the rapid rise of large language models (LLMs), phone automation has
undergone transformative changes. This paper systematically reviews LLM-driven
phone GUI agents, highlighting their evolution from script-based automation to
intelligent, adaptive systems. We first contextualize key challenges, (i)
limited generality, (ii) high maintenance overhead, and (iii) weak intent
comprehension, and show how LLMs address these issues through advanced language
understanding, multimodal perception, and robust decision-making. We then
propose a taxonomy covering fundamental agent frameworks (single-agent,
multi-agent, plan-then-act), modeling approaches (prompt engineering,
training-based), and essential datasets and benchmarks. Furthermore, we detail
task-specific architectures, supervised fine-tuning, and reinforcement learning
strategies that bridge user intent and GUI operations. Finally, we discuss open
challenges such as dataset diversity, on-device deployment efficiency,
user-centric adaptation, and security concerns, offering forward-looking
insights into this rapidly evolving field. By providing a structured overview
and identifying pressing research gaps, this paper serves as a definitive
reference for researchers and practitioners seeking to harness LLMs in
designing scalable, user-friendly phone GUI agents.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.19654v1' target='_blank'>Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep
  Learning Refined SLAM</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Leon Davies, Baihua Li, Mohamad Saada, Simon Sølvsten, Qinggang Meng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-28 10:13:47</h6>
<p class='card-text'>SLAM (Simultaneous Localisation and Mapping) is a crucial component for
robotic systems, providing a map of an environment, the current location and
previous trajectory of a robot. While 3D LiDAR SLAM has received notable
improvements in recent years, 2D SLAM lags behind. Gradual drifts in odometry
and pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in
large complex environments. Dynamic robotic motion coupled with inherent
estimation based SLAM processes introduce noise and errors, degrading map
quality. Occupancy Grid Mapping (OGM) produces results that are often noisy and
unclear. This is due to the fact that evidence based mapping represents maps
according to uncertain observations. This is why OGMs are so popular in
exploration or navigation tasks. However, this also limits OGMs' effectiveness
for specific mapping based tasks such as floor plan creation in complex scenes.
To address this, we propose our novel Transformation and Translation Occupancy
Grid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation
techniques from 3D SLAM to the world of 2D and mitigate errors to improve map
quality using Generative Adversarial Networks (GANs). We introduce a novel data
generation method via deep reinforcement learning (DRL) to build datasets large
enough for training a GAN for SLAM error correction. We demonstrate our SLAM in
real-time on data collected at Loughborough University. We also prove its
generalisability on a variety of large complex environments on a collection of
large scale well-known 2D occupancy maps. Our novel approach enables the
creation of high quality OGMs in complex scenes, far surpassing the
capabilities of current SLAM algorithms in terms of quality, accuracy and
reliability.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.17838v1' target='_blank'>CaRL: Learning Scalable Planning Policies with Simple Rewards</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, Andreas Geiger</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-24 17:56:01</h6>
<p class='card-text'>We investigate reinforcement learning (RL) for privileged planning in
autonomous driving. State-of-the-art approaches for this task are rule-based,
but these methods do not scale to the long tail. RL, on the other hand, is
scalable and does not suffer from compounding errors like imitation learning.
Contemporary RL approaches for driving use complex shaped rewards that sum
multiple individual rewards, \eg~progress, position, or orientation rewards. We
show that PPO fails to optimize a popular version of these rewards when the
mini-batch size is increased, which limits the scalability of these approaches.
Instead, we propose a new reward design based primarily on optimizing a single
intuitive reward term: route completion. Infractions are penalized by
terminating the episode or multiplicatively reducing route completion. We find
that PPO scales well with higher mini-batch sizes when trained with our simple
reward, even improving performance. Training with large mini-batch sizes
enables efficient scaling via distributed data parallelism. We scale PPO to
300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The
resulting model achieves 64 DS on the CARLA longest6 v2 benchmark,
outperforming other RL methods with more complex rewards by a large margin.
Requiring only minimal adaptations from its use in CARLA, the same method is
the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and
90.6 in reactive traffic on the Val14 benchmark while being an order of
magnitude faster than prior work.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.16916v1' target='_blank'>Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual
  Servoing of Soft Continuum Arms</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hsin-Jung Yang, Mahsa Khosravi, Benjamin Walt, Girish Krishnan, Soumik Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-23 17:41:55</h6>
<p class='card-text'>Soft continuum arms (SCAs) soft and deformable nature presents challenges in
modeling and control due to their infinite degrees of freedom and non-linear
behavior. This work introduces a reinforcement learning (RL)-based framework
for visual servoing tasks on SCAs with zero-shot sim-to-real transfer
capabilities, demonstrated on a single section pneumatic manipulator capable of
bending and twisting. The framework decouples kinematics from mechanical
properties using an RL kinematic controller for motion planning and a local
controller for actuation refinement, leveraging minimal sensing with visual
feedback. Trained entirely in simulation, the RL controller achieved a 99.8%
success rate. When deployed on hardware, it achieved a 67% success rate in
zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This
approach offers a scalable solution for SCAs in 3D visual servoing, with
potential for further refinement and expanded applications.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.16855v1' target='_blank'>Monte Carlo Planning with Large Language Model for Text-Based Game
  Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zijing Shi, Meng Fang, Ling Chen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-23 16:23:15</h6>
<p class='card-text'>Text-based games provide valuable environments for language-based autonomous
agents. However, planning-then-learning paradigms, such as those combining
Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably
time-consuming due to extensive iterations. Additionally, these algorithms
perform uncertainty-driven exploration but lack language understanding and
reasoning abilities. In this paper, we introduce the Monte Carlo planning with
Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages
the language understanding and reasoning capabilities of Large Language Models
(LLMs) alongside the exploratory advantages of tree search algorithms.
Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,
enabling them to learn from past experiences and dynamically adjust action
evaluations during planning. We conduct experiments on a series of text-based
games from the Jericho benchmark. Our results demonstrate that the MC-DML
algorithm significantly enhances performance across various games at the
initial planning phase, outperforming strong contemporary methods that require
multiple iterations. This demonstrates the effectiveness of our algorithm,
paving the way for more efficient language-grounded planning in complex
environments.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.15876v2' target='_blank'>Bidirectional Task-Motion Planning Based on Hierarchical Reinforcement
  Learning for Strategic Confrontation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Qizhen Wu, Lei Chen, Kexin Liu, Jinhu Lü</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-22 13:22:58</h6>
<p class='card-text'>In swarm robotics, confrontation scenarios, including strategic
confrontations, require efficient decision-making that integrates discrete
commands and continuous actions. Traditional task and motion planning methods
separate decision-making into two layers, but their unidirectional structure
fails to capture the interdependence between these layers, limiting
adaptability in dynamic environments. Here, we propose a novel bidirectional
approach based on hierarchical reinforcement learning, enabling dynamic
interaction between the layers. This method effectively maps commands to task
allocation and actions to path planning, while leveraging cross-training
techniques to enhance learning across the hierarchical framework. Furthermore,
we introduce a trajectory prediction model that bridges abstract task
representations with actionable planning goals. In our experiments, it achieves
over 80% in confrontation win rate and under 0.01 seconds in decision time,
outperforming existing approaches. Demonstrations through large-scale tests and
real-world robot experiments further emphasize the generalization capabilities
and practical applicability of our method.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.15129v1' target='_blank'>A General Infrastructure and Workflow for Quadrotor Deep Reinforcement
  Learning and Reality Deployment</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Kangyao Huang, Hao Wang, Yu Luo, Jingyu Chen, Jintao Chen, Xiangkui Zhang, Xiangyang Ji, Huaping Liu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-21 14:25:23</h6>
<p class='card-text'>Deploying robot learning methods to a quadrotor in unstructured outdoor
environments is an exciting task. Quadrotors operating in real-world
environments by learning-based methods encounter several challenges: a large
amount of simulator generated data required for training, strict demands for
real-time processing onboard, and the sim-to-real gap caused by dynamic and
noisy conditions. Current works have made a great breakthrough in applying
learning-based methods to end-to-end control of quadrotors, but rarely mention
the infrastructure system training from scratch and deploying to reality, which
makes it difficult to reproduce methods and applications. To bridge this gap,
we propose a platform that enables the seamless transfer of end-to-end deep
reinforcement learning (DRL) policies. We integrate the training environment,
flight dynamics control, DRL algorithms, the MAVROS middleware stack, and
hardware into a comprehensive workflow and architecture that enables
quadrotors' policies to be trained from scratch to real-world deployment in
several minutes. Our platform provides rich types of environments including
hovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and
planning in unknown environments, as a physical experiment benchmark. Through
extensive empirical validation, we demonstrate the efficiency of proposed
sim-to-real platform, and robust outdoor flight performance under real-world
perturbations. Details can be found from our website
https://emnavi.tech/AirGym/.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14894v1' target='_blank'>Never too Cocky to Cooperate: An FIM and RL-based USV-AUV Collaborative
  System for Underwater Tasks in Extreme Sea Conditions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Jingzehua Xu, Guanwen Xie, Jiwei Tang, Yimian Ding, Weiyi Liu, Shuai Zhang, Yi Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-21 06:47:46</h6>
<p class='card-text'>This paper develops a novel unmanned surface vehicle (USV)-autonomous
underwater vehicle (AUV) collaborative system designed to enhance underwater
task performance in extreme sea conditions. The system integrates a dual
strategy: (1) high-precision multi-AUV localization enabled by Fisher
information matrix-optimized USV path planning, and (2) reinforcement
learning-based cooperative planning and control method for multi-AUV task
execution. Extensive experimental evaluations in the underwater data collection
task demonstrate the system's operational feasibility, with quantitative
results showing significant performance improvements over baseline methods. The
proposed system exhibits robust coordination capabilities between USV and AUVs
while maintaining stability in extreme sea conditions. To facilitate
reproducibility and community advancement, we provide an open-source simulation
toolkit available at: https://github.com/360ZMEM/USV-AUV-colab .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14709v1' target='_blank'>Exposing the Copycat Problem of Imitation-based Planner: A Novel
  Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hui Zhou, Shaoshuai Shi, Hongsheng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-20 18:51:26</h6>
<p class='card-text'>Machine learning (ML)-based planners have recently gained significant
attention. They offer advantages over traditional optimization-based planning
algorithms. These advantages include fewer manually selected parameters and
faster development. Within ML-based planning, imitation learning (IL) is a
common algorithm. It primarily learns driving policies directly from supervised
trajectory data. While IL has demonstrated strong performance on many open-loop
benchmarks, it remains challenging to determine if the learned policy truly
understands fundamental driving principles, rather than simply extrapolating
from the ego-vehicle's initial state. Several studies have identified this
limitation and proposed algorithms to address it. However, these methods often
use original datasets for evaluation. In these datasets, future trajectories
are heavily dependent on initial conditions. Furthermore, IL often overfits to
the most common scenarios. It struggles to generalize to rare or unseen
situations.
  To address these challenges, this work proposes: 1) a novel closed-loop
simulator supporting both imitation and reinforcement learning, 2) a causal
benchmark derived from the Waymo Open Dataset to rigorously assess the impact
of the copycat problem, and 3) a novel framework integrating imitation learning
and reinforcement learning to overcome the limitations of purely imitative
approaches. The code for this work will be released soon.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14264v1' target='_blank'>Generative emulation of chaotic dynamics with coherent prior</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Juan Nathaniel, Pierre Gentine</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-19 11:14:40</h6>
<p class='card-text'>Data-driven emulation of nonlinear dynamics is challenging due to long-range
skill decay that often produces physically unrealistic outputs. Recent advances
in generative modeling aim to address these issues by providing uncertainty
quantification and correction. However, the quality of generated simulation
remains heavily dependent on the choice of conditioning priors. In this work,
we present an efficient generative framework for dynamics emulation, unifying
principles of turbulence with diffusion-based modeling: Cohesion. Specifically,
our method estimates large-scale coherent structure of the underlying dynamics
as guidance during the denoising process, where small-scale fluctuation in the
flow is then resolved. These coherent priors are efficiently approximated using
reduced-order models, such as deep Koopman operators, that allow for rapid
generation of long prior sequences while maintaining stability over extended
forecasting horizon. With this gain, we can reframe forecasting as trajectory
planning, a common task in reinforcement learning, where conditional denoising
is performed once over entire sequences, minimizing the computational cost of
autoregressive-based generative methods. Empirical evaluations on chaotic
systems of increasing complexity, including Kolmogorov flow, shallow water
equations, and subseasonal-to-seasonal climate dynamics, demonstrate Cohesion
superior long-range forecasting skill that can efficiently generate
physically-consistent simulations, even in the presence of partially-observed
guidance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.14239v1' target='_blank'>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to
  Deliberative Reasoners</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, Fei Wu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-19 09:25:55</h6>
<p class='card-text'>Multimodal Large Language Models (MLLMs) have powered Graphical User
Interface (GUI) Agents, showing promise in automating tasks on computing
devices. Recent works have begun exploring reasoning in GUI tasks with
encouraging results. However, many current approaches rely on manually designed
reasoning templates, which may result in reasoning that is not sufficiently
robust and adaptive for complex GUI environments. Meanwhile, some existing
agents continue to operate as Reactive Actors, relying primarily on implicit
reasoning that may lack sufficient depth for GUI tasks demanding planning and
error recovery. We argue that advancing these agents requires a shift from
reactive acting towards acting based on deliberate reasoning. To facilitate
this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed
through our Actor2Reasoner framework, a reasoning-centric, two-stage training
approach designed to progressively evolve agents from Reactive Actors to
Deliberative Reasoners. The first stage, Reasoning Injection, focuses on
establishing a basic reasoner. We employ Spatial Reasoning Distillation to
transfer cross-modal spatial reasoning capabilities from teacher models to
MLLMs through trajectories with explicit reasoning steps, enabling models to
integrate GUI visual-spatial information with logical reasoning before action
generation. The second stage, Deliberation Enhancement, refines the basic
reasoner into a deliberative one using Reinforcement Learning. This stage
introduces two approaches: Sub-goal Guidance, which rewards models for
generating accurate intermediate sub-goals, and Error Recovery Scenario
Construction, which creates failure-and-recovery training scenarios from
identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves
strong performance in GUI grounding and trajectory tasks. Resources at
https://github.com/Reallm-Labs/InfiGUI-R1.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.13032v1' target='_blank'>InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction
  Graphs for LLM-Based Task Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zheng Wang, Shu Xian Teo, Jun Jie Chew, Wei Shi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-17 15:41:39</h6>
<p class='card-text'>Recent advancements in large language models (LLMs) have enabled their use as
agents for planning complex tasks. Existing methods typically rely on a
thought-action-observation (TAO) process to enhance LLM performance, but these
approaches are often constrained by the LLMs' limited knowledge of complex
tasks. Retrieval-augmented generation (RAG) offers new opportunities by
leveraging external databases to ground generation in retrieved information. In
this paper, we identify two key challenges (enlargability and transferability)
in applying RAG to task planning. We propose InstructRAG, a novel solution
within a multi-agent meta-reinforcement learning framework, to address these
challenges. InstructRAG includes a graph to organize past instruction paths
(sequences of correct actions), an RL-Agent with Reinforcement Learning to
expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to
improve task generalization for transferability. The two agents are trained
end-to-end to optimize overall planning performance. Our experiments on four
widely used task planning datasets demonstrate that InstructRAG significantly
enhances performance and adapts efficiently to new tasks, achieving up to a
19.2% improvement over the best existing approach.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.10831v1' target='_blank'>Hallucination-Aware Generative Pretrained Transformer for Cooperative
  Aerial Mobility Control</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Hyojun Ahn, Seungcheol Oh, Gyu Seon Kim, Soyi Jung, Soohyun Park, Joongheon Kim</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-15 03:21:08</h6>
<p class='card-text'>This paper proposes SafeGPT, a two-tiered framework that integrates
generative pretrained transformers (GPTs) with reinforcement learning (RL) for
efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In
the proposed design, a Global GPT module assigns high-level tasks such as
sector allocation, while an On-Device GPT manages real-time local route
planning. An RL-based safety filter monitors each GPT decision and overrides
unsafe actions that could lead to battery depletion or duplicate visits,
effectively mitigating hallucinations. Furthermore, a dual replay buffer
mechanism helps both the GPT modules and the RL agent refine their strategies
over time. Simulation results demonstrate that SafeGPT achieves higher delivery
success rates compared to a GPT-only baseline, while substantially reducing
battery consumption and travel distance. These findings validate the efficacy
of combining GPT-based semantic reasoning with formal safety guarantees,
contributing a viable solution for robust and energy-efficient UAV logistics.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.13192v2' target='_blank'>CheatAgent: Attacking LLM-Empowered Recommender Systems via LLM Agent</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, Feiran Huang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-13 05:31:37</h6>
<p class='card-text'>Recently, Large Language Model (LLM)-empowered recommender systems (RecSys)
have brought significant advances in personalized user experience and have
attracted considerable attention. Despite the impressive progress, the research
question regarding the safety vulnerability of LLM-empowered RecSys still
remains largely under-investigated. Given the security and privacy concerns, it
is more practical to focus on attacking the black-box RecSys, where attackers
can only observe the system's inputs and outputs. However, traditional attack
approaches employing reinforcement learning (RL) agents are not effective for
attacking LLM-empowered RecSys due to the limited capabilities in processing
complex textual inputs, planning, and reasoning. On the other hand, LLMs
provide unprecedented opportunities to serve as attack agents to attack RecSys
because of their impressive capability in simulating human-like decision-making
processes. Therefore, in this paper, we propose a novel attack framework called
CheatAgent by harnessing the human-like capabilities of LLMs, where an
LLM-based agent is developed to attack LLM-Empowered RecSys. Specifically, our
method first identifies the insertion position for maximum impact with minimal
input modification. After that, the LLM agent is designed to generate
adversarial perturbations to insert at target positions. To further improve the
quality of generated perturbations, we utilize the prompt tuning technique to
improve attacking strategies via feedback from the victim RecSys iteratively.
Extensive experiments across three real-world datasets demonstrate the
effectiveness of our proposed attacking method.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>