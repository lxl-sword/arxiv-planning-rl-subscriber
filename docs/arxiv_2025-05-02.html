<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-05-02</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-05-02</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.00703v1' target='_blank'>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level
  and Token-level CoT</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-01 17:59:46</h6>
<p class='card-text'>Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.21318v1' target='_blank'>Phi-4-reasoning Technical Report</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, Guoqing Zheng</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-30 05:05:09</h6>
<p class='card-text'>We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that
achieves strong performance on complex reasoning tasks. Trained via supervised
fine-tuning of Phi-4 on carefully curated set of "teachable" prompts-selected
for the right level of complexity and diversity-and reasoning demonstrations
generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains
that effectively leverage inference-time compute. We further develop
Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based
reinforcement learning that offers higher performance by generating longer
reasoning traces. Across a wide range of reasoning tasks, both models
outperform significantly larger open-weight models such as
DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full
DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and
scientific reasoning, coding, algorithmic problem solving, planning, and
spatial understanding. Interestingly, we observe a non-trivial transfer of
improvements to general-purpose benchmarks as well. In this report, we provide
insights into our training data, our training methodologies, and our
evaluations. We show that the benefit of careful data curation for supervised
fine-tuning (SFT) extends to reasoning language models, and can be further
amplified by reinforcement learning (RL). Finally, our evaluation points to
opportunities for improving how we assess the performance and robustness of
reasoning models.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.21111v1' target='_blank'>How to Coordinate UAVs and UGVs for Efficient Mission Planning?
  Optimizing Energy-Constrained Cooperative Routing with a DRL Framework</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Md Safwan Mondal, Subramanian Ramasamy, Luca Russo, James D. Humann, James M. Dotterweich, Pranav Bhounsule</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 18:43:59</h6>
<p class='card-text'>Efficient mission planning for cooperative systems involving Unmanned Aerial
Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) requires addressing energy
constraints, scalability, and coordination challenges between agents. UAVs
excel in rapidly covering large areas but are constrained by limited battery
life, while UGVs, with their extended operational range and capability to serve
as mobile recharging stations, are hindered by slower speeds. This
heterogeneity makes coordination between UAVs and UGVs critical for achieving
optimal mission outcomes. In this work, we propose a scalable deep
reinforcement learning (DRL) framework to address the energy-constrained
cooperative routing problem for multi-agent UAV-UGV teams, aiming to visit a
set of task points in minimal time with UAVs relying on UGVs for recharging
during the mission. The framework incorporates sortie-wise agent switching to
efficiently manage multiple agents, by allocating task points and coordinating
actions. Using an encoder-decoder transformer architecture, it optimizes routes
and recharging rendezvous for the UAV-UGV team in the task scenario. Extensive
computational experiments demonstrate the framework's superior performance over
heuristic methods and a DRL baseline, delivering significant improvements in
solution quality and runtime efficiency across diverse scenarios.
Generalization studies validate its robustness, while dynamic scenario
highlights its adaptability to real-time changes with a case study. This work
advances UAV-UGV cooperative routing by providing a scalable, efficient, and
robust solution for multi-agent mission planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20969v1' target='_blank'>XPG-RL: Reinforcement Learning with Explainable Priority Guidance for
  Efficiency-Boosted Mechanical Search</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yiting Zhang, Shichen Li, Elena Shrestha</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 17:37:45</h6>
<p class='card-text'>Mechanical search (MS) in cluttered environments remains a significant
challenge for autonomous manipulators, requiring long-horizon planning and
robust state estimation under occlusions and partial observability. In this
work, we introduce XPG-RL, a reinforcement learning framework that enables
agents to efficiently perform MS tasks through explainable, priority-guided
decision-making based on raw sensory inputs. XPG-RL integrates a task-driven
action prioritization mechanism with a learned context-aware switching strategy
that dynamically selects from a discrete set of action primitives such as
target grasping, occlusion removal, and viewpoint adjustment. Within this
strategy, a policy is optimized to output adaptive threshold values that govern
the discrete selection among action primitives. The perception module fuses
RGB-D inputs with semantic and geometric features to produce a structured scene
representation for downstream decision-making. Extensive experiments in both
simulation and real-world settings demonstrate that XPG-RL consistently
outperforms baseline methods in task success rates and motion efficiency,
achieving up to 4.5$\times$ higher efficiency in long-horizon tasks. These
results underscore the benefits of integrating domain knowledge with learnable
decision-making policies for robust and efficient robotic manipulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2504.20782v1' target='_blank'>Integrating Human Feedback into a Reinforcement Learning-Based Framework
  for Adaptive User Interfaces</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daniel Gaspar-Figueiredo, Marta Fernández-Diego, Silvia Abrahão, Emilio Insfran</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-04-29 14:00:22</h6>
<p class='card-text'>Adaptive User Interfaces (AUI) play a crucial role in modern software
applications by dynamically adjusting interface elements to accommodate users'
diverse and evolving needs. However, existing adaptation strategies often lack
real-time responsiveness. Reinforcement Learning (RL) has emerged as a
promising approach for addressing complex, sequential adaptation challenges,
enabling adaptive systems to learn optimal policies based on previous
adaptation experiences. Although RL has been applied to AUIs,integrating RL
agents effectively within user interactions remains a challenge.
  In this paper, we enhance a RL-based Adaptive User Interface adaption
framework by incorporating personalized human feedback directly into the
leaning process. Unlike prior approaches that rely on a single pre-trained RL
model, our approach trains a unique RL agent for each user, allowing
individuals to actively shape their personal RL agent's policy, potentially
leading to more personalized and responsive UI adaptations. To evaluate this
approach, we conducted an empirical study to assess the impact of integrating
human feedback into the RL-based Adaptive User Interface adaption framework and
its effect on User Experience (UX). The study involved 33 participants
interacting with AUIs incorporating human feedback and non-adaptive user
interfaces in two domains: an e-learning platform and a trip-planning
application. The results suggest that incorporating human feedback into
RL-driven adaptations significantly enhances UX, offering promising directions
for advancing adaptive capabilities and user-centered design in AUIs.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>