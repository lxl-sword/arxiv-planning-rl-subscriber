<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<title>arXiv Papers - 2025-05-23</title>
<link href='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css' rel='stylesheet'>
<link href='https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap' rel='stylesheet'>
<link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'>
<style>
body {
font-family: 'Roboto', sans-serif;
background-color: #f5f7fa;
padding: 20px;
}
.card {
    border-radius: 10px;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}
.card-title {
    font-weight: 700;
}
.card:hover {
    transform: scale(1.02);
    transition: 0.3s ease-in-out;
}
</style>
</head>
<body>
<div class='container'>
<h1 class='text-center my-4'><i class='fas fa-book'></i>arXiv Papers - 2025-05-23</h1>
<div class='row'>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16833v1' target='_blank'>Strategically Linked Decisions in Long-Term Planning and Reinforcement
  Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Alihan Hüyük, Finale Doshi-Velez</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 16:04:17</h6>
<p class='card-text'>Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16787v1' target='_blank'>Gaze Into the Abyss -- Planning to Seek Entropy When Reward is Scarce</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Ashish Sundar, Chunbo Luo, Xiaoyang Wang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 15:28:50</h6>
<p class='card-text'>Model-based reinforcement learning (MBRL) offers an intuitive way to increase
the sample efficiency of model-free RL methods by simultaneously training a
world model that learns to predict the future. MBRL methods have progressed by
largely prioritising the actor; optimising the world model learning has been
neglected meanwhile. Improving the fidelity of the world model and reducing its
time to convergence can yield significant downstream benefits, one of which is
improving the ensuing performance of any actor it may train. We propose a novel
approach that anticipates and actively seeks out high-entropy states using
short-horizon latent predictions generated by the world model, offering a
principled alternative to traditional curiosity-driven methods that chase
once-novel states well after they were stumbled into. While many model
predictive control (MPC) based methods offer similar alternatives, they
typically lack commitment, synthesising multi step plans after every step. To
mitigate this, we present a hierarchical planner that dynamically decides when
to replan, planning horizon length, and the weighting between reward and
entropy. While our method can theoretically be applied to any model that trains
its own actors with solely model generated data, we have applied it to just
Dreamer as a proof of concept. Our method finishes the Miniworld procedurally
generated mazes 50% faster than base Dreamer at convergence and the policy
trained in imagination converges in only 60% of the environment steps that base
Dreamer needs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16547v1' target='_blank'>Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for
  Occlusion Aware Plant Manipulation</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 11:37:39</h6>
<p class='card-text'>This paper presents an end-to-end deep reinforcement learning (RL) framework
for occlusion-aware robotic manipulation in cluttered plant environments. Our
approach enables a robot to interact with a deformable plant to reveal hidden
objects of interest, such as fruits, using multimodal observations. We decouple
the kinematic planning problem from robot control to simplify zero-shot
sim2real transfer for the trained policy. Our results demonstrate that the
trained policy, deployed using our framework, achieves up to 86.7% success in
real-world trials across diverse initial conditions. Our findings pave the way
toward autonomous, perception-driven agricultural robots that intelligently
interact with complex foliage plants to "find the fruit" in challenging
occluded scenarios, without the need for explicitly designed geometric and
dynamic models of every plant scenario.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16394v1' target='_blank'>Raw2Drive: Reinforcement Learning with Aligned World Models for
  End-to-End Autonomous Driving (in CARLA v2)</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhenjie Yang, Xiaosong Jia, Qifeng Li, Xue Yang, Maoqing Yao, Junchi Yan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:46:53</h6>
<p class='card-text'>Reinforcement Learning (RL) can mitigate the causal confusion and
distribution shift inherent to imitation learning (IL). However, applying RL to
end-to-end autonomous driving (E2E-AD) remains an open problem for its training
difficulty, and IL is still the mainstream paradigm in both academia and
industry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated
promising results in neural planning; however, these methods typically require
privileged information as input rather than raw sensor data. We fill this gap
by designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently
train an auxiliary privileged world model paired with a neural planner that
uses privileged information as input. Subsequently, we introduce a raw sensor
world model trained via our proposed Guidance Mechanism, which ensures
consistency between the raw sensor world model and the privileged world model
during rollouts. Finally, the raw sensor world model combines the prior
knowledge embedded in the heads of the privileged world model to effectively
guide the training of the raw sensor policy. Raw2Drive is so far the only RL
based end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it
achieves state-of-the-art performance.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.16377v1' target='_blank'>VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with
  World Models for Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Yansong Qu, Zilin Huang, Zihao Sheng, Jiancong Chen, Sikai Chen, Samuel Labi</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-22 08:29:59</h6>
<p class='card-text'>Reinforcement learning (RL)-based autonomous driving policy learning faces
critical limitations such as low sample efficiency and poor generalization; its
reliance on online interactions and trial-and-error learning is especially
unacceptable in safety-critical scenarios. Existing methods including safe RL
often fail to capture the true semantic meaning of "safety" in complex driving
contexts, leading to either overly conservative driving behavior or constraint
violations. To address these challenges, we propose VL-SAFE, a world
model-based safe RL framework with Vision-Language model
(VLM)-as-safety-guidance paradigm, designed for offline safe policy learning.
Specifically, we construct offline datasets containing data collected by expert
agents and labeled with safety scores derived from VLMs. A world model is
trained to generate imagined rollouts together with safety estimations,
allowing the agent to perform safe planning without interacting with the real
environment. Based on these imagined trajectories and safety evaluations,
actor-critic learning is conducted under VLM-based safety guidance to optimize
the driving policy more safely and efficiently. Extensive evaluations
demonstrate that VL-SAFE achieves superior sample efficiency, generalization,
safety, and overall performance compared to existing baselines. To the best of
our knowledge, this is the first work that introduces a VLM-guided world
model-based approach for safe autonomous driving. The demo video and code can
be accessed at: https://ys-qu.github.io/vlsafe-website/</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15793v2' target='_blank'>HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for
  Autonomous Driving</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhiwen Chen, Bo Leng, Zhuoren Li, Hanming Deng, Guizhe Jin, Ran Yu, Huanxi Wen</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 17:47:24</h6>
<p class='card-text'>Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15754v1' target='_blank'>Improving planning and MBRL with temporally-extended actions</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Palash Chatterjee, Roni Khardon</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 16:59:32</h6>
<p class='card-text'>Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15607v1' target='_blank'>From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with
  Pedagogy using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 15:00:07</h6>
<p class='card-text'>Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.15146v1' target='_blank'>lmgame-Bench: How Good are LLMs at Playing Games?</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-21 06:02:55</h6>
<p class='card-text'>Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.14970v1' target='_blank'>Self-Evolving Curriculum for LLM Reasoning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 23:17:15</h6>
<p class='card-text'>Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.14443v1' target='_blank'>Semantically-driven Deep Reinforcement Learning for Inspection Path
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Grzegorz Malczyk, Mihir Kulkarni, Kostas Alexis</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 14:45:16</h6>
<p class='card-text'>This paper introduces a novel semantics-aware inspection planning policy
derived through deep reinforcement learning. Reflecting the fact that within
autonomous informative path planning missions in unknown environments, it is
often only a sparse set of objects of interest that need to be inspected, the
method contributes an end-to-end policy that simultaneously performs semantic
object visual inspection combined with collision-free navigation. Assuming
access only to the instantaneous depth map, the associated segmentation image,
the ego-centric local occupancy, and the history of past positions in the
robot's neighborhood, the method demonstrates robust generalizability and
successful crossing of the sim2real gap. Beyond simulations and extensive
comparison studies, the approach is verified in experimental evaluations
onboard a flying robot deployed in novel environments with previously unseen
semantics and overall geometric configurations.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13921v1' target='_blank'>APEX: Empowering LLMs with Physics-Based Task Planning for Real-time
  Insight</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 04:34:58</h6>
<p class='card-text'>Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13834v1' target='_blank'>Toward Real-World Cooperative and Competitive Soccer with Quadrupedal
  Robot Teams</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, Yi Wu, Koushil Sreenath</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 02:20:54</h6>
<p class='card-text'>Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13831v1' target='_blank'>TelePlanNet: An AI-Driven Framework for Efficient Telecom Network
  Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Zongyuan Deng, Yujie Cai, Qing Liu, Shiyao Mu, Bin Lyu, Zhen Yang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-20 02:19:10</h6>
<p class='card-text'>The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13372v1' target='_blank'>Exploiting Symbolic Heuristics for the Synthesis of Domain-Specific
  Temporal Planning Guidance using Reinforcement Learning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Irene Brugnara, Alessandro Valentini, Andrea Micheli</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 17:19:13</h6>
<p class='card-text'>Recent work investigated the use of Reinforcement Learning (RL) for the
synthesis of heuristic guidance to improve the performance of temporal planners
when a domain is fixed and a set of training problems (not plans) is given. The
idea is to extract a heuristic from the value function of a particular
(possibly infinite-state) MDP constructed over the training problems.
  In this paper, we propose an evolution of this learning and planning
framework that focuses on exploiting the information provided by symbolic
heuristics during both the RL and planning phases. First, we formalize
different reward schemata for the synthesis and use symbolic heuristics to
mitigate the problems caused by the truncation of episodes needed to deal with
the potentially infinite MDP. Second, we propose learning a residual of an
existing symbolic heuristic, which is a "correction" of the heuristic value,
instead of eagerly learning the whole heuristic from scratch. Finally, we use
the learned heuristic in combination with a symbolic heuristic using a
multiple-queue planning approach to balance systematic search with imperfect
learned information. We experimentally compare all the approaches, highlighting
their strengths and weaknesses and significantly advancing the state of the art
for this planning and learning schema.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.12872v1' target='_blank'>From Grunts to Grammar: Emergent Language from Cooperative Foraging</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Maytus Piriyajitakonkij, Rujikorn Charakorn, Weicheng Tao, Wei Pan, Mingfei Sun, Cheston Tan, Mengmi Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 08:57:30</h6>
<p class='card-text'>Early cavemen relied on gestures, vocalizations, and simple signals to
coordinate, plan, avoid predators, and share resources. Today, humans
collaborate using complex languages to achieve remarkable results. What drives
this evolution in communication? How does language emerge, adapt, and become
vital for teamwork? Understanding the origins of language remains a challenge.
A leading hypothesis in linguistics and anthropology posits that language
evolved to meet the ecological and social demands of early human cooperation.
Language did not arise in isolation, but through shared survival goals.
Inspired by this view, we investigate the emergence of language in multi-agent
Foraging Games. These environments are designed to reflect the cognitive and
ecological constraints believed to have influenced the evolution of
communication. Agents operate in a shared grid world with only partial
knowledge about other agents and the environment, and must coordinate to
complete games like picking up high-value targets or executing temporally
ordered actions. Using end-to-end deep reinforcement learning, agents learn
both actions and communication strategies from scratch. We find that agents
develop communication protocols with hallmark features of natural language:
arbitrariness, interchangeability, displacement, cultural transmission, and
compositionality. We quantify each property and analyze how different factors,
such as population size and temporal dependencies, shape specific aspects of
the emergent language. Our framework serves as a platform for studying how
language can evolve from partial observability, temporal reasoning, and
cooperative goals in embodied multi-agent settings. We will release all data,
code, and models publicly.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.12752v1' target='_blank'>MOON: Multi-Objective Optimization-Driven Object-Goal Navigation Using a
  Variable-Horizon Set-Orienteering Planner</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Daigo Nakajima, Kanji Tanaka, Daiki Iwata, Kouki Terashima</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 06:20:37</h6>
<p class='card-text'>Object-goal navigation (ON) enables autonomous robots to locate and reach
user-specified objects in previously unknown environments, offering promising
applications in domains such as assistive care and disaster response. Existing
ON methods -- including training-free approaches, reinforcement learning, and
zero-shot planners -- generally depend on active exploration to identify
landmark objects (e.g., kitchens or desks), followed by navigation toward
semantically related targets (e.g., a specific mug). However, these methods
often lack strategic planning and do not adequately address trade-offs among
multiple objectives. To overcome these challenges, we propose a novel framework
that formulates ON as a multi-objective optimization problem (MOO), balancing
frontier-based knowledge exploration with knowledge exploitation over
previously observed landmarks; we call this framework MOON (MOO-driven ON). We
implement a prototype MOON system that integrates three key components: (1)
building on QOM [IROS05], a classical ON system that compactly and
discriminatively encodes landmarks based on their semantic relevance to the
target; (2) integrating StructNav [RSS23], a recently proposed training-free
planner, to enhance the navigation pipeline; and (3) introducing a
variable-horizon set orienteering problem formulation to enable global
optimization over both exploration and exploitation strategies. This work
represents an important first step toward developing globally optimized,
next-generation object-goal navigation systems.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.13549v1' target='_blank'>TD-GRPC: Temporal Difference Learning with Group Relative Policy
  Constraint for Humanoid Locomotion</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Khang Nguyen, Khai Nguyen, An T. Le, Jan Peters, Manfred Huber, Ngo Anh Vien, Minh Nhat Vu</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 04:32:14</h6>
<p class='card-text'>Robot learning in high-dimensional control settings, such as humanoid
locomotion, presents persistent challenges for reinforcement learning (RL)
algorithms due to unstable dynamics, complex contact interactions, and
sensitivity to distributional shifts during training. Model-based methods,
\textit{e.g.}, Temporal-Difference Model Predictive Control (TD-MPC), have
demonstrated promising results by combining short-horizon planning with
value-based learning, enabling efficient solutions for basic locomotion tasks.
However, these approaches remain ineffective in addressing policy mismatch and
instability introduced by off-policy updates. Thus, in this work, we introduce
Temporal-Difference Group Relative Policy Constraint (TD-GRPC), an extension of
the TD-MPC framework that unifies Group Relative Policy Optimization (GRPO)
with explicit Policy Constraints (PC). TD-GRPC applies a trust-region
constraint in the latent policy space to maintain consistency between the
planning priors and learned rollouts, while leveraging group-relative ranking
to assess and preserve the physical feasibility of candidate trajectories.
Unlike prior methods, TD-GRPC achieves robust motions without modifying the
underlying planner, enabling flexible planning and policy learning. We validate
our method across a locomotion task suite ranging from basic walking to highly
dynamic movements on the 26-DoF Unitree H1-2 humanoid robot. Through simulation
results, TD-GRPC demonstrates its improvements in stability and policy
robustness with sampling efficiency while training for complex humanoid control
tasks.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.12648v1' target='_blank'>SafeMove-RL: A Certifiable Reinforcement Learning Framework for Dynamic
  Motion Constraints in Trajectory Planning</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Tengfei Liu, Haoyang Zhong, Jiazheng Hu, Tan Zhang</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-19 03:00:44</h6>
<p class='card-text'>This study presents a dynamic safety margin-based reinforcement learning
framework for local motion planning in dynamic and uncertain environments. The
proposed planner integrates real-time trajectory optimization with adaptive gap
analysis, enabling effective feasibility assessment under partial observability
constraints. To address safety-critical computations in unknown scenarios, an
enhanced online learning mechanism is introduced, which dynamically corrects
spatial trajectories by forming dynamic safety margins while maintaining
control invariance. Extensive evaluations, including ablation studies and
comparisons with state-of-the-art algorithms, demonstrate superior success
rates and computational efficiency. The framework's effectiveness is further
validated on both simulated and physical robotic platforms.</p>
</div>
</div>
</div>
<div class='col-md-6 mb-4'>
<div class='card'>
<div class='card-body'>
<h4 class='card-title'><a href='http://arxiv.org/abs/2505.12204v1' target='_blank'>Of Mice and Machines: A Comparison of Learning Between Real World Mice
  and RL Agents</a></h5>
<h5 class='card-subtitle mb-2 text-muted'>Authors:Shuo Han, German Espinosa, Junda Huang, Daniel A. Dombeck, Malcolm A. MacIver, Bradly C. Stadie</h6>
<h6 class='card-subtitle mb-2 text-muted'>Date:2025-05-18 02:40:16</h6>
<p class='card-text'>Recent advances in reinforcement learning (RL) have demonstrated impressive
capabilities in complex decision-making tasks. This progress raises a natural
question: how do these artificial systems compare to biological agents, which
have been shaped by millions of years of evolution? To help answer this
question, we undertake a comparative study of biological mice and RL agents in
a predator-avoidance maze environment. Through this analysis, we identify a
striking disparity: RL agents consistently demonstrate a lack of
self-preservation instinct, readily risking ``death'' for marginal efficiency
gains. These risk-taking strategies are in contrast to biological agents, which
exhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging
this gap between the biological and artificial, we propose two novel mechanisms
that encourage more naturalistic risk-avoidance behaviors in RL agents. Our
approach leads to the emergence of naturalistic behaviors, including strategic
environment assessment, cautious path planning, and predator avoidance patterns
that closely mirror those observed in biological systems.</p>
</div>
</div>
</div>
</div>
</div>
<script src='https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js'></script>
</body>
</html>